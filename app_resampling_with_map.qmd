---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Map across resamples { .unnumbered}

We can use map (and foreach) to fit and evaluate models over resamples/splits manually

- This can be done to replace `fit_resamples()` and `tune_grid()`
- Useful if we need to fit models outside of full tidymodels framework (e.g., when using keras for neural networks)
- Useful for nested resampling, which is not supported by `fit_resamples()` and `tune_grid()`

## Set up

Lets start by....

- loading libraries
```{r}
library(tidyverse)
library(tidymodels)
```

- creating a simple data set
```{r}
set.seed(123456)
n_obs <- 300
n_boots <- 50
d <- tibble(x1 = rnorm(n_obs), x2 = rnorm(n_obs), y = 2*x1 + 3*x2 + rnorm(n_obs))
```  

- getting bootstrap resamples of d.  It has two columns and n_boots rows.  Each row is a bootstrap sample of the data.  The columns are:

  - `splits` - contains a bootstrap sample of the data that includes held-in raw data and OOB held-out raw data subsamples
  - `id` - name of the resample

```{r}
resamples <- bootstraps(d, times = n_boots)

resamples
```

- setting up a simple recipe for feature engineering all our models (just use raw x and y)
```{r}
rec <- recipe(y ~ ., data = d)
```

## Using map() to replace fit_resamples() - step x step

In this first example, we will combine `map()` with the use of list columns to save all the intermediate products that are produced when fitting and evaluating a single model configuration for a logistic regression across resamples.

To do this, we will need a function to fit logistic regression to held-in training data.  We can use the typical tidymodels code here.
```{r}  
fit_lm <- function(held_in) {
  linear_reg() |> 
    set_engine("lm") |> 
    fit(y ~ ., data = held_in)
}
```

Then we use `map()` and list columns to save the individual steps for evaluating the model in each resample.  The following steps are done for EACH resample using `map()` or `map2()`

- prep the recipe with held-in data
- bake the recipe using `new_data = NULL` to get held-in features
- bake the recipe using `new_data = assessment(split)` to get held-out features
- fit the model using the held-in features
- get predictions using the model with the held-out features
- calculate the accuracy of the model

```{r}
resamples_ex1 <- resamples |> 
  mutate(prep_recs = map(splits, 
                         \(split) prep(rec, training = analysis(split))),
         held_ins = map2(resamples$splits, prep_recs, 
                         \(split, prep_rec) bake(prep_rec, new_data = NULL)),
         held_outs = map2(resamples$splits, prep_recs, 
                          \(split, prep_rec) bake(prep_rec, 
                                                  new_data = assessment(split))),
         models = map(held_ins, 
                      \(held_in) fit_lm(held_in)),
         predictions = map2(models, held_outs, 
                            \(model, held_out) predict(model, held_out)$.pred),
         errors = map2_dbl(predictions, held_outs, 
                           \(pred, held_out) rmse_vec(held_out$y, pred)))
```

The pipline above creates a tibble with columns for each of the intermediate products and the rmse/error of the model in each resample.  All but the last columns are list columns that can hold objects of any time (e.g., prepped recipes, data frames, model objects).  The final column is a double column that holds the rmse of the model in each resample.  That is why we used `map_dbl()` to create the error column.
```{r}
resamples_ex1 |> glimpse()
```

We can now look at rmses across the 50 bootstraps.  For example, we can make a histogram using ggplot from the errors column in the fits tibble
```{r}
resamples_ex1 |> 
  ggplot(aes(errors)) +
  geom_histogram(binwidth = 0.05)
```

And we can summarize the min, max, mean, median and stdev of the error column in the fits tibble
```{r}
resamples_ex1 |> 
  summarize(n = n(),
            min = min(errors), 
            max = max(errors), 
            mean = mean(errors), 
            median = median(errors),
            std_dev = sd(errors))
```

Easy peasy!  But remember, it is easier still using `fit_reamples()`.  We demo this only to make clear what `fit_resamples()` is doing and to give you an alternative workflow in case you can't use `fit_resamples()`.  It is also a demonstration of the use of `map()` and list columns, which has many uses.

```{r}
resamples_tidy_ex1 <-
  linear_reg() |> 
    set_engine("lm") |> 
    fit_resamples(preprocessor = rec, 
                  resamples = resamples, 
                  metrics = metric_set(rmse))

resamples_tidy_ex1 |> 
  collect_metrics()

resamples_tidy_ex1 |> 
  collect_metrics(summarize = FALSE)
```

## Using map() to replace fit_resamples() - one function

If we wanted to generate the held-out error using resampling but didnt need/want the intermediate products, we could wrap all the steps in one function and just map that single function.  
```{r}
fit_and_eval <- function(split, rec) {
  # prep the recipe with held-in data
  prep_rec <- prep(rec, training = analysis(split))
  
  # bake the recipe using new_data = NULL to pull out the held-in features
  held_in <- bake(prep_rec, new_data = NULL)
  
  # bake the recipe using new_data = assessment(split) to get held-out features
  held_out <- bake(prep_rec, new_data = assessment(split))
  
  # fit the model using the held-in features
  model <- 
    linear_reg() |> 
    set_engine("lm") |> 
    fit(y ~ ., data = held_in)
  
  # get predictions using the model with the held-out features
  pred <- predict(model, held_out)$.pred
  
  # calculate the accuracy of the model
  rmse_vec(held_out$y, pred)
}
```

Now map this function over the splits to get a vector of rmse.  Same results, but not saving intermediate steps by using one function.
```{r}
resamples_ex2 <- resamples |> 
  mutate(errors = map_dbl(splits, \(split) fit_and_eval(split, rec)))
```

Here is what the resamples_ex2 tibble now looks like and summary stats on the resampled rmse
```{r}
resamples_ex2

resamples_ex2 |> 
  summarize(n = n(),
            min = min(errors), 
            max = max(errors), 
            mean = mean(errors), 
            median = median(errors),
            std_dev = sd(errors))
```


## Using two map()s to replace tune_grid()

Now we can make this a bit more complicated by adding a grid of hyperparameters to tune.  Lets keep it simple and tune only k in a knn model.  To tune k, we would normally use `tune_grid()` but we can do it again with two loops using `map()`.  We use an outer `map()` to loop over the resamples and an inner `map()` to loop over the values of lambda.   

Lets start by setting up a grid of hyperparameters to tune over.
```{r}
grid_k = tibble(neighbors = c(2, 4, 8, 16))
```

We need a single function to repeatedly fit and eval over our grid of parameters
```{r}
eval_grid <- function(split, rec, grid_k) {
 
  # get held-in and held-out features for split 
  # we calcuate features inside the function where fit all models across the grid 
  # because we want to make sure we only need to prep and bake the recipe once
  # per split.  Otherwise, we would waste a lot of computational time
  prep_rec <- prep(rec, training = analysis(split))
  held_in <- bake(prep_rec, new_data = NULL)
  held_out <- bake(prep_rec, new_data = assessment(split))

  # function fit fit and eval model for a specific lambda using held-in/held-out
  # for this split
  fit_eval <- function(k, held_in, held_out) {
    model <- 
      nearest_neighbor(neighbors = k) |>   
        set_engine("kknn") |>   
        set_mode("regression") |>  
        fit(y ~ ., data = held_in)
    
    pred <- predict(model, held_out)$.pred
    
    tibble(k = k, 
           rmse = rmse_vec(held_out$y, pred))
  }
  
  # loop through grid_k and fit/eval model for each k 
  # this is the inner loop from tune_grid()
  # use list_rbind() to bind the separate rows for each tibble into one larger tibble
  grid_k$neighbors |> 
    map(\(k) fit_eval(k, held_in, held_out)) |> 
    list_rbind()
}
```

Now map this function over the splits. This is the outer loop from `tune_grid()`.  `eval_grid()` will return a tibble with rows for each value of k.  We will save one tibble for each split in a list column called `rmses``. 

```{r}
resamples_ex3 <- resamples |> 
  mutate(rmses = map(splits, 
              \(split) eval_grid(split, rec, grid_k)))
```

The `rmses` column contains the rmse for each value of k for each split/resample in a tibble.  Each tibble has 4 rows (one for each k) and 2 columns (k and rmse).
```{r}
resamples_ex3
```

We can `unnest()` the rmses column to get a tibble with one row for each k value in each resample.  No need to display the original splits column so we will select it out.  For example.
```{r}
resamples_ex3 |> 
  unnest(rmses) |> 
  select(-splits) |>
  print(n = 30)
```

We can pipe this unnested tibble into a `group_by()` and `summarize()` to get the median (or mean) across resamples and then arrange to find the best k 

```{r}
resamples_ex3 |> 
  unnest(rmses) |> 
  select(-splits) |>
  group_by(k) |> 
  summarize(n = n(),
            mean_rmse = mean(rmse)) |> 
  arrange(mean_rmse)
```

The code above makes it clear that using resampling to tune a grid of hyperparameters is just a matter of looping over the resamples in an outer loop and looping over a  grid of hyperparameters in an inner loop. 

But of course, this can be done more easily with `tune_grid()`.  Here is the tidymodels version with `tune_grid()`.  Its clearly more concise code but the looping is not transparent.  
```{r}
resamples_tidy_ex3 <- 
  nearest_neighbor(neighbors = tune()) |>   
    set_engine("kknn") |>   
    set_mode("regression") |>  
    tune_grid(preprocessor = rec, 
              resamples = resamples, 
              grid = grid_k, 
              metrics = metric_set(rmse))

resamples_tidy_ex3 |> 
  collect_metrics() |> 
  arrange(mean)

resamples_tidy_ex3 |> 
  collect_metrics(summarize = FALSE)
```

## Using map() and foreach() to do nested cv

Now lets do the most complicated version of this.   Nested resampling involves looping over outer splits where the held out data are test sets used to evaluate best model configurations for each outer split and the inner loop makes validation sets that are used to select the best model configuration for each outer fold.  However, if we are tuning a grid of hyperparameters, there is even a further nested loop inside the inner resampling loop to get performance metrics for each value of the hyperparameter in the validation sets.

rsample supports creating a nested resamplinng object.  You can specify different resampling for the inner and outer loops.  k-fold for the outer loop and bootstraps for the inner loop is a common choice.
```{r}
resamples_nested <- d |> 
  nested_cv(outside = vfold_cv(v = 5), inside = bootstraps(times = 10))
```

Lets take a look at it.  

- It has five rows for each of the five outer splits.  The outer splits are the k-folds.
- The inner_resamples column contains the inner splits associated with each outer fold.  Each inner split is a bootstrap resample with 10 bootstraps.

```{r}
resamples_nested
```

Lets look a little more carefully at this structure because its critical to understanding nested cv. 

- Below we pull out and look at the first outer k-fold split. 
- The total sample size is 300 and the outer k-fold split has 60 observations held-out to eventually use as a test set and 240 held-in that will eventually be used as training data when we train models in the outer loop to get our final performance metrics for best values of k.
```{r}
resamples_nested |> 
  slice(1) |> 
  pull(splits)

# NOTE: sometimes its easier to use base R list notation to do this
# resamples_nested[[1]]$splits
```

Now look at the inner resamples for the first outer fold.  

- For each outer k-fold split, we take the held-in data (240 observations in this case) and split it further, using the inner resampling method (10 bootstraps here).
- Notice that each of the 10 bootstraps associated with the first outer k-fold split have 240 observation (because we used the held-in data form that outer k-fold split).  These will be used as training data for the inner loop of nested.  We will train models with various values of k with these data
- Notice also that each of these 10 boostraps have held-out (OOB) observations that will be used as validation sets for the inner loop of nested.  We will use these held-out validation sets to calculate performance metrics for the models withh diffferent value of k.  This will let us select the best value of k for each outer fold.

```{r}
# going to switch to base R notation 
resamples_nested[[1]]$inner_resamples
```

Now that we have an appropriate resamples structure to use for nested cv, lets get some functions together to calculate rmses for inner validation sets and outer test sets

We will need a function again that takes a split, uses the recipe to get held-in/held-out features, and then fits and evaluates a model for each value of k in the grid.  This is the exact same function as we  used earlier (because we are using knn again).   We include it below again for your review.

- We will use this function twice.
- In the inner loop of our nested cv, we will map this function over all 10 of our bootstraps for each inner fold using all values of k.
- In the outer loop of our nested cv, we will map this function over all 5 of our outer folds using the best value of k that was identified for eacd fold using its respective 10 bootstraps.
```{r}
#| eval: false

eval_grid <- function(split, rec, grid_k) {
 
  # get held-in and held-out features for split 
  prep_rec <- prep(rec, training = analysis(split))
  held_in <- bake(prep_rec, new_data = NULL)
  held_out <- bake(prep_rec, new_data = assessment(split))

  # function fit fit and eval model for a specific lambda and resample
  fit_eval <- function(k, held_in, held_out) {
    model <- 
      nearest_neighbor(neighbors = k) |>   
        set_engine("kknn") |>   
        set_mode("regression") |>  
        fit(y ~ ., data = held_in)
    
    pred <- predict(model, held_out)$.pred
    
    tibble(k = k, 
           rmse = rmse_vec(held_out$y, pred))
  }
  
  # loop through grid_k and fit/eval model for each k 
  # this is the inner loop
  # use list_rbind() to bind the separate rows for each tibble into one larger tibble
  grid_k$neighbors |> 
    map(\(k) fit_eval(k, held_in, held_out)) |> 
    list_rbind()
}
```

To loop eval_grid over the 10 bootstraps in the nested cv inner loop, we will need another simple function that applies eval_grid over each bootstrap with a set of 10 and binds the results into a single tibble.  
```{r}
bind_bootstraps <- function(bootstraps, rec, grid_k) {
 
  bootstraps$splits |>  
    map(\(bootstrap_split) eval_grid(bootstrap_split, rec, grid_k)) |> 
    list_rbind()
}
```

Now we are ready to get performance metrics for each k in `grid_k` for each of the inner 10 bootstraps in each of the five outer folds. Remember that `bind_bootstraps()` just calls `eval_grid()` for each of the 10 bootstraps in the inner loop and binds the results.  

```{r}
resamples_nested_ex4 <- resamples_nested |> 
  mutate(inner_rmses = map(inner_resamples, 
                          \(inner_resample) bind_bootstraps(inner_resample, 
                                                            rec, 
                                                            grid_k)))
```

Lets look at what we just added to our resamples object

- We now have a tibble with 40 rows and 2 columns associated with each set of 10 inner boostraps splits.
```{r}
resamples_nested_ex4 
```

If we look at one of them, we see

- The 40 rows are because we have 10 boostraps for each of the 4 values of k in the grid.  So 10 * 4 = 40 rmses
- we have columns for k and rmse
```{r}
resamples_nested_ex4$inner_rmses[[1]]
```

We now need to calculate the best k for each of the sets of 10 boostraps (associated with one outer k-fold split). To do this, we average the 10 rmses together for each value of k and then choose the k with the lowest rmse.  We will write a function to do this because we can then map that function over each of the sets of 10 boostraps associatged with each outer split. 
```{r}
get_best_k <- function(rmses) {
  rmses |>  
    group_by(k) |> 
    summarize(mean_rmse = mean(rmse)) |> 
    arrange(mean_rmse) |> 
    slice(1) |> 
    pull(k)
}

resamples_nested_ex4 <- resamples_nested_ex4 |> 
  mutate(best_k = map_dbl(inner_rmses, \(rmses) get_best_k(rmses)))
```

Now we have determined the best value of k for each of the 5 outer folds.  
```{r}
resamples_nested_ex4
```

Now we move up to the outer k-fold splits.  We will use the best k for each outer fold to fit a model using the held-in data from the outer fold and then evaluate that model using the held-out test data.  Those test data were NEVER used before so the will allow us to see how that model with a best value of k (selected by boostrap resampling) performs in new data

We can reuse the `eval_grid()` function we used before to fit and evaluate the model using the held-in data and the held-out test data.  But now we just use one value of k, the one that was best for each fold

```{r}
resamples_nested_ex4 <- resamples_nested_ex4 |> 
  mutate(test_rmse = map2(splits, best_k, 
                               \(split, k) eval_grid(split, rec, tibble(neighbors = k))))
```

We can pull the one row tibbles out of this final column and bind them together to get a tibble with one row for each outer fold.  We have retained the best value of k for each fold for our information (these don't need to be the same for each k-fold split).   If we want to know the true best k to use when we train a final model to implement using all our data, we need select it by using our inner reasampling method (10 boostraps) with all our data.  
```{r}
resamples_nested_ex4$test_rmse |> 
  list_rbind() |> 
  mutate(outer_fold = 1:length(resamples_nested_ex4$test_rmse)) |> 
  relocate(outer_fold)
```

