[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Applied Machine Learning",
    "section": "",
    "text": "Course Syllabus",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-syllabus",
    "href": "index.html#course-syllabus",
    "title": "Introduction to Applied Machine Learning",
    "section": "",
    "text": "Instructor\nJohn Curtin\n\nOffice hours: Thursdays, 1-2 pm or by appointment in Brogden 326\nEmail: jjcurtin@wisc.edu (but please use Slack DM or channel posts for all course communications during this semester)\n\n\n\nTeaching Assistants\nMichelle Marji\n\nOffice hours: Wednesdays, 10-11 am in Brogden 391 or by appointment\nEmail: michelle.marji@wisc.edu\n\nKendra Wyant\n\nOffice hours: Tuesdays, 12:30-1:30 pm in Brodgen 325 or by appointment\nEmail: kpaquette2@wisc.edu\n\n\n\nCourse Website\nhttps://jjcurtin.github.io/book_iaml/\n\n\nCommunications\nAll course communications will occur in the course’s Slack workspace (https://iaml-2024.slack.com/). You should have received an invitation to join the workspace. If you have difficulty joining, please contact me by my email above. The TAs and I will respond to all Slack messages within 1 business day (and often much quicker). Please plan accordingly (e.g., weekend messages may not receive a response until Monday). For general questions about class, coding assignments, etc., please post the question to the appropriate public channel. If you have the question, you are probably not alone. For issues relevant only to you (e.g., class absences, accommodations, etc.), you can send a direct message in Slack to me. However, I may share the DM with the TAs unless you request otherwise. In general, we prefer that all course communication occur within Slack rather than by email so that it is centralized in one location.\n\n\nMeeting Times\nThe scheduled course meeting times are Tuesdays and Thursdays from 11:00 - 12:15 pm. Tuesdays are generally used by the TAs to discuss application issues from the homework or in the course more generally. Thursdays are generally led by John and used to discuss topics from the video lectures and readings.\nAll required videos, readings, and application assignments are described on the course website at the beginning of each unit.\n\n\nCourse Description\nThis course is designed to introduce students to a variety of computational approaches in machine learning. The course is designed with two key foci. First, students will focus on the application of common, “out-of-the-box” statistical learning algorithms that have good performance and are implemented in tidymodels in R. Second, students will focus on the application of these approaches in the context of common questions in behavioral science in academia and industry.\n\n\nRequisites\nStudents are required to have completed Psychology 610 with a grade of B or better or a comparable course with my consent.\n\n\nLearning Outcomes\n\nStudents will develop and refine best practices for data wrangling, general programming, and analysis in R.\nStudents will distinguish among a variety of machine learning settings: supervised learning vs. unsupervised learning, regression vs. classification\nStudents will be able to implement a broad toolbox of well-supported machine-learning methods: decision trees, nearest neighbor, general and generalized linear models, penalized models including ridge, lasso, and elastic-nets, neural nets, random forests.\nStudents will develop expertise with common feature extraction techniques for quantitative and categorical predictors.\nStudents will be able to use natural language processing approaches to extract meaningful features from text data.\nStudents will know how to characterize how well their regression and classification models perform and they will employ appropriate methodology for evaluating their: cross validation, ROC and PR curves, hypothesis testing.\nStudents will learn to apply their skills to common learning problems in psychology and behavioral sciences more generally.\n\n\n\nCourse Topics\n\nOverview of Machine Learning Concepts and Uses\nData wrangling in R using tidyverse and tidymodels\nIterations, functions, simulations in R\nRegression models\nClassification models\nModel performance metrics\nROCs\nCross validation and other resampling methods\nModel selection and regularization\nApproaches to parallel processing\nFeature engineering techniques\nNatural language processing\nTree based methods\nBagging and boosting\nNeural networks\nDimensionality reduction and feature selection\nExplanatory methods including variable importance, partial dependence plots, etc\nEthics and privacy issues\n\n\n\nSchedule\nThe course is organized around 14 weeks of academic instruction covering the following topics:\n\nIntroduction to course and machine learning\nExploratory data analysis\nRegression models\nClassification models\nResampling methods for model selection and evaluation\nRegularization and penalized models\nMidterm exam/project\nAdvanced performance metrics\nModel comparisons\nAdvanced models: Random forests and ensemble methods (bagging, boosting, stacking)\nAdvanced models: Neural networks\nNatural Language Processing I: Text processing and feature engineering\nApplications\nEthics\n\n\nThe final exam is during exam week on Tuesday May 7th from 11 - 12:15 in our normal classroom.\nThe final project is due during exam week on Wednesday May 8th at 8 pm.\n\n\n\nRequired Textbooks and Software\nAll required textbooks are freely available online (though hard copies can also be purchased if desired). There are eight required textbooks for the course. The primary text for which we will read many chapters is:\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. An Introduction to Statistical Learning: With Applications in R (2023; 2nd Edition). (website)\n\nWe will also read sections to chapters in each of the following texts:\n\nWickham, H. & Grolemund, G. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (1st ed.). Sebastopol, CA: O’Reilly Media, Inc. (website)\nHvitfeldt, E. & Silge, J. Supervised Machine Learning for Text Analysis in R (website)\nKuhn, M. & Johnson, K. Applied Predictive Modeling. New York, NYL Springer Science. (website)\nKuhn, M., & Johnson, K. Feature Engineering and Selection: A Practical Approach for Predictive Models (1st ed.). Boca Raton, FL: Chapman and Hall/CRC. (website)\nKuhn, M. & Silge, J. Tidy Modeling with R. (website)\nMolnar, C. Intepretable Machine Learning: A Guide for Makiong Black Box Models Explainable (2nd ed.). (website\nSilge, J., & Robinson, D. Text Mining with R: A Tidy Approach (1st ed.). Beijing; Boston: O’Reilly Media. (website)\nWickham, H. The Tidy Style Guide. (website)\nBoehmke, Brad and Greenwell, Brandon M. (2019). Hands-On Machine Learning with R. Chapman and Hall/CRC. (website)\nNg, Andrew (2018). Machine Learning Yearning: Technical Strategy for AI Engineers in the Age of Deep Learning. DeepLearning.AI. (website)\nWickham, H. (2019). Advanced R. Chapman and Hall/CRC. (website)\n\nAdditional articles will be assigned and provided by pdf through the course website.\nAll data processing and analysis will be accomplished using R (and we recommend the RStudio IDE). R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS.\n\n\nGrading\n\nQuizzes (13 anticipated): 15%\nApplication assignments (11 anticipated): 25%\nMidterm application exam: 15%\nMidterm conceptual exam: 15%\nFinal application exam: 15%\nFinal conceptual exam: 15%\n\nFinal letter grades may be curved upward, but a minimum guarantee is made of an A for 93 or above, AB for 88 - 92, B for 83 - 87, BC for 78 - 82, C for 70 - 77, D for 60-69, and F for &lt; 60.\n\n\nExams, Application Assignments and Quizzes\n\nThe midterm application exam will be due during the 7th week of the course on Wednesday, March 6th at 8 pm.\nThe midterm conceptual exam will be administered during class on Thursday, March 7th.\nThe final exam is during exam week on Tuesday May 7th from 11 - 12:15 in our normal classroom.\nThe final project is due during exam week on Wednesday May 8th at 8 pm.\nApproximately weekly quizzes will be administered through Canvas and due each Wednesday at 8 pm\nApproximately weekly application assignments will be submitted via Canvas and due each Wednesday at 8 pm.\n\n\n\nApplication Assignments\nThe approximately weekly application assignments are due on Wednesdays at 8 pm through Canvas. These assignments are to be done individually. Please do not share answers or code. You are also encouraged to make use of online resources (e.g., stack overflow) for assistance. All assignments will be completed using R markdown to provide both the code and documentation as might be provided to your mentor or employer to fully describe your solution. Late assignments are not accepted because problem solutions are provided immediately after the due date. Application assignments are graded on a three-point scale (0 = not completed, 1 = partially completed and/or with many errors, 2 = fully completed and at least mostly correct). Grades for each assignment will be posted by the following Monday at the latest.\n\n\nChatGPT\nI suspect you have all seen discussions of all that ChatGPT can do by now and its impact on teaching and assessment. I believe that AI like ChatGPT will eventually become an incredible tool for data scientists and programmers. As such, I view these advances with excitement. Of course, I don’t plan to assign a grade to ChatGPT so I want to make sure that we are clear on when you can and when you cannot use it. Given that I expect AI like ChatGPT to become a useful tool in our workflow as professionals, now is the time to start to learn how it can help. Therefore, you are free to use it during any of our application assignments AND the application questions on the mid-term and final exams. Code from ChatGPT is unlikely to be sufficient in either context (and my testing suggests it can be flat out wrong in some instances!) but I suspect that it will still be useful. In contrast, you CANNOT use ChatGPT to answer the conceptual questions on the two exams or the weekly quizzes. Those questions are designed to assess your working knowledge about concepts and best practices. That information must be in YOUR head and I want to be 100% clear that use of ChatGPT to answer those questions will be considered cheating and handled as such if detected. There will be a zero tolerance policy for such cheating. It will be reported to the Dean of Students on first offense.\n\n\nStudent Ethics\nThe members of the faculty of the Department of Psychology at UW-Madison uphold the highest ethical standards of teaching and research. They expect their students to uphold the same standards of ethical conduct. By registering for this course, you are implicitly agreeing to conduct yourself with the utmost integrity throughout the semester.\nIn the Department of Psychology, acts of academic misconduct are taken very seriously. Such acts diminish the educational experience for all involved – students who commit the acts, classmates who would never consider engaging in such behaviors, and instructors. Academic misconduct includes, but is not limited to, cheating on assignments and exams, stealing exams, sabotaging the work of classmates, submitting fraudulent data, plagiarizing the work of classmates or published and/or online sources, acquiring previously written papers and submitting them (altered or unaltered) for course assignments, collaborating with classmates when such collaboration is not authorized, and assisting fellow students in acts of misconduct. Students who have knowledge that classmates have engaged in academic misconduct should report this to the instructor.\n\n\nDiversity and Inclusion\nInstitutional statement on diversity: “Diversity is a source of strength, creativity, and innovation for UW-Madison. We value the contributions of each person and respect the profound ways their identity, culture, background, experience, status, abilities, and opinion enrich the university community. We commit ourselves to the pursuit of excellence in teaching, research, outreach, and diversity as inextricably linked goals.\nThe University of Wisconsin-Madison fulfills its public mission by creating a welcoming and inclusive community for people from every background – people who as students, faculty, and staff serve Wisconsin and the world.” https://diversity.wisc.edu/\n\n\nAcademic Integrity\nBy enrolling in this course, each student assumes the responsibilities of an active participant in UW-Madison’s community of scholars in which everyone’s academic work and behavior are held to the highest academic integrity standards. Academic misconduct compromises the integrity of the university. Cheating, fabrication, plagiarism, unauthorized collaboration, and helping others commit these acts are examples of academic misconduct, which can result in disciplinary action. This includes but is not limited to failure on the assignment/course, disciplinary probation, or suspension. Substantial or repeated cases of misconduct will be forwarded to the Office of Student Conduct & Community Standards for additional review. For more information, refer to http://studentconduct.wiscweb.wisc.edu/academic-integrity\n\n\nAccommodations Polices\nMcBurney Disability Resource Center syllabus statement: “The University of Wisconsin-Madison supports the right of all enrolled students to a full and equal educational opportunity. The Americans with Disabilities Act (ADA), Wisconsin State Statute (36.12), and UW-Madison policy (Faculty Document 1071) require that students with disabilities be reasonably accommodated in instruction and campus life. Reasonable accommodations for students with disabilities is a shared faculty and student responsibility. Students are expected to inform faculty [me] of their need for instructional accommodations by the end of the third week of the semester, or as soon as possible after a disability has been incurred or recognized. Faculty [I], will work either directly with the student [you] or in coordination with the McBurney Center to identify and provide reasonable instructional accommodations. Disability information, including instructional accommodations as part of a student’s educational record, is confidential and protected under FERPA.” http://mcburney.wisc.edu/facstaffother/faculty/syllabus.php\nUW-Madison students who have experienced sexual misconduct (which can include sexual harassment, sexual assault, dating violence and/or stalking) also have the right to request academic accommodations. This right is afforded them under Federal legislation (Title IX). Information about services and resources (including information about how to request accommodations) is available through Survivor Services, a part of University Health Services: https://www.uhs.wisc.edu/survivor-services/\n\n\nComplaints\nOccasionally, a student may have a complaint about a TA or course instructor. If that happens, you should feel free to discuss the matter directly with the TA or instructor. If the complaint is about the TA and you do not feel comfortable discussing it with the individual, you should discuss it with the course instructor. Complaints about mistakes in grading should be resolved with the TA and/or instructor in the great majority of cases. If the complaint is about the instructor (other than ordinary grading questions) and you do not feel comfortable discussing it with the individual, make an appointment to speak to the Associate Chair for Graduate Studies, Professor Shawn Green, cshawngreen@wisc.edu.\nIf you have concerns about climate or bias in this class, or if you wish to report an incident of bias or hate that has occurred in class, you may contact the Chair of the Department, Professor Allyson Bennett (allyson.j.bennett@wisc.edu) or the Chair of the Psychology Department Climate & Diversity Committee, Martha Alibali (martha.alibali@wisc.edu). You may also use the University’s bias incident reporting system\n\n\nPrivacy of Student Information & Digital Tools\nThe privacy and security of faculty, staff and students’ personal information is a top priority for UW-Madison. The university carefully reviews and vets all campus-supported digital tools used to support teaching and learning, to help support success through learning analytics, and to enable proctoring capabilities. UW-Madison takes necessary steps to ensure that the providers of such tools prioritize proper handling of sensitive data in alignment with FERPA, industry standards and best practices. Under the Family Educational Rights and Privacy Act (FERPA which protects the privacy of student education records), student consent is not required for the university to share with school officials those student education records necessary for carrying out those university functions in which they have legitimate educational interest. 34 CFR 99.31(a)(1)(i)(B). FERPA specifically allows universities to designate vendors such as digital tool providers as school officials, and accordingly to share with them personally identifiable information from student education records if they perform appropriate services for the university and are subject to all applicable requirements governing the use, disclosure and protection of student data.\n\n\nPrivacy of Student Records & the Use of Audio Recorded Lectures\nSee information about privacy of student records and the usage of audio-recorded lectures.\nLecture materials and recordings for this course are protected intellectual property at UW-Madison. Students in this course may use the materials and recordings for their personal use related to participation in this class. Students may also take notes solely for their personal use. If a lecture is not already recorded, you are not authorized to record my lectures without my permission unless you are considered by the university to be a qualified student with a disability requiring accommodation. [Regent Policy Document 4-1] Students may not copy or have lecture materials and recordings outside of class, including posting on internet sites or selling to commercial entities. Students are also prohibited from providing or selling their personal notes to anyone else or being paid for taking notes by any person or commercial firm without the instructor’s express written permission. Unauthorized use of these copyrighted lecture materials and recordings constitutes copyright infringement and may be addressed under the university’s policies, UWS Chapters 14 and 17, governing student academic and non-academic misconduct.\n\n\nAcademic Calendar & Religious Observances\nStudents who wish to inquire about religious observance accommodations for exams or assignments should contact the instructor within the first two weeks of class, following the university’s policy on religious observance conflicts",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "001_overview.html",
    "href": "001_overview.html",
    "title": "1  Overview of Machine Learning",
    "section": "",
    "text": "1.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Machine Learning</span>"
    ]
  },
  {
    "objectID": "001_overview.html#overview-of-unit",
    "href": "001_overview.html#overview-of-unit",
    "title": "1  Overview of Machine Learning",
    "section": "",
    "text": "1.1.1 Learning Objectives\n\nUnderstand uses for machine learning models\nBecome familiar with key terminology (presented in bold throughout this unit)\nUnderstand differences between models\n\nSupervised vs. unsupervised\nRegression vs. classification\nOptions for statistical algorithms\nFeatures vs. predictors\n\nRelationships between:\n\nData generating processes\nStatistical algorithms\nModel flexibility\nModel interpretability\nPrediction vs. explanation\n\nUnderstand Bias-Variance Trade-off\n\nReducible and irreducible error\nWhat is bias and variance?\nWhat affects bias and variance?\nWhat is overfitting and how does it relate to bias, variance, and also p-hacking\nUse of training and test sets to assess bias and variance\n\n\n\n\n\n1.1.2 Readings\n\nYarkoni and Westfall (2017) paper\nJames et al. (2023) Chapter 2, pp 15 - 42\n\nPost questions to the readings channel in Slack\n\n\n1.1.3 Lecture & Discussion Videos\nNotes: You can adjust the playback speed of the videos to meet your needs. Closed captioning is also available for all videos.\n\nLecture 1: An Introductory Framework ~ 9 mins\nLecture 2: More Details on Supervised Techniques ~ 23 mins\nLecture 3: Key Terminology in Context ~ 11 mins\nLecture 4: An Example of Bias-Variance Tradeoff ~ 27 mins\n\nPost questions to the video-lectures channel in Slack\n\n\n\n1.1.4 Application Assignment and Quiz\n\nNo application assignment this unit!\nThe unit quiz is due by 8 pm on Wednesday January 24th",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Machine Learning</span>"
    ]
  },
  {
    "objectID": "001_overview.html#an-introductory-framework-for-machine-learning",
    "href": "001_overview.html#an-introductory-framework-for-machine-learning",
    "title": "1  Overview of Machine Learning",
    "section": "1.2 An Introductory Framework for Machine Learning",
    "text": "1.2 An Introductory Framework for Machine Learning\nMachine (Statistical) learning techniques have developed in parallel in statistics and computer science\nTechniques can be coarsely divided into supervised and unsupervised approaches\n\nSupervised approaches involve models that predict an outcome using features\nUnsupervised approaches involve finding structure (e.g., clusters, factors) among a set of variables without any specific outcome specified\nThis course will focus primarily on supervised machine learning problems\nHowever supervised approaches often use unsupervised approaches in early stages as part of feature engineering\n\n\nExamples of supervised approaches include:\n\nPredicting relapse day-by-day among recovering patients with substance use disorders based on cellular communications and GPS.\nScreening someone as positive or negative for substance use disorder based on their Facebook activity\nPredicting the sale price of a house based on characteristics of the house and its neighborhood\n\nExamples of unsupervised approaches include:\n\nDetermining the factor structure of a set of personality items\nIdentifying subgroups among patients with alcohol use disorder based on demographics, use history, addiction severity, and other patient characteristics\nIdentifying the common topics present in customer reviews of some new product or app\n\n\nSupervised machine learning approaches can be categorized as either regression or classification techniques\n\nRegression techniques involve numeric (quantitative) outcomes.\n\nRegression techniques are NOT limited to “regression” (i.e., the general linear model)\n\nThere are many more types of statistical models that are appropriate for numeric outcomes\n\nClassification techniques involve nominal (categorical) outcomes\nMost regression and classification techniques can handle categorical predictors\n\nAmong the earlier supervised model examples, predicting sale price was a regression technique and screening individuals as positive or negative for substance use disorder was a classification technique",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Machine Learning</span>"
    ]
  },
  {
    "objectID": "001_overview.html#more-details-on-supervised-techniques",
    "href": "001_overview.html#more-details-on-supervised-techniques",
    "title": "1  Overview of Machine Learning",
    "section": "1.3 More Details on Supervised Techniques",
    "text": "1.3 More Details on Supervised Techniques\nFor supervised machine learning problems, we assume \\(Y\\) (outcome) is a function of some data generating process (DGP, \\(f\\)) involving a set of Xs (features) plus the addition of random error (\\(\\epsilon\\)) that is independent of X and with mean of 0\n\\(Y = f(X) + \\epsilon\\)\n\nTerminology sidebar: Throughout the course we will distinguish between the raw predictors available in a dataset and the features that are derived from those raw predictors through various transformations.\n\nWe estimate \\(f\\) (the DGP) for two main reasons: prediction and/or inference (i.e., explanation per Yarkoni and Westfall, 2017)\n\\(\\hat{Y} = \\hat{f}(X)\\)\nFor prediction, we are most interested in the accuracy of \\(\\hat{Y}\\) and typically treat \\(\\hat{f}\\) as a black box\nFor inference, we are typically interested in the way that \\(Y\\) is affected by \\(X\\)\n\nWhich predictors are associated with \\(Y\\)?\nWhich are the strongest/most important predictors of \\(Y\\)\nWhat is the relationship between the outcome and the features associated with each predictor. Is the overall relationship between a predictor and \\(Y\\) positive, negative, dependent on other predictors? What is the shape of relationship (e.g., linear or more complex)?\nDoes the model as a whole improve prediction beyond a null model (no features from predictors) or beyond a compact model?\nWe care about good (low error) predictions even when we care about inference (we want small \\(\\epsilon\\))\n\nThey will also be tested with low power\nParameter estimates from models that don’t predict well may be incorrect or at least imprecise\n\n\n\nModel error includes both reducible and irreducible error.\nIf we consider both \\(X\\) and \\(\\hat{f}\\) to be fixed, then:\n\n\\(E(Y - \\hat{Y})^2 = (f(X) + \\epsilon - \\hat{f}(X))^2\\)\n\\(E(Y - \\hat{Y})^2 = [f(X) - \\hat{f}(X)]^2 + Var(\\epsilon)\\)\n\n\\(Var(\\epsilon)\\) is irreducible\n\nIrreducible error results from other important \\(X\\) that we fail to measure and from measurement error in \\(X\\) and \\(Y\\)\nIrreducible error serves as an (unknown) bounds for model accuracy (without collecting additional Xs)\n\n\\([f(X) - \\hat{f}(X)]^2\\) is reducible\n\nReducible error results from a mismatch between \\(\\hat{f}\\) and the true \\(f\\)\nThis course will focus on techniques to estimate \\(f\\) with the goal of minimizing reducible error\n\n\n\n1.3.1 How Do We Estimate \\(f\\)?\n\nWe need a sample of \\(N\\) observations of \\(Y\\) and \\(X\\) that we will call our training set\nThere are two types of statistical algorithms that we can use for \\(\\hat{f}\\):\n\nParametric algorithms\nNon-parametric algorithms\n\n\n\nParametric algorithms:\n\nFirst, make an assumption about the functional form or shape of \\(f\\).\n\nFor example, the general linear model assumes: \\(f(X) = \\beta_0 + \\beta_1*X_1 + \\beta_2*X2 + ... + \\beta_p*X_p\\)\nNext, a model using that algorithm is fit to the training set. In other words, the parameter estimates (e.g., \\(\\beta_0, \\beta_1\\)) are derived to minimize some cost function (e.g., mean squared error for the linear model)\nParametric algorithms reduce the problem of estimating \\(f\\) down to one of only estimating some set of parameters for a chosen model\nParametric algorithms often yield more interpretable models\nBut they are often not very flexible. If you chose the wrong algorithm (shape for \\(\\hat{f}\\) that does not match \\(f\\)) the model will not fit well in the training set (and more importantly not in the new test set either)\n\nTerminology sidebar: A training set is a subset of your full dataset that is used to fit a model. In contrast, a validation set is a subset that has not been included in the training set and is used to select a best model from among competing model configurations. A test set is a third subset of the full dataset that has not been included in either the training or validation sets and is used for evaluating the performance of your fitted final/best model.\n\nNon-parametric algorithms:\n\nDo not make any assumption about the form/shape of \\(f\\)\nCan fit well for a wide variety of forms/shapes for \\(f\\)\nThis flexibility comes with costs\n\nThey generally require larger \\(N\\) in the training set than parametric algorithms to achieve comparable performance\nThey may overfit the training set. This happens when they begin to fit the noise in the training set. This will yield low error in training set but much higher error in new validation or test sets.\nThey are often less interpretable\n\n\n\nGenerally:\n\nFlexibility and interpretability are inversely related\nModels need to be flexible enough to fit \\(f\\) well\nAdditional flexibility beyond this can produce overfitting\nParametric algorithms are generally less flexible than non-parametric algorithms\nParametric algorithms can become more flexible by increasing the number of features (\\(p\\) from 610/710; e.g., using more predictors, more complex, non-linear forms to when deriving features from predictors)\nParametric algorithms can be made less flexible through regularization. There are techniques to make some non-parametric algorithms less flexible as well\nYou want the sweet spot for prediction. You may want even less flexible for inference in increase interpretability.\n\n\n\n\n\n1.3.2 How Do We Assess Model Performance?\nThere is no universally best statistical algorithm\n\nDepends on the true \\(f\\) and your goal (prediction or inference)\nWe often compare multiple statistical algorithms (various parametric and non-parametric options) and model configurations more generally (combinations of different algorithms with different sets of features)\nWhen comparing models/configurations, we need to both fit these models and then select the best one\n\n\nBest needs to be defined with respect to some performance metric in new (validation or test set) data\n\nThere are many performance metrics you might use\nRoot Mean squared error (RMSE) is common for regression problems\nAccuracy is common for classification problems\n\nWe will learn many other performance metrics in a later unit\n\nTwo types of performance problems are typical\n\nModels are underfit if they don’t adequately represent the true \\(f\\), typically because they have oversimplied the relationship (e.g., linear function fit to quadratic DGP, missing key interaction terms)\n\nUnderfit models will yield biased predictions. In other words, they will systematically either under-predict or over-predict \\(Y\\) in some regions of the function.\nBiased models will perform poorly in both training and test sets\n\nModels are overfit if they are too flexible and begin to fit the noise in the training set.\n\nOverfit models will perform well (too well actually) in the training set but poorly in test or validation sets\nThey will show high variance such that the model and its predictions change drastically depending on the training set where it is fit\n\n\n\nMore generally, these problems and their consequences for model performance are largely inversely related\n\nThis is known as the Bias-Variance trade-off\nWe previously discussed reducible and irreducible error\n\nReducible error can be parsed into components due to bias and variance\nGoal is to minimize the sum of bias and variance error (i.e., the reducible error overall)\nWe will often trade off a little bias if it provides a big reduction in variance\n\n\nBut before we dive further into the Bias-Variance trade-off, lets review some key terminology that we will use throughout this course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Machine Learning</span>"
    ]
  },
  {
    "objectID": "001_overview.html#key-terminology-in-context",
    "href": "001_overview.html#key-terminology-in-context",
    "title": "1  Overview of Machine Learning",
    "section": "1.4 Key Terminology in Context",
    "text": "1.4 Key Terminology in Context\nIn the following pages:\n\nWe will present the broad steps for developing and evaluating machine learning models\nWe will situate key terms in this context (along with other synonymous terms used by others) and highlight them in bold.\n\nMachine learning has emerged in parallel from developments in statistics and computer science.\n\nAs a result, there is a lot of terminology and often multiple terms used for the same concept. This is not my fault!\n\nI will try to use one set of terms, but you need to be familiar with other terms you will encounter\n\n\nWhen developing a supervised machine learning model to predict or explain an outcome (also called DV, label, output):\n\nOur goal is for the model to match as close as possible (given the limits due to irreducible error) the true data generating process for Y.\nWe typically consider multiple (often many) candidate model configurations to achieve this goal.\n\n\nCandidate model configurations can vary with respect to:\n\nthe statistical algorithm used\nthe algorithm’s hyperparameters\nthe features used in the model to predict the outcome\n\n\nStatistical algorithms can be coarsely categorized as parametric or non-parametric.\nBut we will mostly focus on a more granular description of the specific algorithm itself\nExamples of specific statistical algorithms we will learn in this course include the linear model, generalized linear model, elastic net, LASSO, ridge regression, neural networks, KNN, random forest.\n\nThe set of candidate model configurations often includes variations of the same statistical algorithm with different hyperparameter (also called tuning parameter) values that control aspects of the algorithm’s operation.\n\nExamples include \\(k\\) in the KNN algorithm and \\(lambda\\) in LASSO, Ridge and Elastic Net algorithms.\nWe will learn more about hyperparameters and their effects later in this course.\n\n\nThe set of candidate model configurations can vary with respect to the features that are included.\n\nA recipe describes how to transform raw data for predictors (also called IVs) into features (also called regressors, inputs) that are included in the feature matrix (also called design matrix, model matrix).\n\nThis process of transforming predictors into features in a feature matrix is called feature engineering.\n\n\nCrossing variation on statistical algorithms, hyperparameter values, and alternative sets of features can increase the number of candidate model configurations dramatically\n\ndeveloping a machine learning model can easily involve fitting thousands of model configurations.\nIn most implementations of machine learning, the number of candidate model configurations nearly ensures that some fitted models will overfit the dataset in which they are developed such that they capitalize on noise that is unique to the dataset in which they were fit.\n\nFor this reason, model configurations are assessed and selected on the basis of their relative performance for new data (observations that were not involved in the fitting process).\n\n\nWe have ONE full dataset but we use resampling techniques to form subsets of that dataset to enable us to assess models’ performance in new data.\nCross-validation and bootstrapping are both examples of classes of resampling techniques that we will learn in this course.\nBroadly, resampling techniques create multiple subsets that consist of random samples of the full dataset. These different subsets can be used for model fitting, model selection, and model evaluation.\n\nTraining sets are subsets that are used for model fitting (also called model training). During model fitting, models with each candidate model configuration are fit to the data in the training set. For example, during fitting, model parameters are estimated for regression algorithms, and weights are established for neural network algorithms. Some non-parametric algorithms, like k-nearest neighbors, do not estimate parameters but simply “memorize” the training sets for subsequent predictions.\nValidation sets are subsets that are used for model selection (or, more accurately, for model configuration selection). During model selection, each (fitted) model — one for every candidate model configuration — is used to make predictions for observations in a validation set that, importantly, does not overlap with the model’s training set. On the basis of each model’s performance in the validation set, the relatively best model configuration (i.e., the configuration of the model that performs best relative to all other model configurations) is identified and selected. If you have only one model configuration, validation set(s) are not needed because there is no need to select among model configurations.\nTest sets are subsets that are used for model evaluation. Generally, a model with the previously identified best configuration is re-fit to all available data other than the test set. This fitted model is used to predict observations in the test set to estimate how well this model is expected to perform for new observations.\n\n\nThere are three broad steps to develop and evaluate a machine learning model:\n\nFitting models with multiple candidate model configurations (in training set(s))\nAssessing each model to select the best configuration (in validation set(s))\nEvaluating how well a model with that best configuration will perform with new observations (in test sets(s))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Machine Learning</span>"
    ]
  },
  {
    "objectID": "001_overview.html#an-example-of-the-bias-variance-trade-off",
    "href": "001_overview.html#an-example-of-the-bias-variance-trade-off",
    "title": "1  Overview of Machine Learning",
    "section": "1.5 An Example of the Bias-Variance Trade-off",
    "text": "1.5 An Example of the Bias-Variance Trade-off\n\n1.5.1 Overview of Example\nThe concepts of underfitting vs. overfitting and the bias-variance trade-off are critical to understand\n\nIt is also important to understand how model flexibility can affect both the bias and variance of that model’s performance\nIt can help to make these abstract concepts concrete by exploring real models that are fit in actual data\nWe will conduct a very simple simulation to demonstrate these concepts\n\nThe code in this example is secondary to understanding the concepts of underfittinng, overfitting, bias, variance, and the bias-variance trade-off\n\nWe will not display much of it so that you can maintain focus on the concepts\nYou will have plenty of time to learn the underlying\n\n\nWhen modeling, our goal is typically to approximate the data generating process (DGP) as close as possible, but in the real world we never know the true DGP.\nA key advantage of many simulations is that we do know the DGP because we define it ourselves.\n\nFor example, in this simulation, we know that \\(Y\\) is a cubic function of \\(X\\) and noise (random error).\nIn fact, we know the exact equation for calculating \\(Y\\) as a function of \\(X\\).\n\n\\(y = 1100 - 4.0 * x - 0.4 * x^2 + 0.1 * (x - h)^3 + noise\\), where:\n\nb0 = 1100\nb1 = -4.0\nb2 = -0.4\nb3 = 0.1\nh = -20.0\nnoise has mean = 0 and sd = 150\n\n\n\nWe will attempt to model this cubic DGP with three different model configurations\n\nA simple linear model that uses only \\(X\\) as a feature\nA (20th order) polynomial linear model that uses 20 polynomials of \\(X\\) as features\nA (20th order) polynomial LASSO model that uses the same 20 polynomials of \\(X\\) as features but “regularizes” to remove unimportant features from the model\n\n\n\n\n\n\n\nQuestion: If the DGP for y is a cubic function of x, what do we know about the expected bias for our three candidate model configurations in this example?\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model will underfit the true DGP and therefore it will be biased b/c \nit can only represent Y as a linear function of X.  \n\nThe two polynomial models will be generally unbiased b/c they have X represented \nwith 20th order polynomials.  \n\nLASSO will be slightly biased due to regularization but more on that in a later unit\n\n\n\n\n\n\n\n\n1.5.2 Stimulation Steps\nWith that introduction complete, lets start our simulation of the bias-variance trade-off\n\nLets simulate four separate research teams, each working to estimate the DGP for Y\n\n\nEach team will get their own random sample of training data (N = 100) to fit models\n\nHere are plots of these four simulated training sets (one for each team) with a dotted line for the data generating process (DGP)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe get one more large random sample (N = 5000) with the same DGP to use as a test set to evaluate all the models that will be fit in the separate training sets across the four teams.\n\n\nWe will let each team use this same test set to keep things simple\nThe key is that the test set contains new observations not present in any of the training sets\n\n\n\n\n\n\n\n\n\n\n\n\nEach of the four teams fit their three model configurations in their training sets\nThey use the resulting models to make predictions for observations in the same training set in which they were fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can you see evidence of bias for any model configuration? Look in any training set.\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model is clearly biased.  It systemically underestimates Y in \nsome portions of the X distribution and overestimates Y in other portions of the \nX distribution.  This is true across training sets for all teams.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can you see any evidence of overfitting for any model configuration?\n\n\n\n\n\n\n\nShow Answer\nThe polynomial linear model appears to overfit the data in the training set.  In \nother words, it seems to follow both the signal/DGP and the noise.  However, in practice\nnone of the teams could not be certain of this with only their training set.  It is\npossible that the wiggles in the prediction line represent the real DGP.   They need\nto look at the model's performance in the test set to be certain about the degree of\noverfitting.  (Of course, we know because these are simulated data and we know the DGP.)\n\n\n\n\n\n\n\nNow the teams use their 3 trained models to make predictions for new observations in the test set\n\n\nRemember that the test set has NEW observations of X and Y that weren’t used for fitting any of the models.\nLets look at each model configuration’s performance in test separately\n\n\n\nHere are predictions from the four simple linear models (fit in the training sets for each team) in the test set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can you see evidence of bias for the simple linear models?\n\n\n\n\n\n\n\nShow Answer\nYes, consistent with what we saw in the training sets, the simple linear model\nsystematically overestimates Y in some places and underestimates it in others.  \nThe DGP is clearly NOT linear but this simple model can only make linear predictions.\nIt is a fairly biased model that underfits the true DGP.  This bias will make a \nlarge contribution to the reducible error of the model\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How much variance across the simple linear models is present?\n\n\n\n\n\n\n\nShow Answer\nThere is not much variance in the prediction lines across the models that were \nfit by different teams in different training sets.  The slopes are very close across\nthe different team's models and the intercepts only vary by a small amount.   \nThe simple linear model configuration does not appear to have high variance (across teams) \nand therefore model variance will not contribute much to its reducible error.\n\n\n\n\n\n\n\nHere are predictions from the polynomial linear models from the four teams in the test set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Are these polynomial models systematically biased?\n\n\n\n\n\n\n\nShow Answer\nThere is not much systematic bias.  The overall function is generally cubic for \nall four teams  - just like the DGP. Bias will not contribute much to the model's \nreducible error.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does the variance of these polynomial models compare to the variance of the simple linear models?\n\n\n\n\n\n\n\nShow Answer\nThere is much higher model variance for this polynomial linear model relative to \nthe simple linear model. Although all four models generally predict Y as a cubic\nfunction of X, there is also a non-systematic wiggle that is different for each \nteam's models.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does this demonstrate the connection between model overfitting and model variance?\n\n\n\n\n\n\n\nShow Answer\nModel variance (across teams) is a result of overfitting to the training set.  \nIf a model fits noise in its training set, that noise will be different in every dataset.\nTherefore, you end up with different models depending on the training set in which they \nare fit.  And none of those models will do well with new data as you can see in this\ntest set because noise is random and different in each dataset.\n\n\n\n\n\n\n\nHere are predictions from the polynomial LASSO models from each team in the test set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does their bias compare to the simple and polynomial linear models?\n\n\n\n\n\n\n\nShow Answer\nThe LASSO models have low bias much like the polynomial linear model. They are able \nto capture the true cubic DGP fairly well.  The regularization process slightly reduced the\nmagnitude of the cubic (the prediction line is a little straighter than it should be),\nbut not by much.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does their variance compare to the simple and polynomial linear models?\n\n\n\n\n\n\n\nShow Answer\nAll four LASSO models, fit in different training sets, resulted in very similar \nprediction lines. Therefore, these LASSO models have low variance, much like the simple linear model. \nIn contrast, the LASSO model variance is clearly lower than the more flexible \npolynomimal model.\n\n\n\n\n\n\n\nNow we will quantify the performance of these models in training and test sets with the root mean square error performance metric. This is the standard deviation of the error when comparing the predicted values for Y to the actual values (ground truth) for Y.\n\n\n\n\n\n\n\nQuestion: What do we expect about RMSE for the three models in train and test?\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model is underfit to the TRUE DGP.  Therfore it is \nsystematically biased everywhere it is used.  It won't fit well in train or test \nfor this reason.  However, it’s not very flexible so it won’t be overfit to the noise \nin train and therefore should fit comparably in train and test.  \n\nThe polynomial linear model will not be biased at all given that the DGP is polynomial.  \nHowever, it is overly flexible (20th order) and so will substantially overfit the \ntraining data such that it will show high variance and its performance will be poor in test.  \n\nThe polynomial LASSO will be the sweet spot in bias-variance trade-off.  It has \na little bias but not much.  However, it is not as flexible due to regularization\nby lambda so it won’t be overfit to its training set.  Therefore, it should do \nwell in the test set.\n\n\n\n\n\n\nTo better understand this:\n\nCompare RMSE across the three model configurations within the training sets (turquoise line)\nCompare how RMSE changes for each model configuration across its training set and the test set\nCompare RMSE across the three model configurations within the test set (red line)?\nSpecifically compare the performance of simple linear model (least flexible) with the polynomial linear model (most flexible)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Would these observations about bias and variance of these three model configurations always be the same regardless of the DGP?\n\n\n\n\n\n\n\nShow Answer\nNo.  A model configuration needs to be flexible enough and/or well designed to \nrepresent the DGP for the data that you are modeling.   The two polynomial models\nin this example were each able to represent a cubic DGP.  The simple linear model\nwas not.  The polynomial linear model was too flexible for a cubic given that it \nhad 20 polynomials of X.  Therefore, it was overfit to its training set and had \nhigh variance.  However, if the DGP was a different shape, the story would be \ndifferent.  If the DGP was linear the simple linear model would not have been \nbiased and would have performed best. If this DGP was some other form (step function),\nit may be that none of the models would work well.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Machine Learning</span>"
    ]
  },
  {
    "objectID": "001_overview.html#discussion---tuesdaythursday",
    "href": "001_overview.html#discussion---tuesdaythursday",
    "title": "1  Overview of Machine Learning",
    "section": "1.6 Discussion - Tuesday/Thursday",
    "text": "1.6 Discussion - Tuesday/Thursday\n\n1.6.1 Course Overview\n\nIntroductions (preferred name, pronouns, program/department and year)\n\n\n\nStructure of course\n\nSame flow each week\n\nWeek starts on Thursdays at 12:15 pm\nAssignments include:\n\nPre-recorded lectures\nWeb book material\nReadings (James et al. (2023) and other sources; All free)\nApplication assignment\n\nAsychronous discussion and questions on Slack\nLab section on Tuesdays at 11:00 am - address previous week’s code\nQuiz and application assignments due Wednesdays at 8 pm\nWrap-up discussion and conceptual questions on Thursdays at 11 am. Not a lecture\n\nSelf-paced (except for due dates and discussion)\nQuizzes used to encourage and assess reading. Also to guide discussion.\nWorkload similar to 610/710\nOffice hours\n\nJohn - Thursdays, 1 – 2 pm\nMichelle - Wednesdays, 10 - 11 am\nKendra - Mondays, 2:30 - 3:30 pm\nPersonal appointments & Slack\n\n\n\n\n\nThe web book\n\nPrimary source for all course materials\nOrganized by units (with syllabus at front)\nLinks to pre-recorded lectures, readings, and quiz\nProvides primary source for code examples (all you need for this course)\nLecture follows book\n\n\n\n\nCourse as guided learning\n\nConcepts in lectures and readings\nApplications in web book and application assignment\nDiscussion section is discussion/questions (not pre-planned lecture)\nSlack CAN be a good source for discussion and questions as well\nGrades are secondary (quizzes, application assignments, exams)\n\n\n\n\nWhy these tools?\n\nQuarto\n\nScientific publishing system (reproducible code, with output, presentations, papers)\nTool for collaboration\nInteractive with static product (render to html or pdf)\nApplication assignments, web book, and slides\n\ntidyverse?\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures\nUnmatched for data wrangling, EDA, and data visualization\n\ntidymodels?\n\nConsistent interface to hundreds of statistical algorithms\nUnder VERY active development\nWell-supported and documented (tidymodels.org)\nBe careful with other web documentation (often out of date)\n\n\n\n\n\nWhy me?\n\nPrimary tools in our research program\nModel for progression from 610/710\nCan do AND can collaborate with CS and expert data scientists\n\n\n\n\nEnvironment\n\nSafe and respectful\nVERY encouraging of questions, discussion, and tangents\nHave fun\nAccomodations and Complaints\n\n\n\n\nChatGPT\n\nYay! May develop into an amazing tool in your workflow\nUse as tool (like Stack Overflow) for applications (application assignments, application questions on exams)\nCheck carefully - it can be wrong even when it looks right\nDo NOT use for conceptual questions (quizzes, conceptual exam questions). This type of info needs to be in your head to be effective data scientist.\n\nAcademic Integrity\n\nDo not cheat! Only you lose.\nNo collaboration with classmates, peers, previous students on anything graded (including application assignments)\nAll cheating reported to Department and Dean of Students. If application assignment or quizzes, zero on all of them because I can’t trust them. If exam, zero on exam.\n\n\n\n\n\n1.6.2 Association vs. Prediction\n\n\n\n\n\n\nQuestion: What is the difference between association vs. prediction?\n\n\n\n\n\n\n\nShow Answer\nAssociation quantifies the relationship between variables within a sample \n(predictors-outcome).  Prediction requires using an established model to \npredict (future?) outcomes for new (\"out-of-sample, \"held-out\") participants.\n\n\n\n\n\n\n\nMuch research in psychology demonstrates association but calls it prediction!\nAssociation (sometimes substantially) overestimates the predictive strength of our models\n\nCoefficients are derived to mimimize SSE (or maximize \\(R^2\\))\n\\(R^2\\) from GLM (using one sample) indexes how well on average any GLM that is fit to a sample will account for variance in that sample when specific coefficients are estimated in the same sample they are evaluated\n\\(R^2\\) does NOT tell you how well a specific GLM (including its coefficients) will work with new data for prediction\n\\(R^2\\) itself is positively biased even as estimate of how well a sample specific GLM will predict in that sample (vs. adjusted \\(R^2\\) and other corrections)\n\n\n\n\n\n1.6.3 Prediction vs. Explanation\n\nExamples of valuable prediction without explanation?\nCan you have explanation without prediction?\nPrediction models can provide insight or tests of explanatory theories (e.g., do causes actually predict in new data; variable importance)\nGoal of scientific psychology is to understand human behavior. It involves both explaining behavior (i.e., identifying causes) and predicting (yet to be observed) behaviors.\n\nWe overemphasize explanatory goals in this department, IMO\n\nMachine learning is well positioned for prediction, but also for explanation\n\n\n\n\n1.6.4 Basic framework and terminology for machine learning\n\nSupervised vs. unsupervised machine learning?\nSupervised regression vs classification?\n\n\n\n\n1.6.5 Data Generating Process\nWhat is a data generating process?\n\n\\(Y = f(X) + \\epsilon\\)\nboth function and Xs are generally unknown\n\\(\\hat{Y} = \\hat{f}(X)\\)\n\nWhy do we estimate the data generating process?\n\n\n\n1.6.6 Cross Validation\n\nWhat is it and why do we do it?\nHow are replication and cross-validation different?\n\n\nReducible vs. Irreducible error?\n\nOur predictions will have error\nYou learned to estimate parameters in the GLM to minimize error in 610\nBut error remained non-zero (in your sample and more importantly with same model in new samples) unless you perfectly estimated the DGP\nThat error can be divided into two sources\nIrreducible error comes from measurement error in X, Y and missing X because predictors (causes) not measured.\n\nIrreducible without collecting new predictors and/or with new measures\nIt places a ceiling on the performance of the best model you can develop with your available data\n\nReducible error comes from mismatch between \\(\\hat{f}(X)\\) and the true \\(f(X)\\).\n\nWe can reduce this without new data.\n\nJust need better model (\\(\\hat{f}(X)\\)).\n\nYou didn’t consider this (much) in 610 because you were limited to one statistical algorithm (GLM) AND it didn’t have hyperparameters.\nYou did reduce error by coding predictors (feature engineering) differently (interactions \\(X1*X2\\), polynomial terms \\(X^2\\), power transformations of X) This course will teach you methods to decrease reducible error (and validly estimate total error of that best model)\n\n\n\nWhat are the three general steps by which we estimate and evaluate the data generating process with a sample of data? Lets use all this vocabulary!\n\nCandidate model configurations\n\nStatistical algorithms\nHyperparameters\nFeatures (vs. predictors?), feature matrix, feature engineering, recipe (tidymodels specific)\n\nModel fitting (training), selection, and evaluation\nResampling techniques\n\ncross validation techniques (k-fold)\nboostrapping for cross validation\n\nTraining, validation, and test sets (terms vary in literature!)\n\n\n\n\n1.6.7 Bias-variance tradeoff\nWhat is underfitting, overfitting, bias, and variance?\nBias and variance are general concepts to understand during any estimation process\n\nEstimate mean, median, standard deviation\nParameter estimates in GLM\nEstimate DGP - \\(\\hat{Y} = \\hat{f}(X)\\)\n\n\nConceptual example of bias-variance: Darts from Yarkoni and Westfall (2017)\n\n\nSecond Conceptual Example: Models (e.g. many scales made by Acme Co.) to measure my weight\n\n\n\n1.6.8 Bias - A deeper dive\nBiased models are generally less complex models (i.e., underfit) than the data-generating process for your outcome\n\nBiased models lead to errors in prediction because the model will systematically over- or under-predict outcomes (scores or probabilities) for specific values of predictor(s) (bad for prediction goals!)\nParameter estimates from biased models may over or under-estimate the true effect of a predictor (bad for explanatory goals!)\n\n\n\n\n\n\n\n\nQuestion: Are GLMs biased models?\n\n\n\n\n\n\n\nShow Answer\nGLM parameter estimates are BLUE - best **linear** unbiased estimators. Parameter\nestimates from any sample are unbiased estimates of the linear model coefficients\nfor population model but if DGP is not linear, this linear model will produce \nbiased predictions and have biased parameter estimates.\n\n\n\n\n\n\nBias seems like a bad thing.\n\nBoth bias (due to underfitting) and variance (due to overfitting) are sources of (reducible) prediction errors (and imprecise/inaccurate parameter estimates). They are also often inversely related (i.e., the trade-off).\nA model configuration needs to be flexible enough to represent the true DGP.\n\nAny more flexibility will lead to overfitting.\n\nAny less flexibility will lead to underfitting.\n\nIdeally, if your model configuration is perfectly matched to the DGP, it will have very low bias and very low variance (assuming sufficiently large N)\nThe world is complex. In many instances,\n\nWe can’t perfectly represent the DGP\nWe trade off a little bias for big reduction in variance to produce the most accurate predictions (and stable parameter estimates across samples for explanatory goals)\nOr we trade off a little variance (slightly more flexible model) to get a big reduction in bias\nEither way, we get models that predict well and may be useful for explanatory goals\n\n\n\n\n\n1.6.9 Variance - A deeper dive\n\n\n\n\n\n\nQuestion: Consider example of p = n - 1 in general linear model. What happens in this situation? How is this related to overfitting and model flexibility?\n\n\n\n\n\n\n\nShow Answer\nThe  model will perfectly fit the sample data even when there is no relationship\nbetween the predictors and the outcome.  e.g., Any two points can be fit perfectly\nwith one predictor (line), any three points can be fit perfectly with two predictors\n(plane).  This model will NOT predict well in new data.  This model is overfit \nbecause n-1 predictors is too flexible for the linear model. You will fit the \nnoise in the training data.\n\n\n\n\n\n\nFactors that increase overfitting\n\nSmall N\nComplex models (e.g, many predictors, p relative to n, non-parametric models)\nWeak effects of predictors (lots of noise available to overfit)\nCorrelated predictors (for some algorithms like the GLM)\nChoosing between many model configurations (e.g. different predictors or predictor sets, transformations, types of statistical models) - lets return to this when we consider p-hacking\n\n\nYou might have noticed that many of the above factors contribute to the standard error of a parameter estimate/model coefficient from the GLM\n\nSmall N\nBig p\nSmall \\(R^2\\) (weak effects)\nCorrelated predictors\n\nThe standard error increases as model overfitting increases due to these factors\n\n\n\n\n\n\n\nQuestion: Explain the link between model variance/overfitting, standard errors, and sampling distributions?\n\n\n\n\n\n\n\nShow Answer\nAll parameter estimates have a sampling distribution.  This is the distribution \nof estimates that you would get if you repeatedly fit the same model to new samples.\nWhen a model is overfit, that means that aspects of the model (its parameter \nestimates, its predictions) will vary greatly from sample to sample.  This is \nrepresented by a large standard error (the SD of the sampling distribution) for \nthe model's parameter estimates.  It also means that the predictions you will make\nin new data will be very different depending on the sample that was used to \nestimate the parameters.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Describe problem of p-hacking with respect to overfitting?\n\n\n\n\n\n\n\nShow Answer\nWhen you p-hack, you are overfitting the training set (your sample).  You try out\nmany, many different model configurations and choose the one you like best rather\nthan what works well in new data.  This model likely capitalizes on noise in your\nsample.  It won't fit well in another sample.  In other words, your conclusions \nare not linked to the true DGP and would be different if you used a different sample.\nIn a different vein, your significance test is wrong.  The SE does not reflect \nthe model variance that resulted from testing many different configurations b/c \nyour final model didn't \"know\" about the other models.  Statistically invalid \nconclusion!\n\n\n\n\n\n\nParameter estimates from an overfit model are specific to the sample within which they were trained and are not true for other samples or the population as a whole\n\nParameter estimates from overfit models have big (TRUE) SE and so they may be VERY different in other samples\n\nThough if the overfitting is due to fitting many models, it won’t be reflected in the SE from any one model because each model doesn’t know the other models exist! p-hacking!!\n\nWith traditional (one-sample) statistics, this can lead us to incorrect conclusions about the effect of predictors associated with these parameter estimates (bad for explanatory goals!).\nIf the parameter estimates are very different sample to sample (and different from the true population parameters), this means the model will predict poorly in new samples (bad for prediction goals!). We fix this by using resampling to evaluate model performance.\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag.\n\n\nYarkoni, Tal, and Jacob Westfall. 2017. “Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning.” Perspectives on Psychological Science 12 (6): 1100–1122.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Machine Learning</span>"
    ]
  },
  {
    "objectID": "002_exploratory_data_analysis.html",
    "href": "002_exploratory_data_analysis.html",
    "title": "2  Exploratory Data Analysis",
    "section": "",
    "text": "2.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "002_exploratory_data_analysis.html#overview-of-unit",
    "href": "002_exploratory_data_analysis.html#overview-of-unit",
    "title": "2  Exploratory Data Analysis",
    "section": "",
    "text": "2.1.1 Learning Objectives\n\nStages of Analysis\nBest practices for data storage, variable classing, data dictionaries\nProblems and solutions regarding data leakage\nKey goals and techniques cleaning EDA\n\nTidying names and response labels\nAppropriate visualizations based on variable class\nSummary statistics based on variable class\n\nProper splitting for training/validation and test sets\nKey goals and techniques modeling EDA\n\nAppropriate visualizations based on variable class\nSummary statistics based on variable class\n\nIntroductory use of recipes for feature engineering\n\n\n\n\n2.1.2 Readings\n[NOTE: These are short chapters. You are reading to understand the framework of visualizing data in R. Don’t feel like you have to memorize the details. These are reference materials that you can turn back to when you need to write code!]\n\nWickham, Çetinkaya-Rundel, and Grolemund (2023) Chapter 1, Data Visualization\nWickham, Çetinkaya-Rundel, and Grolemund (2023) Chapter 9, Layers\nWickham, Çetinkaya-Rundel, and Grolemund (2023) Chapter 10, Exploratory Data Analysis\n\nPost questions to the readings channel in Slack\n\n\n2.1.3 Lecture Videos\n\nLecture 1: Stages of Data Analysis and Model Development ~ 10 mins\nLecture 2: Best Practices and Other Recommendations ~ 27 mins\nLecture 3: EDA for Data Cleaning ~ 41 mins\nLecture 4: EDA for Modeling - Univariate ~ 24 mins\nLecture 5: EDA for Modeling - Bivariate ~ 20 mins\nLecture 6: Working with Recipes ~ 13 mins\n\nPost questions to the video-lectures channel in Slack\n\n\n\n2.1.4 Application Assignment and Quiz\n\ndata\ndata dictionary\ncleaning EDA: qmd\nmodeling EDA: qmd\nsolutions: cleaning EDA; modeling EDA\n\nPost questions to the application-assignments channel in Slack\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, January 31st",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "002_exploratory_data_analysis.html#overview-of-exploratory-data-analysis",
    "href": "002_exploratory_data_analysis.html#overview-of-exploratory-data-analysis",
    "title": "2  Exploratory Data Analysis",
    "section": "2.2 Overview of Exploratory Data Analysis",
    "text": "2.2 Overview of Exploratory Data Analysis\n\n2.2.1 Stages of Data Analysis and Model Development\nThese are the main stages of data analysis for machine learning and the data that are used\n\nEDA: Cleaning (full dataset)\nEDA: Split data into training, validation and test set(s)\nEDA: Modeling (training sets)\nModel Building: Feature engineering (training sets)\nModel Building: Fit many models configurations (training set)\nModel Building: Evaluate many models configurations (validation sets)\nFinal Model Evaluation: Select final/best model configuration (validation sets)\nFinal Model Evaluation: Fit best model configuration (use both training and validation sets)\nFinal Model Evaluation: Evaluate final model configuration (test sets)\nFinal Model Evaluation: Fit best model configuration to ALL data (training, validation, and test sets) if you plan to use it for applications.\n\n\nThe earlier stages are highly iterative:\n\nYou may iterate some through EDA stages 1-3 if you find further errors to clean in stage 3 [But make sure you resplit into the same sets]\nYou will iterate many times though stages 3-6 as you learn more about your data both through EDA for modeling and evaluating actual models in validation\n\nYou will NOT iterate back to earlier stages after you select a final model configuration\n\nStages 7 - 10 are performed ONLY ONCE\nOnly one model configuration is selected and re-fit and only that model is brought into test for evaluation\nAny more than this is essentially equivalent to p-hacking in traditional analyses\nStep 10 only happens if you plan to use the model in some application",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "002_exploratory_data_analysis.html#best-practices-and-other-recommendations",
    "href": "002_exploratory_data_analysis.html#best-practices-and-other-recommendations",
    "title": "2  Exploratory Data Analysis",
    "section": "2.3 Best Practices and Other Recommendations",
    "text": "2.3 Best Practices and Other Recommendations\n\n2.3.1 Data file formats\nWe generally store data as CSV [comma-separated value] files\n\nEasy to view directly in a text editor\nEasy to share because others can use/import into any data analysis platform\nWorks with version control (e.g. git, svn)\nuse read_csv() and write_csv()\n\nExceptions include:\n\nWe may consider binary (.rds) format for very big files because read/write can be slow for csv files.\n\nBinary file format provides a very modest additional protection for sensitive data (which we also don’t share)\nuse read_rds() and write_rds()\n\nSee chapter 7 - Data Import in Wickham, Çetinkaya-Rundel, and Grolemund (2023) for more details and advanced techniques for importing data using read_csv()\n\n\n\n2.3.2 Classing Variables\nWe store and class variables in R based on their data type (level of measurement).\n\nSee Wikipedia definitions for levels of measurement for a bit more precision that we will provide here.\n\nCoarsely, there are four levels:\n\nnominal: qualitative categories, no inherent order (e.g., marital status, sex, car color)\nordinal: qualitative categories (sometimes uses number), inherent order but not equidistant spacing (e.g., Likert scale; education level)\ninterval and ratio (generally treated the same in social sciences): quantitative scores, ordered, equidistant spacing. Ratio has true 0. (e.g., temperature in Celsius vs. Kelvin scales)\n\nWe generally refer to nominal and ordinal variables as categorical and interval/ratio as quantitative or numeric\n\nFor nominal variables\n\nWe store (in csv files) these variables as character class with descriptive text labels for the levels\n\nEasier to share/document\nReduces errors\n\nWe class these variables in R as factors when we load them (using read_csv())\nIn some cases, we should pay attention to the order of the levels of the variable. e.g.,\n\nFor a dichotomous outcome variable, the positive/event level of dichotomous factor outcome should be first level of the factor\nThe order of levels may also matter for factor predictors (e.g., step_dummy() uses first level as reference).\n\n\n\nFor ordinal variables:\n\nWe store (in csv files) these variables as character class with descriptive text labels for the levels\n\nEasier to share/document\nReduces errors\n\nWe class these variables in R as factors (just like nominal variables)\n\nIt is easier to do EDA with these variables classes as factors\nWe use standard factors (not ordered)\n\nConfirm that the order of the levels is set up correctly. This is very important for ordinal variables.\nDuring feature engineering stage, we can then either\n\nTreat as a nominal variable and create features using step_dummy()\nTreat as an interval variable using step_ordinalscore()\n\n\nSimilar EDA approaches are used with both nominal and ordinal variable\nOrdinal variables may show non-linear relations b/c they may not be evenly spaced. In these instances, we can use feature engineering approaches that are also used for nominal variables\n\nFor interval and ratio variables:\n\nWe store these variables as numeric\nWe class these variables as numeric (either integer or double - let R decide) during the read and clean stage (They are typically already in this class when read in)\n\nSimilar EDA approaches are used with both interval and ratio variables\nSimilar feature engineering approaches are used with both\n\n\n\n2.3.3 Data Dictionaries\nYou should always make a data dictionary for use with your data files.\n\nIdeally, these are created during the planning phase of your study, prior to the start of data collection\nStill useful if created at the start of data analysis\n\nData dictionaries:\n\nhelp you keep track of your variables and their characteristics (e.g., valid ranges, valid responses)\ncan be used by you to check your data during EDA\ncan be provided to others when you share your data (data are not generally useful to others without a data dictionary)\n\nWe will see a variety of data dictionaries throughout the course. Many are not great as you will learn.\n\n\n\n2.3.4 The Ames Housing Prices Dataset\nWe will use the Ames Housing Prices dataset as a running example this unit (and some future units and application assignments as well)\n\nYou can read more about the original dataset created by Dean DeCock\nThe data set contains data from home sales of individual residential property in Ames, Iowa from 2006 to 2010\nThe original data set includes 2930 observations of sales price and a large number of explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous)\nThis is the original data dictionary\nThe challenge with this dataset is to build the best possible prediction model for the sale price of the homes.\n\n\n\n\n2.3.5 Packages and Conflicts\nFirst, lets set up our environment with functions from important packages. I strongly recommend reviewing our recommendations for best practices regarding managing function conflicts now. It will save you a lot of headaches in the future.\n\nWe set a conflicts policy that will produce errors if we have unanticipated conflicts.\nWe source a library of functions that we use for common tasks in machine learning.\n\nThis includes a function (tidymodels_conflictRules()) that sets conflict rules to allow us to attach tidymodels functions without conflicts with tidyverse functions.\n\nYou can review that function to see what it does (search for that function name at the link)\n\nThen we use that function\n\n\noptions(conflicts.policy = \"depends.ok\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\ntidymodels_conflictRules()\n\n\nNext we load packages for functions that we will use regularly. There are five things to note RE best practices\n\nIf we will use a lot of functions from a package (e.g., tidyverse, tidymodels), we attach the full package\nIf we will use only several functions from a package (but plan to use them repeatedly), we use the include.only parameter to just attach those functions.\nAt times, if we plan to use a single function from a package only 1-2x times, we may not even attach that function at all. Instead, we just call it using its namespace (i.e. packagename::functionname)\nIf a package has a function that conflicts with our primary packages and we don’t plan to use that function, we load the package but exclude the function. If we really needed it, we can call it with its namespace as per option 3 above.\nPay attention to conflicts that were allowed to make sure you understand and accept them. (I left the package messages and warnings in the book this time to see them. I will hide them to avoid cluttering book in later units but you should always review them.)\n\n\n1library(janitor, include.only = \"clean_names\")\n2library(cowplot, include.only = \"plot_grid\")\n3library(kableExtra, exclude = \"group_rows\")\nlibrary(tidyverse) \n4library(tidymodels)\n\n\n1\n\nAs an alternative, we could have skipped loading the package and instead called the function as janitor::clean_names()\n\n2\n\nSame is true for cowplot package\n\n3\n\nWhen loading kableExtra (which we use often), you will always need to exclude groups_rows() to prevent a conflict with dplyr package in the tidyverse\n\n4\n\nLoading tidymodels will produce conflicts unless you source and call my function tidymodels_conflictRules() (see above)\n\n\n\n\n\n\n\n2.3.6 Source and Other Environment Settings\nWe will also source (from github) two other libraries of functions that we use commonly for exploratory data analyses. You should review these function scripts (fun_eda.R; fun_plots.R to see the code for these functions.\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\n\nFinally, we tune our environment a bit more by setting plot themes and print options that we prefer\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf, tibble.print_max = Inf)\n\nAnd we set a relative path to our data. This assumes you are using an RStudio project with the path to the data relative to that project file. I’ve provided more detail elsewhere on best practices for managing files and paths.\n\npath_data &lt;- \"data\"\n\n\n\n\n2.3.7 Read and Glimpse Dataframe\nLets read in the data and glimpse the subset of observations we will work with in Units 2-3 and the first two application assignments.\n\n1data_all &lt;- read_csv(here::here(path_data, \"ames_raw_class.csv\"),\n2                     col_types = cols()) |&gt;\n3  glimpse()\n\n\n1\n\nFirst we read data using a relative path and the here::here() function. This is a replacement for file.path() that works better for both interactive use and rendering in Quarto when using projects.\n\n2\n\nWe use col_types = cols() to let R guess the correct class for each column. This suppresses messages that aren’t important at this point prior to EDA.\n\n3\n\nIt is good practice to always glimpse() data after you read it.\n\n\n\n\nRows: 1,955\nColumns: 81\n$ PID               &lt;chr&gt; \"0526301100\", \"0526350040\", \"0526351010\", \"052710501…\n$ `MS SubClass`     &lt;chr&gt; \"020\", \"020\", \"020\", \"060\", \"120\", \"120\", \"120\", \"06…\n$ `MS Zoning`       &lt;chr&gt; \"RL\", \"RH\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\"…\n$ `Lot Frontage`    &lt;dbl&gt; 141, 80, 81, 74, 41, 43, 39, 60, 75, 63, 85, NA, 47,…\n$ `Lot Area`        &lt;dbl&gt; 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, …\n$ Street            &lt;chr&gt; \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pav…\n$ Alley             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Lot Shape`       &lt;chr&gt; \"IR1\", \"Reg\", \"IR1\", \"IR1\", \"Reg\", \"IR1\", \"IR1\", \"Re…\n$ `Land Contour`    &lt;chr&gt; \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"HLS\", \"Lvl\", \"Lv…\n$ Utilities         &lt;chr&gt; \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"A…\n$ `Lot Config`      &lt;chr&gt; \"Corner\", \"Inside\", \"Corner\", \"Inside\", \"Inside\", \"I…\n$ `Land Slope`      &lt;chr&gt; \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gt…\n$ Neighborhood      &lt;chr&gt; \"NAmes\", \"NAmes\", \"NAmes\", \"Gilbert\", \"StoneBr\", \"St…\n$ `Condition 1`     &lt;chr&gt; \"Norm\", \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"No…\n$ `Condition 2`     &lt;chr&gt; \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Nor…\n$ `Bldg Type`       &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"TwnhsE\", \"TwnhsE\", …\n$ `House Style`     &lt;chr&gt; \"1Story\", \"1Story\", \"1Story\", \"2Story\", \"1Story\", \"1…\n$ `Overall Qual`    &lt;dbl&gt; 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6…\n$ `Overall Cond`    &lt;dbl&gt; 5, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 2, 5, 6, 6…\n$ `Year Built`      &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993…\n$ `Year Remod/Add`  &lt;dbl&gt; 1960, 1961, 1958, 1998, 2001, 1992, 1996, 1999, 1994…\n$ `Roof Style`      &lt;chr&gt; \"Hip\", \"Gable\", \"Hip\", \"Gable\", \"Gable\", \"Gable\", \"G…\n$ `Roof Matl`       &lt;chr&gt; \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg…\n$ `Exterior 1st`    &lt;chr&gt; \"BrkFace\", \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"CemntBd…\n$ `Exterior 2nd`    &lt;chr&gt; \"Plywood\", \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"CmentBd…\n$ `Mas Vnr Type`    &lt;chr&gt; \"Stone\", \"None\", \"BrkFace\", \"None\", \"None\", \"None\", …\n$ `Mas Vnr Area`    &lt;dbl&gt; 112, 0, 108, 0, 0, 0, 0, 0, 0, 0, 0, 0, 603, 0, 350,…\n$ `Exter Qual`      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"TA\", \"TA\"…\n$ `Exter Cond`      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\"…\n$ Foundation        &lt;chr&gt; \"CBlock\", \"CBlock\", \"CBlock\", \"PConc\", \"PConc\", \"PCo…\n$ `Bsmt Qual`       &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\"…\n$ `Bsmt Cond`       &lt;chr&gt; \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\"…\n$ `Bsmt Exposure`   &lt;chr&gt; \"Gd\", \"No\", \"No\", \"No\", \"Mn\", \"No\", \"No\", \"No\", \"No\"…\n$ `BsmtFin Type 1`  &lt;chr&gt; \"BLQ\", \"Rec\", \"ALQ\", \"GLQ\", \"GLQ\", \"ALQ\", \"GLQ\", \"Un…\n$ `BsmtFin SF 1`    &lt;dbl&gt; 639, 468, 923, 791, 616, 263, 1180, 0, 0, 0, 637, 36…\n$ `BsmtFin Type 2`  &lt;chr&gt; \"Unf\", \"LwQ\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Un…\n$ `BsmtFin SF 2`    &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0, 0, 0, 0,…\n$ `Bsmt Unf SF`     &lt;dbl&gt; 441, 270, 406, 137, 722, 1017, 415, 994, 763, 789, 6…\n$ `Total Bsmt SF`   &lt;dbl&gt; 1080, 882, 1329, 928, 1338, 1280, 1595, 994, 763, 78…\n$ Heating           &lt;chr&gt; \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"Gas…\n$ `Heating QC`      &lt;chr&gt; \"Fa\", \"TA\", \"TA\", \"Gd\", \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"Gd\"…\n$ `Central Air`     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n$ Electrical        &lt;chr&gt; \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\"…\n$ `1st Flr SF`      &lt;dbl&gt; 1656, 896, 1329, 928, 1338, 1280, 1616, 1028, 763, 7…\n$ `2nd Flr SF`      &lt;dbl&gt; 0, 0, 0, 701, 0, 0, 0, 776, 892, 676, 0, 0, 1589, 67…\n$ `Low Qual Fin SF` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Gr Liv Area`     &lt;dbl&gt; 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655,…\n$ `Bsmt Full Bath`  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0…\n$ `Bsmt Half Bath`  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Full Bath`       &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 1, 1, 2, 2…\n$ `Half Bath`       &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0…\n$ `Bedroom AbvGr`   &lt;dbl&gt; 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 4, 4, 1, 2, 3, 3…\n$ `Kitchen AbvGr`   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ `Kitchen Qual`    &lt;chr&gt; \"TA\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"TA\"…\n$ `TotRms AbvGrd`   &lt;dbl&gt; 7, 5, 6, 6, 6, 5, 5, 7, 7, 7, 5, 4, 12, 8, 8, 4, 7, …\n$ Functional        &lt;chr&gt; \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Ty…\n$ Fireplaces        &lt;dbl&gt; 2, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1…\n$ `Fireplace Qu`    &lt;chr&gt; \"Gd\", NA, NA, \"TA\", NA, NA, \"TA\", \"TA\", \"TA\", \"Gd\", …\n$ `Garage Type`     &lt;chr&gt; \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"A…\n$ `Garage Yr Blt`   &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993…\n$ `Garage Finish`   &lt;chr&gt; \"Fin\", \"Unf\", \"Unf\", \"Fin\", \"Fin\", \"RFn\", \"RFn\", \"Fi…\n$ `Garage Cars`     &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2…\n$ `Garage Area`     &lt;dbl&gt; 528, 730, 312, 482, 582, 506, 608, 442, 440, 393, 50…\n$ `Garage Qual`     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\"…\n$ `Garage Cond`     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\"…\n$ `Paved Drive`     &lt;chr&gt; \"P\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n$ `Wood Deck SF`    &lt;dbl&gt; 210, 140, 393, 212, 0, 0, 237, 140, 157, 0, 192, 0, …\n$ `Open Porch SF`   &lt;dbl&gt; 62, 0, 36, 34, 0, 82, 152, 60, 84, 75, 0, 54, 36, 12…\n$ `Enclosed Porch`  &lt;dbl&gt; 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `3Ssn Porch`      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Screen Porch`    &lt;dbl&gt; 0, 120, 0, 0, 0, 144, 0, 0, 0, 0, 0, 140, 210, 0, 0,…\n$ `Pool Area`       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Pool QC`         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence             &lt;chr&gt; NA, \"MnPrv\", NA, \"MnPrv\", NA, NA, NA, NA, NA, NA, NA…\n$ `Misc Feature`    &lt;chr&gt; NA, NA, \"Gar2\", NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Misc Val`        &lt;dbl&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Mo Sold`         &lt;dbl&gt; 5, 6, 6, 3, 4, 1, 3, 6, 4, 5, 2, 6, 6, 6, 6, 6, 2, 1…\n$ `Yr Sold`         &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010…\n$ `Sale Type`       &lt;chr&gt; \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\"…\n$ `Sale Condition`  &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"N…\n$ SalePrice         &lt;dbl&gt; 215000, 105000, 172000, 189900, 213500, 191500, 2365…\n\n\n\nDataset Notes:\n\nDataset has N = 1955 rather than 2930.\n\nI have held out remaining observations to serve as a test set for a friendly competition in Unit 3\nI will judge your models’ performance with this test set at that time!\nMore on the importance of held out test sets as we progress through the course\n\nThis full dataset has 81 variables. For the lecture examples in units 2-3 we will only use a subset of the predictors\nYou will use different predictors in the next two application assignments\n\n\nHere we select the variables we will use for lecture\n\ndata_all &lt;- data_all |&gt; \n  select(SalePrice,\n         `Gr Liv Area`, \n         `Lot Area`, \n         `Year Built`, \n         `Overall Qual`, \n         `Garage Cars`,\n         `Garage Qual`,\n         `MS Zoning`,\n         `Lot Config` ,\n1         `Bldg Type`) |&gt;\n  glimpse()\n\n\n1\n\nNotice that the dataset used non-standard variable names that include spaces. We need to use back-ticks around the variable names to allow us reference those variables. We will fix this during the cleaning process and you should never use spaces in variable names when setting up your own data!!!\n\n\n\n\nRows: 1,955\nColumns: 10\n$ SalePrice      &lt;dbl&gt; 215000, 105000, 172000, 189900, 213500, 191500, 236500,…\n$ `Gr Liv Area`  &lt;dbl&gt; 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 14…\n$ `Lot Area`     &lt;dbl&gt; 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 100…\n$ `Year Built`   &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, 1…\n$ `Overall Qual` &lt;dbl&gt; 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, 7…\n$ `Garage Cars`  &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2…\n$ `Garage Qual`  &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"…\n$ `MS Zoning`    &lt;chr&gt; \"RL\", \"RH\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"…\n$ `Lot Config`   &lt;chr&gt; \"Corner\", \"Inside\", \"Corner\", \"Inside\", \"Inside\", \"Insi…\n$ `Bldg Type`    &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"TwnhsE\", \"TwnhsE\", \"Tw…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-data-cleaning",
    "href": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-data-cleaning",
    "title": "2  Exploratory Data Analysis",
    "section": "2.4 Exploratory Data Analysis for Data Cleaning",
    "text": "2.4 Exploratory Data Analysis for Data Cleaning\nEDA could be done using either tidyverse packages and functions or tidymodels (mostly using the recipes package.)\n\nWe prefer to use the richer set of functions available in the tidyverse (and dplyr and purrr packages in particular).\nWe will reserve the use of recipes for feature engineering only when we are building features for models that we will fit in our training sets and evaluation in our validation and test sets.\n\n\n\n2.4.1 Data Leakage Issues\nData leakage refers to a mistake made by the developer of a machine learning model in which they accidentally share information between their training set and held-out validation or test sets\n\nTraining sets are used to fit models with different configurations\nValidation sets are used to select the best model among those with different configurations (not needed if you only have one configuration)\nTest sets are used to evaluate a best model\nWhen splitting data-sets into training, validation and test sets, the goal is to ensure that no data (or information more broadly) are shared between the three sets\n\nNo data or information from test should influence either fitting or selecting models\nTest should only be used once to evaluate a best/final model\nTrain and validation set also must be segregated (although validation sets may be used to evaluate many model configurations)\nInformation necessary for transformations and other feature engineering (e.g., means/sds for centering/scaling, procedures for missing data imputation) must all be based only on training data.\nData leakage is common if you are not careful.\n\n\nIn particular, if we begin to use test data or information about test during model fitting\n\nWe risk overfitting\nThis is essentially the equivalent of p-hacking in traditional analyses\nOur estimate of model performance will be too optimistic, which could have harmful real-world consequences.\n\n\n\n\n2.4.2 Tidy variable names\nUse snake case for variable names\n\nclean_names() from janitor package is useful for this.\nMay need to do further correction of variable names using rename()\nSee more details about tidy names for objects (e.g., variables, dfs, functions) per Tidy Style Guide\n\n\ndata_all &lt;- data_all |&gt; \n  clean_names(\"snake\")\n\ndata_all |&gt; names()\n\n [1] \"sale_price\"   \"gr_liv_area\"  \"lot_area\"     \"year_built\"   \"overall_qual\"\n [6] \"garage_cars\"  \"garage_qual\"  \"ms_zoning\"    \"lot_config\"   \"bldg_type\"   \n\n\n\n\n\n2.4.3 Explore variable classes\nAt this point, we should class all of our variables as either numeric or factor\n\nInterval and ratio variables use numeric classes (dbl or int)\nNominal and ordinal variable use factor class\nUseful for variable selection later (e.g., where(is.numeric), where(is.factor))\n\nSubsequent cleaning steps are clearer if we have this established/confirmed now\n\nWe have a number of nominal or ordinal variables that are classed as character.\nWe have one ordinal variable (overall_qual) that is classed as numeric (because the levels were coded with numbers rather than text)\n\nread_csv() thought was numeric by the levels are coded using numbers\nThe data dictionary indicates that valid values range from 1 - 10.\n\n\ndata_all |&gt; glimpse()\n\nRows: 1,955\nColumns: 10\n$ sale_price   &lt;dbl&gt; 215000, 105000, 172000, 189900, 213500, 191500, 236500, 1…\n$ gr_liv_area  &lt;dbl&gt; 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 1465…\n$ lot_area     &lt;dbl&gt; 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 10000…\n$ year_built   &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, 199…\n$ overall_qual &lt;dbl&gt; 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, 7, …\n$ garage_cars  &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, …\n$ garage_qual  &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA…\n$ ms_zoning    &lt;chr&gt; \"RL\", \"RH\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL…\n$ lot_config   &lt;chr&gt; \"Corner\", \"Inside\", \"Corner\", \"Inside\", \"Inside\", \"Inside…\n$ bldg_type    &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"TwnhsE\", \"TwnhsE\", \"Twnh…\n\n\n\nWe can the recode overall_qual first and set its levels\nWe can recode all the character variables to factor in one step. Most are nominal. We will handle the order for garage_qual later.\n\n1oq_levels &lt;- 1:10\n\ndata_all &lt;-  data_all |&gt; \n  mutate(overall_qual = factor(overall_qual, \n2                               levels = oq_levels)) |&gt;\n3  mutate(across(where(is.character), factor)) |&gt;\n  glimpse()\n\n\n1\n\nIt is always best to explicitly set the levels of an ordinal factor in the order you prefer. It is not necessary here because overall_qual was numeric and therefore sorts in the expected order. However, if it had been numbers stored as characters, it could sort incorrectly (e.g., 1, 10, 2, 3, …). And obviously if the orders levels were names, the order would have to be specified.\n\n2\n\nWe indicate the levels here.\n\n3\n\nWe use a mutate to re-class all character data to factors. I prefer factor() to forcats::fct() because factor orders the levels alphabetically. Be aware that this could change if your code is used in a region of the world where this sorting is different. I still prefer this to the alternative (in fct()) that orders by the order the levels are found in your data.\n\n\n\n\nRows: 1,955\nColumns: 10\n$ sale_price   &lt;dbl&gt; 215000, 105000, 172000, 189900, 213500, 191500, 236500, 1…\n$ gr_liv_area  &lt;dbl&gt; 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 1465…\n$ lot_area     &lt;dbl&gt; 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 10000…\n$ year_built   &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, 199…\n$ overall_qual &lt;fct&gt; 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, 7, …\n$ garage_cars  &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, …\n$ garage_qual  &lt;fct&gt; TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, T…\n$ ms_zoning    &lt;fct&gt; RL, RH, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, R…\n$ lot_config   &lt;fct&gt; Corner, Inside, Corner, Inside, Inside, Inside, Inside, I…\n$ bldg_type    &lt;fct&gt; 1Fam, 1Fam, 1Fam, 1Fam, TwnhsE, TwnhsE, TwnhsE, 1Fam, 1Fa…\n\n\n\n\n\n2.4.4 Skimming the data\nskim() from the skimr package is a wonderful and customizable function for summary statistics\n\nIt is highly customizable so we can write our own versions for our own needs\nWe use different versions for cleaning and modeling EDA\nFor cleaning EDA, we just remove some stats that we don’t want to see at this time\nWe can get many of the summary stats for cleaning in one call\nWe have a custom skim defined in the fun_eda.R function library that we use regularly. Here is the code but you can use the function directly if you sourced fun_eda.R (as we did above)\n\n\nskim_some &lt;- skim_with(numeric = sfl(mean = NULL, sd = NULL, p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL))\n\n\nHere is what we get with our new skim_some() function\n\nWe will refer to this again for each characteristic we want to review for instructional purposes\nWe can already see that we can use skim_some() to confirm that we only have numeric and factor classes\n\n\ndata_all |&gt; \n  skim_some()\n\n\nData summary\n\n\nName\ndata_all\n\n\nNumber of rows\n1955\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\noverall_qual\n0\n1.00\nFALSE\n10\n5: 556, 6: 487, 7: 403, 8: 233\n\n\ngarage_qual\n109\n0.94\nFALSE\n5\nTA: 1745, Fa: 79, Gd: 16, Po: 4\n\n\nms_zoning\n0\n1.00\nFALSE\n7\nRL: 1530, RM: 297, FV: 91, C (: 19\n\n\nlot_config\n0\n1.00\nFALSE\n5\nIns: 1454, Cor: 328, Cul: 114, FR2: 55\n\n\nbldg_type\n0\n1.00\nFALSE\n5\n1Fa: 1631, Twn: 145, Dup: 77, Twn: 64\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\nsale_price\n0\n1\n12789\n745000\n\n\ngr_liv_area\n0\n1\n438\n5642\n\n\nlot_area\n0\n1\n1476\n215245\n\n\nyear_built\n0\n1\n1875\n2010\n\n\ngarage_cars\n1\n1\n0\n4\n\n\n\n\n\n\nCoding sidebar 1:\n\nWrite functions whenever you will repeat code often. You can now reuse skim_some()\nskim_with() is an example of a function factory - a function that is used to create a new function\n\npartial() and compose() are two other function factories we will use at times\nMore details on function factories is available in Advanced R\n\n\n\nCoding sidebar 2:\n\nGather useful functions together in a script that you can reuse.\nAll of the reusable functions in this and later units are available to you in one of my public github repositories.\nYou can load these functions into your workspace directly from github using devtools::source_url(). For example: devtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\")\nYou should start to gather your favorite custom functions together in your own script(s).\n\nYou can save your own scripts in a local file and load them into your workspace using source() or you can make your own github repo so you can begin to share your code with others!\n\n\n\n\n2.4.5 Missing Data - All variables\nskim_some() provides us with missing data counts and complete data proportions for each variable\n\ndata_all |&gt; \n  skim_some() |&gt; \n1  select(skim_variable, n_missing, complete_rate)\n\n\n1\n\nskim_some() returns a dataframe so you can select only the subset of columns to focus its output on what you want. Or just print it all!\n\n\n\n\n# A tibble: 10 × 3\n   skim_variable n_missing complete_rate\n   &lt;chr&gt;             &lt;int&gt;         &lt;dbl&gt;\n 1 overall_qual          0         1    \n 2 garage_qual         109         0.944\n 3 ms_zoning             0         1    \n 4 lot_config            0         1    \n 5 bldg_type             0         1    \n 6 sale_price            0         1    \n 7 gr_liv_area           0         1    \n 8 lot_area              0         1    \n 9 year_built            0         1    \n10 garage_cars           1         0.999\n\n\n\nYou likely should view the full observation for missing values\nWe will show you a few methods to do this in your rendered output\n\nprint() will print only 20 rows and the number of columns that will display for width of page\n\nSet options() if you will do a lot of printing and want full dataframe printed\n\nUse kbl() from kableExtra package for formatted tables (two methods below)\n\nDon’t forget that you can also use View() interactively in R Studio\n\nOption 1 (Simple): Use print() with options()\n\n1options(tibble.width = Inf, tibble.print_max = Inf)\n\ndata_all |&gt; filter(is.na(garage_cars)) |&gt; \n  print()\n\n\n1\n\nThis sets print to print all rows and columns. Note that we set these options at the start of the unit b.c. we like to see our full tibbles. If we want only a subset of the first (or last) rows, we use head() or tail()\n\n\n\n\n# A tibble: 1 × 10\n  sale_price gr_liv_area lot_area year_built overall_qual garage_cars\n       &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;              &lt;dbl&gt;\n1     150909        1828     9060       1923 5                     NA\n  garage_qual ms_zoning lot_config bldg_type\n  &lt;fct&gt;       &lt;fct&gt;     &lt;fct&gt;      &lt;fct&gt;    \n1 &lt;NA&gt;        RM        Inside     1Fam     \n\n\n\nHere are some more advanced options using kbl() for the df with many rows\n\nkable() tables from knitr package and kableExtra extensions (including kbl()) are very useful during EDA and also final publication quality tables\nuse library(kableExtra)\nsee vignettes for kableExtra\n\nOption 2 (more advanced): Use a function for kables that we created. Code is displayed here but the function is available to you if you source fun_eda.R from Github\n\n# Might want to use height = \"100%\" if only printing a few rows\n1print_kbl &lt;- function(data, height = \"500px\") {\n  data |&gt; \n    kbl(align = \"r\") |&gt; \n    kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) |&gt; \n2    scroll_box(height = height, width = \"100%\")\n}\n\n\n1\n\nDefaults to a output box of height = “500px”. Can set to other values if preferred.\n\n2\n\nMight want to use height = \"100%\" if only printing a few rows.\n\n\n\n\n\nLet’s use this function to see its output\n\ndata_all |&gt; filter(is.na(garage_qual)) |&gt; \n  print_kbl()\n\n\n\n\n\n\nsale_price\ngr_liv_area\nlot_area\nyear_built\noverall_qual\ngarage_cars\ngarage_qual\nms_zoning\nlot_config\nbldg_type\n\n\n\n\n115000\n864\n10500\n1971\n4\n0\nNA\nRL\nFR2\n1Fam\n\n\n128950\n1225\n9320\n1959\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n84900\n1728\n13260\n1962\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n116500\n858\n7207\n1958\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n76500\n1306\n5350\n1940\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n76500\n2256\n9045\n1910\n5\n0\nNA\nRM\nInside\n2fmCon\n\n\n159900\n1560\n12900\n1912\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n55000\n1092\n5600\n1930\n4\n0\nNA\nRM\nInside\n2fmCon\n\n\n93369\n1884\n6449\n1907\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n94000\n1020\n6342\n1875\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n136000\n1832\n10773\n1967\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n100000\n1664\n9825\n1965\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n90000\n960\n6410\n1958\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n100000\n1666\n9839\n1931\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n139000\n1824\n9400\n1971\n6\n0\nNA\nRL\nCorner\nDuplex\n\n\n76000\n1092\n1476\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n75500\n630\n1491\n1972\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n88250\n1092\n1900\n1970\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n136000\n1792\n9000\n1974\n5\n0\nNA\nRL\nFR2\nDuplex\n\n\n142000\n1114\n13072\n2004\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n82500\n708\n5330\n1940\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n129000\n1464\n9900\n1910\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n94550\n1701\n7627\n1920\n4\n0\nNA\nRM\nCorner\n2fmCon\n\n\n103000\n1447\n10134\n1910\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n37900\n968\n5925\n1910\n3\n0\nNA\nRM\nInside\n1Fam\n\n\n113000\n1452\n4456\n1920\n4\n0\nNA\nRM\nInside\n2fmCon\n\n\n58500\n816\n3300\n1910\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n34900\n720\n7879\n1920\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n60000\n800\n6120\n1936\n2\n0\nNA\nRM\nInside\n1Fam\n\n\n62500\n2128\n3000\n1922\n5\n0\nNA\nRM\nInside\nDuplex\n\n\n97500\n1864\n5852\n1902\n7\n0\nNA\nRM\nCorner\n2fmCon\n\n\n70000\n892\n5160\n1923\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n179000\n1200\n10800\n1987\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n179000\n1200\n10800\n1987\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n61000\n904\n10020\n1922\n1\n0\nNA\nRL\nInside\n1Fam\n\n\n118000\n698\n9405\n1947\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n99900\n864\n4060\n1922\n5\n0\nNA\nRL\nCorner\n1Fam\n\n\n119900\n1678\n10926\n1959\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n112000\n833\n8780\n1985\n5\n0\nNA\nRL\nCorner\n1Fam\n\n\n141000\n1080\n7500\n2004\n7\n0\nNA\nRL\nInside\n1Fam\n\n\n106250\n1294\n10800\n1900\n4\n0\nNA\nRL\nInside\n2fmCon\n\n\n130000\n1800\n8513\n1961\n5\n0\nNA\nRL\nCorner\nDuplex\n\n\n120000\n1027\n5400\n1920\n7\n0\nNA\nRM\nInside\n1Fam\n\n\n95000\n1080\n5914\n1890\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n65000\n1588\n12205\n1949\n3\n0\nNA\nRM\nInside\n1Fam\n\n\n129400\n1540\n6000\n1905\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n160000\n1984\n8094\n1910\n6\n1\nNA\nRM\nInside\n2fmCon\n\n\n89500\n1406\n7920\n1920\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n79900\n1198\n5586\n1920\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n82375\n1344\n10320\n1915\n3\n0\nNA\nRM\nInside\n2fmCon\n\n\n127500\n1355\n10106\n1940\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n80000\n1006\n9000\n1959\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n260000\n1518\n19550\n1940\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n99600\n864\n9350\n1975\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n107500\n1347\n7000\n1910\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n79000\n1096\n9600\n1924\n6\n0\nNA\nRL\nCorner\n1Fam\n\n\n85000\n796\n8777\n1910\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n145900\n2200\n8777\n1900\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n82000\n1152\n6040\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n82000\n1152\n6012\n1955\n4\n0\nNA\nRL\nCorner\nDuplex\n\n\n118000\n1440\n12108\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n82500\n1152\n6845\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n91900\n784\n6931\n1955\n4\n0\nNA\nRL\nInside\n2fmCon\n\n\n120000\n1053\n12180\n1938\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n96000\n1137\n8050\n1947\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n98000\n864\n5604\n1925\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n67000\n864\n8248\n1914\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n135900\n1716\n5687\n1912\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n119000\n1200\n8155\n1930\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n81000\n630\n1890\n1972\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n146000\n1100\n7500\n2006\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n64000\n670\n3500\n1945\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n103200\n882\n5500\n1956\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n148000\n1534\n10800\n1895\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n110500\n866\n3880\n1945\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n127000\n1355\n6882\n1914\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n200500\n3086\n18030\n1946\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n150000\n1440\n7711\n1977\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n86000\n605\n9098\n1920\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n123600\n990\n8070\n1994\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n98500\n1195\n8741\n1946\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n79000\n774\n4270\n1931\n3\n0\nNA\nRH\nInside\n1Fam\n\n\n200000\n3395\n10896\n1914\n6\n0\nNA\nRH\nInside\n2fmCon\n\n\n150000\n2592\n10890\n1923\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n115000\n1517\n8500\n1919\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n150909\n1828\n9060\n1923\n5\nNA\nNA\nRM\nInside\n1Fam\n\n\n119600\n1991\n8250\n1895\n5\n0\nNA\nC (all)\nInside\n2fmCon\n\n\n147000\n1120\n8402\n2007\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n93900\n1092\n1495\n1970\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n84500\n630\n1936\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n139500\n1142\n7733\n2005\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n132000\n1131\n13072\n2005\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n85500\n869\n5900\n1923\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n135000\n1192\n10800\n1949\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n119000\n1556\n8512\n1960\n5\n0\nNA\nRL\nCorner\nDuplex\n\n\n124000\n1025\n7000\n1962\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n64500\n1020\n4761\n1918\n3\n0\nNA\nC (all)\nCorner\n1Fam\n\n\n100000\n788\n7446\n1941\n4\n0\nNA\nRL\nCorner\n1Fam\n\n\n80500\n912\n6240\n1947\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n72000\n819\n9000\n1919\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n117250\n914\n8050\n2002\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n81000\n1184\n8410\n1910\n5\n0\nNA\nRL\nFR2\n1Fam\n\n\n83000\n1414\n8248\n1922\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n102000\n1522\n6000\n1926\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n72000\n672\n8534\n1925\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n115000\n1396\n9000\n1951\n5\n0\nNA\nC (all)\nInside\n2fmCon\n\n\n78000\n936\n8520\n1916\n3\n0\nNA\nC (all)\nInside\n1Fam\n\n\n92000\n630\n1533\n1970\n5\n0\nNA\nRM\nInside\nTwnhs\n\n\n90500\n1092\n1936\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n\n\n\n\n\n\n\nCoding sidebar:\n\nIn the above example, we created a function (print_kbl()) from scratch (rather than using a function factory)\nSee functions chapter in Wickham, Çetinkaya-Rundel, and Grolemund (2023) for help.\nSee functionals chapter in Wickham (2019).\n\n\nOption 3 (Most advanced): Line by line kable table. You can make this as complicated and customized as you like. We use kable (and kableExtra) for publication quality tables. This is a simple example of options\n\ndata_all |&gt; filter(is.na(garage_qual)) |&gt; \n  kbl(align = \"r\") |&gt; \n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) |&gt; \n  scroll_box(height = \"500px\", width = \"100%\")\n\n\n\n\n\n\nsale_price\ngr_liv_area\nlot_area\nyear_built\noverall_qual\ngarage_cars\ngarage_qual\nms_zoning\nlot_config\nbldg_type\n\n\n\n\n115000\n864\n10500\n1971\n4\n0\nNA\nRL\nFR2\n1Fam\n\n\n128950\n1225\n9320\n1959\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n84900\n1728\n13260\n1962\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n116500\n858\n7207\n1958\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n76500\n1306\n5350\n1940\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n76500\n2256\n9045\n1910\n5\n0\nNA\nRM\nInside\n2fmCon\n\n\n159900\n1560\n12900\n1912\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n55000\n1092\n5600\n1930\n4\n0\nNA\nRM\nInside\n2fmCon\n\n\n93369\n1884\n6449\n1907\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n94000\n1020\n6342\n1875\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n136000\n1832\n10773\n1967\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n100000\n1664\n9825\n1965\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n90000\n960\n6410\n1958\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n100000\n1666\n9839\n1931\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n139000\n1824\n9400\n1971\n6\n0\nNA\nRL\nCorner\nDuplex\n\n\n76000\n1092\n1476\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n75500\n630\n1491\n1972\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n88250\n1092\n1900\n1970\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n136000\n1792\n9000\n1974\n5\n0\nNA\nRL\nFR2\nDuplex\n\n\n142000\n1114\n13072\n2004\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n82500\n708\n5330\n1940\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n129000\n1464\n9900\n1910\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n94550\n1701\n7627\n1920\n4\n0\nNA\nRM\nCorner\n2fmCon\n\n\n103000\n1447\n10134\n1910\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n37900\n968\n5925\n1910\n3\n0\nNA\nRM\nInside\n1Fam\n\n\n113000\n1452\n4456\n1920\n4\n0\nNA\nRM\nInside\n2fmCon\n\n\n58500\n816\n3300\n1910\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n34900\n720\n7879\n1920\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n60000\n800\n6120\n1936\n2\n0\nNA\nRM\nInside\n1Fam\n\n\n62500\n2128\n3000\n1922\n5\n0\nNA\nRM\nInside\nDuplex\n\n\n97500\n1864\n5852\n1902\n7\n0\nNA\nRM\nCorner\n2fmCon\n\n\n70000\n892\n5160\n1923\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n179000\n1200\n10800\n1987\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n179000\n1200\n10800\n1987\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n61000\n904\n10020\n1922\n1\n0\nNA\nRL\nInside\n1Fam\n\n\n118000\n698\n9405\n1947\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n99900\n864\n4060\n1922\n5\n0\nNA\nRL\nCorner\n1Fam\n\n\n119900\n1678\n10926\n1959\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n112000\n833\n8780\n1985\n5\n0\nNA\nRL\nCorner\n1Fam\n\n\n141000\n1080\n7500\n2004\n7\n0\nNA\nRL\nInside\n1Fam\n\n\n106250\n1294\n10800\n1900\n4\n0\nNA\nRL\nInside\n2fmCon\n\n\n130000\n1800\n8513\n1961\n5\n0\nNA\nRL\nCorner\nDuplex\n\n\n120000\n1027\n5400\n1920\n7\n0\nNA\nRM\nInside\n1Fam\n\n\n95000\n1080\n5914\n1890\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n65000\n1588\n12205\n1949\n3\n0\nNA\nRM\nInside\n1Fam\n\n\n129400\n1540\n6000\n1905\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n160000\n1984\n8094\n1910\n6\n1\nNA\nRM\nInside\n2fmCon\n\n\n89500\n1406\n7920\n1920\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n79900\n1198\n5586\n1920\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n82375\n1344\n10320\n1915\n3\n0\nNA\nRM\nInside\n2fmCon\n\n\n127500\n1355\n10106\n1940\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n80000\n1006\n9000\n1959\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n260000\n1518\n19550\n1940\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n99600\n864\n9350\n1975\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n107500\n1347\n7000\n1910\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n79000\n1096\n9600\n1924\n6\n0\nNA\nRL\nCorner\n1Fam\n\n\n85000\n796\n8777\n1910\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n145900\n2200\n8777\n1900\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n82000\n1152\n6040\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n82000\n1152\n6012\n1955\n4\n0\nNA\nRL\nCorner\nDuplex\n\n\n118000\n1440\n12108\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n82500\n1152\n6845\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n91900\n784\n6931\n1955\n4\n0\nNA\nRL\nInside\n2fmCon\n\n\n120000\n1053\n12180\n1938\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n96000\n1137\n8050\n1947\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n98000\n864\n5604\n1925\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n67000\n864\n8248\n1914\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n135900\n1716\n5687\n1912\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n119000\n1200\n8155\n1930\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n81000\n630\n1890\n1972\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n146000\n1100\n7500\n2006\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n64000\n670\n3500\n1945\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n103200\n882\n5500\n1956\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n148000\n1534\n10800\n1895\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n110500\n866\n3880\n1945\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n127000\n1355\n6882\n1914\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n200500\n3086\n18030\n1946\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n150000\n1440\n7711\n1977\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n86000\n605\n9098\n1920\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n123600\n990\n8070\n1994\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n98500\n1195\n8741\n1946\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n79000\n774\n4270\n1931\n3\n0\nNA\nRH\nInside\n1Fam\n\n\n200000\n3395\n10896\n1914\n6\n0\nNA\nRH\nInside\n2fmCon\n\n\n150000\n2592\n10890\n1923\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n115000\n1517\n8500\n1919\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n150909\n1828\n9060\n1923\n5\nNA\nNA\nRM\nInside\n1Fam\n\n\n119600\n1991\n8250\n1895\n5\n0\nNA\nC (all)\nInside\n2fmCon\n\n\n147000\n1120\n8402\n2007\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n93900\n1092\n1495\n1970\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n84500\n630\n1936\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n139500\n1142\n7733\n2005\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n132000\n1131\n13072\n2005\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n85500\n869\n5900\n1923\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n135000\n1192\n10800\n1949\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n119000\n1556\n8512\n1960\n5\n0\nNA\nRL\nCorner\nDuplex\n\n\n124000\n1025\n7000\n1962\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n64500\n1020\n4761\n1918\n3\n0\nNA\nC (all)\nCorner\n1Fam\n\n\n100000\n788\n7446\n1941\n4\n0\nNA\nRL\nCorner\n1Fam\n\n\n80500\n912\n6240\n1947\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n72000\n819\n9000\n1919\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n117250\n914\n8050\n2002\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n81000\n1184\n8410\n1910\n5\n0\nNA\nRL\nFR2\n1Fam\n\n\n83000\n1414\n8248\n1922\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n102000\n1522\n6000\n1926\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n72000\n672\n8534\n1925\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n115000\n1396\n9000\n1951\n5\n0\nNA\nC (all)\nInside\n2fmCon\n\n\n78000\n936\n8520\n1916\n3\n0\nNA\nC (all)\nInside\n1Fam\n\n\n92000\n630\n1533\n1970\n5\n0\nNA\nRM\nInside\nTwnhs\n\n\n90500\n1092\n1936\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n\n\n\n\n\n\n\nIn this instance, if we consult our data dictionary, we see that NA for garage_qual should be coded as “no garage”. We will correct this in our data set.\nThis is a pretty poor choice on the part of the researchers who created the dataset because it becomes impossible to distinguish between NA that means no garage vs. true NA for the variable. In fact, if you later do really careful EDA on the full data set with all variables, you will see this problem likely exists in this dataset\nAnyway, let’s correct all the NA for garage_qual to “no_garage” using mutate()\n\ndata_all &lt;- data_all |&gt; \n1  mutate(garage_qual = fct_expand(garage_qual, \"no_garage\"),\n2         garage_qual = replace_na(garage_qual, \"no_garage\"))\n\n\n1\n\nFirst add a new level to the factor\n\n2\n\nThen recode NA to that new level\n\n\n\n\nWe will leave the NA for garage_cars as NA because its not clear if that is truly missing or not, based on further EDA not shown here.\n\nWe have one other issue with garage_qual. It is an ordinal variable but we never reviewed the order of its levels. The data dictionary indicates the levels are ordered (best to worst) as:\n\nEx (excellent)\nGd (good)\nTA (typical/average)\nFa (fair)\nPo (poor)\n\nAnd we might assume that no garage is even worse than a poor garage. Lets see what they are.\n\ndata_all$garage_qual |&gt; levels()\n\n[1] \"Ex\"        \"Fa\"        \"Gd\"        \"Po\"        \"TA\"        \"no_garage\"\n\n\nTo fix this, we can use forcats::fct_relevel().\n\n1gq_levels &lt;- c(\"no_garage\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\")\ndata_all &lt;- data_all |&gt; \n2  mutate(garage_qual = fct_relevel(garage_qual, gq_levels))\n\n3data_all$garage_qual |&gt; levels()\n\n\n1\n\nMake a vector that indicates the valid levels in order\n\n2\n\nPass that into fct_relevel(). See ?fct_relevel for other ways to adjust the levels of a factor.\n\n3\n\nConfirm that the levels are now correct\n\n\n\n\n[1] \"no_garage\" \"Po\"        \"Fa\"        \"TA\"        \"Gd\"        \"Ex\"       \n\n\n\n\n\n2.4.6 Explore Min/Max Response for Numeric Variables\nWe should explore mins and maxes for all numeric variables to detect out of valid range numeric responses\n\nCould also do this for ordinal variables that are coded with numbers\n\ne.g., overall_qual (1-10) vs. garage_qual (no_garage, Po, Fa, TA, Gd, Ex)\n\nThis is only a temporary mutation of overall_qual for this check. We don’t assign to new df to an object\nWe can use skim_some() again\n\np0 = min\np100 = max\n\n\n\ndata_all |&gt;\n  mutate(overall_qual = as.numeric(overall_qual)) |&gt; \n  skim_some() |&gt; \n1  filter(skim_type == \"numeric\") |&gt;\n2  select(skim_variable, numeric.p0, numeric.p100)\n\n\n1\n\nSelect only numeric variables since min/max only apply to them\n\n2\n\nSelect relevant stats (min/max)\n\n\n\n\n# A tibble: 6 × 3\n  skim_variable numeric.p0 numeric.p100\n  &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n1 sale_price         12789       745000\n2 gr_liv_area          438         5642\n3 lot_area            1476       215245\n4 year_built          1875         2010\n5 overall_qual           1           10\n6 garage_cars            0            4\n\n\n\n\n\n2.4.7 Explore All Responses for Categorical Variables\nWe should explore all unique responses for nominal variables\nMight also do this for ordinal variables that are coded with labels vs. numbers.\n\ndata_all |&gt; \n  select(where(is.factor)) |&gt;\n  walk(\\(column) print(levels(column)))\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n[1] \"no_garage\" \"Po\"        \"Fa\"        \"TA\"        \"Gd\"        \"Ex\"       \n[1] \"A (agr)\" \"C (all)\" \"FV\"      \"I (all)\" \"RH\"      \"RL\"      \"RM\"     \n[1] \"Corner\"  \"CulDSac\" \"FR2\"     \"FR3\"     \"Inside\" \n[1] \"1Fam\"   \"2fmCon\" \"Duplex\" \"Twnhs\"  \"TwnhsE\"\n\n\nCoding sidebar:\n\nOn the previous page, we demonstrated the use of an anonymous function (\\(column) print(levels(column))), which is a function we use once that we don’t bother to assign a name (since we won’t reuse it). We often use anonymous functions when using the functions from the purrr package (e.g., map(), walk())\nWe use walk() from the purrr package to apply our anonymous function to all columns of the data frame at once\nJust copy this code for now\nWe will see simpler uses later that will help you understand iteration with purrr functions\nSee the chapter on iteration in R for Data Science (2e) for more info on map() and walk()\n\n\n\n\n2.4.8 Tidy Responses for Categorical Variables\nFeature engineering with nominal and ordinal variables typically involves\n\nConverting to factors (already did this!)\nOften creating dummy features from these factors\n\nThis feature engineering will use response labels for naming new features\n\nTherefore, it is a good idea to have the responses snake-cased and cleaned up a bit so that these new feature names are clean/clear.\n\nHere is an easy way to convert responses for character variables to snake case using a function (tidy_responses()) we share in fun_eda.R (reproduced here).\n\nThis uses regular expressions (regex), which will will learn about in a later unit on text processing.\nYou could expand this cleaning function if you encounter other issues that need to be cleaned in the factor levels.\n\n\ntidy_responses &lt;- function(column){\n  # replace all non-alphanumeric with _\n  column &lt;- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\W\", \"_\"))\n  # replace whitespace with _\n  column &lt;- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\s+\", \"_\"))\n  # replace multiple _ with single _\n  column &lt;- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\_+\", \"_\"))\n  #remove _ at end of string\n  column &lt;- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\_$\", \"\"))\n  # remove _ at start of string\n  column &lt;- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\^_\", \"\"))\n  # convert to lowercase\n  column &lt;- fct_relabel(column, tolower)\n  factor(column)\n}\n\n\nLet’s use the function\n\ndata_all &lt;- data_all |&gt; \n1  mutate(across(where(is.factor), tidy_responses))\n\n\n1\n\nWe use the tidy selection helper function to limit our mutate to only factors. See more details on the tidy selection helpers like all_of() and where()\n\n\n\n\n\nAlas, these response labels were pretty poorly chosen so some didn’t convert well. And some are really hard to understand too.\n\nAvoid this problem and choose good response labels from the start for your own data\nHere, we show you what we got from using tidy_responses()\n\n\ndata_all |&gt; \n  select(where(is.factor)) |&gt;\n  walk(\\(column) print(levels(column)))\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n[1] \"no_garage\" \"po\"        \"fa\"        \"ta\"        \"gd\"        \"ex\"       \n[1] \"a_agr\" \"c_all\" \"fv\"    \"i_all\" \"rh\"    \"rl\"    \"rm\"   \n[1] \"corner\"  \"culdsac\" \"fr2\"     \"fr3\"     \"inside\" \n[1] \"1fam\"   \"2fmcon\" \"duplex\" \"twnhs\"  \"twnhse\"\n\n\n\nLets clean them up a bit more manually\n\ndata_all &lt;- data_all |&gt; \n  mutate(ms_zoning = fct_recode(ms_zoning,\n                                res_low = \"rl\",\n                                res_med = \"rm\",\n                                res_high = \"rh\",\n                                float = \"fv\",\n                                agri = \"a_agr\",\n                                indus = \"i_all\",\n                                commer = \"c_all\"),\n1         bldg_type = fct_recode(bldg_type,\n                                one_fam = \"1fam\",\n                                two_fam = \"2fmcon\",\n                                town_end = \"twnhse\",\n                                town_inside = \"twnhs\"))\n\n\n1\n\nNote that I did not need to list all levels in the recode. Only the levels I wanted to change.\n\n\n\n\nThe full dataset is now clean!\n\n\n\n2.4.9 Train/Validate/Test Splits\nThe final task we typically do as part of the data preparation process is to split the full dataset into training, validation and test sets.\n\nTest sets are “typically” between 20-30% of your full dataset\n\nThere are costs and benefits to larger test sets\nWe will learn about these costs/benefits in the unit on resampling\nI have already held out the test set\n\nThere are many approaches to validation sets\n\nFor now (until unit 5) we will use a single validation set approach\nWe will use 25% of the remaining data (after holding out the test set) as a validation set for this example\n\nIt is typical to split data on the outcome within strata\n\nFor a categorical outcome, this makes the proportions of the response categories more balanced across the train, validation, and test sets\nFor a numeric outcome, we first break up the distribution into temporary bins (see breaks = 4 below) and then we split within these bins\n\nIMPORTANT: Set a seed so that you can reproduce these splits if you later do more cleaning\n\n\nset.seed(20110522)\nsplits &lt;- data_all |&gt; \n  initial_split(prop = 3/4, strata = \"sale_price\", breaks = 4)\n\n\nWe then extract the training set from the splits and save it\n\nTraining sets are used for “analysis”- hence the name of the function\n\n\nsplits |&gt; \n1  analysis() |&gt;\n  glimpse() |&gt; \n  write_csv(here::here(path_data, \"ames_clean_class_trn.csv\"))\n\n\n1\n\nanalysis() pulls out the training set from our splits of data_all\n\n\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nWe will not need the validation set for modeling EDA\n\nIt should NOT be used for anything other than evaluating models to select the best model configuration\nWe do NOT do Modeling EDA or Model Fitting with the validation set\nSave it in this clean form for easy use when you need it\nWe use the validation set to “assess” models that we have fit in training sets - hence the name of the function\n\n\nsplits |&gt; \n1  assessment() |&gt;\n  glimpse() |&gt; \n  write_csv(here::here(path_data, \"ames_clean_class_val.csv\"))\n\n\n1\n\nassessment() pulls out the validation set from our splits of data_all\n\n\n\n\nRows: 490\nColumns: 10\n$ sale_price   &lt;dbl&gt; 215000, 189900, 189000, 171500, 212000, 164000, 394432, 1…\n$ gr_liv_area  &lt;dbl&gt; 1656, 1629, 1804, 1341, 1502, 1752, 1856, 1004, 1092, 106…\n$ lot_area     &lt;dbl&gt; 31770, 13830, 7500, 10176, 6820, 12134, 11394, 11241, 168…\n$ year_built   &lt;dbl&gt; 1960, 1997, 1999, 1990, 1985, 1988, 2010, 1970, 1971, 197…\n$ overall_qual &lt;fct&gt; 6, 5, 7, 7, 8, 8, 9, 6, 5, 6, 7, 9, 8, 8, 7, 8, 6, 5, 5, …\n$ garage_cars  &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 3, 2, 3, 1, 1, 2, …\n$ garage_qual  &lt;fct&gt; ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, t…\n$ ms_zoning    &lt;fct&gt; res_low, res_low, res_low, res_low, res_low, res_low, res…\n$ lot_config   &lt;fct&gt; corner, inside, inside, inside, corner, inside, corner, c…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, one_fam, town_end, one_fam, on…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-modeling",
    "href": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-modeling",
    "title": "2  Exploratory Data Analysis",
    "section": "2.5 Exploratory Data Analysis for Modeling",
    "text": "2.5 Exploratory Data Analysis for Modeling\nNow let’s begin our Modeling EDA\nWe prefer to write separate scripts for Cleaning vs. Modeling EDA (but not displayed here)\n\nThis keeps these two processes separate in our minds\nCleaning EDA is done with full dataset but Modeling EDA is only done with a training set - NEVER use validation or test set\nYou will use two separate scripts for the application assignment for this unit\n\n\nLets re-load (and glimpse) our training set to pretend we are at the start of a new script.\n\n data_trn &lt;- read_csv(here::here(path_data, \"ames_clean_class_trn.csv\")) |&gt; \n  glimpse()\n\nRows: 1465 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): garage_qual, ms_zoning, lot_config, bldg_type\ndbl (6): sale_price, gr_liv_area, lot_area, year_built, overall_qual, garage...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;dbl&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;chr&gt; \"ta\", \"ta\", \"no_garage\", \"ta\", \"ta\", \"ta\", \"ta\", \"no_gara…\n$ ms_zoning    &lt;chr&gt; \"res_high\", \"res_low\", \"res_low\", \"res_low\", \"res_low\", \"…\n$ lot_config   &lt;chr&gt; \"inside\", \"corner\", \"fr2\", \"fr2\", \"inside\", \"inside\", \"in…\n$ bldg_type    &lt;chr&gt; \"one_fam\", \"one_fam\", \"one_fam\", \"town_inside\", \"town_end…\n\n\n\nWe have some work to do (again)\n\nNotice that overall_qual is back to being classed as numeric (dbl).\n\nNotice that your factors are back to character\n\nThis is because csv files don’t save anything other than the values (labels for factors). They are the cleaned labels though!\n\nYou should class all variables using the same approach as before (often just a copy/paste).\n\n\n data_trn &lt;- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n1           col_types = cols()) |&gt;\n2  mutate(across(where(is.character), factor)) |&gt;\n3  mutate(overall_qual = factor(overall_qual, levels = 1:10),\n         garage_qual = fct_relevel(garage_qual, c(\"no_garage\", \"po\", \"fa\", \n4                                                  \"ta\", \"gd\", \"ex\"))) |&gt;\n  glimpse()\n\n\n1\n\nuse col_types = cols() to suppress messages about default class assignments\n\n2\n\nuse mutate() with across() to change all character variables to factors\n\n3\n\nuse mutate() with factor() to change numeric variable to factor.\n\n4\n\nuse mutate() with fct_relevel() to explicitly set levels of an ordinal factor. Also notice the warning about the unknown level. Always explore warnings! In this instance, its fine. There were only two observations with ex and neither ended up in the training split. Still best to include this level to note it exists!\n\n\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nCoding sidebar: We will likely re-class the Ames dataset many times (for training, validation, test). We could copy/paste these mutates each time but whenever you do something more than twice, I recommend writing a function. We might write this one to re-class the ames variables\n\nclass_ames &lt;- function(df){\n  \n  df |&gt;\n    mutate(across(where(is.character), factor)) |&gt; \n    mutate(overall_qual = factor(overall_qual, levels = 1:10), \n           garage_qual = fct_relevel(garage_qual, c(\"no_garage\", \"po\", \"fa\", \n                                                    \"ta\", \"gd\", \"ex\")))\n}\n\n\nNow we can use this function every time we read in one of the Ames datasets\n\ndata_trn &lt;- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           col_types = cols()) |&gt; \n1  class_ames() |&gt;\n  glimpse()\n\n\n1\n\nUsing our new function!\n\n\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nThere are 3 basic types of Modeling EDA you should always do\n\nExplore missingness for predictors\nExplore univariate distributions for outcome and predictors\nExplore bivariate relationships between predictors and outcome\n\nAs a result of this exploration, we will:\n\nIdentify promising predictors\nDetermine appropriate feature engineering for those predictors (e.g., transformations)\nIdentify outliers and consider how to handle when model building\nConsider how to handle imputation for missing data (if any)\n\n\n\n2.5.1 Overall Summary of Feature Matrix\nBefore we dig into individual variables and their distributions and relationships with the outcome, it’s nice to start with a big picture of the dataset\n\nWe use another customized version of skim() from the skimr package to provide this\nJust needed to augment it with skewness and kurtosis statistics for numeric variables\nand remove histogram b/c we don’t find that small histogram useful\nincluded in fun_eda.R on github\n\n\nskew_na &lt;- partial(e1071::skewness, na.rm = TRUE)\nkurt_na &lt;- partial(e1071::kurtosis, na.rm = TRUE)\n\nskim_all &lt;- skimr::skim_with(numeric = skimr::sfl(skew = skew_na, \n                                                  kurtosis = kurt_na, \n                                                  hist = NULL))\n\n\nCareful review of this output provides a great orientation to our data\n\ndata_trn |&gt; \n  skim_all()\n\n\nData summary\n\n\nName\ndata_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\ntop_counts\n\n\n\n\noverall_qual\n0\n1\n10\n5: 424, 6: 350, 7: 304, 8: 176\n\n\ngarage_qual\n0\n1\n5\nta: 1312, no_: 81, fa: 57, gd: 13\n\n\nms_zoning\n0\n1\n7\nres: 1157, res: 217, flo: 66, com: 13\n\n\nlot_config\n0\n1\n5\nins: 1095, cor: 248, cul: 81, fr2: 39\n\n\nbldg_type\n0\n1\n5\none: 1216, tow: 108, dup: 63, tow: 46\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789\n129500\n160000\n213500\n745000\n1.64\n4.60\n\n\ngr_liv_area\n0\n1\n1506.84\n511.44\n438\n1128\n1450\n1759\n5642\n1.43\n5.19\n\n\nlot_area\n0\n1\n10144.16\n8177.55\n1476\n7500\n9375\n11362\n164660\n11.20\n182.91\n\n\nyear_built\n0\n1\n1971.35\n29.65\n1880\n1953\n1972\n2000\n2010\n-0.54\n-0.62\n\n\ngarage_cars\n1\n1\n1.78\n0.76\n0\n1\n2\n2\n4\n-0.26\n0.10\n\n\n\n\n\n\n\n\n2.5.2 Univariate Distributions\nExploration of univariate distributions are useful to\n\nUnderstand variation and distributional shape\nMay suggest need to consider transformations as part of feature engineering\nCan identify univariate outliers (valid but disconnected from distribution so not detected in cleaning)\n\nWe generally select different visualizations and summary statistics for categorical vs. numeric variables\n\n\n\n2.5.3 Barplots for Categorical Variables (Univariate)\nThe primary visualization for categorical variables is the bar plot\n\nWe use it for both nominal and ordinal variables\nDefine and customize it within a function for repeated use.\n\nWe share this and all the remaining plots used in this unit in fun_plot.R. Source it to use them without having to re-code each time\n\n\nplot_bar &lt;- function(df, x){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\n\nCoding sidebar: When defining functions, generally put data as first argument so you can pipe in data using tidy pipelines\nThere are pros and cons to writing functions that accept variable names that are quoted vs. unquoted\n\nIt depends a bit on how you will use them.\n.data[[argument]] is used in functions with quoted arguments\nembracing {{}} is used for unquoted arguments\nFor these plot functions, I use quoted variable names and then pipe those into map() to make multiple plots (see below)\nsee ?vignette(\"programming\") or info on tidy evaluation in Wickham, Çetinkaya-Rundel, and Grolemund (2023) for more details\n\n\nBar plots reveal low frequency responses for nominal and ordinal variables\n\nSee bldg_type\n\n\ndata_trn |&gt; plot_bar(\"bldg_type\")\n\n\n\n\n\n\n\n\n\nBar plots can display distributional shape for ordinal variables. May suggest the need for transformations if we later treat the ordinal variable as numeric\n\nSee overall_qual. Though it is not very skewed.\n\n\ndata_trn |&gt; plot_bar(\"overall_qual\")\n\n\n\n\n\n\n\n\n\nCoding sidebar:\nWe can make all of our plots iteratively using map() from the `purrr package.\n\ndata_trn |&gt; \n1  select(where(is.factor)) |&gt;\n2  names() |&gt;\n3  map(\\(name) plot_bar(df = data_trn, x = name)) |&gt;\n4  plot_grid(plotlist = _, ncol = 2)\n\n\n1\n\nSelect only the factor columns\n\n2\n\nGet their names as strings (that is why we use quoted variables in these plot functions\n\n3\n\nUse map() to iterative plot_bar() over every column. (see iteration in Wickham, Çetinkaya-Rundel, and Grolemund (2023))\n\n4\n\nUse plot_grid() from cowplot package to display the list of plots in a grid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 Tables for Categorical Variables (Univariate)\nWe tend to prefer visualizations vs. summary statistics for EDA. However, tables can be useful.\nHere is a function that was described in Wickham, Çetinkaya-Rundel, and Grolemund (2023) that we like because\n\nIt includes counts and proportions\nIt includes NA as a category\n\nWe have included it in fun_eda.R for your use.\n\ntab &lt;- function(df, var, sort = FALSE) {\n  df |&gt;  dplyr::count({{ var }}, sort = sort) |&gt; \n    dplyr::mutate(prop = n / sum(n))\n} \n\n\nTables can be used to identify responses that have very low frequency and to think about the need to handle missing values\n\nSee ms_zoning\nMay want to collapse low frequency (or low percentage) categories to reduce the number of features needed to represent the predictor\n\n\ndata_trn |&gt; tab(ms_zoning)\n\n# A tibble: 7 × 3\n  ms_zoning     n     prop\n  &lt;fct&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 agri          2 0.00137 \n2 commer       13 0.00887 \n3 float        66 0.0451  \n4 indus         1 0.000683\n5 res_high      9 0.00614 \n6 res_low    1157 0.790   \n7 res_med     217 0.148   \n\n\n\n\nWe could also view the table sorted if we prefer\n\n\ndata_trn |&gt; tab(ms_zoning, sort = TRUE)\n\n# A tibble: 7 × 3\n  ms_zoning     n     prop\n  &lt;fct&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 res_low    1157 0.790   \n2 res_med     217 0.148   \n3 float        66 0.0451  \n4 commer       13 0.00887 \n5 res_high      9 0.00614 \n6 agri          2 0.00137 \n7 indus         1 0.000683\n\n\n\nbut could see all this detail with plot as well\n\ndata_trn |&gt; plot_bar(\"ms_zoning\")\n\n\n\n\n\n\n\n\n\n\n\n2.5.5 Histograms for Numeric Variables (Univariate)\nHistograms are a useful/common visualization for numeric variables\nLet’s define a histogram function (included in fun_plots.r)\n\nBin size should be explored a bit to find best representation\nSomewhat dependent on n (my default here is based on this training set)\nThis is one of the limitations of histograms\n\n\nplot_hist &lt;- function(df, x, bins = 100){\n  df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_histogram(bins = bins) +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n\n\nLet’s look at sale_price\n\nIt is positively skewed\nMay suggest units (dollars) are not interval in nature (makes sense)\nCould cause problems for some algorithms (e.g., lm) when features are normal\n\n\ndata_trn |&gt; plot_hist(\"sale_price\")\n\n\n\n\n\n\n\n\n\n\n\n2.5.6 Smoothed Frequency Polygons for Numeric Variables (Univariate)\nFrequency polygons are also commonly used\n\nDefine a frequency polygon function and use it (included in fun_plots.r)\n\n\nplot_freqpoly &lt;- function(df, x, bins = 50){\n  df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_freqpoly(bins = bins) +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n\n\n\nBins may matter again\n\n\ndata_trn |&gt; plot_freqpoly(\"sale_price\")\n\n\n\n\n\n\n\n\n\n\n\n2.5.7 Simple Boxplots for Numeric Variables (Univariate)\nBoxplots display\n\nMedian as line\n25%ile and 75%ile as hinges\nHighest and lowest points within 1.5 * IQR (interquartile-range: difference between scores at 25% and 75%iles)\nOutliers outside of 1.5 * IQR\n\nDefine a boxplot function and use it (included in fun_plots.r)\n\nplot_boxplot &lt;- function(df, x){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1))\n}\n\n\nHere is the plot for sale_price\n\ndata_trn |&gt; plot_boxplot(\"sale_price\")\n\n\n\n\n\n\n\n\n\n\n\n2.5.8 Combined Boxplot and Violin Plots for Numeric Variables (Univariate)\nThe combination of a boxplot and violin plot is particularly useful\n\nThis is our favorite\nGet all the benefits of the boxplot\nCan clearly see shape of distribution given the violin plot overlay\nCan also clearly see the tails\n\nDefine a combined plot (included in fun_plots.r)\n\nplot_box_violin &lt;- function(df, x){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_violin(aes(y = 0), fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1))\n}\n\n\nHere is the plot for sale_price\n\nIn this instance, the skew is NOT due to only a few outliers\n\n\ndata_trn |&gt; plot_box_violin(\"sale_price\")\n\n\n\n\n\n\n\n\n\nCoding sidebar:\n\nYou can make figures for all numeric variables at once using select() and map() as before\n\n\ndata_trn |&gt; \n1  select(where(is.numeric)) |&gt;\n  names() |&gt; \n  map(\\(name) plot_box_violin(df = data_trn, x = name)) |&gt; \n  plot_grid(plotlist = _, ncol = 2)\n\n\n1\n\nNow select numeric rather than factor but otherwise same as previous example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.9 Summary Statistics for Numeric Variables (Univariate)\nskim_all() provided all the summary statistics you likely needed for numeric variables\n\nmean & median (p50)\nsd & IQR (see difference between p25 and p75)\nskew & kurtosis\n\nYou can get skim of only numeric variables if you like\n\ndata_trn |&gt; \n  skim_all() |&gt; \n  filter(skim_type == \"numeric\")\n\n\nData summary\n\n\nName\ndata_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789\n129500\n160000\n213500\n745000\n1.64\n4.60\n\n\ngr_liv_area\n0\n1\n1506.84\n511.44\n438\n1128\n1450\n1759\n5642\n1.43\n5.19\n\n\nlot_area\n0\n1\n10144.16\n8177.55\n1476\n7500\n9375\n11362\n164660\n11.20\n182.91\n\n\nyear_built\n0\n1\n1971.35\n29.65\n1880\n1953\n1972\n2000\n2010\n-0.54\n-0.62\n\n\ngarage_cars\n1\n1\n1.78\n0.76\n0\n1\n2\n2\n4\n-0.26\n0.10\n\n\n\n\n\n\n\n\n2.5.10 Bivariate Relationships with Outcome\nBivariate relationships with the outcome are useful to detect\n\nWhich predictors display some relationship with the outcome\nWhat feature engineering (transformations) might maximize that relationship\nAre there any bivariate (model) outliers\n\nAgain, we prefer visualizations but summary statistics are also available\n\n\n\n2.5.11 Scatterplots for Numeric Variables (Bivariate)\nScatterplots are the preferred visualization when both variables are numeric\nDefine a scatterplot function (included in fun_plots.r)\n\nadd a simple line\nadd a LOWESS line (Locally Weighted Scatterplot Smoothing)\nThese lines are useful for considering shape of relationship\n\n\nplot_scatter &lt;- function(df, x, y){\n  df |&gt;\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"green\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n\n\nLet’s consider relationship between gr_liv_area and sale_price\n\nCare most about influential points (both model outlier and leverage)\nCan be typically spotted in bivariate plots (but could do more sophisticated assessments)\nWe might:\n\nretain as is\ndrop\nbring to fence\n\n\nIf bivariate outliers are detected, you should return to cleaning mode to verify that they aren’t result of scoring/coding errors. If they are:\n\nFix in full dataset\nUse same train/test split after fixing\n\n\ndata_trn |&gt; plot_scatter(\"gr_liv_area\", \"sale_price\")\n\n\n\n\n\n\n\n\n\nHere is another example where the relationship might be non-linear\n\ndata_trn |&gt; plot_scatter(\"year_built\", \"sale_price\")\n\n\n\n\n\n\n\n\n\nA transformation of sale_price might help the relationship with \\(year\\_built\\) but might hurt gr_liv_area\nMaybe need to transform both sale_price and gr_liv_area as both were skewed\nThis might require some more EDA but here is a start\n\nQuick and temporary Log (base e) of sale_price\nThis doesn’t seem promising by itself\n\n\ndata_trn |&gt; \n  mutate(sale_price = log(sale_price)) |&gt; \n  plot_scatter(\"gr_liv_area\", \"sale_price\")\n\n\n\n\n\n\n\ndata_trn |&gt; \n  mutate(sale_price = log(sale_price)) |&gt;\n  plot_scatter(\"year_built\", \"sale_price\")\n\n\n\n\n\n\n\n\n\nCan make scatterplots for ordinal variables as well\n\nBut other (perhaps better) options also exist for this combination of variable classes.\nUse as.numeric() to allow for lm and LOWESS lines on otherwise categorical variable\n\n\ndata_trn |&gt; \n  mutate(overall_qual = as.numeric(overall_qual)) |&gt; \n  plot_scatter(\"overall_qual\", \"sale_price\")\n\n\n\n\n\n\n\n\n\nCoding sidebar: Use jitter() with x to help with overplotting\n\ndata_trn |&gt; \n  mutate(overall_qual = jitter(as.numeric(overall_qual))) |&gt; \n  plot_scatter(\"overall_qual\", \"sale_price\")\n\n\n\n\n\n\n\n\n\n\n\n2.5.12 Correlations & Correlation Plots for Numeric Variables (Bivariate)\nCorrelations are useful summary statistics for numeric variables\nSome statistical algorithms are sensitive to high correlations among features (multi-collinearity)\nAt best, highly correlated features add unnecessary flexibility and can lead to overfitting\n\nWe can visualize correlations among predictors/features using corrplot.mixed() from corrplot package\n\nBest for numeric variables\nCan include ordinal or two level nominal variables if transformed to numeric\nCan include nominal variables with &gt; 2 levels if first transformed appropriately (e.g., dummy features, not demonstrated yet)\nWorks best with relatively small set of variables\n\n\n\ndata_trn |&gt; \n  mutate(overall_qual = as.numeric(overall_qual),\n         garage_qual = as.numeric(garage_qual)) |&gt; \n  select(where(is.numeric)) |&gt; \n  cor(use = \"pairwise.complete.obs\") |&gt; \n1  corrplot::corrplot.mixed()\n\n\n1\n\nNote use of namespace (corrplot::corrplot.mixed()) to call this function from corrplot package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.13 Grouped Box + Violin Plots for Categorical and Numeric (Bivariate)\nA grouped version of the combined box and violin plot is our preferred visualization for relationship between categorical and numeric variables (included in fun_plots.r)\n\nOften best when feature is categorical and outcome is numeric but can reverse\nCan use with both nominal and ordinal categorical variable\nWickham, Çetinkaya-Rundel, and Grolemund (2023) also describes use of grouped frequency polygons for this combination of variable classes\n\n\nplot_grouped_box_violin &lt;- function(df, x, y){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\n\nHere is the relationship between overall_qual and sale_price\n\nTend to prefer this over the scatterplot (with as.numeric()) for ordinal variables\nIncreasing spread of sale_price at higher levels of overall_qual is clearer in this plot\n\n\ndata_trn |&gt; plot_grouped_box_violin(\"overall_qual\", \"sale_price\")\n\n\n\n\n\n\n\n\n\nHere is a grouped box + violin with a nominal variable\n\nMore variation and skew in sale_price for one family homes (additional features, moderators?)\nPosition of townhouse (interior vs. exterior) seems to matter (don’t collapse?)\n\n\ndata_trn |&gt; plot_grouped_box_violin(\"bldg_type\", \"sale_price\")\n\n\n\n\n\n\n\n\n\nWhen we have a categorical predictor and a numeric outcome, we often want to see both the relationship between the variables AND the variability on the categorical variable alone.\nWe like this combined plot enough when doing EDA to provide a specific function (included in fun_plots.r)!\nIt is our go to for understanding the potential effect of a categorical predictor\n\nplot_categorical &lt;- function(df, x, y, ordered = FALSE){\n  if (ordered) {\n    df &lt;- df |&gt;\n      mutate(!!x := fct_reorder(.data[[x]], .data[[y]]))\n  }\n  \n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  p_bar &lt;- df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_bar()  +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n  \n  p_box &lt;- df |&gt;\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n  \n  return(list(p_bar, p_box))\n}\n\n\nsale_price by bldg_type\n\ndata_trn |&gt; plot_categorical(\"bldg_type\", \"sale_price\") |&gt; \n  plot_grid(plotlist = _, ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\n2.5.14 Stacked Barplots for Categorical (Bivariate)\nStacked Barplots:\n\nCan be useful with both nominal and ordinal variables\nCan create with either raw counts or percentages.\n\nDisplays different perspective (particularly with uneven distributions across levels)\nDepends on your question\n\nOften, you will place the outcome on the x-axis and the feature is coded by fill\n\n\nplot_grouped_barplot_count &lt;- function(df, x, y){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[y]], fill = .data[[x]])) +\n    geom_bar(position = \"stack\") +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\nplot_grouped_barplot_percent &lt;- function(df, x, y){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[y]], fill = .data[[x]])) +\n    geom_bar(position = \"fill\") +\n    labs(y = \"Proportion\") +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\n\nFor example, if we wanted to learn about how bldg_type varies by lot_config, see these plots\n\ndata_trn |&gt; plot_grouped_barplot_count(\"lot_config\", \"bldg_type\")\n\n\n\n\n\n\n\n\n\ndata_trn |&gt; plot_grouped_barplot_percent(\"lot_config\", \"bldg_type\")\n\n\n\n\n\n\n\n\n\nMay want to plot both ways\n\ndata_trn |&gt; plot_grouped_barplot_percent(\"lot_config\", \"bldg_type\")\n\n\n\n\n\n\n\n\n\ndata_trn |&gt; plot_grouped_barplot_percent(\"bldg_type\", \"lot_config\")\n\n\n\n\n\n\n\n\n\n\n\n2.5.15 Tile Plot for Ordinal Categorical (Bivariate)\nTile plots may be useful if both categorical variables are ordinal\n\nplot_tile &lt;- function(df, x, y){\n  df |&gt;\n    count(.data[[x]], .data[[y]]) |&gt;\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_tile(mapping = aes(fill = n))\n}\n\n\n\ndata_trn |&gt; plot_tile(\"overall_qual\", \"garage_qual\")\n\n\n\n\n\n\n\n\n\nYou might also consider a scatterplot with jitter in this instance\n\ndata_trn |&gt; \n  mutate(overall_qual = jitter(as.numeric(overall_qual)),\n         garage_qual = as.numeric(garage_qual)) |&gt; \n  plot_scatter(\"overall_qual\", \"garage_qual\")\n\n\n\n\n\n\n\n\n\n\n\n2.5.16 Two-way Tables for Categorical Variables (Bivariate)\nTwo-way tables are sometimes a useful summary statistic for two categorical variables. We can use a 2-variable form of the tab function, tab2(), from my function scripts for this\nFor example, the relationship between bldg_type and lot_config\n\ndata_trn |&gt; tab2(bldg_type, lot_config)\n\n   bldg_type corner culdsac fr2 fr3 inside\n      duplex     13       0   3   0     47\n     one_fam    221      73  29   1    892\n    town_end      8       5   3   0     92\n town_inside      0       3   3   0     40\n     two_fam      6       0   1   1     24",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "002_exploratory_data_analysis.html#working-with-recipes",
    "href": "002_exploratory_data_analysis.html#working-with-recipes",
    "title": "2  Exploratory Data Analysis",
    "section": "2.6 Working with Recipes",
    "text": "2.6 Working with Recipes\nRecipes are used for feature engineering in tidymodels using the recipes package\n\nUsed for transforming raw predictors into features used in our models\nDescribes all steps to make feature matrix. For example:\n\nTransforming factors into “dummy” features if needed\nLinear and non-linear transformations (e.g., log, box-cox)\nPolynomials and interactions (i.e., x1 * x1 or x1 * x2)\nMissing value imputations\n\nProper use of recipes is an important tool to prevent data leakage between train and either validation or test.\nRecipes use only information from the training set in all feature engineering\nConsider example of standardizing x1 for a feature in train vs. validation and test. Must use mean and sd from TRAIN to standardize x1 in train, validate, and test. VERY IMPORTANT.\n\n\nWe use recipes in a two step process - prep() and bake()\n\n“Prepping” a recipe involves calculating any statistics needed for the transformations that will be applied to engineer features (e.g., mean and standard deviation to normalize a numeric variable).\n\nPrepping is done with the prep() function.\nPrepping is always done only with training data. A “prepped” recipe does not derive any statistics from validation or test sets.\n\n“Baking” is the process of calculating the features\n\nBaking is done with the bake() function.\nWe used our prepped recipe when we bake.\nWhereas we only prep a recipe with training data, we can use a prepped recipe to bake features from any dataset (training, validation, or test).\n\n\n\nWe will work with recipes extensively when model building starting in unit 3\nFor now, we will only use the recipe to indicate roles as a gentle introduction.\nWe will expand on this recipe in unit 3\nRecipe syntax is very similar to generic tidyverse syntax (created by same group)\n\nActually a subset of tidyverse functions\nLess flexible/powerful but focused on our needs and easier to learn\nYou will eventually know both\n\n\nRecipes are used in Modeling scripts (which is a third type of script after cleaning and modeling EDA scripts)\n\nLets reload training again to pretend we are starting a new script\n\n\n data_trn &lt;- read_csv(file.path(path_data, \"ames_clean_class_trn.csv\"), \n                      col_types = cols()) |&gt; \n1  class_ames() |&gt;\n  glimpse()\n\n\n1\n\nRemember our function for classing!\n\n\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nRecipes can be used to indicate the outcome and predictors that will be used in the model\n\nCan use . to indicate all predictors\n\nCurrently, our preferred method for larger data sets\nWe can exclude some predictors later by changing their role, removing them with a later recipe step (\\(step\\_rm()\\)), or specifying a more precise formula when we fit the model\nSee Roles in Recipes for more info\n\nCan use specific names of predictors along with \\(+\\)\n\nThis is our preferred method when we have a smaller set of predictors (We will show you both approaches)\ne.g., sale_price ~ lot_area + year_built + overall_qual\n\nDo NOT indicate interactions here\n\nAll predictors are combined with +\nInteractions are specified by a later explicit feature engineering step\n\n\n\nrec &lt;- recipe(sale_price ~ ., data = data_trn)\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 9\n\nsummary(rec)\n\n# A tibble: 10 × 4\n   variable     type      role      source  \n   &lt;chr&gt;        &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 gr_liv_area  &lt;chr [2]&gt; predictor original\n 2 lot_area     &lt;chr [2]&gt; predictor original\n 3 year_built   &lt;chr [2]&gt; predictor original\n 4 overall_qual &lt;chr [3]&gt; predictor original\n 5 garage_cars  &lt;chr [2]&gt; predictor original\n 6 garage_qual  &lt;chr [3]&gt; predictor original\n 7 ms_zoning    &lt;chr [3]&gt; predictor original\n 8 lot_config   &lt;chr [3]&gt; predictor original\n 9 bldg_type    &lt;chr [3]&gt; predictor original\n10 sale_price   &lt;chr [2]&gt; outcome   original\n\n\n\n\n2.6.1 Prepping and Baking a Recipe\nLet’s make a feature matrix from the training set\nThere are two discrete (and important) steps\n\nprep()\nbake()\n\nFirst we prep the recipe using the training data\n\n1rec_prep &lt;- rec |&gt;\n2  prep(training = data_trn)\n\n\n1\n\nWe start by prepping our raw/original recipe (rec)\n\n2\n\nWe use the prep() function on on the training data. Recipes are ALWAYS prepped using training data. This makes sure that are recipes will always only use information from the training set when making features for any subsequent dataset. As part of the prepping process, prep() calculates and saves the features for the training data for later use\n\n\n\n\n\nSecond, we bake the training data using this prepped recipe to get a feature matrix from it.\n\n\nWe set new_data = NULL because don’t need features for new data. Instead, we want features from the training data that were used to prep the recipe\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(new_data = NULL)\n\n\nFinally, we should generally at least glimpse and/or skim (and typically do some more EDA) on our features to make sure our recipe is doing what we expect.\n\nglimpse\n\n\nfeat_trn |&gt; glimpse()\n\nRows: 1,465\nColumns: 10\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n\n\n\n\nskim\n\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\ntop_counts\n\n\n\n\noverall_qual\n0\n1\n10\n5: 424, 6: 350, 7: 304, 8: 176\n\n\ngarage_qual\n0\n1\n5\nta: 1312, no_: 81, fa: 57, gd: 13\n\n\nms_zoning\n0\n1\n7\nres: 1157, res: 217, flo: 66, com: 13\n\n\nlot_config\n0\n1\n5\nins: 1095, cor: 248, cul: 81, fr2: 39\n\n\nbldg_type\n0\n1\n5\none: 1216, tow: 108, dup: 63, tow: 46\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n1506.84\n511.44\n438\n1128\n1450\n1759\n5642\n1.43\n5.19\n\n\nlot_area\n0\n1\n10144.16\n8177.55\n1476\n7500\n9375\n11362\n164660\n11.20\n182.91\n\n\nyear_built\n0\n1\n1971.35\n29.65\n1880\n1953\n1972\n2000\n2010\n-0.54\n-0.62\n\n\ngarage_cars\n1\n1\n1.78\n0.76\n0\n1\n2\n2\n4\n-0.26\n0.10\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789\n129500\n160000\n213500\n745000\n1.64\n4.60\n\n\n\n\n\n\nWe can now use our features from training to train models, but that will take place in the next unit!\nWe could also use the prepped recipe to bake validation or test data to evaluate trained models. That too will happen in the next unit!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "002_exploratory_data_analysis.html#discussion-topics",
    "href": "002_exploratory_data_analysis.html#discussion-topics",
    "title": "2  Exploratory Data Analysis",
    "section": "2.7 Discussion Topics",
    "text": "2.7 Discussion Topics\n\nHouse keeping\n\nUnit 2 solutions\nCourse evals for extra credit (to quiz score)!\nUnit 3 homework\n\ntest set predictions\nfree lunch!\n\nQuestions to all three of us\nAn attempt at a reprex is generally expected now for code help\n\nQuiz review\nStudent Questions\n\ngreat questions!\nwrite them down as you read and watch lectures\nnot all questions will fit (slack as an alternative for follow-up)\n\n\n# Possible topics\n\nReview\n\nGoal is to develop model that closely approximates DGP\nGoal is to evaluate (estimate) how close our model is to the DGP (how much error) with as little error as possible\nBias, overfitting/variance for any estimate (model and performance of model)\ncandidate model configurations\nfit, select, evaluate\ntraining, validation, test\n\nReview: 2.2.1 Stages of Data Analysis and Model Development\nBest practices (discuss quickly)\n\ncsv for data sharing, viewing, git (though be careful with data in github or other public repo!)\nvariable values saved as text when nominal and ordinal (self-documenting)\nCreate data dictionary - Documentation is critical!!\nsnake_case for variables and self-documenting names (systematic names too)\n\nReview: 2.3.1 Data Leakage Issues\n\nReview section in webbook\nCleaning EDA is done with full dataset (but univariate). Very limited (variable names, values, find errors)\nModeling EDA is only done with a training set (or even “eyeball” sample) - NEVER use validate or test set\nNever estimate anything with full data set (e.g., missing values, standardize, etc)\nUse recipes, prep (all estimation) with held in data than bake the appropriate set\nPut test aside\nYou work with validation but never explore with validation (will still catch leakage with test but will be mislead to be overly optimistic and spoil test)\n\nFunctions sidenote - fun_modeling.R on github\nReview: 2.4.2 Prepping and Baking a Recipe\n\nReview section in web book\nprep always with held in data, bake with held in & out data.\n\nEDA for modeling\n\nlimitless, just scratched the surface\nDiffers some based on dimenstionality of dataset\nLearning about DGP\n\nunderstand univariate distributions, frequencies\nbivariate relationships\ninteractions (3 or more variables)\npatterns in data\n\n\nExtra topics, time permitting\n\n8.1. Missing data\n- Exclude vs. Impute in training data.  Outcomes?\n- How to impute\n- Missing in validate or test (can't exclude?). Exclude cases with missing outcomes.\n8.2. Outliers - Drop or fix errors! - Goal is always to estimate DGP - Exclude - Retain - Bring to fence - Don’t exclude/change outcome in validate/test\n8.3. Issues with high dimensionality\n- Hard to do predictor level EDA\n- Common choices (normality transformations)\n- observed vs. predicted plots\n- Methods for automated variable selection (glmnet)\n8.4. Distributional Shape\n- Measurement issues (interval scale)\n- Implications for relationships with other variables\n- Solutions?\n8.5. Linearity vs. More Complex Relationships\n- Transformations\n- Choice of statistical algorithm\n- Do you need a linear model?\n8.6. Interactions\n- Domain expertise\n- Visual options for interactions\n- But what do do with high dimensional data?\n- Explanatory vs. prediction goals (algorithms that accommodate interactions)\n8.7. How to handle all of these decisions in the machine learning framework\n- Goal is to develop a model that most closely approximates the DGP\n- How does validation and test help this?\n- Preregistration?\n  - Pre-reg for performance metric, resampling method   \n  - Use of resampling for other decisions\n  - Use of resampling to find correct model to test explanatory goals\n8.8. Model Assumptions\n- Why do we make assumptions?\n  - Inference\n  - Flexibility wrt DGP\n\n\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. https://adv-r.hadley.nz/.\n\n\nWickham, Hadley, Çetinkaya-Rundel Mine, and Garrett Grolemund. 2023. R for Data Science: Visualize, Model, Transform, and Import Data. 2nd ed. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "003_regression.html",
    "href": "003_regression.html",
    "title": "3  Introduction to Regression Models",
    "section": "",
    "text": "3.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "003_regression.html#overview-of-unit",
    "href": "003_regression.html#overview-of-unit",
    "title": "3  Introduction to Regression Models",
    "section": "",
    "text": "3.1.1 Learning Objectives\n\nUse of root mean square error (RMSE) in training and validation sets for model performance evaluation\nThe General Linear Model as a machine learning model\n\nExtensions to categorical variables (Dummy coding features)\nExtensions to interactive and non-linear effects of features\n\nK Nearest Neighbor (KNN)\n\nHyperparameter \\(k\\)\nScaling predictors\nExtensions to categorical variables\n\n\n\n\n\n3.1.2 Readings\n\nJames et al. (2023) Chapter 3, pp 59 - 109\n\nPost questions to the Readings channel in Slack\n\n\n3.1.3 Lecture Videos\n\nLecture 1: Overview ~ 13 mins\nLecture 2: The Simple Linear Model, Part 1 ~ 16 mins\nLecture 3: The Simple Linear Model, Part 2 ~ 12 mins\nLecture 4: Extension to Multiple Predictors ~ 15 mins\nLecture 5: Extension to Categorical Predictors ~ 30 mins\nLecture 6: Extension to Interactions and Non-Linear Effects ~ 11 mins\nLecture 7: Introduction to KNN ~ 9 mins\nLecture 8: The hyperparameter k ~ 13 mins\nLecture 9: Distance and Scaling in KNN ~ 9 mins\nLecture 10: KNN with Ames ~ 12 mins\n\nPost questions to the video-lectures channel in Slack\n\n\n\n3.1.4 Application Assignment and Quiz\n\nclean data: train; validate; test\ndata dictionary\nlm qmd\nknn qmd\nsolution: lm; knn\n\nPost questions to the application-assignments channel in Slack\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, February 7th\n\nOur goal in this unit is to build a machine learning regression model that can accurately (we hope) predict the sale_price for future sales of houses (in Iowa? more generally?)\nTo begin this project we need to:\n\nSet up conflicts policies\nWe will hide this in future units\n\n\noptions(conflicts.policy = \"depends.ok\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\n\nℹ SHA-1 hash of file is \"77e91675366f10788c6bcb59fa1cfc9ee0c75281\"\n\ntidymodels_conflictRules()\n\n\n\nLoad the packages we will need. I am shifting here to only loading tidymodels and tidyverse because the other functions we need are only called occasionally (so we will call them by namespace to see how that “feels”.)\n\n\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\n\n\n\nsource additional class functions libraries\nWe will hide this in future units\n\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\n\nℹ SHA-1 hash of file is \"c045eee2655a18dc85e715b78182f176327358a7\"\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\nℹ SHA-1 hash of file is \"def6ce26ed7b2493931fde811adff9287ee8d874\"\n\n\n\nset display options\nWe will hide this in future units\n\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\n\n\nhandle paths\n\n\npath_data &lt;- \"./data\"\n\n\n\nSet up function to class ames data (copied with one improvement from last unit)\n\n\nclass_ames &lt;- function(df){\n  df |&gt;\n    mutate(across(where(is.character), factor)) |&gt; \n    mutate(overall_qual = factor(overall_qual, levels = 1:10), \n1           garage_qual = suppressWarnings(fct_relevel(garage_qual,\n                                                      c(\"no_garage\", \"po\", \"fa\", \n                                                    \"ta\", \"gd\", \"ex\"))))\n}\n\n\n1\n\nWarnings should be considered errors until investigated. Once investigated, they can be ignored. To explicitly ignore, use suppressWarnings()\n\n\n\n\n\n\nOpen the cleaned training set\n\n\ndata_trn &lt;- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           col_types = cols()) |&gt;  \n  class_ames() |&gt; \n  glimpse()\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\n\nOpen the cleaned validation set\n\n\ndata_val &lt;- read_csv(here::here(path_data, \"ames_clean_class_val.csv\"),\n                     col_types = cols()) |&gt; \n  class_ames() |&gt; \n  glimpse()\n\nRows: 490\nColumns: 10\n$ sale_price   &lt;dbl&gt; 215000, 189900, 189000, 171500, 212000, 164000, 394432, 1…\n$ gr_liv_area  &lt;dbl&gt; 1656, 1629, 1804, 1341, 1502, 1752, 1856, 1004, 1092, 106…\n$ lot_area     &lt;dbl&gt; 31770, 13830, 7500, 10176, 6820, 12134, 11394, 11241, 168…\n$ year_built   &lt;dbl&gt; 1960, 1997, 1999, 1990, 1985, 1988, 2010, 1970, 1971, 197…\n$ overall_qual &lt;fct&gt; 6, 5, 7, 7, 8, 8, 9, 6, 5, 6, 7, 9, 8, 8, 7, 8, 6, 5, 5, …\n$ garage_cars  &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 3, 2, 3, 1, 1, 2, …\n$ garage_qual  &lt;fct&gt; ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, t…\n$ ms_zoning    &lt;fct&gt; res_low, res_low, res_low, res_low, res_low, res_low, res…\n$ lot_config   &lt;fct&gt; corner, inside, inside, inside, corner, inside, corner, c…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, one_fam, town_end, one_fam, on…\n\n\nNOTE: Remember, I have held back an additional test set that we will use only once to evaluate the final model that we each develop in this unit.\n\nWe will also make a dataframe to track validation error across the models we fit\n\nerror_val &lt;- tibble(model = character(), rmse_val = numeric()) |&gt; \n  glimpse()\n\nRows: 0\nColumns: 2\n$ model    &lt;chr&gt; \n$ rmse_val &lt;dbl&gt; \n\n\n\nWe will fit regression models with various model configurations.\nThese configurations will differ with respect to statistical algorithm:\n\nA General Linear Model (lm) - a parametric approach\nK Nearest Neighbor (KNN) - a non-parametric approach\n\nThese configurations will differ with respect to the features\n\nSingle feature (i.e., simple regression)\nVarious sets of multiple features that vary by:\n\nRaw predictors used\nTransformations applied to those predictors as part of feature engineering\nInclusion (or not) of interactions among features\n\nThe KNN model configurations will also differ with respect to its hyperparameter- \\(k\\)\n\n\nTo build models that will work well in new data (e.g., the data that I have held back from you so far):\n\nWe have split the remaining data into a training and validation set for our own use during model building\nWe will fit models in train\nWe will evaluate them in validation\n\nRemember that we:\n\nUsed a 75/25 stratified (on sale_price) split of the data at the end of cleaning EDA to create training and validation sets\nAre only using a subset of the available predictors. The same ones I used for the EDA unit\n\nYou will work with all of my predictors and all the predictors you used for your EDA when you do the application assignment for this unit\n\nPause for a moment to answer this question:\n\n\n\n\n\n\nQuestion: Why do we need independent validation data to select the best model configuration? In other words, why cant we just fit and evaluate all of the models in our one training set?\n\n\n\n\n\n\n\nShow Answer\nThese models will all overfit the dataset within which they are fit to some degree.   \nIn other words, they will predict both systematic variance (the DGP) and some noise in \nthe training set.  However, they will differ in how much they overfit the training set.   \nAs the models get more flexible they will have the potential to overfit to a greater degree.\nModels with a larger number of features (e.g., more predictors, features based on interactions\nas well as raw predictors) will overfit to a greater degree.  All other things equal, the\nnon-parametric KNN will also be more flexible than the general linear model so it may \noverfit to a greater degree as well if the true DGP is linear on the features.  \nTherefore, just because a model fits the training set well does not mean it will work \nwell in new data because the noise will be different in every new dataset.  \nThis overfitting will be removed from our performance estimate if we calculate it \nwith new data (the validation set).\n\n\n\n\n\n\nLet’s take a quick look at the available raw predictors in the training set\n\ndata_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\ndata_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\ntop_counts\n\n\n\n\noverall_qual\n0\n1\n10\n5: 424, 6: 350, 7: 304, 8: 176\n\n\ngarage_qual\n0\n1\n5\nta: 1312, no_: 81, fa: 57, gd: 13\n\n\nms_zoning\n0\n1\n7\nres: 1157, res: 217, flo: 66, com: 13\n\n\nlot_config\n0\n1\n5\nins: 1095, cor: 248, cul: 81, fr2: 39\n\n\nbldg_type\n0\n1\n5\none: 1216, tow: 108, dup: 63, tow: 46\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789\n129500\n160000\n213500\n745000\n1.64\n4.60\n\n\ngr_liv_area\n0\n1\n1506.84\n511.44\n438\n1128\n1450\n1759\n5642\n1.43\n5.19\n\n\nlot_area\n0\n1\n10144.16\n8177.55\n1476\n7500\n9375\n11362\n164660\n11.20\n182.91\n\n\nyear_built\n0\n1\n1971.35\n29.65\n1880\n1953\n1972\n2000\n2010\n-0.54\n-0.62\n\n\ngarage_cars\n1\n1\n1.78\n0.76\n0\n1\n2\n2\n4\n-0.26\n0.10\n\n\n\n\n\n\nRemember from our modeling EDA that we have some issues to address as part of our feature engineering:\n\nMissing values\nPossible transformation of sale_price\nPossible transformation of other numeric predictors\nWe will need to use some feature engineering techniques to handle categorical variables\nWe may need to consider interactions among features\n\nAll of this will be accomplished with a recipe\nBut first, let’s discuss/review our first statistical algorithm",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "003_regression.html#the-simple-general-linear-model-lm",
    "href": "003_regression.html#the-simple-general-linear-model-lm",
    "title": "3  Introduction to Regression Models",
    "section": "3.2 The Simple (General) Linear Model (LM)",
    "text": "3.2 The Simple (General) Linear Model (LM)\nWe will start with only a quick review of the use of the simple (one feature) linear model (LM) as a machine learning model because you should be very familiar with this statistical model at this point\n\n\\(Y = \\beta_0 + \\beta_1*X_1 + \\epsilon\\)\n\nApplied to our regression problem, we might fit a model such as:\n\n\\(sale\\_price = \\beta_0 + \\beta_1*gr\\_liv\\_area + \\epsilon\\)\n\nThe (general) linear model is a parametric model. We need to estimate two parameters using our training set\n\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\nYou already know how to do this using lm() in base R. However, we will use the tidymodels modeling approach.\n\nWe use tidymodels because:\n\nIt provides a consistent interface to many (and growing numbers) of statistical algorithms\nIt provides very strong and easy feature engineering routines (e.g., missing data, scaling, transformations, near-zero variance, collinearity) via recipes\nIt simplifies model performance evaluation using resampling approaches (that you don’t know yet!)\nIt supports numerous performance metrics\nIt is tightly integrated within the tidyverse\nIt is under active development and support\nYou can see documentation for all of the packages at the tidymodels website. It is worth a quick review now to get a sense of what is available\n\n\nTo fit a model with a specific configuration, we need to:\n\nSet up a feature engineering recipe\nUse the recipe to make a feature matrix\n\nprep() it with training data\nbake() it with data you want to use to calculate feature matrix\n\nSelect and define the statistical algorithm\nFit the algorithm in the feature matrix in our training set\n\nThese steps are accomplished with functions from the recipes and parsnip packages.\n\nWe will start with a simple model configuration\n\nGeneral linear model (lm)\nOne feature (raw gr_liv_area)\nFit in training data\n\n\nSet up a VERY SIMPLE feature engineering recipe\n\nInclude outcome on the left size of ~\nInclude raw predictors (not yet features) on the right side of ~.\n\nIndicate the training data\n\n\nrec &lt;- \n  recipe(sale_price ~ gr_liv_area, data = data_trn) \n\n\nWe can see a summary of it to verify it is doing what you expect by calling\n\nrec\nsummary(rec)\n\n\n\nWe can then prep the recipe and bake the data to make our feature matrix from the training dataset\n\nAgain, remember we always prep a recipe with training data but use the prepped recipe to bake any data\nIn this instance we will prep with data_trn and then bake data_trn so that we have features from our training set to train/fit the model.\n\n\nrec_prep &lt;- rec |&gt; \n  prep(training = data_trn)\n\n\n\nAnd now we can bake the training data to make a feature matrix\nTraining features were already saved in the prepped recipe so we set new_data = NULL\n\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(new_data = NULL)\n\n\nYou should always review the feature matrix to make sure it looks as you expect\n\nincludes outcome (sale_price)\nincludes expected feature (gr_liv_area)\nSample size is as expected\nNo missing data\n\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n1506.84\n511.44\n438\n1128\n1450\n1759\n5642\n1.43\n5.19\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789\n129500\n160000\n213500\n745000\n1.64\n4.60\n\n\n\n\n\n\nNow let’s consider the statistical algorithm\n\ntidymodels breaks this apart into two pieces for clarity\nFirst, you specify the broad category of algorithm\n\ne.g., linear_reg(), nearest_neighbor(), logistic_reg()\n\nNext, you set_mode() to indicate if if the model is for regression or classification broadly\n\nNot needed if the engine can only be used for one mode (e.g., 'linear_reg() is only for regression.\n\nThen you select a function from a specific R package (or base R) that will implement the algorithm\n\ntidymodels calls this setting the engine\ne.g., lm, kknn, glm, glmnet\n\n\n\nYou can see the available engines (and modes: regression vs. classification) for the broad classes of algorithms\nWe will work with many of these algorithms later in the course\n\nshow_engines(\"linear_reg\")\n\n# A tibble: 7 × 2\n  engine mode      \n  &lt;chr&gt;  &lt;chr&gt;     \n1 lm     regression\n2 glm    regression\n3 glmnet regression\n4 stan   regression\n5 spark  regression\n6 keras  regression\n7 brulee regression\n\nshow_engines(\"nearest_neighbor\")\n\n# A tibble: 2 × 2\n  engine mode          \n  &lt;chr&gt;  &lt;chr&gt;         \n1 kknn   classification\n2 kknn   regression    \n\nshow_engines(\"logistic_reg\")\n\n# A tibble: 7 × 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification\n\nshow_engines(\"decision_tree\")\n\n# A tibble: 5 × 2\n  engine mode          \n  &lt;chr&gt;  &lt;chr&gt;         \n1 rpart  classification\n2 rpart  regression    \n3 C5.0   classification\n4 spark  classification\n5 spark  regression    \n\nshow_engines(\"rand_forest\")\n\n# A tibble: 6 × 2\n  engine       mode          \n  &lt;chr&gt;        &lt;chr&gt;         \n1 ranger       classification\n2 ranger       regression    \n3 randomForest classification\n4 randomForest regression    \n5 spark        classification\n6 spark        regression    \n\nshow_engines(\"mlp\")\n\n# A tibble: 6 × 2\n  engine mode          \n  &lt;chr&gt;  &lt;chr&gt;         \n1 keras  classification\n2 keras  regression    \n3 nnet   classification\n4 nnet   regression    \n5 brulee classification\n6 brulee regression    \n\n\n\nYou can load even more engines from the discrim package. We will use some of these later too. You need to load the package to use these engines.\n\nlibrary(discrim, exclude = \"smoothness\") # needed for these engines\n\nshow_engines(\"discrim_linear\") \n\n# A tibble: 4 × 2\n  engine        mode          \n  &lt;chr&gt;         &lt;chr&gt;         \n1 MASS          classification\n2 mda           classification\n3 sda           classification\n4 sparsediscrim classification\n\nshow_engines(\"discrim_regularized\") \n\n# A tibble: 1 × 2\n  engine mode          \n  &lt;chr&gt;  &lt;chr&gt;         \n1 klaR   classification\n\nshow_engines(\"naive_Bayes\")\n\n# A tibble: 2 × 2\n  engine     mode          \n  &lt;chr&gt;      &lt;chr&gt;         \n1 klaR       classification\n2 naivebayes classification\n\n\n\nYou can also better understand how the engine will be called using translate()\nNot useful here but will be with more complicated algorithms\n\nlinear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n\n\nLet’s combine our feature matrix with an algorithm to fit a model in our training set using only raw gr_liv_area as a feature\nNote the specification of\n\nThe category of algorithm\nThe engine (no need to set mode of engine b/c lm are only for the regression mode)\nThe use of the . to indicate all features in the matrix.\n\nnot that useful here because there is only one feature: gr_liv_area\nwill be useful when we have many features in the matrix\n\nWe use the the feature matrix (rather than raw data) from the training set to fit the model.\n\n\nfit_lm_1 &lt;- \n1  linear_reg() |&gt;\n2  set_engine(\"lm\") |&gt;\n3  fit(sale_price ~ ., data = feat_trn)\n\n\n1\n\ncategory of algorithm\n\n2\n\nengine\n\n3\n\nuse of . for all features and use of feature matrix from training set\n\n\n\n\n\nWe can get the parameter estimates, standard errors, and statistical tests for each \\(\\beta\\) = 0 for this model using tidy() from the broom package (loaded as part of the tidyverse)\n\nfit_lm_1 |&gt;  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   16561.   4537.        3.65 2.72e-  4\n2 gr_liv_area     109.      2.85     38.2  4.78e-222\n\n\n\nThere are a variety of ways to pull out the estimates for each feature (and intercept)\nOption 1: Pull all estimates from the tidy object\n\nfit_lm_1 |&gt; \n  tidy() |&gt; \n  pull(estimate)\n\n[1] 16560.9991   108.9268\n\n\n\nOption 2: Extract a single estimate using $ and row number. Be careful that order of features won’t change! This assumes the feature coefficient for the relevant feature is always the second coefficient.\n\ntidy(fit_lm_1)$estimate[[2]]\n\n[1] 108.9268\n\n\n\nOption 3: Filter tidy df to the relevant row (using term ==) and pull the estimate. Safer!\n\nfit_lm_1 |&gt; \n  tidy() |&gt; \n  filter(term == \"gr_liv_area\") |&gt; \n  pull(estimate)\n\n[1] 108.9268\n\n\n\nOption 4: Write a function if we plan to do this a lot. We include this function in the fun_ml.R script in our repo. Better still (safe and code efficient)!\n\nget_estimate &lt;- function(the_fit, the_term){\n  the_fit |&gt; \n    tidy() |&gt; \n    filter(term == the_term) |&gt; \n    pull(estimate)\n}\n\nand then use this function\n\nget_estimate(fit_lm_1, \"gr_liv_area\")\n\n[1] 108.9268\n\nget_estimate(fit_lm_1, \"(Intercept)\")\n\n[1] 16561\n\n\n\nRegardless of the method, we now have a simple parametric model for sale_price\n\\(\\hat{sale\\_price} = 1.6561\\times 10^{4} + 108.9 * gr\\_liv\\_area\\)\n\nWe can get the predicted values for sale_price (i.e., \\(\\hat{sale\\_price}\\)) in our validation set using predict()\nHowever, we first need to make a feature matrix for our validation set\n\nWe use the same recipe that we previously prepped with training data (data_trn)\nBut now we bake new data that were not used to train the recipe - the validation data, data_val\n\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(new_data = data_val)\n\n\nAs always, we should skim these new features\n\nSample size matches what we expect for validation set\nNo missing data\nIncludes expected outcome and features\n\n\nfeat_val |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_val\n\n\nNumber of rows\n490\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n1493.0\n483.78\n480\n1143.5\n1436\n1729.5\n3608\n0.92\n1.16\n\n\nsale_price\n0\n1\n178512.8\n75493.59\n35311\n129125.0\n160000\n213000.0\n556581\n1.42\n2.97\n\n\n\n\n\n\nNow we can get predictions using our model with validation features\npredict() returns a dataframe with one column named .pred and one row for every observation in dataframe (e.g., validation feature set)\n\npredict(fit_lm_1, feat_val)\n\n# A tibble: 490 × 1\n     .pred\n     &lt;dbl&gt;\n 1 196944.\n 2 194003.\n 3 213065.\n 4 162632.\n 5 180169.\n 6 207401.\n 7 218729.\n 8 125923.\n 9 135509.\n10 133004.\n# ℹ 480 more rows\n\n\n\nWe can visualize how well this model performs in the validation set by plotting predicted sale_price (\\(\\hat{sale\\_price}\\)) vs. sale_price (ground truth in machine learning terminology) for these data\nWe might do this a lot so let’s write a function. We have also included this function in fun_ml.R\n\nplot_truth &lt;- function(truth, estimate) {\n  ggplot(mapping = aes(x = truth, y = estimate)) + \n    geom_abline(lty = 2, color = \"red\") + \n    geom_point(alpha = 0.5) + \n    labs(y = \"predicted outcome\", x = \"outcome\") +\n    coord_obs_pred()   # scale axes uniformly (from tune package)\n}\n\n\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_1, feat_val)$.pred)\n\n\n\n\n\n\n\n\nPerfect performance would have all the points right on the dotted line (same value for actual and predicted outcome)\n\nOur model doesn’t do that well yet. Not surprising\nPattern also has some indication of fanning of residuals AND some non-linearity with higher outcome scores that suggests need for a power transformation of outcome (e.g., log)\nThis is consistent with our earlier modeling EDA\nPerhaps not that bad here b/c both sale_price and gr_liv_area were positively skewed\nWe will need consider this eventually\n\n\nWe can quantify model performance by selecting a performance metric\n\nThe yardstick package within the tidymodels framework supports calculation of many performance metrics for regression and classification models\nSee the list of all currently available metrics\n\nRoot mean square error (RMSE) is a common performance metric for regression models\n\nYou focused on a related metric, sum of squared error (SSE), in PSY 610/710\nRMSE simply divides SSE by N (to get mean squared error; MSE) and then takes the square root to return the metric to the original units for the outcome variable\nIt is easy to calculate using rmse_vec() from the yardstick package\n\n\nrmse_vec(truth = feat_val$sale_price, \n         estimate = predict(fit_lm_1, feat_val)$.pred)\n\n[1] 51375.08\n\n\n\nLet’s record how well this model performed in validation so we can compare it to subsequent models\n\nerror_val &lt;- bind_rows(error_val, \n                       tibble(model = \"simple linear model\", \n                              rmse_val = rmse_vec(truth = feat_val$sale_price, \n                                                  estimate = predict(fit_lm_1,\n                                                                     feat_val)$.pred)))\nerror_val\n\n# A tibble: 1 × 2\n  model               rmse_val\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 simple linear model   51375.\n\n\nNOTE: I will continue to bind RMSE to this dataframe for newer models but plan to hide this code chuck to avoid distractions. You can reuse this code repeatedly to track your own models if you like. (Perhaps we should write a function??)\n\nFor explanatory purposes, we might want to visualize the relationship between a feature and the outcome (in addition to examining the parameter estimates and the associated statistical tests)\n\nHere is a plot of \\(\\hat{sale\\_price}\\) by gr_liv_area superimposed over a scatterplot of the raw data from the validation set\n\n\nfeat_val |&gt; \n  ggplot(aes(x = gr_liv_area)) +\n    geom_point(aes(y = sale_price), color = \"gray\") +\n    geom_line(aes(y = predict(fit_lm_1, data_val)$.pred), \n              linewidth = 1.25, color = \"blue\") + \n    ggtitle(\"Validation Set\")\n\n\n\n\n\n\n\n\n\nAs expected, there is a moderately strong positive relationship between gr_liv_area and sale_price.\n\nWe can also again see the heteroscadasticity in the errors that might be corrected by a power transformation of sale_price (or gr_liv_area)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "003_regression.html#extension-of-lm-to-multiple-predictors",
    "href": "003_regression.html#extension-of-lm-to-multiple-predictors",
    "title": "3  Introduction to Regression Models",
    "section": "3.3 Extension of LM to Multiple Predictors",
    "text": "3.3 Extension of LM to Multiple Predictors\nWe can improve model performance by moving from simple linear model to a linear model with multiple features derived from multiple predictors\nWe have many other numeric variables available to use, even in this pared down version of the dataset.\n\ndata_trn |&gt;  names()\n\n [1] \"sale_price\"   \"gr_liv_area\"  \"lot_area\"     \"year_built\"   \"overall_qual\"\n [6] \"garage_cars\"  \"garage_qual\"  \"ms_zoning\"    \"lot_config\"   \"bldg_type\"   \n\n\nLet’s expand our model to also include lot_area, year_built, and garage_cars\nAgain, we need:\n\nA feature engineering recipe\nTraining (and eventually validation) feature matrices\nAn algorithm to fit in training feature matrix\n\n\nWith the addition of new predictors, we now have a feature engineering task\n\nWe have missing data on garage_cars in the training set\nWe need to decide how we will handle it\n\nA simple solution is to do median imputation - substitute the median of the non-missing scores for any missing score.\n\nThis is fast and easy to understand\nIt works OK (but there are certainly better options that we will consider later in the course)\n\nsee other options in Step Functions - Imputation section on tidymodels website\n\nThere is only one missing value so it likely doesn’t matter much anyway\n\n\nLet’s add this to our recipe. All of the defaults are appropriate but you should see ?step_impute_median() to review them\n\nrec &lt;- \n1  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars,\n         data = data_trn) |&gt; \n  step_impute_median(garage_cars)\n\n\n1\n\nNotice we now list four predictors for our recipe using + between them\n\n\n\n\n\nNow we need to\n\nFirst prep recipe\n\n\nrec_prep &lt;- rec |&gt; \n1  prep(data_trn)\n\n\n1\n\nI am going to stop using training = at this point. Remember, we prep recipes with training data.\n\n\n\n\n\n\nNext, bake the training data with prepped recipe to get training features\n\n\nfeat_trn &lt;- rec_prep |&gt; \n1  bake(data_trn)\n\n\n1\n\nI am going to stop using new_data = but remember, we use NULL if we want the previously saved training features. If instead, we want features for new data, we provide that dataset.\n\n\n\n\n\n\nAnd take a quick look at the features\n\nSample size is correct\n4 features and the outcome variable\nAll features are numeric\nNo missing data for garage_qual\n\n\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n1506.84\n511.44\n438\n1128\n1450\n1759\n5642\n1.43\n5.19\n\n\nlot_area\n0\n1\n10144.16\n8177.55\n1476\n7500\n9375\n11362\n164660\n11.20\n182.91\n\n\nyear_built\n0\n1\n1971.35\n29.65\n1880\n1953\n1972\n2000\n2010\n-0.54\n-0.62\n\n\ngarage_cars\n0\n1\n1.78\n0.76\n0\n1\n2\n2\n4\n-0.26\n0.10\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789\n129500\n160000\n213500\n745000\n1.64\n4.60\n\n\n\n\n\n\n\nAnd finally, bake the validation data with the same prepped recipe to get validation features\n\n\nfeat_val &lt;- rec_prep |&gt; \n1  bake(data_val)\n\n\n1\n\nNotice that here we are now baking data_val\n\n\n\n\n\n\nAnd take a quick look\n\nCorrect sample size (N = 490)\n4 features and outcome\nAll numeric\nNo missing data\n\n\n\nfeat_val |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_val\n\n\nNumber of rows\n490\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n1493.00\n483.78\n480\n1143.5\n1436.0\n1729.50\n3608\n0.92\n1.16\n\n\nlot_area\n0\n1\n10462.08\n10422.55\n1680\n7500.0\n9563.5\n11780.75\n215245\n15.64\n301.66\n\n\nyear_built\n0\n1\n1971.08\n30.96\n1875\n1954.0\n1975.0\n2000.00\n2010\n-0.66\n-0.41\n\n\ngarage_cars\n0\n1\n1.74\n0.76\n0\n1.0\n2.0\n2.00\n4\n-0.24\n0.22\n\n\nsale_price\n0\n1\n178512.82\n75493.59\n35311\n129125.0\n160000.0\n213000.00\n556581\n1.42\n2.97\n\n\n\n\n\n\nNow let’s combine our algorithm and training features to fit this model configuration with 4 features\n\nfit_lm_4 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n1  fit(sale_price ~ ., data = feat_trn)\n\n\n1\n\nthe . is a bit more useful now\n\n\n\n\n\nThis yields these parameter estimates (which as we know from 610/710 were selected to minimize SSE in the training set):\n\nfit_lm_4 |&gt; tidy()\n\n# A tibble: 5 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -1665041.    89370.       -18.6  1.14e- 69\n2 gr_liv_area       76.8       2.62      29.3  3.56e-149\n3 lot_area           0.514     0.146      3.51 4.60e-  4\n4 year_built       854.       46.2       18.5  7.66e- 69\n5 garage_cars    22901.     1964.        11.7  4.09e- 30\n\n\n\nHere is our parametric model\n\n\\(\\hat{sale\\_price} = -1.6650409\\times 10^{6} + 76.8 * gr\\_liv\\_area + 0.5 * lot\\_area + 854.3 * year\\_built + 2.29008\\times 10^{4} * garage\\_cars\\)\n\nCompared with our previous simple regression model:\n\n\\(\\hat{sale\\_price} = 1.6561\\times 10^{4} + 108.9 * gr\\_liv\\_area\\)\n\n\nOf course, these four features are correlated both with sale_price but also with each other\nLet’s look at correlations in the training set.\n\nfeat_trn |&gt; \n  cor() |&gt; \n  corrplot::corrplot.mixed()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What are the implications of the correlations among many of these predictors?\n\n\n\n\n\n\n\nShow Answer\nThe multiple regression model coefficients represent unique effects, controlling for \nall other variables in the model.  You can see how the unique effect of `gr_liv_area` \nis smaller than its overall effect from the simple regression.   This also means \nthat the overall predictive strength of the model will not be a sum of the effects \nof each predictor considered in isolation - it will likely be less.  Also, if the \ncorrelations are high, problems with multicollinearity will emerge.  This will yield \nlarge standard errors which means that the models will start to have more variance when \nfit in different training datasets!  We will soon learn about other regularized \nversions of the GLM that do not have these issues with correlated predictors.\n\n\n\n\n\n\nHow well does this more complex model perform in validation? Let’s compare the previous and current visualizations of \\(sale\\_price\\) vs. \\(\\hat{sale\\_price}\\)\n\nLooks like the errors are smaller (closer to the diagonal line that would represent prefect prediction)\nClear signs of non-linearity are now present as well. Time for more Modeling EDA!!\n\n\nplot_1 &lt;- plot_truth(truth = feat_val$sale_price, \n                     estimate = predict(fit_lm_1, feat_val)$.pred)\n\nplot_4 &lt;- plot_truth(truth = feat_val$sale_price, \n                     estimate = predict(fit_lm_4, feat_val)$.pred)\n\ncowplot::plot_grid(plot_1, plot_4, \n          labels = list(\"1 feature\", \"4 features\"), hjust = -1.5)\n\n\n\n\n\n\n\n\nCoding sidebar: Notice the use of plot_grid() from the cowplot package to make side by side plots. This also required returning the individual plots as objects (just assign to a object name, e.g., plot_1)\n\nLet’s compare model performance for the two models using RMSE in the validation set\n\nThe one feature simple linear model\n\n\nrmse_vec(feat_val$sale_price, \n         predict(fit_lm_1, feat_val)$.pred)\n\n[1] 51375.08\n\n\n\nThe four feature linear model. A clear improvement!\n\n\nrmse_vec(feat_val$sale_price, \n         predict(fit_lm_4, feat_val)$.pred)\n\n[1] 39903.25\n\n\n\n\nLet’s bind the new performance metric to our results table\n\n\n\n# A tibble: 2 × 2\n  model                  rmse_val\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 simple linear model      51375.\n2 4 feature linear model   39903.\n\n\n\nGiven the non-linearity suggested by the truth vs. estimate plots, we might wonder if we could improve the fit if we transformed our features to be closer to normal\n\nThere are a number of recipe functions that do transformations (see Step Functions - Individual Transformations)\nWe will apply step_YeoJohnson(), which is similar to a Box-Cox transformation but can be more broadly applied because the scores don’t need to be strictly positive\n\n\nLet’s do it all again, now with transformed features!\n\nDefine the feature engineering recipe\n\n\nrec &lt;- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars, \n         data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_YeoJohnson(lot_area, gr_liv_area, year_built, garage_cars)\n\n\n\nPrep the recipe with training set\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\n\nUse prepped recipe to bake the training set into features\nNotice the features are now less skewed (but sale_price is still skewed)\n\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n5.22\n0.16\n4.60\n5.11\n5.23\n5.33\n5.86\n0.00\n0.12\n\n\nlot_area\n0\n1\n14.10\n1.14\n10.32\n13.69\n14.20\n14.64\n21.65\n0.08\n5.46\n\n\nyear_built\n0\n1\n1971.35\n29.65\n1880.00\n1953.00\n1972.00\n2000.00\n2010.00\n-0.54\n-0.62\n\n\ngarage_cars\n0\n1\n2.12\n0.98\n0.00\n1.11\n2.37\n2.37\n5.23\n-0.03\n0.04\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789.00\n129500.00\n160000.00\n213500.00\n745000.00\n1.64\n4.60\n\n\n\n\n\n\n\nUse same prepped recipe to bake the validation set into features\nAgain, features are less skewed\n\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\nfeat_val |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_val\n\n\nNumber of rows\n490\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n5.22\n0.16\n4.65\n5.11\n5.23\n5.32\n5.66\n-0.17\n0.19\n\n\nlot_area\n0\n1\n14.14\n1.17\n10.57\n13.69\n14.24\n14.72\n22.44\n0.11\n6.12\n\n\nyear_built\n0\n1\n1971.08\n30.96\n1875.00\n1954.00\n1975.00\n2000.00\n2010.00\n-0.66\n-0.41\n\n\ngarage_cars\n0\n1\n2.06\n0.96\n0.00\n1.11\n2.37\n2.37\n5.23\n0.01\n0.24\n\n\nsale_price\n0\n1\n178512.82\n75493.59\n35311.00\n129125.00\n160000.00\n213000.00\n556581.00\n1.42\n2.97\n\n\n\n\n\n\n\nFit model\n\n\nfit_lm_4yj &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\n\nView truth vs. estimate plot\n\n\nplot_truth(truth = feat_val$sale_price,\n           estimate = predict(fit_lm_4yj, feat_val)$.pred)\n\n\n\n\n\n\n\n\n\n\nand look at the error\n\n\n\n# A tibble: 3 × 2\n  model                          rmse_val\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 simple linear model              51375.\n2 4 feature linear model           39903.\n3 4 feature linear model with YJ   41660.\n\n\n\nThat didn’t help at all. Error still high and still non-linearity in plot.\n\n\nWe may need to consider\n\na transformation of sale_price (We will leave that to you for the application assignment if you are daring!)\nor a different algorithm that can handle non-linear relationships better",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "003_regression.html#extension-to-categorical-predictors",
    "href": "003_regression.html#extension-to-categorical-predictors",
    "title": "3  Introduction to Regression Models",
    "section": "3.4 Extension to Categorical Predictors",
    "text": "3.4 Extension to Categorical Predictors\nMany important predictors in our models may be categorical (nominal and some ordinal predictors)\n\nSome statistical algorithms (e.g., random forest) can accept even nominal predictors as features without any further feature engineering\nBut many cannot. Linear models cannot.\nThe type of feature engineering may differ for nominal vs. ordinal categorical predictors\nFor nominal categorical predictors:\n\nWe need to learn a common approach to transform them to numeric features - dummy coding. We will learn the concept in general AND how to accomplish within a feature engineering recipe.\n\nFor ordinal predictors:\n\nWe can treat them like numeric predictors\nWe can treat them like nominal categorical predictors\n\nSee article on Categorical Predictors on the tidymodels website for more details\n\n\n\n3.4.1 Dummy Coding\nFor many algorithms, we will need to use feature engineering to convert a categorical predictor to numeric features. One common technique is to use dummy coding. When dummy coding a predictor, we transform the original categorical predictor with m levels into m-1 dummy coded features.\nTo better understand how and why we do this, lets consider a version of ms_zoning in the Ames dataset.\n\ndata_trn |&gt; \n  pull(ms_zoning) |&gt; \n  table()\n\n\n    agri   commer    float    indus res_high  res_low  res_med \n       2       13       66        1        9     1157      217 \n\n\nWe will recode ms_zoning to have only 3 levels to make our example simple (though dummy codes can be used for predictors with any number of levels)\n\ndata_dummy &lt;- data_trn |&gt; \n1  select(sale_price, ms_zoning)  |&gt;\n2  mutate(ms_zoning3 = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\",\n                                                   \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n3                                 \"floating\" = \"float\")) |&gt;\n4  select(-ms_zoning)\n\n\n1\n\nMake a df (dataframe) with only sale_price and ms_zoning\n\n2\n\nfct_collapse() from the forcats package is our preferred way to collapse levels of a factor. See fct_recode() for more generic recoding of levels.\n\n3\n\nWe could have left this line out and float would have stayed as a level named float\n\n4\n\nRemove original ms_zoning predictor\n\n\n\n\n\nTake a look at the new predictor\n\ndata_dummy |&gt; \n  pull(ms_zoning3) |&gt; \n  table() \n\n\n commercial    floating residential \n         16          66        1383 \n\n\n\n\n\n\n\n\n\nQuestion: Why can’t we simply recode each level with a different consecutive value (e.g., commercial = 1, floating =2 , residential = 3)?\n\n\n\n\n\n\n\nShow Answer\nThere is no meaningful way to order the numbers that we assign to the levels of \nthis unordered categorical predictor.  The shape and strength of the relationship\nbetween it and sale_price will completely change based on arbitrary ordering of \nthe levels.\n\n\n\n\n\n\nImagine fitting a straight line to predict sale_price from ms_zoning3 using these three different ways to arbitrarily assign numbers to levels.\n\ndata_dummy |&gt; \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 1,\n                                ms_zoning3 == \"commercial\" ~ 2,\n                                ms_zoning3 == \"floating\" ~ 3)) |&gt; \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n\n\n\n\n\n\n\n\n\ndata_dummy |&gt; \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 2,\n                                ms_zoning3 == \"commercial\" ~ 1,\n                                ms_zoning3 == \"floating\" ~ 3)) |&gt; \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n\n\n\n\n\n\n\n\n\ndata_dummy |&gt; \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 3,\n                                ms_zoning3 == \"commercial\" ~ 1,\n                                ms_zoning3 == \"floating\" ~ 2)) |&gt; \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n\n\n\n\n\n\n\n\n\nDummy coding resolves this issue.\n\nWhen using dummy codes, we transform (i.e., feature engineer) our original m-level categorical predictor to m-1 dummy features.\nEach of these m-1 features represents a contrast between a specific level of the categorical variable and a reference level\nThe full set of m-1 features represents the overall effect of the categorical predictor variable.\nWe assign values of 0 or 1 to each observation on each feature in a meaningful pattern (see below)\n\n\nFor example, with our three-level predictor: ms_zoning3\n\nWe need 2 dummmy features (d1, d2) to represent this 3-level categorical predictor\nDummy feature 1 is coded 1 for residential and 0 for all other levels\nDummy feature 2 is coded 1 for floating and 0 for all other levels\n\nHere is this coding scheme displayed in a table\n\n\n# A tibble: 3 × 3\n  ms_zoning3     d1    d2\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 commercial      0     0\n2 residential     1     0\n3 floating        0     1\n\n\n\nWith this coding:\n\nCommercial properties are coded 0 for both d1 and d2.\n\nThis means that commercial properties will become the reference level against which both residential and floating village are compared.\nBecause we are focused on prediction, the choice of reference level is mostly arbitrary. For explanatory goals, you might consider which level is best suited to be the reference.\nThere is much deeper coverage of dummy and other contrast coding in 610/710\n\n\nWe can add these two features manually to the data frame and view a handful of observations to make this coding scheme more concrete\n\n\n# A tibble: 8 × 4\n  sale_price ms_zoning3     d1    d2\n       &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1     105000 residential     1     0\n2     126000 residential     1     0\n3      13100 commercial      0     0\n4     115000 residential     1     0\n5     149500 floating        0     1\n6      40000 commercial      0     0\n7     120000 residential     1     0\n8     151000 floating        0     1\n\n\n\nIf we now fit a model where we predict sale_price from these two dummy coded features, each feature would represent the contrast of the mean sale_price for the level coded 1 vs. the mean sale_price for the level that is coded 0 for all features (i.e., commercial)\n\nd1 is the contrast of mean sale_price for residential vs. commercial\nd2 is the contrast of mean sale_price for floating vs. commercial\nThe combined effect of these two features represents the overall effect of ms_zoning3 on sale_price\n\n\nLets do this quickly in base R using lm() as you have done previously in 610.\n\nm &lt;- lm(sale_price ~ d1 + d2, data = data_dummy) \n\nm |&gt; summary()\n\n\nCall:\nlm(formula = sale_price ~ d1 + d2, data = data_dummy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-166952  -50241  -20241   31254  565259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    81523      19409   4.200 2.83e-05 ***\nd1             98219      19521   5.031 5.47e-07 ***\nd2            143223      21634   6.620 5.03e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 77640 on 1462 degrees of freedom\nMultiple R-squared:  0.03151,   Adjusted R-squared:  0.03018 \nF-statistic: 23.78 on 2 and 1462 DF,  p-value: 6.858e-11\n\n\n\nThe mean sale price of residential properties is 9.8219^{4} dollars higher than commercial properties.\nThe mean sale price of floating villages is 1.43223^{5} dollars higher than commercial properties.\n\n\nTo understand this conceptually, it is easiest to visualize the linear model that would predict sale_price with these two dichotomous features.\n\nThere are only three columns of sale_price because the only possible values for d1 and d2 (which are both dichotomous) are\n\n0,0 (commercial)\n1,0 (residential)\n0,1 (floating village)\n\nThis regression with two features yields a prediction plane (displayed)\nThe left/right tilt of the plane will be the parameter estimate for d1 and it is the contrast of residential vs. commercial\nThe front/back tilt of the plane will be the parameter estimate for d2 and it is the contrast of floating village vs. commercial\n\n\n\n\n\n\n\n\n\n\n\nStatistical sidebar:\n\nAny full rank (# levels - 1) set of features regardless of coding system predicts exactly the same (e.g., dummy, helmert, contrast coding)\nPreference among coding systems is simply to get single df contrasts of theoretical importance (i.e., for explanation rather than prediction)\nFinal (mth) dummy feature is not included b/c its is completely redundant (perfectly multicollinear) with other dummy features. This would also prevent a linear model from fitting (‘dummy variable trap’).\nHowever, some statistical algorithms do not have problems with perfect multicollinearity (e.g., LASSO, ridge regression).\n\nFor these algorithms, you will sometimes see modified version of dummy coding called one-hot coding.\n\nThis approach uses one additional dummy coded feature for the final category.\n\nWe won’t spend time on this but you should be familiar with the term b/c it is often confused with dummy coding.\n\n\n\nCoding Sidebar\nWhen creating dummy coded features from factors that have levels with infrequent observations, you may occasionally end up with novel levels in your validation or test sets that were not present in your training set.\n\nThis will cause you issues.\n\nThese issues are mostly resolved if you make sure to explicitly list all possible levels for a factor when classing that factor in the training data, even if the level doesn’t exist in the training data.\n\nWe provide more detail on this issue in an appendix.\n\n\n\n\n3.4.2 Nominal Predictors\nNow that we understand how to use dummy coding to feature engineer nominal predictors, let’s consider some potentially important ones that are available to us.\nWe can discuss if any look promising.\n\nLets return first to ms_zoning\n\ndata_trn |&gt; \n  plot_categorical(\"ms_zoning\", \"sale_price\") |&gt; \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\n\n\n\n\n\n\n\nWe might:\n\nRepresent it with 6 dummy features (because there are 7 raw levels) but many of the categories are very low n - won’t account for much variance?\nCombine all the commercial categories (agri, commer, indus), which would take care of most of the low n groups. They also all tend to have the lower prices.\nCombine all the residential to get a better feature to variance accounted ratio. They all tend to have similar prices on average and res_high is also pretty low n. \n\nData dictionary entry: Identifies the general zoning classification of the sale.\n\nagri: Agriculture\ncommer: Commercial\nfloat: Floating Village Residential\nindus: Industrial\nres_high: Residential High Density\nres_med: Residential Medium Density\nres_low: Residential Low Density\n\n\nlot_config\n\ndata_trn |&gt; \n  plot_categorical(\"lot_config\", \"sale_price\") |&gt; \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n\n\n\n\n\n\n\n\nWe see that:\n\nMost are inside lots, some of the lot categories are low n\nMedian sale_price is not very different between configurations\nNot very promising but could help some (particularly given the large sample size)\n\nData dictionary entry: Lot configuration\n\ninside: Inside lot\ncorner: Corner lot\nculdsac: Cul-de-sac\nfr2: Frontage on 2 sides of property\nfr3: Frontage on 3 sides of property\n\n\nbldg_type\n\ndata_trn |&gt; \n  plot_categorical(\"bldg_type\", \"sale_price\") |&gt; \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n\n\n\n\n\n\n\n\nWe see that:\n\nMost of the houses are in one category - one_fam\nThere is not much difference in median sale_price among categories\nNot very promising\n\nData dictionary entry: Type of dwelling\n\none_fam: Single-family Detached\n\ntwo_fam: Two-family Conversion; originally built as one-family dwelling\nduplex: Duplex\ntown_end: Townhouse End Unit\ntown_inside: Townhouse Inside Unit\n\n\nLet’s do some feature engineering with ms_zoning. We can now do this formally in a recipe so that it can be used in our modeling workflow.\n\nFirst, if you noticed earlier, there are some levels for ms_zoning that are pretty infrequent. Lets make sure both data_trn and data_val have all levels set for this factor.\n\n\ndata_trn |&gt; pull(ms_zoning) |&gt; levels()\n\n[1] \"agri\"     \"commer\"   \"float\"    \"indus\"    \"res_high\" \"res_low\"  \"res_med\" \n\ndata_val |&gt; pull(ms_zoning) |&gt; levels()\n\n[1] \"commer\"   \"float\"    \"indus\"    \"res_high\" \"res_low\"  \"res_med\" \n\n\n\nAs expected, we are missing a level (agri) in data_val. Lets fix that here\n\n\ndata_val &lt;- data_val |&gt; \n  mutate(ms_zoning = factor(ms_zoning, \n                            levels = c(\"agri\", \"commer\", \"float\", \"indus\", \n                                       \"res_high\", \"res_low\", \"res_med\")))\n\n[Note: Ideally, you would go back to cleaning EDA and add this level to the full dataset and then re-split into training, validation and test. This is a sloppy shortcut!]\n\nWith that fixed, let’s proceed:\n\nWe will collapse categories down to three levels (commercial, residential, floating village) as before but now using step_mutate() combined with fct_collapse() to do this inside of our recipe.\n\nWe will convert to dummy features using step_dummy(). The first level of the factor will be set to the reference level when we call step_dummy().\nstep_dummy() is a poor choice for function name. It actually uses whatever contrast coding we have set up in R. However, the default is are dummy coded contrasts (R calls this treatment contrasts). See ?contrasts and options(\"contrasts\") for more info.\n\n\nrec &lt;- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + ms_zoning, \n         data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\")) |&gt;\n  step_dummy(ms_zoning)\n\n\nCoding Sidebar\nYou should also read more about some other step_() functions that you might use for categorical predictors: - step_other() to combine all low frequency categories into a single “other” category. - step_unknown() to assign missing values their own category - You can use selector functions. For example, you could make dummy variables out of all of your factors in one step using step_dummy(all_nominal_predictors()).\nSee the Step Functions - Dummy Variables and Encoding section on the tidymodels website for additional useful functions.\nWe have also described these in the section on factor steps in Appendix 1\n\nLet’s see if the addition of ms_zoning helped\n\nNotice the addition of the dummy coded features to the feature matrix\nNotice the removal of the factor ms_zoning\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\n\nskim\n\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n1506.84\n511.44\n438\n1128\n1450\n1759\n5642\n1.43\n5.19\n\n\nlot_area\n0\n1\n10144.16\n8177.55\n1476\n7500\n9375\n11362\n164660\n11.20\n182.91\n\n\nyear_built\n0\n1\n1971.35\n29.65\n1880\n1953\n1972\n2000\n2010\n-0.54\n-0.62\n\n\ngarage_cars\n0\n1\n1.78\n0.76\n0\n1\n2\n2\n4\n-0.26\n0.10\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789\n129500\n160000\n213500\n745000\n1.64\n4.60\n\n\nms_zoning_floating\n0\n1\n0.05\n0.21\n0\n0\n0\n0\n1\n4.38\n17.22\n\n\nms_zoning_residential\n0\n1\n0.94\n0.23\n0\n1\n1\n1\n1\n-3.86\n12.90\n\n\n\n\nfeat_val |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_val\n\n\nNumber of rows\n490\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n1493.00\n483.78\n480\n1143.5\n1436.0\n1729.50\n3608\n0.92\n1.16\n\n\nlot_area\n0\n1\n10462.08\n10422.55\n1680\n7500.0\n9563.5\n11780.75\n215245\n15.64\n301.66\n\n\nyear_built\n0\n1\n1971.08\n30.96\n1875\n1954.0\n1975.0\n2000.00\n2010\n-0.66\n-0.41\n\n\ngarage_cars\n0\n1\n1.74\n0.76\n0\n1.0\n2.0\n2.00\n4\n-0.24\n0.22\n\n\nsale_price\n0\n1\n178512.82\n75493.59\n35311\n129125.0\n160000.0\n213000.00\n556581\n1.42\n2.97\n\n\nms_zoning_floating\n0\n1\n0.05\n0.22\n0\n0.0\n0.0\n0.00\n1\n4.07\n14.58\n\n\nms_zoning_residential\n0\n1\n0.93\n0.25\n0\n1.0\n1.0\n1.00\n1\n-3.51\n10.33\n\n\n\n\n\n\n\nNow lets fit a model with these features\n\n\nfit_lm_6 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\n\nplot it\n\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_6, feat_val)$.pred)\n\n\n\n\n\n\n\n\n\n\nAnd evaluate it\n\n\n\n# A tibble: 4 × 2\n  model                              rmse_val\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 simple linear model                  51375.\n2 4 feature linear model               39903.\n3 4 feature linear model with YJ       41660.\n4 6 feature linear model w/ms_zoning   39846.\n\n\n\nRemoving Yeo Johnson transformation but adding dummy coded ms_zoning may have helped a little\n\n\n\n\n\n\n\n\nQuestion: Will the addition of new predictors/features to a model always reduce RMSE in train? in validation?\n\n\n\n\n\n\n\nShow Answer\nAs you know, the estimation procedure in linear models is OLS.  Parameter estimates\nare derived to minimize the SSE in the data set in which they are derived.  For this\nreason, adding a predictor will never increase RMSE in the training set and it will\nusually lower it even when it is not part of the DGP.   However, this is not true in\nvalidation.  A predictor will only meaningfully lower RMSE in validation if it is\npart of the DGP.  Also, a bad predictor could even increase RMSE in validation due to\noverfitting.\n\n\n\n\n\n\n\n\n3.4.3 Ordinal Predictors\nWe have two paths to pursue for ordinal predictors\n\nWe can treat them like nominal predictors (e.g., dummy code)\nWe can treat them like numeric predictors (either raw or with an added transformation if needed)\n\n\nLet’s consider overall_qual\n\ndata_trn |&gt; \n  plot_categorical(\"overall_qual\", \"sale_price\") |&gt; \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n\n\n\n\n\n\n\n\nObservations:\n\nLow frequency for low and to some degree high quality response options. If dummy coding, may want to collapse some (1-2)\nThere is a monotonic relationship (mostly linear) with sale_price. Treat as numeric?\nNot skewed so doesn’t likely need to be transformed if treated as numeric\nNumeric will take one feature vs. many (9?) features for dummy codes.\n\nDummy codes are more flexible but we may not need this flexibility (and unnecessary flexibility increases overfitting)\n\n\nLet’s add overall_qual to our model as numeric\nRemember that this predictor was ordinal so we paid special attention to the order of the levels when we classed this factor. Lets confirm they are in order\n\ndata_trn |&gt; pull(overall_qual) |&gt; levels()\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n\n\n\nTo convert overall_qual to numeric (with levels in the specified order), we can use another simple mutate inside our recipe.\n\nrec &lt;- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |&gt;\n  step_dummy(ms_zoning)\n\nCoding Sidebar\nThere is a step function called step_ordinalscore() but it requires that the factor is classed as an ordered factor. It is also more complicated than needed in our opinion. Just use as.numeric()\n\nLet’s evaluate this model\n\nMaking features\nSkipping the skim to save space (we promised we checked it previously!)\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\n\nFitting model\n\n\nfit_lm_7 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\n\nPlotting results\n\n\nplot_truth(truth = feat_val$sale_price, \n                                 estimate = predict(fit_lm_7, feat_val)$.pred)\n\n\n\n\n\n\n\n\n\n\nQuantifying held out error\n\n\n\n# A tibble: 5 × 2\n  model                              rmse_val\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 simple linear model                  51375.\n2 4 feature linear model               39903.\n3 4 feature linear model with YJ       41660.\n4 6 feature linear model w/ms_zoning   39846.\n5 7 feature linear model               34080.\n\n\n\nThat helped!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "003_regression.html#extensions-to-interactive-models-and-non-linear-models",
    "href": "003_regression.html#extensions-to-interactive-models-and-non-linear-models",
    "title": "3  Introduction to Regression Models",
    "section": "3.5 Extensions to Interactive Models and Non-linear Models",
    "text": "3.5 Extensions to Interactive Models and Non-linear Models\n\n3.5.1 Interactions\nThere may be interactive effects among our predictors\n\nSome statistical algorithms (e.g., KNN) can naturally accommodate interactive effects without any feature engineering\nLinear models cannot\nNothing to fear, tidymodels makes it easy to feature engineer interactions\n[BUT - as we will learn, we generally think that if you expect lots of interactions, the linear model may not be the best model to use]\n\n\nFor example, it may be that the relationship between year_built and sale_price depends on overall_qual.\n\nOld houses are expensive if they are in good condition\nbut old houses are very cheap if they are in poor condition\n\n\nIn the tidymodels framework\n\nCoding interactions is done by feature engineering, not by formula (Note that formula does not change below in recipe)\nThis seems appropriate to us as we are making new features to represent interactions\nWe still use an R formula like interface to specify the interaction term features that will be created\nsee more details on the tidymodels website\n\n\nrec &lt;- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |&gt;\n  step_dummy(ms_zoning) |&gt; \n  step_interact(~ overall_qual:year_built)\n\n\nLet’s prep, bake, fit, and evaluate!\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\n\nNote the new interaction term (we just skim feat_trn here)\nNamed using “x” to specify the interaction\n\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n1506.84\n511.44\n438\n1128\n1450\n1759\n5642\n1.43\n5.19\n\n\nlot_area\n0\n1\n10144.16\n8177.55\n1476\n7500\n9375\n11362\n164660\n11.20\n182.91\n\n\nyear_built\n0\n1\n1971.35\n29.65\n1880\n1953\n1972\n2000\n2010\n-0.54\n-0.62\n\n\ngarage_cars\n0\n1\n1.78\n0.76\n0\n1\n2\n2\n4\n-0.26\n0.10\n\n\noverall_qual\n0\n1\n6.08\n1.41\n1\n5\n6\n7\n10\n0.20\n-0.03\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789\n129500\n160000\n213500\n745000\n1.64\n4.60\n\n\nms_zoning_floating\n0\n1\n0.05\n0.21\n0\n0\n0\n0\n1\n4.38\n17.22\n\n\nms_zoning_residential\n0\n1\n0.94\n0.23\n0\n1\n1\n1\n1\n-3.86\n12.90\n\n\noverall_qual_x_year_built\n0\n1\n12015.69\n2907.93\n1951\n9800\n11808\n14021\n20090\n0.24\n-0.11\n\n\n\n\n\n\n\nfit model\n\n\nfit_lm_8 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(sale_price ~., \n      data = feat_trn)\n\n\n\nplot\n\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_8, feat_val)$.pred)\n\n\n\n\n\n\n\n\n\n\ncalculate held out error\n\n\n\n# A tibble: 6 × 2\n  model                                rmse_val\n  &lt;chr&gt;                                   &lt;dbl&gt;\n1 simple linear model                    51375.\n2 4 feature linear model                 39903.\n3 4 feature linear model with YJ         41660.\n4 6 feature linear model w/ms_zoning     39846.\n5 7 feature linear model                 34080.\n6 8 feature linear model w/interaction   32720.\n\n\n\nThat helped!\n\n\nYou can also feature engineer interactions with nominal (and ordinal predictors treated as nominal) predictors\n\nThe nominal predictors should first be converted to dummy code features\nYou will indicate the interactions using the variable names that will be assigned to these dummy code features\nUse starts_with() or matches() to make it easy if there are many features associated with a categorical predictor\nCan use “~ .^2” to include all two way interactions (be careful if you have dummy coded features!)\n\n\nLet’s code an interaction between ms_zoning & year_built.\n\nOld homes are cool\nOld commercial spaces are never cool\nMaybe this is why the main effect of ms_zoning wasn’t useful\n\n\nrec &lt;- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |&gt;\n  step_dummy(ms_zoning) |&gt; \n  step_interact(~ overall_qual:year_built) |&gt; \n  step_interact(~ starts_with(\"ms_zoning_\"):year_built)  \n\n\n\nprep, bake\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\n\nYup, we have two new interaction features as expected\n\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n1506.84\n511.44\n438\n1128\n1450\n1759\n5642\n1.43\n5.19\n\n\nlot_area\n0\n1\n10144.16\n8177.55\n1476\n7500\n9375\n11362\n164660\n11.20\n182.91\n\n\nyear_built\n0\n1\n1971.35\n29.65\n1880\n1953\n1972\n2000\n2010\n-0.54\n-0.62\n\n\ngarage_cars\n0\n1\n1.78\n0.76\n0\n1\n2\n2\n4\n-0.26\n0.10\n\n\noverall_qual\n0\n1\n6.08\n1.41\n1\n5\n6\n7\n10\n0.20\n-0.03\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789\n129500\n160000\n213500\n745000\n1.64\n4.60\n\n\nms_zoning_floating\n0\n1\n0.05\n0.21\n0\n0\n0\n0\n1\n4.38\n17.22\n\n\nms_zoning_residential\n0\n1\n0.94\n0.23\n0\n1\n1\n1\n1\n-3.86\n12.90\n\n\noverall_qual_x_year_built\n0\n1\n12015.69\n2907.93\n1951\n9800\n11808\n14021\n20090\n0.24\n-0.11\n\n\nms_zoning_floating_x_year_built\n0\n1\n90.29\n415.84\n0\n0\n0\n0\n2009\n4.38\n17.22\n\n\nms_zoning_residential_x_year_built\n0\n1\n1860.03\n453.95\n0\n1948\n1968\n1997\n2010\n-3.83\n12.78\n\n\n\n\n\n\n\nFit model\n\n\nfit_lm_10 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\n\nPlot\n\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_10, feat_val)$.pred)\n\n\n\n\n\n\n\n\n\n\nQuantify held out error\n\n\nerror_val &lt;- error_val |&gt; \n  bind_rows(tibble(model = \"10 feature linear model w/interactions\", \n                   rmse_val = rmse_vec(feat_val$sale_price,\n                                       predict(fit_lm_10,\n                                               feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 7 × 2\n  model                                  rmse_val\n  &lt;chr&gt;                                     &lt;dbl&gt;\n1 simple linear model                      51375.\n2 4 feature linear model                   39903.\n3 4 feature linear model with YJ           41660.\n4 6 feature linear model w/ms_zoning       39846.\n5 7 feature linear model                   34080.\n6 8 feature linear model w/interaction     32720.\n7 10 feature linear model w/interactions   32708.\n\n\n\nNot really any better\nShouldn’t just include all interactions without reason\n\nEither you have done EDA to support them or\nYou have substantive interest in them (explanatory question)\nIf you want all interactions, use a statistical algorithm that supports those relationships without feature engineering (e.g., KNN, random forest and other decision trees)\n\n\n\n\n\n3.5.2 Non-linear Models\nWe may also want to model non-linear effects of our predictors\n\nSome non-parametric models can accommodate non-linear effects without feature engineering (e.g., KNN, Random Forest).\nNon-linear effects can be accommodated in a linear model with feature engineering\n\nTransformations of Y or X. See Step Functions - Individual Transformations on tidymodels website\nOrdinal predictors can be coded with dummy variables\nNumeric predictors can be split at threshold\nPolynomial contrasts for numeric or ordinal predictors (see step_poly())\n\nWe will continue to explore these options throughout the course",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "003_regression.html#knn-regression",
    "href": "003_regression.html#knn-regression",
    "title": "3  Introduction to Regression Models",
    "section": "3.6 KNN Regression",
    "text": "3.6 KNN Regression\nK Nearest Neighbor\n\nIs a non-parametric regression and classification statistical algorithm\n\nIt does not yield specific parameter estimates for features/predictors (or statistical tests for those parameter estimates)\nThere are still ways to use it to address explanatory questions (visualizations, model comparisons, feature importance)\n\nVery simple but also powerful (listed commonly among top 10 algorithms)\n\nBy powerful, it is quite flexible and can accommodate many varied DGPs without the need for much feature engineering with its predictors\nMay not need most transformations of X or Y\nMay not need to model interactions\nStill need to handle missing data, outliers, and categorical predictors\n\n\n\nK Nearest Neighbor\n\nAlgorithm “memorizes” the training set (lazy learning)\n\nLazy learning is most useful for large, continuously changing datasets with few attributes (features) that are commonly queried (e.g., online recommendation systems)\n\nPrediction for any new observation is based on \\(k\\) most similar observations from the dataset\n\n\\(k\\) provides direct control over the bias-variance trade-off for this algorithm\n\n\n\nTo better understand KNN let’s simulate training data for three different DGPs (linear - y, polynomial - y2, and step - y3)\n\nLet’s start with a simple example where the DGP for Y is linear on one predictor (X)\nDGP: \\(y = rnorm(150, x, 10)\\)\nThis figure displays:\n\nDGP\nPrediction line from a simple linear model\nRed lines to represent three new observations (X = 10, 50, 90) we want to make predictions for via a standard KNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What would 5-NN predictions look like for each of these three new values of X in the figure above?\n\n\n\n\n\n\n\nShow Answer\nFor x = 10, find the five observations that have X values closest to 10.  Average the \nY values for those 5 observations and that is your predicted Y associated with \nthat new value of X.  Repeat to make predictions for Y for any other value of X,\ne.g., 50, 90, or any other value\n\n\n\n\n\n\nKNN can easily accommodate non-linear relationships between numeric predictors and outcomes without any feature engineering for predictors\nIn fact, it can flexibly handle any shape of relationship\nDGP: \\(y2 = rnorm(150, x^4 / 800000, 8)\\)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nDGP: \\(y3 = if\\_else(x &lt; 40, rnorm(150, 25, 10),  rnorm(150, 75, 10))\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "003_regression.html#the-hyperparameter-k-in-knn",
    "href": "003_regression.html#the-hyperparameter-k-in-knn",
    "title": "3  Introduction to Regression Models",
    "section": "3.7 The Hyperparameter k in KNN",
    "text": "3.7 The Hyperparameter k in KNN\nKNN is our first example of a statistical algorithm that includes a hyperparameter, in this case \\(k\\)\n\nAlgorithm hyperparameters differ from parameters in that they cannot be estimated while fitting the algorithm to the training set\nThey must be set in advance\nk = 5 is the default for kknn(), the engine from the kknn package that we will use to fit a KNN within tidymodels.\n\n\\(kknn()\\) weights observations (neighbors) based on distance.\n\nAn option exists for unweighted as well but not likely used much (default is optimal weighting, use it!).\n\n\n\nUsing the polynomial DGP above, let’s look at a 5-NN yields\n\nNote the new category of algorithm, new engine, and the need to set a mode (because KNN can be used for regression and classification)\nWe can look in the package documentation to better understand what is being done (?kknn::train.kknn).\n\n\nnearest_neighbor() |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  translate()\n\nK-Nearest Neighbor Model Specification (regression)\n\nComputational engine: kknn \n\nModel fit template:\nkknn::train.kknn(formula = missing_arg(), data = missing_arg(), \n    ks = min_rows(5, data, 5))\n\n\n\n\nSet up simple feature engineering recipe and get training features (nothing happening but let’s follow normal routine anyway)\n\n\nrec &lt;- \n  recipe(y2 ~ x, data = data_trn_demo)\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn_demo)\n\nfeat_trn_demo &lt;- rec_prep |&gt; \n  bake(NULL)\n\n\n\nFit 5NN\n\n\nfit_5nn_demo &lt;- \n  nearest_neighbor() |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(y2 ~ ., data = feat_trn_demo)\n\n\n\nGet features for a validation set (a new sample using same polynomial DGP)\n\n\nfeat_val_demo &lt;- rec_prep |&gt; \n  bake(data_val_demo)\n\n\n\nDisplay 5NN predictions in validation\n\nKNN (with k = 5) does a pretty good job of representing the shape of the DGP (low bias)\nKNN displays some (but minimal) evidence of overfitting\nSimple linear model does not perform well (clear/high bias)\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s pause and consider our conceptual understanding of the impact of \\(k\\) on the bias-variance trade-off\n\n\n\n\n\n\nQuestion: How will the size of k influence model performance (e.g., bias, overfitting/variance)?\n\n\n\n\n\n\n\nShow Answer\nSmaller values of k will tend to increase overfitting (and therefore variance \nacross training samples) but decrease bias.  Larger values of k will tend to decrease\noverfitting but increase bias.  We need to find the Goldilocks \"sweet spot\"\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How will k = 1 perform in training and validation sets?\n\n\n\n\n\n\n\nShow Answer\nk = 1 will perfectly fit the training set.  Therefore it is very dependent on the training \nset (high variance).  It will fit both the DGP and the noise in the training set.  \nClearly it will likely not do as well in validation (it will be overfit to training).  \nk needs to be larger if there is more noise (to average over more cases).  k needs \nto be smaller if the relationships are complex. (More on choosing k by resampling in \nunit 5.\n\n\n\n\n\n\nk = 1\n\nFit new model\nRecipe and features have not changed\n\n\nfit_1nn_demo &lt;- \n1  nearest_neighbor(neighbors = 1) |&gt;\n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(y2 ~ ., data = feat_trn_demo)\n\n\n1\n\nSet k with neighbors =\n\n\n\n\n\nVisualize prediction models in Train and Validation\n\n\n\n\n\n\n\n\n\n\nCalculate RMSE in validation for two KNN models\nk = 1\n\nrmse_vec(feat_val_demo$y2, \n         predict(fit_1nn_demo, feat_val_demo)$.pred)\n\n[1] 10.91586\n\n\nk = 5\n\nrmse_vec(feat_val_demo$y2, \n         predict(fit_5nn_demo, feat_val_demo)$.pred)\n\n[1] 8.387035\n\n\n\nWhat if we go the other way and increase \\(k\\) to 75\n\nfit_75nn_demo &lt;- \n  nearest_neighbor(neighbors = 75) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(y2 ~ ., data = feat_trn_demo)\n\n\nVisualize prediction models in Train and Validation\n\n\n\n\n\n\n\n\n\n\nCalculate RMSE in validation for three KNN models\nThis is the bias-variance trade-off in action\n\nk = 1 - high variance\n\n\nrmse_vec(feat_val_demo$y2, \n         predict(fit_1nn_demo, feat_val_demo)$.pred)\n\n[1] 10.91586\n\n\n\nk = 5 - just right (well better at least)\n\n\nrmse_vec(feat_val_demo$y2, \n         predict(fit_5nn_demo, feat_val_demo)$.pred)\n\n[1] 8.387035\n\n\n\nk = 75 - high bias\n\n\nrmse_vec(feat_val_demo$y2, \n         predict(fit_75nn_demo, feat_val_demo)$.pred)\n\n[1] 15.34998",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "003_regression.html#distance-and-scaling-in-knn",
    "href": "003_regression.html#distance-and-scaling-in-knn",
    "title": "3  Introduction to Regression Models",
    "section": "3.8 Distance and Scaling in KNN",
    "text": "3.8 Distance and Scaling in KNN\n\n3.8.1 Defining “Nearest”\nTo make a prediction for some new observation, we need to identify the observations from the training set that are nearest to it\n\nNeed a distance measure to define “nearest”\nIMPORTANT: We care only about:\n\nDistance between a validation observation and all the training observations\nNeed to find the \\(k\\) observations in training that are nearest to the validation observation (i.e., its neighbors)\nDistance is defined based on these observations’ features, not their outcomes\n\nThere are a number of different distance measures available (e.g., Euclidean, Manhattan, Chebyshev, Cosine, Minkowski)\n\nEuclidean is most commonly used in KNN\n\n\n\nEuclidean distance between any two points is an n-dimensional extension of the Pythagorean formula (which applies explicitly with 2 features/2 dimensional space).\n\\(C^2 = A^2 + B^2\\)\n\\(C = \\sqrt{A^2 + B^2}\\)\n…where C is the distance between two points\n\nThe Euclidean distance between 2 points (p and q) in two dimensions (2 predictors, x1 = A, x2 = B)\n\\(Distance = \\sqrt{A^2 + B^2}\\)\n\\(Distance = \\sqrt{(q1 - p1)^2 + (q2 - p2)^2}\\)\n\\(Distance = \\sqrt{(2 - 1)^2 + (5 - 2)^2}\\)\n\\(Distance = 3.2\\)\n\n\n\n\n\n\n\n\n\n\nOne dimensional (one feature) is simply the subtraction of scores on that feature (x1) between p and q\n\\(Distance = \\sqrt{(q1 - p1)^2}\\)\n\\(Distance = \\sqrt{(2 - 1)^2}\\)\n\\(Distance = 1\\)\n\n\n\n\n\n\n\n\n\nN-dimensional generalization for n features:\n\\(Distance = \\sqrt{(q1 - p1)^2 + (q2 - p2)^2 + ... + (qn - pn)^2}\\)\n\nManhattan distance is also referred to as city block distance\n\nTravel down the “A” street for 1 unit\nTravel down the “B” street for 3 units\nTotal distance = 4 units\n\nFor two features/dimensions\n\\(Distance = |A + B|\\)\n\n\n\n\n\n\n\n\n\n\nkknn() uses Minkowski distance (see Wikipedia or less mathematical description)\n\nIt is a more complex parameterized distance formula\n\nThis parameter is called p, referred to as distance in kknn()\n\nEuclidean and Manhattan distances are special cases where p = 2 and 1, respectively\nThe default p in kknn() = 2 (Euclidean distance)\n\nThis default (like all defaults) can be changed when you define the algorithm using nearest_neighbor()\n\n\n\n\n3.8.2 Scaling X\nDistance is dependent on scales of all the features. We need to put all features on the same scale\n\nScale all features to SD = 1 (using step_scale(all_numeric_predictors()))\nRange correct [0, 1] all features (using step_range(all_numeric_predictors()))\n\n\n\n\n3.8.3 Categorical Predictors\nKNN requires numeric features (for distance calculation).\n\nFor categorical predictors, you will need to use dummy coding or other feature engineering that results in numeric features.\ne.g., step_dummy(all_nominal_predictors())",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "003_regression.html#knn-with-ames-housing-prices",
    "href": "003_regression.html#knn-with-ames-housing-prices",
    "title": "3  Introduction to Regression Models",
    "section": "3.9 KNN with Ames Housing Prices",
    "text": "3.9 KNN with Ames Housing Prices\nLet’s use KNN with Ames\n\nTrain a model using only numeric predictors and overall_qual as numeric\nUse the default k = 5 algorithm\nSet SD = 1 for all features\n\n\nrec &lt;- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + overall_qual, \n         data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(overall_qual = as.numeric(overall_qual)) |&gt; \n1  step_scale(all_numeric_predictors())\n\n\n1\n\nRemember to take advantage of these selectors for easier code! See ?has_role for more details\n\n\n\n\n\n\nprep, bake\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt;\n  bake(NULL)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\n\nSkim training features. Note all SD = 1\n\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n1465\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n2.95\n1.00\n0.86\n2.21\n2.84e+00\n3.44\n11.03\n1.43\n5.19\n\n\nlot_area\n0\n1\n1.24\n1.00\n0.18\n0.92\n1.15e+00\n1.39\n20.14\n11.20\n182.91\n\n\nyear_built\n0\n1\n66.48\n1.00\n63.40\n65.86\n6.65e+01\n67.45\n67.79\n-0.54\n-0.62\n\n\ngarage_cars\n0\n1\n2.33\n1.00\n0.00\n1.31\n2.62e+00\n2.62\n5.23\n-0.26\n0.10\n\n\noverall_qual\n0\n1\n4.30\n1.00\n0.71\n3.54\n4.24e+00\n4.95\n7.07\n0.20\n-0.03\n\n\nsale_price\n0\n1\n180696.15\n78836.41\n12789.00\n129500.00\n1.60e+05\n213500.00\n745000.00\n1.64\n4.60\n\n\n\n\n\n\n\nSkim validation features. Note SD. Why not exactly 1?\n\n\nfeat_val |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_val\n\n\nNumber of rows\n490\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ngr_liv_area\n0\n1\n2.92\n0.95\n0.94\n2.24\n2.81\n3.38\n7.05\n0.92\n1.16\n\n\nlot_area\n0\n1\n1.28\n1.27\n0.21\n0.92\n1.17\n1.44\n26.32\n15.64\n301.66\n\n\nyear_built\n0\n1\n66.47\n1.04\n63.23\n65.90\n66.61\n67.45\n67.79\n-0.66\n-0.41\n\n\ngarage_cars\n0\n1\n2.27\n0.99\n0.00\n1.31\n2.62\n2.62\n5.23\n-0.24\n0.22\n\n\noverall_qual\n0\n1\n4.28\n0.98\n0.71\n3.54\n4.24\n4.95\n7.07\n0.00\n0.35\n\n\nsale_price\n0\n1\n178512.82\n75493.59\n35311.00\n129125.00\n160000.00\n213000.00\n556581.00\n1.42\n2.97\n\n\n\n\n\n\n\nFit 5NN\n\n\nfit_5nn_5num &lt;- \n  nearest_neighbor() |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\n\nerror_val &lt;- bind_rows(error_val, \n                        tibble(model = \"5 numeric predictor 5nn\", \n                               rmse_val = rmse_vec(feat_val$sale_price, \n                                                   predict(fit_5nn_5num, feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 8 × 2\n  model                                  rmse_val\n  &lt;chr&gt;                                     &lt;dbl&gt;\n1 simple linear model                      51375.\n2 4 feature linear model                   39903.\n3 4 feature linear model with YJ           41660.\n4 6 feature linear model w/ms_zoning       39846.\n5 7 feature linear model                   34080.\n6 8 feature linear model w/interaction     32720.\n7 10 feature linear model w/interactions   32708.\n8 5 numeric predictor 5nn                  32837.\n\n\n\nNot bad!\n\n\nKNN also mostly solved the linearity problem\n\nWe might be able to improve the linear models with better transformations of X and Y\nHowever, this wasn’t needed for KNN!\n\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_5nn_5num, feat_val)$.pred)\n\n\n\n\n\n\n\n\n\nBut 5NN may be overfit. k = 5 is pretty low\nAgain with k = 20\n\nfit_20nn_5num &lt;- \n  nearest_neighbor(neighbors = 20) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\n\n\n# A tibble: 9 × 2\n  model                                  rmse_val\n  &lt;chr&gt;                                     &lt;dbl&gt;\n1 simple linear model                      51375.\n2 4 feature linear model                   39903.\n3 4 feature linear model with YJ           41660.\n4 6 feature linear model w/ms_zoning       39846.\n5 7 feature linear model                   34080.\n6 8 feature linear model w/interaction     32720.\n7 10 feature linear model w/interactions   32708.\n8 5 numeric predictor 5nn                  32837.\n9 5 numeric predictor 20nn                 30535.\n\n\n\nThat helped some\n\n\nOne more time with k = 50 to see where we are in the bias-variance function\n\nfit_50nn_5num &lt;- \n  nearest_neighbor(neighbors = 50) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\n\n\n# A tibble: 10 × 2\n   model                                  rmse_val\n   &lt;chr&gt;                                     &lt;dbl&gt;\n 1 simple linear model                      51375.\n 2 4 feature linear model                   39903.\n 3 4 feature linear model with YJ           41660.\n 4 6 feature linear model w/ms_zoning       39846.\n 5 7 feature linear model                   34080.\n 6 8 feature linear model w/interaction     32720.\n 7 10 feature linear model w/interactions   32708.\n 8 5 numeric predictor 5nn                  32837.\n 9 5 numeric predictor 20nn                 30535.\n10 5 numeric predictor 50nn                 31055.\n\n\n\nToo high, now we have bias……\nWe will learn a more rigorous method for selecting the optimal value for \\(k\\) (i.e., tuning this hyperparameter) in unit 5\n\n\nTo better understand bias-variance trade-off, let’s look at error across these three values of \\(k\\) in train and validation for Ames\nTraining\n\nRemember that training error would be 0 for k = 1\nTraining error is increasing as \\(k\\) increases b/c it KNN is overfitting less (so its not fitting the noise in train as well)\n\n\nrmse_vec(feat_trn$sale_price, \n         predict(fit_5nn_5num, feat_trn)$.pred)\n\n[1] 19012.94\n\nrmse_vec(feat_trn$sale_price, \n         predict(fit_20nn_5num, feat_trn)$.pred)\n\n[1] 27662.2\n\nrmse_vec(feat_trn$sale_price, \n         predict(fit_50nn_5num, feat_trn)$.pred)\n\n[1] 31069.12\n\n\n\nValidation\n\nValidation error is first going down as \\(k\\) increases (and it would have been very high for k = 1)\nBias is likely increasing a bit\nBut this is compensated by big decreases in overfitting variance\nThe trade-off is good for k = 20 relative to 5 and 1\nAt some point, as \\(k\\) increases the increase in bias outweighed the decrease in variance and validation error increased too.\n\n\nrmse_vec(feat_val$sale_price, \n         predict(fit_5nn_5num, feat_val)$.pred)\n\n[1] 32837.37\n\nrmse_vec(feat_val$sale_price, \n         predict(fit_20nn_5num, feat_val)$.pred)\n\n[1] 30535.04\n\nrmse_vec(feat_val$sale_price, \n         predict(fit_50nn_5num, feat_val)$.pred)\n\n[1] 31054.6\n\n\n\nLet’s do one final example and add one of our nominal variables into the model: ms_zoning\n\nNeed to collapse levels and then dummy\n\n\nrec &lt;- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + \n           overall_qual + ms_zoning, data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(overall_qual = as.numeric(overall_qual)) |&gt; \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\")) |&gt;\n  step_dummy(ms_zoning) |&gt; \n  step_scale(all_numeric_predictors())\n\n\n\nprep, bake\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\n\nFit\n\n\nfit_20nn_5num_mszone &lt;- \n  nearest_neighbor(neighbors = 20) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\n\nevaluate\n\n\n\n# A tibble: 11 × 2\n   model                                   rmse_val\n   &lt;chr&gt;                                      &lt;dbl&gt;\n 1 simple linear model                       51375.\n 2 4 feature linear model                    39903.\n 3 4 feature linear model with YJ            41660.\n 4 6 feature linear model w/ms_zoning        39846.\n 5 7 feature linear model                    34080.\n 6 8 feature linear model w/interaction      32720.\n 7 10 feature linear model w/interactions    32708.\n 8 5 numeric predictor 5nn                   32837.\n 9 5 numeric predictor 20nn                  30535.\n10 5 numeric predictor 50nn                  31055.\n11 5 numeric predictor 20nn with ms_zoning   30172.\n\n\n\nNow it helps.\n\nMight have to do with interactions with other predictors that we didn’t model in the linear model\nKNN automatically accommodates interactions. Why?\nThis model is a bit more complex and might benefit further from higher \\(k\\)\n\n\nAs a teaser, here is another performance metric for this model - \\(R^2\\). Not too shabby! Remember, there is certainly some irreducible error in sale_price that will put a ceiling on \\(R^2\\) and a floor on RMSE\n\nrsq_vec(feat_val$sale_price, \n        predict(fit_20nn_5num_mszone, feat_val)$.pred)\n\n[1] 0.8404044\n\n\nOverall, we now have a model that predicts housing prices with about 30K of RMSE and accounting for 84% of the variance. I am sure you can improve on this!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "003_regression.html#discussion",
    "href": "003_regression.html#discussion",
    "title": "3  Introduction to Regression Models",
    "section": "3.10 Discussion",
    "text": "3.10 Discussion\n\n3.10.1 Announcements\n\nRoom change for two days\n\nThursday, 2/15\nThursday, 2/29\nRoom 121\nIf we like it……\n\nFeedback - THANKS!\n\nconsistent feedback will be implemented (as best I can!)\ncaptions\nvocabulary and concepts - new appendix\nuse of web “book” - better thought of as lecture slides\nyou can clone book_iaml. Render to slides with your notes, render to pdf\nUse of questions for dicussion\n\nDidn’t work to read directly\nRank order based on frequency and importance\nPut some in slack\nCan’t do all. Ask in slack, ask in office hours or after discussion/lab\n\n\nReprex\n\nWe need you to try to make reprex for help on application assignments\n\nHomework is basically same for unit 4\n\nNew dataset - titanic\nDo EDA but we don’t need to see it\nFit KNN and RDA models (will learn about LDA, QDA and RDA in unit)\nSubmit predictions. Free lunch!\nAnd for this free lunch….\n\n\n\n\nread_csv(here::here(path_data, \"lunch_003.csv\")) |&gt; \n  print_kbl()\n\nRows: 20 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): name\ndbl (1): rmse_test\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\nname\nrmse_test\n\n\n\n\nDong\n25495.53\n\n\nSimental\n28733.22\n\n\nPhipps\n28794.84\n\n\nYang\n30420.01\n\n\nLi\n31534.42\n\n\nYu\n31874.12\n\n\nNA\n32606.15\n\n\nNA\n33614.59\n\n\nNA\n33877.07\n\n\nNA\n33879.19\n\n\nNA\n34315.31\n\n\nNA\n34692.22\n\n\nNA\n35034.74\n\n\nNA\n35154.67\n\n\nNA\n35683.85\n\n\nNA\n36101.95\n\n\nNA\n36266.80\n\n\nNA\n36473.84\n\n\nNA\n38974.43\n\n\nNA\n42367.31\n\n\n\n\n\n\n\n\n\n\n\n3.10.2 Sources of error\n\nWhat are two broad sources of error?\nWhat are two broad sources of reducible error. Describe what they are and factors that affect them?\nWhy do we need independent validation data to select the best model configuration?\nWhy do we need test data if we used validation data to select among many model configurations\nWhat is RMSE? Connect it to metric you already know? How is it being used in lm (two ways)?; in knn (one way)?\nHow does bias and variance manifest when you look at your performance metric (RMSE) in training and validation sets?\nWill the addition of new features to a (lm?) model always reduce RMSE in train? in validation? Connect to concepts of bias and variance\n\n\n\n\n3.10.3 KNN\n\nHow does KNN use training data to make predictions\nWhat is k and how does it get used when making predictions?\nWhat is the impact of k on bias and variance/overfitting?\nk=1: performance in train? in val?\nDistance measures: use Euclidean (default in kknn)!\nTuning K: stay “tuned”\n\n\n\n\n3.10.4 Interaction in KNN - Consider bias first (but also variance) in this example\n\nSimulate data\nFit models for lm and knn with and without interaction\nTook some shortcuts (no recipe, predict back into train)\n\n\nn &lt;- 200\nset.seed(5433)\n\nd &lt;- tibble(x1 = runif(n, 0,100), # uniform\n               x2 = rep(c(0,1), n/2), # dichotomous\n               x1_x2 = x1*x2, # interaction\n               y = rnorm(n, 0 + 1*x1 + 10*x2 + 10* x1_x2, 20)) #DGP + noise\n\nfit_lm &lt;- \n  linear_reg() |&gt;   \n  set_engine(\"lm\") |&gt;   \n  fit(y ~ x1 + x2, data = d)\n\nfit_lm_int &lt;- \n  linear_reg() |&gt;   \n  set_engine(\"lm\") |&gt;   \n  fit(y ~ x1 + x2 + x1_x2, data = d)\n\nfit_knn &lt;- \n  nearest_neighbor(neighbors = 20) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(y ~ x1 + x2, data = d)\n\nfit_knn_int &lt;- \n  nearest_neighbor(neighbors = 20) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(y ~ x1 + x2 + x1_x2, data = d)\n\nd &lt;- d |&gt; \n  mutate(pred_lm = predict(fit_lm, d)$.pred,\n         pred_lm_int = predict(fit_lm_int, d)$.pred,\n         pred_knn = predict(fit_knn, d)$.pred,\n         pred_knn_int = predict(fit_knn_int, d)$.pred)\n\n\n\nPredictions from linear model with and without interaction\n\nYou NEED interaction features with LM\n\n\n\nd |&gt; \n  ggplot(aes(x = x1, group = factor(x2), color = factor(x2))) +\n    geom_line(aes(y = pred_lm)) +\n    geom_point(aes(y = y)) +\n    ggtitle(\"lm without interaction\") +\n    ylab(\"y\") +\n    scale_color_discrete(name = \"x2\")\n\n\n\n\n\n\n\nd |&gt; \n  ggplot(aes(x = x1, group = factor(x2), color = factor(x2))) +\n    geom_line(aes(y = pred_lm_int)) +\n    geom_point(aes(y = y)) +\n    ggtitle(\"lm with interaction\") +\n    ylab(\"y\") +\n    scale_color_discrete(name = \"x2\")\n\n\n\n\n\n\n\n\n\n\nPredictions from KNN with and without interaction\n\nYou do NOT need interaction features with KNN!\n\n\n\nd |&gt; \n  ggplot(aes(x = x1, group = factor(x2), color = factor(x2))) +\n    geom_line(aes(y = pred_knn)) +\n    geom_point(aes(y = y)) +\n    ggtitle(\"KNN without interaction\") +\n    ylab(\"y\") +\n    scale_color_discrete(name = \"x2\")\n\n\n\n\n\n\n\nd |&gt; \n  ggplot(aes(x = x1, group = factor(x2), color = factor(x2))) +\n    geom_line(aes(y = pred_knn_int)) +\n    geom_point(aes(y = y)) +\n    ggtitle(\"KNN with interaction\") +\n    ylab(\"y\") +\n    scale_color_discrete(name = \"x2\")\n\n\n\n\n\n\n\n\n\n\n\n3.10.5 LM vs. KNN better with some predictors or overall?\n\n“Why do some features seem to improve performance more in linear models or only in KNNs?”\n“What are some contexts where KNN doesn’t work well? In other words, what are the advantages/disadvantages of using KNN?”\n\nAlways comes down to bias vs. variance\nFlexibility and N are key moderators of these two key factors.\n\nk? - impact on bias, variance?\n\nKNN for explanation?\n\nVisualizations (think of interaction plot above) make clear the effect\nWill learn more (better visualizations, variable importance, model comparisons) in later unit\n\n\n\n\n\n3.10.6 Normalizing transformations - Yeo Johnson\n\nwhen needed for lm?\nwhen needed for knn?\n\n\n\n\n3.10.7 Dummy coding\n\nWhy do we do it?\nDescribe the values assigned to the dummy coded features\nWhy these values? In other words, how can you interpret the effect of a dummy coded feature?\nHow is it different from one-hot coding. When to use or not use one-hot coding?\n\n\n\n\n3.10.8 Exploration\n\n“I feel that I can come up with models that decrease the RMSE, but I don’t have good priors on whether adding any particular variable or observation will result in an improved model. I still feel a little weird just adding and dropping variables into a KNN and seeing what gets the validation RMSE the lowest (even though because we’re using validation techniques it’s a fine technique)”\n\nExploration is learning. This is research. If you knew the answer you wouldn’t be doing the study\nDomain knowledge is still VERY important\nSome algorithms (LASSO, glmnet) will help with feature selection\nstaying organized\n\nScript structure\nGood documentation - QMD as analysis notebook\n\nSome overfitting to validation will occur? Consequence? Solutions?\n\n\n\n\n\n3.10.9 “Curse of dimensionality” - Bias vs. variance\n\nMissing features produce biased models.\nUnnecessary features or even many features relative to N produce variance\nDoes your available N in your algorithm support the features you need to have low bias.\n\nMostly an empirical question - can’t really tell otherwise outside of simulated data. Validation set is critical!\nFlexible models often need more N holding features constant\nRegularization (unit 6) will work well when lots of features\n\n\n\n\n\n3.10.10 Transformations of numeric predictors\n\nUse of plot_truth() [predicted vs. observed]\nResiduals do not have mean of 0 for every \\(\\hat{y}\\)\n\nConsequence: biased parameter estimates. Linear is bad DGP\nAlso bad test of questions RE the predictor (underestimate? misinform)\n\nNon-normal residuals\n\nConsequence: lm parameter estimates still unbiased (for linear DGP) but more “efficient” solutions exist\nBad for prediction b/c higher variance than other solutions\nMay suggest omission of variables\n\nHeteroscasticity\n\nConsequence: Inefficient and inaccurate standard errors.\nStatistical tests wrong\nPoor prediction for some (where larger variance of residuals) \\(\\hat{y}\\)\nhigher variance overall than other solutions - bad again for prediction\n\nTransformation of outcome?\n\nmetric\nback to raw predictions\n\n\n\n\n\n3.10.11 “In GLM, why correlation/collinearity among predictors will cause larger variance? Is it because of overfitting?”\n\n\n3.10.12 KNN (black box) for explanatory purposes\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Regression Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html",
    "href": "004_classification.html",
    "title": "4  Introduction to Classification Models",
    "section": "",
    "text": "4.1 Unit Overview",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#unit-overview",
    "href": "004_classification.html#unit-overview",
    "title": "4  Introduction to Classification Models",
    "section": "",
    "text": "4.1.1 Learning Objectives\n\nBayes classifier\nLogistic regression\n\nprobability, odds, and logit models\ndefinitions of odds and odds ratios\n\nK nearest neighbors for classification\nLinear discriminant analysis\nQuadratic discriminant analysis\nRegularized discriminant analysis\nDecision boundaries in the two feature space\nRelative costs and benefits of these different statistical algorithms\n\n\n\n\n4.1.2 Readings\n\nJames et al. (2023) Chapter 4, pp 129 - 164\n\nPost questions to the readings channel in Slack\n\n\n4.1.3 Lecture Videos\n\nLecture 1: The Bayes Classifier ~ 9 mins\nLecture 2: Conceptual Overview of Logistic Regression ~ 19 mins\nLecture 3: EDA with the Cars Dataset ~ 12 mins\nLecture 4: Logistic Regression with Cars Dataset ~ 32 mins\nLecture 5: KNN with Cars Dataset ~ 19 mins\nLecture 6: LDA, DQA, RDA with Cars Dataset ~ 16 mins\nLecture 7: Comparisons among Classifiers ~ 11 mins\n\nPost questions to the video-lectures channel in Slack\n\n\n4.1.4 Application Assignment and Quiz\n\ndata: raw; test\ndata dictionary\ncleaning EDA qmd\nrda qmd\nknn qmd\nsolution: modeling EDA rda; knn\n\nPost questions to application-assignments Slack Channel\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, February 14th\n\nOur eventual goal in this unit is for you to build a machine learning classification model that can accurately predict who lived vs. died on the titanic.\n\n\nPrior to that, we will work with an example where we classify a car as high or low fuel efficient (i.e., a dichtomous outcome based on miles per gallon) using features engineered from its characteristics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#bayes-classifier",
    "href": "004_classification.html#bayes-classifier",
    "title": "4  Introduction to Classification Models",
    "section": "4.2 Bayes Classifier",
    "text": "4.2 Bayes Classifier\nFirst, lets introduce the Bayes classifier, which is the classifier that will have the lowest error rate of all classifiers using the same set of features.\n\nThe figure below displays simulated data for a classification problem for K = 2 classes as a function of X1 and X2\n\nThe Bayes classifier assigns each observation its most likely class given its conditional probabilities for the values for X1 and X2\n\n\\(Pr(Y = k | X = x_0) for\\:k = 1:K\\)\nFor K = 2, this means assigning to the class with Pr &gt; .50\nThis decision boundary for the two class problem is displayed in the figure\n\n\nThe Bayes classifier provides the minimum error rate for test data\n\nError rate for any \\(x_0\\) will be \\(1 - max (Pr( Y = k | X = x_0))\\)\nOverall error rate will be the average of this across all possible X\nThis is the irreducible error for classification problems\nThis is a theoretical model b/c (except for simulated data), we don’t know the conditional probabilities based on X\nMany classification models try to estimate these conditionals\n\nLet’s talk now about some of these classification models",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#logistic-regression-a-conceptual-review",
    "href": "004_classification.html#logistic-regression-a-conceptual-review",
    "title": "4  Introduction to Classification Models",
    "section": "4.3 Logistic Regression: A Conceptual Review",
    "text": "4.3 Logistic Regression: A Conceptual Review\nLogistic regression (a special case of the generalized linear model) estimates the conditional probability for each class given X (a specific set of values for our features)\n\nIn the binary outcome case, we will often refer to the two outcomes as the positive class and the negative class\nThis makes most sense in some applied settings where we are most interested in predicting if one of the two classes is likely, e.g.,\n\nPresence of heart disease\nPositive for some psychiatric diagnoses\nLapse back to alcohol use in people with alcohol use disorder\n\n\n\nLogistic regression is used frequently for binary and multi-level outcomes because\n\nThe general linear model makes predictions that are not bounded by [0, 1] and do not represent true estimates of conditional probability for each class\nLogistic regression approaches can be modified to accommodate multi-class outcomes (i.e., more than two levels) even when they are not ordered\nNonetheless, the general linear model is still used at times to predict binary outcomes (see Linear Probability Model) so you should be aware of it. We won’t discuss it further here.\n\n\n\nLogistic regression provides predicted conditional probabilities for one class (positive class) for any specific set of values for our features (X)\n\n\\(Pr(positive\\:class | X) = \\frac{e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}{1 + e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}\\)\nThese conditional probabilities are bounded by [0, 1]\nTo maximize accuracy (as per Bayes classifier),\n\nwe predict the positive case if \\(Pr(positive class | X) &gt; .5\\) for any specific X\notherwise we predict the negative class\n\n\n\nAs a simple parametric model, logistic regression is commonly used for explanatory purposes as well as prediction\n\nFor these reasons, it is worthwhile to fully understand how to work with the logistic function to quantify and describe the effects of your features/predictors in terms of\n\nProbability\nOdds\n[Log-odds or logit]\nOdds ratio\n\n\nThe logistic function yields the probability of the positive class given X\n\nHowever, in some instances (e.g., horse racing, poker), it may make more sense to describe the odds of the positive case occurring rather than probability\n\n\n\nOdds are defined with respect to probabilities as follows:\n\n\\(odds = \\frac{Pr(positive\\:class|X)} {1 - Pr(positive\\:class|X)}\\)\n\n\nFor example, if the UW Badgers have a .5 probability of winning some upcoming game based on X, their odds of winning are 1 (to 1)\n\n\\(\\frac{0.5} {1 - 0.5}\\)\n\n\n\nIf the UW Badgers have a .75 probability of winning some upcoming game based on X, their odds of winning are 3 (:1; ‘3 to 1’)\n\n\\(\\frac{0.75} {1 - 0.75}\\)\n\n\n\n\n\n\n\n\nQuestion: If the UW Badgers have a .25 probability of winning some upcoming game based on X, what are their odds of winning?\n\n\n\n\n\n\n\nShow Answer\n.25 / (1 - .25) = 0.33 or 1:3\n\n\n\n\n\n\nThe logistic function can be modified to provide odds directly:\n\nLogistic function:\n\n\\(Pr(positive\\:class | X) = \\frac{e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}{1 + e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}\\)\n\n\n\nDefinition of odds:\n\n\\(odds = \\frac{Pr(positive\\:class|X)} {1 - Pr(positive\\:class|X)}\\)\n\n\n\nSubstitute logistic function for \\(Pr(positive\\:class|X)\\) on top and bottom and simplify to get:\n\n\\(odds(positive\\:class|X) = e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}\\)\nOdds are bounded by [0, \\(\\infty\\)]\n\n\nThe logistic function can be modified further such that the outcome (log-odds/logit) is a linear function of your features\n\nIf we take the natural log (base e) of both sides of the odds equation, we get:\n\n\\(log(odds(positive\\:class|X)) = \\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p\\)\nLog-odds are unbounded \\([-\\infty, \\infty]\\)\n\n\n\nUse of logit transformation:\n\nprovides the connection between the general linear model and generalized linear models (in this case with link = logit, family = binomial).\n\nNotice that the logit/log-odds is a linear combination of the features (just like in the general linear model)\n\n\nOdds and probability are descriptive but they are not linear functions of X\n\nTherefore, parameter estimates from these models aren’t very useful to describe the effect of features\nThis is because unit change in Y per unit change in any specific feature is not the same for all values of the feature\n\n\nLog-odds are a linear function of X\n\nTherefore, you can say that log-odds of positive class increases by \\(b_1\\) (our estimate of \\(\\beta_1\\)) for every one unit increase in \\(x_1\\)\nHowever, log-odds are NOT very descriptive/intuitive so they are not that useful for explanatory purposes\n\n\nThe odds ratio addresses these problems\n\nOdds are defined at a specific set of values across the features in your model. For example, with one feature:\n\n\\(odds = \\frac{Pr(positive\\:class|x_1)} {1 - Pr(positive\\:class|x_1)}\\)\n\n\n\nThe odds ratio describes the change in odds for a change of c units in your feature. With some manipulation:\n\n\\(Odds\\:ratio  =  \\frac{odds(x+c)}{odds(x)}\\)\n\\(Odds\\:ratio  =  \\frac{e^{\\beta_0 + \\beta_1*(x_1 + c)}}{e^{\\beta_0 + \\beta_1*x_1}}\\)\n\\(Odds\\:ratio  = e^{c*\\beta_1}\\)\n\n\nAs an example, if we fit a logistic regression model to predict the probability of the Badgers winning a home football game given the attendance (measured in individual spectators at the game), we might find \\(b_1\\) (our estimate of \\(\\beta_1\\)) = .000075.\n\nGiven this, the odds ratio associated with every increase in 10,000 spectators:\n\n\\(= e^{c * \\b_1}\\)\n\\(= e^{10000 * .000075}\\)\n\\(= 2.1\\)\nFor every increase of 10,000 spectators, the odds of the Badgers winning doubles",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#the-cars-dataset",
    "href": "004_classification.html#the-cars-dataset",
    "title": "4  Introduction to Classification Models",
    "section": "4.4 The Cars Dataset",
    "text": "4.4 The Cars Dataset\nNow, let’s put this all of this together in the Cars dataset from Carnegie Mellon’s StatLib Dataset Archive\n\n\nOur goal is to build a classifier (machine learning model for a categorical outcome) that classifies cars as either high or low mpg.\n\n\n4.4.1 Cleaning EDA\nLet’s start with some cleaning EDA\n\nOpen and “skim” it RE variable names, classes & missing data\nVariable names are already tidy\nno missing data\nmins (p0) and maxes(p100) for numeric look good\nthere are two character variables (mpq and name) that will need to be re-classed\n\n\ndata_all &lt;- read_csv(here::here(path_data, \"auto_all.csv\"),\n                     col_types = cols()) \n\ndata_all |&gt; skim_some()\n\n\nData summary\n\n\nName\ndata_all\n\n\nNumber of rows\n392\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nmpg\n0\n1\n3\n4\n0\n2\n0\n\n\nname\n0\n1\n6\n36\n0\n301\n0\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\ncylinders\n0\n1\n3\n8.0\n\n\ndisplacement\n0\n1\n68\n455.0\n\n\nhorsepower\n0\n1\n46\n230.0\n\n\nweight\n0\n1\n1613\n5140.0\n\n\nacceleration\n0\n1\n8\n24.8\n\n\nyear\n0\n1\n70\n82.0\n\n\norigin\n0\n1\n1\n3.0\n\n\n\n\n\n\n\nmpg is ordinal, so lets set the levels to indicate the order.\nAfter reviewing the data dictionary, we see that origin is a nominal predictor that is coded numeric (where 1 = American, 2 = European, and 3 = Japanese). Let’s recode as character with meaningful labels and then class as a factor\nand lets not forget to re-class name too\n\n\ndata_all &lt;- data_all |&gt; \n  mutate(mpg = factor(mpg, levels = c(\"low\", \"high\")),\n         name = factor(name), \n         origin = factor (origin),\n1         origin = fct_recode(as.character(origin),\n                             \"american\" = \"1\",\n                            \"european\" = \"2\",\n                            \"japanese\" = \"3\")) \n\n\n1\n\nfct_recode() works on levels stored as characters, so convert 1, 2, 3, to character first.\n\n\n\n\n\nNow, we can explore responses for categorical variables\n\nOther than name, responses for all other variables are tidy\nname has many different responses\nWe won’t know how to handle this until we get to later units in the class on natural language processing!\n\n\ndata_all |&gt; \n  select(where(is.factor)) |&gt;\n  walk(\\(column) print(levels(column)))\n\n[1] \"low\"  \"high\"\n[1] \"american\" \"european\" \"japanese\"\n  [1] \"amc ambassador brougham\"             \n  [2] \"amc ambassador dpl\"                  \n  [3] \"amc ambassador sst\"                  \n  [4] \"amc concord\"                         \n  [5] \"amc concord d/l\"                     \n  [6] \"amc concord dl 6\"                    \n  [7] \"amc gremlin\"                         \n  [8] \"amc hornet\"                          \n  [9] \"amc hornet sportabout (sw)\"          \n [10] \"amc matador\"                         \n [11] \"amc matador (sw)\"                    \n [12] \"amc pacer\"                           \n [13] \"amc pacer d/l\"                       \n [14] \"amc rebel sst\"                       \n [15] \"amc spirit dl\"                       \n [16] \"audi 100 ls\"                         \n [17] \"audi 100ls\"                          \n [18] \"audi 4000\"                           \n [19] \"audi 5000\"                           \n [20] \"audi 5000s (diesel)\"                 \n [21] \"audi fox\"                            \n [22] \"bmw 2002\"                            \n [23] \"bmw 320i\"                            \n [24] \"buick century\"                       \n [25] \"buick century 350\"                   \n [26] \"buick century limited\"               \n [27] \"buick century luxus (sw)\"            \n [28] \"buick century special\"               \n [29] \"buick electra 225 custom\"            \n [30] \"buick estate wagon (sw)\"             \n [31] \"buick lesabre custom\"                \n [32] \"buick opel isuzu deluxe\"             \n [33] \"buick regal sport coupe (turbo)\"     \n [34] \"buick skyhawk\"                       \n [35] \"buick skylark\"                       \n [36] \"buick skylark 320\"                   \n [37] \"buick skylark limited\"               \n [38] \"cadillac eldorado\"                   \n [39] \"cadillac seville\"                    \n [40] \"capri ii\"                            \n [41] \"chevroelt chevelle malibu\"           \n [42] \"chevrolet bel air\"                   \n [43] \"chevrolet camaro\"                    \n [44] \"chevrolet caprice classic\"           \n [45] \"chevrolet cavalier\"                  \n [46] \"chevrolet cavalier 2-door\"           \n [47] \"chevrolet cavalier wagon\"            \n [48] \"chevrolet chevelle concours (sw)\"    \n [49] \"chevrolet chevelle malibu\"           \n [50] \"chevrolet chevelle malibu classic\"   \n [51] \"chevrolet chevette\"                  \n [52] \"chevrolet citation\"                  \n [53] \"chevrolet concours\"                  \n [54] \"chevrolet impala\"                    \n [55] \"chevrolet malibu\"                    \n [56] \"chevrolet malibu classic (sw)\"       \n [57] \"chevrolet monte carlo\"               \n [58] \"chevrolet monte carlo landau\"        \n [59] \"chevrolet monte carlo s\"             \n [60] \"chevrolet monza 2+2\"                 \n [61] \"chevrolet nova\"                      \n [62] \"chevrolet nova custom\"               \n [63] \"chevrolet vega\"                      \n [64] \"chevrolet vega (sw)\"                 \n [65] \"chevrolet vega 2300\"                 \n [66] \"chevrolet woody\"                     \n [67] \"chevy c10\"                           \n [68] \"chevy c20\"                           \n [69] \"chevy s-10\"                          \n [70] \"chrysler cordoba\"                    \n [71] \"chrysler lebaron medallion\"          \n [72] \"chrysler lebaron salon\"              \n [73] \"chrysler lebaron town @ country (sw)\"\n [74] \"chrysler new yorker brougham\"        \n [75] \"chrysler newport royal\"              \n [76] \"datsun 1200\"                         \n [77] \"datsun 200-sx\"                       \n [78] \"datsun 200sx\"                        \n [79] \"datsun 210\"                          \n [80] \"datsun 210 mpg\"                      \n [81] \"datsun 280-zx\"                       \n [82] \"datsun 310\"                          \n [83] \"datsun 310 gx\"                       \n [84] \"datsun 510\"                          \n [85] \"datsun 510 (sw)\"                     \n [86] \"datsun 510 hatchback\"                \n [87] \"datsun 610\"                          \n [88] \"datsun 710\"                          \n [89] \"datsun 810\"                          \n [90] \"datsun 810 maxima\"                   \n [91] \"datsun b-210\"                        \n [92] \"datsun b210\"                         \n [93] \"datsun b210 gx\"                      \n [94] \"datsun f-10 hatchback\"               \n [95] \"datsun pl510\"                        \n [96] \"dodge aries se\"                      \n [97] \"dodge aries wagon (sw)\"              \n [98] \"dodge aspen\"                         \n [99] \"dodge aspen 6\"                       \n[100] \"dodge aspen se\"                      \n[101] \"dodge challenger se\"                 \n[102] \"dodge charger 2.2\"                   \n[103] \"dodge colt\"                          \n[104] \"dodge colt (sw)\"                     \n[105] \"dodge colt hardtop\"                  \n[106] \"dodge colt hatchback custom\"         \n[107] \"dodge colt m/m\"                      \n[108] \"dodge coronet brougham\"              \n[109] \"dodge coronet custom\"                \n[110] \"dodge coronet custom (sw)\"           \n[111] \"dodge d100\"                          \n[112] \"dodge d200\"                          \n[113] \"dodge dart custom\"                   \n[114] \"dodge diplomat\"                      \n[115] \"dodge magnum xe\"                     \n[116] \"dodge monaco (sw)\"                   \n[117] \"dodge monaco brougham\"               \n[118] \"dodge omni\"                          \n[119] \"dodge rampage\"                       \n[120] \"dodge st. regis\"                     \n[121] \"fiat 124 sport coupe\"                \n[122] \"fiat 124 tc\"                         \n[123] \"fiat 124b\"                           \n[124] \"fiat 128\"                            \n[125] \"fiat 131\"                            \n[126] \"fiat strada custom\"                  \n[127] \"fiat x1.9\"                           \n[128] \"ford country\"                        \n[129] \"ford country squire (sw)\"            \n[130] \"ford escort 2h\"                      \n[131] \"ford escort 4w\"                      \n[132] \"ford f108\"                           \n[133] \"ford f250\"                           \n[134] \"ford fairmont\"                       \n[135] \"ford fairmont (auto)\"                \n[136] \"ford fairmont (man)\"                 \n[137] \"ford fairmont 4\"                     \n[138] \"ford fairmont futura\"                \n[139] \"ford fiesta\"                         \n[140] \"ford futura\"                         \n[141] \"ford galaxie 500\"                    \n[142] \"ford gran torino\"                    \n[143] \"ford gran torino (sw)\"               \n[144] \"ford granada\"                        \n[145] \"ford granada ghia\"                   \n[146] \"ford granada gl\"                     \n[147] \"ford granada l\"                      \n[148] \"ford ltd\"                            \n[149] \"ford ltd landau\"                     \n[150] \"ford maverick\"                       \n[151] \"ford mustang\"                        \n[152] \"ford mustang gl\"                     \n[153] \"ford mustang ii\"                     \n[154] \"ford mustang ii 2+2\"                 \n[155] \"ford pinto\"                          \n[156] \"ford pinto (sw)\"                     \n[157] \"ford pinto runabout\"                 \n[158] \"ford ranger\"                         \n[159] \"ford thunderbird\"                    \n[160] \"ford torino\"                         \n[161] \"ford torino 500\"                     \n[162] \"hi 1200d\"                            \n[163] \"honda accord\"                        \n[164] \"honda accord cvcc\"                   \n[165] \"honda accord lx\"                     \n[166] \"honda civic\"                         \n[167] \"honda civic (auto)\"                  \n[168] \"honda civic 1300\"                    \n[169] \"honda civic 1500 gl\"                 \n[170] \"honda civic cvcc\"                    \n[171] \"honda prelude\"                       \n[172] \"maxda glc deluxe\"                    \n[173] \"maxda rx3\"                           \n[174] \"mazda 626\"                           \n[175] \"mazda glc\"                           \n[176] \"mazda glc 4\"                         \n[177] \"mazda glc custom\"                    \n[178] \"mazda glc custom l\"                  \n[179] \"mazda glc deluxe\"                    \n[180] \"mazda rx-4\"                          \n[181] \"mazda rx-7 gs\"                       \n[182] \"mazda rx2 coupe\"                     \n[183] \"mercedes benz 300d\"                  \n[184] \"mercedes-benz 240d\"                  \n[185] \"mercedes-benz 280s\"                  \n[186] \"mercury capri 2000\"                  \n[187] \"mercury capri v6\"                    \n[188] \"mercury cougar brougham\"             \n[189] \"mercury grand marquis\"               \n[190] \"mercury lynx l\"                      \n[191] \"mercury marquis\"                     \n[192] \"mercury marquis brougham\"            \n[193] \"mercury monarch\"                     \n[194] \"mercury monarch ghia\"                \n[195] \"mercury zephyr\"                      \n[196] \"mercury zephyr 6\"                    \n[197] \"nissan stanza xe\"                    \n[198] \"oldsmobile cutlass ciera (diesel)\"   \n[199] \"oldsmobile cutlass ls\"               \n[200] \"oldsmobile cutlass salon brougham\"   \n[201] \"oldsmobile cutlass supreme\"          \n[202] \"oldsmobile delta 88 royale\"          \n[203] \"oldsmobile omega\"                    \n[204] \"oldsmobile omega brougham\"           \n[205] \"oldsmobile starfire sx\"              \n[206] \"oldsmobile vista cruiser\"            \n[207] \"opel 1900\"                           \n[208] \"opel manta\"                          \n[209] \"peugeot 304\"                         \n[210] \"peugeot 504\"                         \n[211] \"peugeot 504 (sw)\"                    \n[212] \"peugeot 505s turbo diesel\"           \n[213] \"peugeot 604sl\"                       \n[214] \"plymouth 'cuda 340\"                  \n[215] \"plymouth arrow gs\"                   \n[216] \"plymouth champ\"                      \n[217] \"plymouth cricket\"                    \n[218] \"plymouth custom suburb\"              \n[219] \"plymouth duster\"                     \n[220] \"plymouth fury\"                       \n[221] \"plymouth fury gran sedan\"            \n[222] \"plymouth fury iii\"                   \n[223] \"plymouth grand fury\"                 \n[224] \"plymouth horizon\"                    \n[225] \"plymouth horizon 4\"                  \n[226] \"plymouth horizon miser\"              \n[227] \"plymouth horizon tc3\"                \n[228] \"plymouth reliant\"                    \n[229] \"plymouth sapporo\"                    \n[230] \"plymouth satellite\"                  \n[231] \"plymouth satellite custom\"           \n[232] \"plymouth satellite custom (sw)\"      \n[233] \"plymouth satellite sebring\"          \n[234] \"plymouth valiant\"                    \n[235] \"plymouth valiant custom\"             \n[236] \"plymouth volare\"                     \n[237] \"plymouth volare custom\"              \n[238] \"plymouth volare premier v8\"          \n[239] \"pontiac astro\"                       \n[240] \"pontiac catalina\"                    \n[241] \"pontiac catalina brougham\"           \n[242] \"pontiac firebird\"                    \n[243] \"pontiac grand prix\"                  \n[244] \"pontiac grand prix lj\"               \n[245] \"pontiac j2000 se hatchback\"          \n[246] \"pontiac lemans v6\"                   \n[247] \"pontiac phoenix\"                     \n[248] \"pontiac phoenix lj\"                  \n[249] \"pontiac safari (sw)\"                 \n[250] \"pontiac sunbird coupe\"               \n[251] \"pontiac ventura sj\"                  \n[252] \"renault 12 (sw)\"                     \n[253] \"renault 12tl\"                        \n[254] \"renault 5 gtl\"                       \n[255] \"saab 99e\"                            \n[256] \"saab 99gle\"                          \n[257] \"saab 99le\"                           \n[258] \"subaru\"                              \n[259] \"subaru dl\"                           \n[260] \"toyota carina\"                       \n[261] \"toyota celica gt\"                    \n[262] \"toyota celica gt liftback\"           \n[263] \"toyota corolla\"                      \n[264] \"toyota corolla 1200\"                 \n[265] \"toyota corolla 1600 (sw)\"            \n[266] \"toyota corolla liftback\"             \n[267] \"toyota corolla tercel\"               \n[268] \"toyota corona\"                       \n[269] \"toyota corona hardtop\"               \n[270] \"toyota corona liftback\"              \n[271] \"toyota corona mark ii\"               \n[272] \"toyota cressida\"                     \n[273] \"toyota mark ii\"                      \n[274] \"toyota starlet\"                      \n[275] \"toyota tercel\"                       \n[276] \"toyouta corona mark ii (sw)\"         \n[277] \"triumph tr7 coupe\"                   \n[278] \"vokswagen rabbit\"                    \n[279] \"volkswagen 1131 deluxe sedan\"        \n[280] \"volkswagen 411 (sw)\"                 \n[281] \"volkswagen dasher\"                   \n[282] \"volkswagen jetta\"                    \n[283] \"volkswagen model 111\"                \n[284] \"volkswagen rabbit\"                   \n[285] \"volkswagen rabbit custom\"            \n[286] \"volkswagen rabbit custom diesel\"     \n[287] \"volkswagen rabbit l\"                 \n[288] \"volkswagen scirocco\"                 \n[289] \"volkswagen super beetle\"             \n[290] \"volkswagen type 3\"                   \n[291] \"volvo 144ea\"                         \n[292] \"volvo 145e (sw)\"                     \n[293] \"volvo 244dl\"                         \n[294] \"volvo 245\"                           \n[295] \"volvo 264gl\"                         \n[296] \"volvo diesel\"                        \n[297] \"vw dasher (diesel)\"                  \n[298] \"vw pickup\"                           \n[299] \"vw rabbit\"                           \n[300] \"vw rabbit c (diesel)\"                \n[301] \"vw rabbit custom\"                    \n\n\n\nRemove name\n\n\ndata_all &lt;- data_all |&gt; \n  select(-name)\n\n\nFinally, let’s make and save our training and validation sets. If we were doing model building for prediction we would also need a test set but we will focus this unit on just selecting the best model but not rigorously evaluating it.\n\nLet’s use a 75/25 split, stratified on mpg\nDon’t forget to set a seed in case you need to re-split again in the future!\n\n\nset.seed(20110522) \n\nsplits &lt;- data_all |&gt; \n  initial_split(prop = .75, strata = \"mpg\")\n\nsplits |&gt; \n  analysis() |&gt; \n  glimpse() |&gt; \n  write_csv(here::here(path_data, \"auto_trn.csv\"))\n\nRows: 294\nColumns: 8\n$ mpg          &lt;fct&gt; high, high, high, high, high, high, high, high, high, hig…\n$ cylinders    &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ displacement &lt;dbl&gt; 113.0, 97.0, 97.0, 110.0, 107.0, 104.0, 121.0, 97.0, 140.…\n$ horsepower   &lt;dbl&gt; 95, 88, 46, 87, 90, 95, 113, 88, 90, 95, 86, 90, 70, 76, …\n$ weight       &lt;dbl&gt; 2372, 2130, 1835, 2672, 2430, 2375, 2234, 2130, 2264, 222…\n$ acceleration &lt;dbl&gt; 15.0, 14.5, 20.5, 17.5, 14.5, 17.5, 12.5, 14.5, 15.5, 14.…\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 7…\n$ origin       &lt;fct&gt; japanese, japanese, european, european, european, europea…\n\nsplits |&gt; \n  assessment() |&gt; \n  glimpse() |&gt; \n  write_csv(here::here(path_data, \"auto_val.csv\"))\n\nRows: 98\nColumns: 8\n$ mpg          &lt;fct&gt; low, low, low, low, low, low, low, low, low, low, low, lo…\n$ cylinders    &lt;dbl&gt; 8, 8, 8, 8, 8, 6, 6, 6, 6, 8, 8, 4, 4, 4, 8, 8, 8, 4, 8, …\n$ displacement &lt;dbl&gt; 350, 304, 302, 440, 455, 198, 200, 225, 250, 400, 318, 14…\n$ horsepower   &lt;dbl&gt; 165, 150, 140, 215, 225, 95, 85, 105, 100, 175, 150, 72, …\n$ weight       &lt;dbl&gt; 3693, 3433, 3449, 4312, 4425, 2833, 2587, 3439, 3329, 446…\n$ acceleration &lt;dbl&gt; 11.5, 12.0, 10.5, 8.5, 10.0, 15.5, 16.0, 15.5, 15.5, 11.5…\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 72, 7…\n$ origin       &lt;fct&gt; american, american, american, american, american, america…\n\n\n\n\n\n\n\n\n\nQuestion: Any concerns about using this training-validation split?\n\n\n\n\n\n\n\nShow Answer\nThese sample sizes are starting to get a little small.  Fitted models will have \nhigher variance with only 75% (N = 294) observations and performance in validation \n(with only N = 98 observations) may not be a very precise estimate of true validation \nerror.  We will learn more robust methods in the unit on resampling.\n\n\n\n\n\n\n\n\n4.4.2 Modeling EDA\nLet’s do some quick modeling EDA to get a sense of the data.\n\nWe will keep it quick and dirty.\nFirst, we should make a function to class Cars since we may open it frequently\nEven though its simple, still better this way (e.g., what if we decide to change how to handle classing - will only need to make that change in one place!)\n\n\nclass_cars &lt;- function(d) { \n  d |&gt; \n    mutate(mpg = factor(mpg, levels = c(\"low\", \"high\")),\n           origin = factor (origin))\n}\n\n\n\nOpen train (we don’t need validate for modeling EDA)\n[We are pretending this is a new script….]\nClass the predictors\n\n\ndata_trn &lt;- read_csv(here::here(path_data, \"auto_trn.csv\"),\n                     col_type = cols()) |&gt; \n  class_cars() |&gt; \n  glimpse()\n\nRows: 294\nColumns: 8\n$ mpg          &lt;fct&gt; high, high, high, high, high, high, high, high, high, hig…\n$ cylinders    &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ displacement &lt;dbl&gt; 113.0, 97.0, 97.0, 110.0, 107.0, 104.0, 121.0, 97.0, 140.…\n$ horsepower   &lt;dbl&gt; 95, 88, 46, 87, 90, 95, 113, 88, 90, 95, 86, 90, 70, 76, …\n$ weight       &lt;dbl&gt; 2372, 2130, 1835, 2672, 2430, 2375, 2234, 2130, 2264, 222…\n$ acceleration &lt;dbl&gt; 15.0, 14.5, 20.5, 17.5, 14.5, 17.5, 12.5, 14.5, 15.5, 14.…\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 7…\n$ origin       &lt;fct&gt; japanese, japanese, european, european, european, europea…\n\n\n\n\nBar plot for outcome\n\nOutcome is balanced\nUnbalanced outcomes can be more complicated (more on this later)\n\n\n\ndata_trn |&gt; plot_bar(\"mpg\")\n\n\n\n\n\n\n\n\n\n\nGrouped (by mpg) box/violin plots for numeric predictors\n\n\ndata_trn |&gt; \n  select(where(is.numeric)) |&gt; \n  names() |&gt; \n  map(\\(name) plot_grouped_box_violin(df = data_trn, x = \"mpg\", y = name)) |&gt; \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nGrouped barplot for origin\n\n\ndata_trn |&gt; \n  plot_grouped_barplot_percent(x = \"origin\", y = \"mpg\")\n\n\n\n\n\n\n\n\n\ndata_trn |&gt; \n  plot_grouped_barplot_percent(x = \"mpg\", y = \"origin\")\n\n\n\n\n\n\n\n\n\n\nCorrelation plots for numeric\nCan dummy code categorical predictors to examine correlations (point biserial, phi) including them\nWe will do this manually using tidyverse here.\nSet american to be reference level (its the first level)\ncylinders, displacement, horsepower, and weight are all essentially the same construct (big, beefy car!)\nAll are strongly correlated with mpg\n\n\ndata_trn |&gt; \n1  mutate(mpg_high = if_else(mpg == \"high\", 1, 0),\n         origin_japan = if_else(origin == \"japanese\", 1, 0),\n         origin_europe = if_else(origin == \"european\", 1, 0)) |&gt; \n2  select(-origin, -mpg) |&gt;\n  cor() |&gt; \n  corrplot::corrplot.mixed() \n\n\n1\n\nIf manually coding a binary outcome variable, best practice is to set the positive class to be 1\n\n2\n\nRemove origin and mpg after converting to dummy-coded features",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#logistic-regression---model-building",
    "href": "004_classification.html#logistic-regression---model-building",
    "title": "4  Introduction to Classification Models",
    "section": "4.5 Logistic Regression - Model Building",
    "text": "4.5 Logistic Regression - Model Building\nNow that we understand our data a little better, let’s build some models\n\nLet’s also try to simultaneously pretend that we:\n\nAre building and selecting a best prediction model that will be evaluated with some additional held out test data (how bad would it have been to split into three sets??!!)\nHave an explanatory question about production year - Are we more likely to have efficient cars more recently because of improvements in “technology”, above and beyond broad car characteristics (e.g., the easy stuff like weight, displacement, etc.)\n\n\n\nTo be clear, prediction and explanation goals are often separate (though prediction is an important foundation explanation)\n\nEither way, we need a validation set\n\nOpen it and class variables\n\n\ndata_val &lt;- read_csv(here::here(path_data, \"auto_val.csv\"),\n                     col_type = cols()) |&gt; \n  class_cars() |&gt; \n  glimpse()\n\nRows: 98\nColumns: 8\n$ mpg          &lt;fct&gt; low, low, low, low, low, low, low, low, low, low, low, lo…\n$ cylinders    &lt;dbl&gt; 8, 8, 8, 8, 8, 6, 6, 6, 6, 8, 8, 4, 4, 4, 8, 8, 8, 4, 8, …\n$ displacement &lt;dbl&gt; 350, 304, 302, 440, 455, 198, 200, 225, 250, 400, 318, 14…\n$ horsepower   &lt;dbl&gt; 165, 150, 140, 215, 225, 95, 85, 105, 100, 175, 150, 72, …\n$ weight       &lt;dbl&gt; 3693, 3433, 3449, 4312, 4425, 2833, 2587, 3439, 3329, 446…\n$ acceleration &lt;dbl&gt; 11.5, 12.0, 10.5, 8.5, 10.0, 15.5, 16.0, 15.5, 15.5, 11.5…\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 72, 7…\n$ origin       &lt;fct&gt; american, american, american, american, american, america…\n\n\n\nLet’s predict high mpg from car beefiness and year\n\nWe likely want to combine the beefy variables into one construct/factor (beef)\n\nWe can use PCA to extract one factor\nInput variables should be centered and scaled for PCA. Easy peasy\nPCA will be calculated with data_trn during prep(). These statistics from data_trn will be used to bake()\n\n\n1rec &lt;- recipe(mpg ~ ., data = data_trn) |&gt;\n  step_pca(cylinders, displacement, horsepower, weight, \n           options = list(center = TRUE, scale. = TRUE), \n           num_comp = 1, \n2           prefix = \"beef_\")\n\n\n1\n\nUsing . now in the recipe to leave all variables in the feature matrix. We can later select the ones we want use for prediction\n\n2\n\nWe will have one PCA component, it will be called beef_1\n\n\n\n\n\nNow, prep() the recipe and bake() some feature for training and validation sets\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\n\nLets skim our training features\nWe can see there are features in there we won’t use\nmpg, year, and beef_1 look good\n\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n294\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\ntop_counts\n\n\n\n\norigin\n0\n1\n3\name: 187, jap: 56, eur: 51\n\n\nmpg\n0\n1\n2\nlow: 147, hig: 147\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\nacceleration\n0\n1\n15.54\n2.64\n8.00\n14.00\n15.50\n17.0\n24.80\n0.24\n0.58\n\n\nyear\n0\n1\n75.94\n3.68\n70.00\n73.00\n76.00\n79.0\n82.00\n0.03\n-1.17\n\n\nbeef_1\n0\n1\n0.00\n1.92\n-2.36\n-1.64\n-0.85\n1.4\n4.36\n0.63\n-0.96\n\n\n\n\n\n\n\nFit the model configuration in train\n\n\nfit_lr_2 &lt;- \n1  logistic_reg() |&gt;\n2  set_engine(\"glm\") |&gt;\n3  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\n1\n\ncategory of algorithm is logistic regression\n\n2\n\nSet engine to be generalized linear model. No need to `set_mode(“classification”) because logistic regression glm is only used for classification\n\n3\n\nNotice that we explicitly indicate which features to use because our feature matrix has more than just these two in it because we used . in our recipe and our datasets have more columns.\n\n\n\n\n\nLet’s look at the logistic model and its parameter estimates from train\n\nfit_lr_2 |&gt; tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -23.4      5.55       -4.23 2.38e- 5\n2 beef_1        -2.49     0.316      -7.87 3.49e-15\n3 year           0.292    0.0718      4.07 4.71e- 5\n\n\n\n\n\n\\(Pr(high|(beef, year)) = \\frac{e^{b0 + b1*beef + b2*year}}{1 + e^{b0 + b1*beef  + b2*year}}\\)\n\nUse predict() to get \\(Pr(high|year)\\) (or predicted class, more on that later) directly from the model for any data\n\nFirst make a dataframe for feature values for prediction\nWe will use this for a plot\nGet probabilities for a range of years (incrementing by .1 year)\nHold beef constant at its mean\n\n\np_high &lt;- \n  tibble(year = seq(min(feat_val$year), max(feat_val$year), .1),\n         beef_1 = mean(feat_val$beef_1)) \n\n\npredict() will:\n\nGive us probabilities for high and low (type = \"prob\"). We will select out the low columns\nAlso provides the 95% conf int around these probabilities (type = \"conf_int\")\nAmbiguity is removed by their choice to label by the actual outcome labels (Nice!)\nWe will bind these predictions to the dataframe we made above\n\n\np_high &lt;- p_high |&gt; \n  bind_cols(predict(fit_lr_2, new_data = p_high, type = \"prob\")) |&gt; \n  bind_cols(predict(fit_lr_2, new_data = p_high, type = \"conf_int\")) |&gt; \n  select(-.pred_low, -.pred_lower_low, -.pred_upper_low) |&gt; \n  glimpse()\n\nRows: 121\nColumns: 5\n$ year             &lt;dbl&gt; 70.0, 70.1, 70.2, 70.3, 70.4, 70.5, 70.6, 70.7, 70.8,…\n$ beef_1           &lt;dbl&gt; 0.008438869, 0.008438869, 0.008438869, 0.008438869, 0…\n$ .pred_high       &lt;dbl&gt; 0.04733721, 0.04867270, 0.05004389, 0.05145161, 0.052…\n$ .pred_lower_high &lt;dbl&gt; 0.01495325, 0.01557265, 0.01621651, 0.01688572, 0.017…\n$ .pred_upper_high &lt;dbl&gt; 0.1398943, 0.1419807, 0.1440989, 0.1462495, 0.1484331…\n\n\n\nHere is a quick look at the head\n\np_high |&gt; head()\n\n# A tibble: 6 × 5\n   year  beef_1 .pred_high .pred_lower_high .pred_upper_high\n  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1  70   0.00844     0.0473           0.0150            0.140\n2  70.1 0.00844     0.0487           0.0156            0.142\n3  70.2 0.00844     0.0500           0.0162            0.144\n4  70.3 0.00844     0.0515           0.0169            0.146\n5  70.4 0.00844     0.0529           0.0176            0.148\n6  70.5 0.00844     0.0544           0.0183            0.151\n\n\n\nCan plot this relationship using the predicted probabilities\n\nplot both prediction line\nand observed Y (with a little jitter)\n\n\nggplot() +\n  geom_point(data = feat_val, aes(x = year, y = as.numeric(mpg) - 1), \n             position = position_jitter(height = 0.05, width = 0.05)) +\n  geom_line(data = p_high, mapping = aes(x = year, y = .pred_high)) +\n  geom_ribbon(data = p_high, \n              aes(x = year, ymin = .pred_lower_high, ymax = .pred_upper_high), \n              linetype = 2, alpha = 0.1) +\n  scale_x_continuous(name = \"Production Year (19XX)\", breaks = seq(70,82,2)) +\n  ylab(\"Pr(High MPG)\")\n\n\n\n\n\n\n\n\n\nCan plot odds instead of probabilities using odds equation and coefficients from the model\n\n\\(odds = \\frac{Pr(positive\\:class|x_1)} {1 - Pr(positive\\:class|x_1)}\\)\n\n\np_high &lt;- p_high |&gt; \n  mutate(.odds_high = .pred_high / (1 - .pred_high),\n         .odds_lower_high = .pred_lower_high / (1 - .pred_lower_high),\n         .odds_upper_high = .pred_upper_high / (1 - .pred_upper_high))\n\n\n\nNOTE: Harder to also plot raw data as scatter plot when plotting odds vs. probabilities. Maybe use second Y axis?\n\n\np_high |&gt; \n  ggplot() +\n    geom_line(mapping = aes(x = year, y = .odds_high)) +\n    geom_ribbon(aes(x = year, ymin = .odds_lower_high, ymax = .odds_upper_high), \n                linetype = 2, alpha = 0.1) +\n    scale_x_continuous(name = \"Production Year (19XX)\", breaks = seq(70, 82, 2)) +\n    ylab(\"Odds(High MPG)\")\n\n\n\n\n\n\n\n\n\nOdds ratio for year:\n\n\\(Odds\\:ratio  = e^{c*\\beta_1}\\)\n\n\n\nGet the parameter estimate/coefficient from the model\n\nFor every one year increase (holding beef constant), the odds of a car being categorized as high mpg increase by a factor of 1.3394301\n\n\nexp(get_estimate(fit_lr_2, \"year\"))\n\n[1] 1.33943\n\n\n\nYou can do this for other values of c as well (not really needed here because a 1 year unit makes sense)\n\n\\(Odds\\:ratio  = e^{c*\\beta_1}\\)\nFor every 10 year increase (holding beef constant), the odds of a car being categorized as high mpg increase by a factor of 18.5866278\n\n\nexp(10 * get_estimate(fit_lr_2, \"year\"))\n\n[1] 18.58663\n\n\n\nTesting parameter estimates\n\ntidy() provides z-tests for the parameter estimates\nThis is NOT the recommended statistical test from 610\n\n\nfit_lr_2 |&gt; tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -23.4      5.55       -4.23 2.38e- 5\n2 beef_1        -2.49     0.316      -7.87 3.49e-15\n3 year           0.292    0.0718      4.07 4.71e- 5\n\n\n\nThe preferred test for parameter estimates from logistic regression is the likelihood ratio test\n\nYou can get this using Anova() from the car package (as you learn in 610/710)\nThe glm object is returned using $fit\n\n\ncar::Anova(fit_lr_2$fit, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\n       LR Chisq Df Pr(&gt;Chisq)    \nbeef_1  239.721  1  &lt; 2.2e-16 ***\nyear     20.317  1  6.562e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAccuracy is a very common performance metric for classification models\n\nHow accurate is our two feature model?\n\nNote the use of type = \"class\"\n\n\n\nWe can calculate accuracy in the training set. This may be somewhat overfit and optimistic!\n\naccuracy_vec(feat_trn$mpg, predict(fit_lr_2, feat_trn, type = \"class\")$.pred_class)\n\n[1] 0.9217687\n\n\n\nAnd in the validation set (better estimate of performance in new data. Though if we use this set multiple times to select a best configuration, it will become overfit eventually too!)\n\naccuracy_vec(feat_val$mpg, predict(fit_lr_2, feat_val, type = \"class\")$.pred_class)\n\n[1] 0.8877551\n\n\n\n\nYou can see some evidence of over-fitting to the training set in that model performance is a bit lower in validation\n\nLet’s take a look at the decision boundary for this model in the validation set\n\nWe need a function to plot the decision boundary because we will use it repeatedly to compare decision boundaries across statistical models\n\nDisplayed here as another example of a function that uses quoted variable names\nThis function is useful for book examples with two features.\n\nOnly can be used with two features, so not that useful in real life!\nNot included in my function scripts (just here to helpy you understand the material)\n\n\nplot_decision_boundary &lt;- function(data, model, x_names, y_name, n_points = 100) {\n  \n  preds &lt;- crossing(X1 = seq(min(data[[x_names[1]]]), \n                                 max(data[[x_names[1]]]), \n                                 length = n_points),\n                   X2 = seq(min(data[[x_names[2]]]), \n                                 max(data[[x_names[2]]]), \n                                 length = n_points))\n  names(preds) &lt;- x_names\n  preds[[y_name]] &lt;- predict(model, preds)$.pred_class\n  preds[[y_name]] &lt;- as.numeric(preds[[y_name]])-1\n  \n  ggplot(data = data, \n         aes(x = .data[[x_names[1]]], \n             y = .data[[x_names[2]]], \n             color = .data[[y_name]])) +\n    geom_point(size = 2, alpha = .5) +\n    geom_contour(data = preds, \n                 aes(x = .data[[x_names[1]]], \n                     y = .data[[x_names[2]]], \n                     z = .data[[y_name]]), \n                 color = \"black\", breaks = .5, linewidth = 2) +\n    labs(x = x_names[1], y = x_names[2], color = y_name) +\n    coord_obs_pred()\n}\n\n\nLogistic regression produces a linear decision boundary when you consider a scatter plot of the points by the two features.\n\nPoints on one side of the line are assigned to one class and points on the other side of the line are assigned to the other class.\n\nThis decision boundary would be a plane if there were three features Harder to visualize in higher dimensional space.\nWe will contrast this decision boundary from logistic regression with other statistical algorithms in a bit.\n\n\n\nHere is the decision boundary for this model in both train and validation sets\n\np_train &lt;- feat_trn |&gt; \n  plot_decision_boundary(fit_lr_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\np_val &lt;- feat_val |&gt; \n  plot_decision_boundary(fit_lr_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\ncowplot::plot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)\n\n\n\n\n\n\n\n\n\nWhat if you wanted to try to improve our predictions?\n\nYou could find the best set of covariates to test the effect of year. [Assuming the best covariates are those that account for the most variance in mpg]?\nFor either prediction or explanation, you need to find this best model\n\n\n\nWe can compare model performance in validation set to find this best model\n\nWe can use that one for our prediction goal\nWe can test the effect of year in that model for our explanation goal\n\nThis is a principled way to decide on the best model for our explanatory goal (vs. p-hacking)\nWe get to explore and we end up with the best model to provide our focal test\n\n\n\nLet’s quickly fit another model we might have considered.\n\nThis model will contain the 4 variables from our PCA but as individual features rather than one PCA component score (beef)\nMake features for this model\n\nNo feature engineering needed because raw variables are all numeric already)\nWe will give the features dfs new names to retain the old features that included beef_1\n\n\n\nrec_raw &lt;- recipe(mpg ~ ., data = data_trn) \n\nrec_raw_prep &lt;- rec_raw |&gt; \n  prep(data_trn)\n\nfeat_raw_trn &lt;- rec_raw_prep |&gt; \n  bake(NULL)\n\nfeat_raw_val &lt;- rec_raw_prep |&gt; \n  bake(data_val)\n\n\nFit PCA features individually + year\n\nfit_lr_raw &lt;- \n  logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(mpg ~ cylinders + displacement + horsepower + weight + year, \n      data = feat_raw_trn)\n\n\nWhich is the best prediction model?\n\nBeef PCA\n\n\naccuracy_vec(feat_val$mpg, predict(fit_lr_2, feat_val)$.pred_class)\n\n[1] 0.8877551\n\n\n\nIndividual raw features from PCA\n\n\naccuracy_vec(feat_raw_val$mpg, predict(fit_lr_raw, feat_raw_val)$.pred_class)\n\n[1] 0.8673469\n\n\n\n\nThe PCA beef model fits best. The model with individual features likely increases overfitting a bit but doesn’t yield a reduction in bias because all the other variables are so highly correlated.\n\nfit_lr_2 is your choice for best model for prediction (at least it is descriptively better)\n\nIf you had an explanatory question about year, how would you have chosen between these two tests of year in these two different but all reasonable models\n\nYou might have chose the model with individual features because the effect of year is stronger.\n\nThat is NOT the model that comes closes to the DGP.\n\nWe believe the appropriate model is the beef model that has higher overall accuracy!\nThis is a start for us to start to consider the use of resampling methods to make decisions about how to best pursue explanatory goals.\nCould you now test your year effect in the full sample? Let’s discuss.\n\n\ncar::Anova(fit_lr_2$fit, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\n       LR Chisq Df Pr(&gt;Chisq)    \nbeef_1  239.721  1  &lt; 2.2e-16 ***\nyear     20.317  1  6.562e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(fit_lr_raw$fit, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\n             LR Chisq Df Pr(&gt;Chisq)    \ncylinders      0.4199  1    0.51697    \ndisplacement   1.3239  1    0.24990    \nhorsepower     6.4414  1    0.01115 *  \nweight        15.6669  1  7.553e-05 ***\nyear          27.0895  1  1.942e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#k-nearest-neighbors---a-conceptual-overview",
    "href": "004_classification.html#k-nearest-neighbors---a-conceptual-overview",
    "title": "4  Introduction to Classification Models",
    "section": "4.6 K Nearest Neighbors - A Conceptual Overview",
    "text": "4.6 K Nearest Neighbors - A Conceptual Overview\nLet’s switch gears to a non-parametric method we already know - KNN.\n\nKNN can be used as a classifier as well as for regression problems\nKNN tries to determine conditional class possibilities for any set of features by looking at observed classes for similar values of for these features in the training set\n\n\nThis figure illustrates the application of 3-NN to a small sample training set (N = 12) with 2 predictors\n\nFor test observation X in the left panel, we would predict class = blue because blue is the majority class (highest probability) among the 3 nearest training observations\nIf we calculated these probabilities for all possible combinations of the two predictors in the training set, it would yield the decision boundaries depicted in the right panel\n\n\n\nKNN can produce complex decision boundaries\n\nThis makes it flexible (can reduce bias)\nThis makes it susceptible to variance/overfitting problems\n\n\n\nRemember that we can control this bias-variance trade-off with K.\n\nAs K increases, variance reduces (but bias may increase).\n\nAs K decreases, bias may be reduced but variance increases.\nChoose a K that produces good performance in new (validation) data\n\n\nThese figures depict the KNN (and Bayes classifier) decision boundaries for the earlier simulated 2 class problem with X1 and X2\n\nK = 10 appears to provide the sweet spot b/c it closely approximates the Bayes decision boundary\nOf course, you wouldn’t know the true Bayes decision boundary if the data were real (not simulated)\nBut K = 10 would also yield the lowest test error (which is how it should be chosen)\n\nBayes classifier test error: .1304\nK = 10 test error: .1363\nK = 1 test error: .1695\nK = 100 test err: .1925\n\n\n\n\n\nYou can NOT make the decision about K based on training error\n\nThis figure depicts training and test error for simulated data example as function of 1/K\n\nTraining error decreases as 1/K increases. At 1 (K=1) training error is 0\nTest error shows expected inverted U\n\nFor high K (left side), error is high because of high variance\nAs move right (lower K), variance is reduced rapidly with little increase in bias. Error is reduced.\nEventually, there is diminishing return from reducing variance but bias starts to increase rapidly. Error increases again.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#a-return-to-cars---now-with-knn",
    "href": "004_classification.html#a-return-to-cars---now-with-knn",
    "title": "4  Introduction to Classification Models",
    "section": "4.7 A Return to Cars - Now with KNN",
    "text": "4.7 A Return to Cars - Now with KNN\nLet’s demonstrate KNN using the Cars dataset\n\nCalculate beef_1 PCA component\nScale both features\nMake train and validation feature matrices\n\n\nrec &lt;- recipe(mpg ~ ., data = data_trn) |&gt; \n  step_pca(cylinders, displacement, horsepower, weight,\n           options = list(center = TRUE, scale. = TRUE), \n           num_comp = 1, prefix = \"beef_\") |&gt; \n  step_scale(year, beef_1)\n\nrec_prep &lt;- rec |&gt; \n  prep(data = data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\nFit models with varying K.\n\nK = 1\n\n\nfit_1nn_2 &lt;- \n  nearest_neighbor(neighbors = 1) |&gt; \n  set_engine(\"kknn\") |&gt;\n1  set_mode(\"classification\") |&gt;\n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\n1\n\nNotice that we now use set_mode(\"classification\")\n\n\n\n\n\nK = 5\n\n\nfit_5nn_2 &lt;- \n  nearest_neighbor(neighbors = 5) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;   \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\nK = 10\n\n\nfit_10nn_2 &lt;- \n  nearest_neighbor(neighbors = 10) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;   \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\nK = 20\n\n\nfit_20nn_2 &lt;- \n  nearest_neighbor(neighbors = 20) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;   \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\nOf course, training accuracy goes down with decreasing flexibility as k increases\n\nK = 1\n\n\naccuracy_vec(feat_trn$mpg, predict(fit_1nn_2, feat_trn)$.pred_class)\n\n[1] 1\n\n\n\nK = 5\n\n\naccuracy_vec(feat_trn$mpg, predict(fit_5nn_2, feat_trn)$.pred_class)\n\n[1] 0.9489796\n\n\n\nK = 10\n\n\naccuracy_vec(feat_trn$mpg, predict(fit_10nn_2, feat_trn)$.pred_class)\n\n[1] 0.9455782\n\n\n\nK = 20\n\n\naccuracy_vec(feat_trn$mpg, predict(fit_20nn_2, feat_trn)$.pred_class)\n\n[1] 0.9285714\n\n\n\nIn contrast, validation accuracy first increases and then eventually decreases as k increase.\n\nK = 1\n\n\naccuracy_vec(feat_val$mpg, predict(fit_1nn_2, feat_val)$.pred_class)\n\n[1] 0.8163265\n\n\n\nK = 5\n\n\naccuracy_vec(feat_val$mpg, predict(fit_5nn_2, feat_val)$.pred_class)\n\n[1] 0.877551\n\n\n\nK = 10 (preferred based on validation accuracy)\n\n\naccuracy_vec(feat_val$mpg, predict(fit_10nn_2, feat_val)$.pred_class)\n\n[1] 0.8877551\n\n\n\nK = 20\n\n\naccuracy_vec(feat_val$mpg, predict(fit_20nn_2, feat_val)$.pred_class)\n\n[1] 0.8673469\n\n\n\nLet’s look at the decision boundaries for 10-NN in training and validation sets\n\nA very complex decision boundary\nClearly trying hard to segregate the points\nIn Ames, the relationships were non-linear and therefore KNN did much better than the linear model\nHere, the decision boundary is pretty linear so the added flexibility of KNN doesn’t get us much.\n\nMaybe we gain a little in bias reduction but lose a little in overfitting\nEnds up performing comparable to logistic regression (a generalized linear model)\n\n\n\np_train &lt;- feat_trn |&gt; \n  plot_decision_boundary(fit_10nn_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\")\n\np_val &lt;- feat_val |&gt; \n  plot_decision_boundary(fit_10nn_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\")\n\ncowplot::plot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How do you think the parametric logistic regression compares to the non-parametric KNN with respect to explanatory goals? Consider our (somewhat artificial) question about the effect of year.\n\n\n\n\n\n\n\nShow Answer\nThe logistic regression provides coefficients (parameter estimates) that can be \nused to describe changes in probability, odds and odds ratio associated with \nchange in year.  These parameter estimates can be tested via inferential procedures.  \nKNN does not provide any parameter estimates.  With KNN, we can visualize decision boundary \n(only in 2 or three dimensions) or the predicted outcome by any feature, controlling for \nother features but these relationships may be complex in shape.  \nOf course, if the relationships are complex, we might not want to hide that.  \nWe will learn more about feature importance for explanation in a later unit.\n\n\n\n\n\n\nPlot of probability of high_mpg by year, holding beef_1 constant at its mean based on 10-NN\n\nGet \\(Pr(high|year)\\) holding beef constant at its mean\npredict() returns probabilities for high and low.\n\n\np_high &lt;- \n  tibble(year = seq(min(feat_val$year), max(feat_val$year), .1),\n         beef_1 = mean(feat_val$beef_1)) \n\np_high &lt;- p_high |&gt; \n  bind_cols(predict(fit_10nn_2, p_high, type = \"prob\")) |&gt; \n  glimpse()\n\nRows: 33\nColumns: 4\n$ year       &lt;dbl&gt; 18.99759, 19.09759, 19.19759, 19.29759, 19.39759, 19.49759,…\n$ beef_1     &lt;dbl&gt; 0.004400185, 0.004400185, 0.004400185, 0.004400185, 0.00440…\n$ .pred_low  &lt;dbl&gt; 0.96, 0.99, 0.99, 0.99, 0.97, 0.87, 0.81, 0.81, 0.81, 0.81,…\n$ .pred_high &lt;dbl&gt; 0.04, 0.01, 0.01, 0.01, 0.03, 0.13, 0.19, 0.19, 0.19, 0.19,…\n\n\n\n\nPlot it\nIn a later unit, we will learn about feature ablation that we can combine with model comparisons to potentially test predictor effects in non-parametric models\n\n\nggplot() +\n  geom_point(data = feat_val, aes(x = year, y = as.numeric(mpg) - 1), \n             position = position_jitter(height = 0.05, width = 0.05)) +\n  geom_line(data = p_high, mapping = aes(x = year, y = .pred_high)) +\n  scale_x_continuous(name = \"Production Year (19XX)\", \n                     breaks = seq(0, 1, length.out = 7), \n                     labels = as.character(seq(70, 82, length.out = 7))) +\n  ylab(\"Pr(High MPG)\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#linear-discriminant-analysis",
    "href": "004_classification.html#linear-discriminant-analysis",
    "title": "4  Introduction to Classification Models",
    "section": "4.8 Linear Discriminant Analysis",
    "text": "4.8 Linear Discriminant Analysis\nLDA models the distributions of the Xs separately for each class\n\nThen uses Bayes theorem to estimate \\(Pr(Y = k | X)\\) for each k and assigns the observation to the class with the highest probability\n\\(Pr(Y = k|X) = \\frac{\\pi_k * f_k(X)}{\\sum_{l = 1}^{K} f_l(X)}\\)\nwhere\n\n\\(\\pi_k\\) is the prior probability that an observation comes from class k (estimated from frequencies of k in training)\n\\(f_k(X)\\) is the density function of X for an observation from class k\n\n\\(f_k(X)\\) is large if there is a high probability that an observation in class k has that set of values for X and small if that probability is low\n\\(f_k(X)\\) is difficult to estimate unless we make some simplifying assumptions (i.e., X is multivariate normal and common covariance matrix (\\(\\sum\\)) across K classes)\nWith these assumptions, we can estimate \\(\\pi_k\\), \\(\\mu_k\\), and \\(\\sigma^2\\) from the training set and calculate \\(Pr(Y = k|X)\\) for each k\n\n\n\nWith a single feature, the probability of any class k, given X is:\n\n\\(Pr(Y = k|X) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2)}{\\sum_{l=1}^{K}\\pi_l\\frac{1}{2\\sigma^2}\\exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)}\\)\nLDA is a parametric model, but is it interpretable?\n\n\nApplication of LDA to Cars data set with two predictors\n\nNotice that LDA produces linear decision boundary (see James et al. (2023) for formula for discriminant function derived from the probability function on last slide)\n\nrec &lt;- recipe(mpg ~ ., data = data_trn) |&gt; \n  step_pca(cylinders, displacement, horsepower, weight, \n           options = list(center = TRUE, scale. = TRUE), \n           num_comp = 1, \n           prefix = \"beef_\")\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\n\nNeed to load the discrm package\nLets look at how the function is called\n\n\nlibrary(discrim, exclude = \"smoothness\")\n\ndiscrim_linear() |&gt; \n  set_engine(\"MASS\") |&gt; \n  translate()\n\nLinear Discriminant Model Specification (classification)\n\nComputational engine: MASS \n\nModel fit template:\nMASS::lda(formula = missing_arg(), data = missing_arg())\n\n\n\nFit the LDA in train\n\nfit_lda_2 &lt;- \n  discrim_linear() |&gt; \n  set_engine(\"MASS\") |&gt; \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\nAccuracy and decision boundary\n\naccuracy_vec(feat_val$mpg, predict(fit_lda_2, feat_val)$.pred_class)\n\n[1] 0.8469388\n\n\n\np_train &lt;- feat_trn |&gt; \n  plot_decision_boundary(fit_lda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\np_val &lt;- feat_val |&gt; \n  plot_decision_boundary(fit_lda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\ncowplot::plot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#quadratic-discriminant-analysis",
    "href": "004_classification.html#quadratic-discriminant-analysis",
    "title": "4  Introduction to Classification Models",
    "section": "4.9 Quadratic Discriminant Analysis",
    "text": "4.9 Quadratic Discriminant Analysis\nQDA relaxes one restrictive assumption of LDA\n\nStill required multivariate normal X\nBut it allows each class to have its own \\(\\sum\\)\nThis makes it:\n\nMore flexible\nAble to model non-linear decision boundaries (see formula for discriminant in James et al. (2023))\nBut requires substantial increase in parameter estimation (more potential to overfit)\n\n\n\nApplication of RDA (Regularized Discriminant Analysis) algorithm to Car data set with two features\n\nThe algorithm that is available in tidymodels is actually a regularized discriminant analysis, rda() from the klaR package\nThere are two hyperparameters, frac_common_cov and frac_identity, that can each vary between 0 - 1\n\nWhen frac_common_cov = 1 and frac_identity = 0, this is an LDA\nWhen frac_common_cov = 0 and frac_identity = 0, this is a QDA\nThese hyperparameters can be tuned to different values to improve the fit dependent on the true DGP\nMore on hyperparameter tuning in unit 5\nThis is a flexible algorithm that likely replaces the need to fit separate LDA and QDA models\n\nsee https://discrim.tidymodels.org/reference/discrim_regularized.html\n\n\nHere is a true QDA using frac_common_cov = 1 and frac_identity = 0\n\ndiscrim_regularized(frac_common_cov = 0, frac_identity = 0) |&gt; \n  set_engine(\"klaR\") |&gt; \n  translate()\n\nRegularized Discriminant Model Specification (classification)\n\nMain Arguments:\n  frac_common_cov = 0\n  frac_identity = 0\n\nComputational engine: klaR \n\nModel fit template:\nklaR::rda(formula = missing_arg(), data = missing_arg(), lambda = 0, \n    gamma = 0)\n\n\n\n\nNow fit it\n\n\nfit_qda_2 &lt;- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) |&gt; \n  set_engine(\"klaR\") |&gt; \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\nAccuracy and decision boundary\n\naccuracy_vec(feat_val$mpg, \n             predict(fit_qda_2, feat_val)$.pred_class)\n\n[1] 0.877551\n\n\n\np_train &lt;- feat_trn |&gt; \n  plot_decision_boundary(fit_qda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\np_val &lt;- feat_val |&gt; \n  plot_decision_boundary(fit_qda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\ncowplot::plot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#comparisons-between-these-four-classifiers",
    "href": "004_classification.html#comparisons-between-these-four-classifiers",
    "title": "4  Introduction to Classification Models",
    "section": "4.10 Comparisons between these four classifiers",
    "text": "4.10 Comparisons between these four classifiers\n\nBoth logistic and LDA are linear functions of X and therefore produce linear decision boundaries\nLDA makes additional assumptions about X (multivariate normal and common \\(\\sum\\)) beyond logistic regression. Relative performance is based on the quality of this assumption\nQDA relaxes the LDA assumption about common \\(\\sum\\) (and RDA can relax it partially)\n\nThis also allows for nonlinear decision boundaries including 2-way interactions among features\nQDA is therefore more flexible, which means possibly less bias but more potential for overfitting\n\nBoth QDA and LDA assume multivariate normal X so may not accommodate categorical predictors very well. Logistic and KNN do accommodate categorical predictors\nKNN is non-parametric and therefore the most flexible\n\nCan also handle interactions and non-linear effects natively (with feature engineering)\nIncreased overfitting, decreased bias?\nNot very interpretable. But LDA/QDA, although parametric, aren’t as interpretable as logistic regression\n\nLogistic regression fails when classes are perfectly separated (but does that ever happen?) and is less stable when classes are well separated\nLDA, KNN, and QDA naturally accommodate more than two classes\n\nLogistic requires additional tweak (Briefly describe: multiple one vs other classes models approach)\n\nLogistic regression requires relatively large sample sizes. LDA may perform better with smaller sample sizes if assumptions are met",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#a-quick-tour-of-many-classifiers",
    "href": "004_classification.html#a-quick-tour-of-many-classifiers",
    "title": "4  Introduction to Classification Models",
    "section": "4.11 A quick tour of many classifiers",
    "text": "4.11 A quick tour of many classifiers\nThe Cars dataset had strong predictors and a mostly linear decision boundary for the two predictors we considered\n\nThis will not be true in many cases\nLet’s consider a more complex two predictor decision boundary in the circle dataset from the mlbench package (lots of cool datasets for ML)\nThis will hopefully demonstrate that the key is to have a algorithm that can model the DGP\nThere is NO best algorithm\nThe best algorithm depends on\n\nThe DGP\nThe goal (prediction vs. explanation)\n\n\n\nThis will also demonstrate the power of tidymodels to allow us to fit many different statistical algorithms which all have their own syntax using a common syntax provided by tidymodels.\n\nThis example has been adapted to tidymodels from a demonstration by Michael Hahsler\nSimulate train and test data\n\n\nlibrary(mlbench, include.only = \"mlbench.circle\")\n\nset.seed(20140102)\ndata_trn &lt;- as_tibble(mlbench.circle(200)) |&gt; \n  rename(x_1 = x.1, x_2 = x.2) |&gt; \n  glimpse()\n\nRows: 200\nColumns: 3\n$ x_1     &lt;dbl&gt; -0.514721263, 0.763312056, 0.312073042, 0.162981535, -0.320294…\n$ x_2     &lt;dbl&gt; 0.47513532, 0.65803777, -0.89824011, 0.38680494, -0.47313964, …\n$ classes &lt;fct&gt; 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2,…\n\ntest &lt;- as_tibble(mlbench.circle(200)) |&gt; \n  rename(x_1 = x.1, x_2 = x.2)\n\n\n\nPlot train data\n\n\ndata_trn |&gt; ggplot(aes(x = x_1, y = x_2, color = classes)) +\n  geom_point(size = 2, alpha = .5)\n\n\n\n\n\n\n\n\n\n\n4.11.1 Logistic Regression\n\nFit\n\n\nfit_lr_bench &lt;- \n  logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\n\n\nAccuracy and Decision Boundary\n\n\naccuracy_vec(test$classes, \n             predict(fit_lr_bench, test)$.pred_class)\n\n[1] 0.62\n\n\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_lr_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_lr_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n\n\n\n\n\n4.11.2 KNN with K = 5 (somewhat arbitrary default)\n\nFit\nNote same syntax as logistic regression (and for all others!)\n\n\nfit_knn_bench &lt;- \n  nearest_neighbor() |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\n\n\nAccuracy and Decision Boundary\nNote same syntax as logistic regression (and for all others!)\nSee what KNN can do relative to LR when the boundaries are non-linear!\n\n\naccuracy_vec(test$classes, \n             predict(fit_knn_bench, test)$.pred_class)\n\n[1] 0.985\n\n\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_knn_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) + \n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_knn_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n\n\n\n\n\n4.11.3 Linear Discriminant Analysis\n\nFit\n\n\nfit_lda_bench &lt;- \n  discrim_linear() |&gt; \n  set_engine(\"MASS\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\n\n\nAccuracy and Decision Boundary\n\n\naccuracy_vec(test$classes, \n             predict(fit_lda_bench, test)$.pred_class)\n\n[1] 0.62\n\n\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_lda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_lda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n\n\n\n\n\n4.11.4 Regularized Discriminant Analysis\n\nFit\nLetting rda() select optimal hyperparameter values\n\n\nfit_rda_bench &lt;- \n  discrim_regularized() |&gt; \n  set_engine(\"klaR\") |&gt;\n  fit(classes ~ x_1 + x_2, data = data_trn)\n\n\n\nAccuracy and Decision Boundary\nLess overfitting?\n\n\naccuracy_vec(test$classes, \n             predict(fit_rda_bench, test)$.pred_class)\n\n[1] 0.985\n\n\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_rda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) + \n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_rda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) + \n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n\n\n\n\n\n4.11.5 Naive Bayes Classifier\nThe Naïve Bayes classifier is a simple probabilistic classifier which is based on Bayes theorem\n\nBut, we assume that the predictor variables are conditionally independent of one another given the response value\nThis algorithm can be fit with either klaR or naivebayes engines\nThere are many tutorials available on this classifier\nFit it\n\n\nfit_bayes_bench &lt;- \n  naive_Bayes() |&gt; \n  set_engine(\"naivebayes\") |&gt;\n  fit(classes ~ x_1 + x_2, data = data_trn)\n\n\n\nAccuracy and Decision Boundary\n\n\naccuracy_vec(test$classes, \n             predict(fit_bayes_bench, test)$.pred_class)\n\n[1] 0.99\n\n\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_bayes_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_bayes_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n\n\n\n\n\n4.11.6 Random Forest\nRandom Forest is a variant of decision trees\n\nIt uses bagging which involves resampling the data to produce many trees and then aggregating across trees for the final classification\nWe will discuss Random Forest in a later unit\nHere we fit it with defaults for its hyperparameters\nFit it\n\n\nfit_rf_bench &lt;- \n  rand_forest() |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\n\n\nAccuracy and Decision Boundary\n\n\naccuracy_vec(test$classes, \n             predict(fit_rf_bench, test)$.pred_class)\n\n[1] 0.985\n\n\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_rf_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_rf_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n\n\n\n\n\n4.11.7 Neural networks\nIt is easy to fit a single layer neural network\n\nWe do this below with varying number of hidden units and all other hyperparameters set to defaults\nWe will have a unit on this later in the semester\n\n\n\n4.11.7.1 Single Layer NN with 1 hidden unit:\n\nFit\n\n\nfit_nn1_bench &lt;- \n mlp(hidden_units = 1) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\n\n\nAccuracy and Decision Boundary\n\n\naccuracy_vec(test$classes, \n             predict(fit_nn1_bench, test)$.pred_class)\n\n[1] 0.66\n\n\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_nn1_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_nn1_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n\n\n\n\n\n4.11.7.2 Single Layer NN with 2 hidden units\n\nFit\n\n\nfit_nn2_bench &lt;- \n mlp(hidden_units = 2) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\n\n\nAccuracy and Decision Boundary\n\n\naccuracy_vec(test$classes, \n             predict(fit_nn2_bench, test)$.pred_class)\n\n[1] 0.765\n\n\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_nn2_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) + \n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_nn2_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n\n\n\n\n\n4.11.7.3 Single Layer NN with 5 hidden units\n\nFit\n\n\nfit_nn5_bench &lt;- \n mlp(hidden_units = 5) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\n\n\nAccuracy and Decision Boundary\n\n\naccuracy_vec(test$classes, \n             predict(fit_nn5_bench, test)$.pred_class)\n\n[1] 0.96\n\n\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_nn5_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_nn5_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#discussion",
    "href": "004_classification.html#discussion",
    "title": "4  Introduction to Classification Models",
    "section": "4.12 Discussion",
    "text": "4.12 Discussion\n\n4.12.1 Anouncements\n\nQuiz performance - 95%\nPlease meet with TA or me if you can’t generate predictions from your models\nRoom 121 again in two weeks\nAnd the winner is…..\n\n\n\nread_csv(here::here(path_data, \"lunch_004.csv\")) |&gt; \n  print_kbl()\n\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): name\ndbl (1): acc_test\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\nname\nacc_test\n\n\n\n\nrf\n0.80\n\n\nwang\n0.80\n\n\nPhipps\n0.80\n\n\ndong\n0.79\n\n\nli\n0.79\n\n\njiang\n0.78\n\n\nShea\n0.77\n\n\nsimental\n0.77\n\n\ncherry\n0.77\n\n\ncrosby\n0.77\n\n\nNA\n0.77\n\n\nNA\n0.76\n\n\nNA\n0.73\n\n\nNA\n0.73\n\n\nNA\n0.72\n\n\nNA\n0.71\n\n\nNA\n0.71\n\n\nNA\n0.69\n\n\nNA\n0.68\n\n\nNA\n0.68\n\n\nNA\n0.68\n\n\nNA\n0.66\n\n\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n4.12.2 Probability, odds, and log-odds\n\nDGP on probability\nwhat is irreducible error for classification?\nDGP on X1 - draw it with varying degrees of error\n\n\n\nDGP and error on two features\n\n\n\n\nProbability vs. odds vs. log-odds\nHow to interpret parameter estimates (effects of X)\n\n\n\n\n4.12.3 Comparisons across algorithms\nLogistic regression models DGP condition probabilities using logistic function\n\nGet parameter estimates for effects of X\nMakes strong assumptions shape of DGP - linear on log-odds(Y)\nYields linear decision boundary\nBetter for binary outcomes but can do more than two levels\nNeeds numeric features but can dummy code categorical variables (as with lm)\nProblems when classes are fully separable (or even mostly separable)\n\n\nLDA uses Bayes theorem to estimate condition probability\n\nLDA models the distributions of the Xs separately for each class\nThen uses Bayes theorem to estimate \\(Pr(Y = k | X)\\) for each k and assigns the observation to the class with the highest probability\n\n\\(Pr(Y = k|X) = \\frac{\\pi_k * f_k(X)}{\\sum_{l = 1}^{K} f_l(X)}\\)\nwhere\n\n\\(\\pi_k\\) is the prior probability that an observation comes from class k (estimated from frequencies of k in training)\n\\(f_k(X)\\) is the density function of X for an observation from class k\n\n\\(f_k(X)\\) is large if there is a high probability that an observation in class k has that set of values for X and small if that probability is low\n\\(f_k(X)\\) is difficult to estimate unless we make some simplifying assumptions\nX is multivariate normal\nCommon covariance matrix (\\(\\sum\\)) across K classesj\nWith these assumptions, we can estimate \\(\\pi_k\\), \\(\\mu_k\\), and \\(\\sigma^2\\) from the training set and calculate \\(Pr(Y = k|X)\\) for each k\n\n\n\n\nParametric model but parameters not useful for interpretation of effects of X\nLinear decision boundary\n\nAssumptions about multivariate normal X and common \\(\\sum\\)\nDummy features may not work well given assumption?\nMay require smaller sample sizes to fit than logistic regression if assumptions met\nCan natively handle more than two level for outcome\n\n\nQDA relaxes one restrictive assumption of LDA\n\nStill required multivariate normal X\nBut it allows each class to have its own \\(\\sum\\)\nThis makes it:\n\nMore flexible\nAble to model non-linear decision boundaries including 2-way intearctions (see formula for discriminant in James et al. (2023))\nhow handle interactions by relaxing common \\(\\sum\\)?\nBut requires substantial increase in parameter estimation (more potential to overfit)\n\nStill problems with dummy features\nCan natively handle more than 2 levels of outcome like LDA\nCompare to LDA and Logisitic Regression on bias-variance trade off?\n\nRDA may be better than both LDA and QDA? More on idea of blending after elastic net\n\n\nKNN works similar to regression\n\nBut now looks at percentage of observations for each class among nearest neighbours to estimate conditional probabilities\nDoesn’t make assumptions about Xs or \\(\\sum\\) for LDA and/or QDA\nDoesn’t not limited to linear decision boundaries like logistic and LDA\nVery flexible - low bias but high variance?\nK can be adjusted to impact bias-variance trade-off\nKNN can handle more than two level outcomes natively",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "004_classification.html#categorical-predictors",
    "href": "004_classification.html#categorical-predictors",
    "title": "4  Introduction to Classification Models",
    "section": "4.13 Categorical predictors",
    "text": "4.13 Categorical predictors\n\nAll algorithms so far require numeric features\nOrdinal can be made numeric sometimes by just substituting ordered vector (i.e. 1, 2, 3, etc)\nNominal needs something more\nOur go to method is Dummy features\n\nWhat is big problem with dummy features?\nCollapsing levels?\nAlso issue of binary scores for LDA/QDA\n\nTarget encoding example\n\nCountry of origin for car example (but maybe think of many countries?)\nWhy not data leakage?\nProblems with step_mutate()\nCan manually do it with our current resampling\nSee embed package\n\n\n\n\n4.13.1 Interactions in LDA and QDA\n\nSimulate multivariate normal distribution for X (x1 and x2) using MASS package\nSeparately for trn and val\nNOTE: I first did this with uniform distributions on X and the models fit more poorly. Why?\n\n\nset.seed(5433)\nmeans &lt;- c(0, 0)\nsigma &lt;- diag(2) * 100\ndata_trn &lt;- MASS::mvrnorm(n = 300, mu = means, Sigma = sigma) |&gt;  \n    magrittr::set_colnames(str_c(\"x\", 1:length(means))) |&gt;  \n    as_tibble()\n\ndata_val &lt;- MASS::mvrnorm(n = 3000, mu = means, Sigma = sigma) |&gt;  \n    magrittr::set_colnames(str_c(\"x\", 1:length(means))) |&gt;  \n    as_tibble()\n\n\n\nWrite function for interactive DGP based on x1 and x2\nWill map this over rows of d\nCan specify any values for b\nb[4] will be interaction parameter estimate\n\n\nb &lt;- c(0, 0, 0, .5)\n\ncalc_p &lt;- function(x, b){\n   exp(b[1] + b[2]*x$x1 + b[3]*x$x2 + b[4]*x$x1*x$x2) /\n     (1 + exp(b[1] + b[2]*x$x1 + b[3]*x$x2 + b[4]*x$x1*x$x2))\n}\n\n\n\nAdd p and then observed classes to trn and val\n\n\ndata_trn &lt;- data_trn |&gt; \n  mutate(p = calc_p(data_trn, b)) |&gt; \n  mutate(y = rbinom(nrow(data_trn), 1, p),\n         y = factor(y, levels = 0:1, labels = c(\"neg\", \"pos\")))\n\nhead(data_trn, 10)\n\n# A tibble: 10 × 4\n        x1     x2        p y    \n     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;\n 1   9.85   -5.53 1.52e-12 neg  \n 2  -2.48    5.64 9.21e- 4 neg  \n 3  -4.74  -13.7  1.00e+ 0 pos  \n 4  -7.02   12.6  6.09e-20 neg  \n 5   0.942  -3.48 1.63e- 1 pos  \n 6   0.151   2.78 5.52e- 1 neg  \n 7   7.74   -6.84 3.24e-12 neg  \n 8  -4.85  -10.1  1.00e+ 0 pos  \n 9  -0.937   2.03 2.78e- 1 neg  \n10 -16.5     6.08 1.83e-22 neg  \n\ndata_val &lt;- data_val |&gt; \n  mutate(p = calc_p(data_val, b)) |&gt; \n  mutate(y = rbinom(nrow(data_val), 1, p),\n         y = factor(y, levels = 0:1, labels = c(\"neg\", \"pos\")))\n\n\n\nLets look at what an interactive DGP looks like for two features and a binary outcome\nParameter estimates set up a “cross-over” interaction\n\n\ndata_val |&gt; \n  ggplot(mapping = aes(x = x1, y = x2, color = y)) +\n    geom_point(size = 2, alpha = .5)\n\n\n\n\n\n\n\n\n\n\nFit models in trn\n\n\nfit_lda &lt;- \n  discrim_linear() |&gt; \n  set_engine(\"MASS\") |&gt; \n  fit(y ~ x1 + x2, data = data_trn)\n\nfit_lda_int &lt;- \n  discrim_linear() |&gt; \n  set_engine(\"MASS\") |&gt; \n  fit(y ~ x1 + x2 + x1*x2, data = data_trn)\n\nfit_qda &lt;- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) |&gt; \n  set_engine(\"klaR\") |&gt; \n  fit(y ~ x1 + x2, data = data_trn)\n\nfit_qda_int &lt;- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) |&gt; \n  set_engine(\"klaR\") |&gt; \n  fit(y ~ x1 + x2 + x1*x2, data = data_trn)\n\n\n\nAdditive LDA model decision boundary and performance in val\n\n\ndata_val |&gt; \n  plot_decision_boundary(fit_lda, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400)\n\n\n\n\n\n\n\n\n\n\nInteractive LDA model decision boundary and performance in val\n\n\ndata_val |&gt; \n  plot_decision_boundary(fit_lda_int, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400) \n\n\n\n\n\n\n\n\n\n\nAdditive QDA model decision boundary\n\n\ndata_val |&gt; \n  plot_decision_boundary(fit_qda, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400)\n\n\n\n\n\n\n\n\n\nInteractive QDA model decision boundary\n\n\n\ndata_val |&gt; \n  plot_decision_boundary(fit_qda_int, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400) \n\n\n\n\n\n\n\n\n\nCosts for QDA vs. LDA intearctive in this example and more generally with more features?\nWhat if you were using RDA, which can model the full range of models between linear and quadratic?\n\n\nPCA\n\nhttps://setosa.io/ev/principal-component-analysis/:w\nhttps://www.cs.cmu.edu/~elaw/papers/pca.pdf\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Classification Models</span>"
    ]
  },
  {
    "objectID": "005_resampling.html",
    "href": "005_resampling.html",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "",
    "text": "5.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#overview-of-unit",
    "href": "005_resampling.html#overview-of-unit",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "",
    "text": "5.1.1 Learning Objectives\n\nBias vs. variance wrt model performance estimates\n\nHow is this different from bias vs. variable of model itself\n\nMethods for computationally intense calculations\n\nParallel processing\nCache\n\nTypes of resampling\n\nValidation set approach\nLeave One Out CV\nK-Fold and Repeated K-Fold\nGrouped K-Fold\nBootstrap resampling\n\nUse of resampling for tuning hyperparameters\n\nCombining these resampling approaches with a Test set\n\nUsed for simultaneous model selection and evaluation\nSingle independent test set\nAdvanced topic: Nested resampling\n\n\n\n\n\n5.1.2 Readings\n\nKuhn and Johnson (2018) Chapter 4, pp 61 - 80\nSupplemental: James et al. (2023) Chapter 5, pp 197 - 208 186\n\nPost questions to the readings channel in Slack\n\n\n5.1.3 Lecture Videos\n\nLecture 1: Overview & Parallel Processing ~ 16 mins\nLecture 2: Introduction to Resampling ~ 11 mins\nLecture 3: Single Validation/Test Set Approach ~ 26 mins\nLecture 4: Leave One Out Cross Validation ~ 9 mins\nLecture 5: K-fold Cross Validation Approaches ~ 21 mins\nLecture 6: Repeated and Grouped K-fold Approaches ~ 11 mins\nLecture 7: Bootstrap Resampling ~ 11 mins\nLecture 8: Using Resampling to Select Best Model Configurations ~ 17 mins\nLecture 9: Resampling for Both Model Selection and Evaluation ~ 11 mins\nLecture 10: Nested Resampling ~ 14 mins\n\nPost questions to the video-lectures channel in Slack\n\n\n\n5.1.4 Application Assignment and Quiz\n\ndata\ndata dictionary\nqmd shell\nsolution\n\nPost questions to application-assignments Slack channel\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, February 21st",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#some-technical-details-for-costly-computations",
    "href": "005_resampling.html#some-technical-details-for-costly-computations",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.2 Some Technical Details for Costly Computations",
    "text": "5.2 Some Technical Details for Costly Computations\nBefore we dive into resampling, we need to introduce two coding techniques that can save us a lot of time when implementing resampling methods\n\nParallel processing\nCaching time-consuming computations\n\n\n\n5.2.1 Parallel Processing\nWhen using resampling, we often end up fitting many, many model configurations\n\nThis can be the same model configuration in many different training sets\nOr many different model configurations in many different training sets (even more computationally demanding)\n\n\n\nCritically\n\nThe fitting process for each of these configurations is independent for the others\nThe order that the configurations are fit doesn’t matter either\nWhen these two criteria are met, the processes can be run in parallel with an often big time savings\n\n\nTo do this in R, we need to set up a parallel processing backend\n\nLots of options and details depending on the code you intend to run in parallel to do it really well\nWe can discuss some of these issues/details and other solutions (i.e., High Throughput Computing at CHTC)\nSome options are OS specific\nProvide more details elsewhere\n\n\nTLDR - copy the following code chunk into your scripts after you load your other libraries (e.g., tidyverse and tidymodels)\n\ncl &lt;- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))\ndoParallel::registerDoParallel(cl)\n\n\n\n\n5.2.2 Using Cache\nEven with parallel processing, resampling procedures can STILL take a lot of time, particularly on notebook computers that don’t have a lot of cores available\n\nIn these instances, you may also want to consider caching the result\n\nWhen you cache some set of calculations, you are essentially saving the results of the calculations\nIf you need to run the script again, you simply load the saved calculations again from disk, rather than re-calculating them (its much quicker to just read them from a file)\n\n\n\nBut…\n\nYou need to redo the calculations if you change anything in your script that could affect them\nThis is called “invalidating the cache”\nYou need to be very careful to reuse the cache when you can but also to invalidate it when the calculations have changed\n\n\nIn other notes, we describe three options to cache calculations that are available in R.\n\nYou should read more about those options if you plan to use one\nOur preferred solution is to use xfun::cache_rds()\nRead the help for this function (?xfun::cache_rds) if you plan to use it\nCache is complicated and can lead to errors.\n\nBut cache can also save you a lot of time during development!\n\n\nStart by loading only that function for the xfun package. You can add this line of code after your other libraries (e.g., tidyverse, tidymodels)\n\nlibrary(xfun, include.only = \"cache_rds\")\n\n\nTo use the function\n\nYou will pass the code for the calculations you want to cache as the first argument (expr) to the function inside a set of curly brackets {}\nYou need to list the path (dir =) and filename (file =) for the rds file that will save the cached calculations.\n\nThe / at the end of the path is needed.\n\nYou should use a meaningful (and distinct) filename.\n\nProvide rerun = FALSE as a third argument.\n\nYou can set this to true temporarily if you need to invalidate the cache to redo the calculations\nWe like to set it up as an environment variable (see rerun_setting below)\nKeep it as FALSE during development\nSet it to TRUE at the end of our development so that we make sure we didn’t make any cache invalidation errors\n\nYou may also provide a list of globals to hash =. See more details at previous link\n\n\ncache_rds(\n  expr = {\n },\n dir = \"cache/\",\n file = \"filename\", \n rerun = rerun_setting \n)\n\n\n\nWe will demonstrate the use of this function throughout the book. BUT you do not need to use it if you find it confusing.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#introduction-to-resampling",
    "href": "005_resampling.html#introduction-to-resampling",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.3 Introduction to Resampling",
    "text": "5.3 Introduction to Resampling\nWe will use resampling for two goals:\n\nTo select among model configurations based on relative performance estimates of these configurations in new data\nTo evaluate the performance of our best/final model configuration in new data\n\nFor both of these goals we are using new data to estimate performance of model configuration(s)\n\nThere are two kinds of problems that can emerge from using a sub-optimal resampling approach\n\nWe can get a biased estimate of model performance (i.e., we can systematically under or over-estimate its performance)\nWe can get an imprecise estimate of model performance (i.e., high variance in our model performance metric if it was repeatedly calculated in different samples of held-out data)\n\n\nEssentially, this is the bias and variance problem again, but now not with respect to the model’s actual performance but instead with our estimate of how the model will perform\nThis is a very important distinction to keep in mind or you will be confused as we discuss bias and variance into the future. We have:\n\nbias and variance of model performance (i.e., the predictions the model makes)\nbias and variance of our estimate of how well the model will perform in new data\ndifferent factors affect each\n\n\nLet’s get a dataset for this unit. We will use the heart disease dataset from the UCI Machine Learning Repository. We will focus on the Cleveland data subset, whose variable are defined in this data dictionary\nThese data are less well prepared\n\nNo variable/column names exist\nNA is coded with ?\nUse rename() to add tidy variable names\n\n\ndata_all &lt;- read_csv(here::here(path_data, \"cleveland.csv\"), \n1                     col_names = FALSE,\n2                     na = \"?\") |&gt;\n  rename(age = X1,\n         sex = X2,\n         cp = X3,\n         rest_bp = X4,\n         chol = X5,\n         fbs = X6,\n         rest_ecg = X7,\n         max_hr = X8,\n         exer_ang = X9,\n         exer_st_depress = X10,\n         exer_st_slope = X11,\n         ca = X12,\n         thal = X13,\n         disease = X14)\n\n\n1\n\nIndicating that column names are NOT on the first row. First row begins with data\n\n2\n\nSpecifying a non-standard value for NA\n\n\n\n\nRows: 303 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nCode categorical variables as factors with meaningful text labels (and no spaces)\n\ndata_all &lt;- data_all |&gt; \n  mutate(disease = factor(disease, levels = 0:4, \n                          labels = c(\"no\", \"yes\", \"yes\", \"yes\", \"yes\")),\n         sex = factor(sex,  levels = c(0, 1), labels = c(\"female\", \"male\")),\n         fbs = factor(fbs, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_ang = factor(exer_ang, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_st_slope = factor(exer_st_slope, levels = 1:3, \n                                labels = c(\"upslope\", \"flat\", \"downslope\")),\n         cp = factor(cp, levels = 1:4, \n                     labels = c(\"typ_ang\", \"atyp_ang\", \"non_anginal\", \"non_anginal\")),\n         rest_ecg = factor(rest_ecg, levels = 0:2, \n                           labels = c(\"normal\", \"abnormal1\", \"abnormal2\")),\n         rest_ecg = fct_collapse(rest_ecg, \n                                abnormal = c(\"abnormal1\", \"abnormal2\")),\n         thal = factor(thal, levels = c(3, 6, 7), \n                       labels = c(\"normal\", \"fixeddefect\", \"reversabledefect\"))) |&gt; \n  glimpse()\n\nRows: 303\nColumns: 14\n$ age             &lt;dbl&gt; 63, 67, 67, 37, 41, 56, 62, 57, 63, 53, 57, 56, 56, 44…\n$ sex             &lt;fct&gt; male, male, male, male, female, male, female, female, …\n$ cp              &lt;fct&gt; typ_ang, non_anginal, non_anginal, non_anginal, atyp_a…\n$ rest_bp         &lt;dbl&gt; 145, 160, 120, 130, 130, 120, 140, 120, 130, 140, 140,…\n$ chol            &lt;dbl&gt; 233, 286, 229, 250, 204, 236, 268, 354, 254, 203, 192,…\n$ fbs             &lt;fct&gt; yes, no, no, no, no, no, no, no, no, yes, no, no, yes,…\n$ rest_ecg        &lt;fct&gt; abnormal, abnormal, abnormal, normal, abnormal, normal…\n$ max_hr          &lt;dbl&gt; 150, 108, 129, 187, 172, 178, 160, 163, 147, 155, 148,…\n$ exer_ang        &lt;fct&gt; no, yes, yes, no, no, no, no, yes, no, yes, no, no, ye…\n$ exer_st_depress &lt;dbl&gt; 2.3, 1.5, 2.6, 3.5, 1.4, 0.8, 3.6, 0.6, 1.4, 3.1, 0.4,…\n$ exer_st_slope   &lt;fct&gt; downslope, flat, flat, downslope, upslope, upslope, do…\n$ ca              &lt;dbl&gt; 0, 3, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ thal            &lt;fct&gt; fixeddefect, normal, reversabledefect, normal, normal,…\n$ disease         &lt;fct&gt; no, yes, yes, no, no, no, yes, no, yes, yes, no, no, y…\n\n\n\nWe won’t do EDA in this unit but lets at least do a quick skim to inform ourselves\n\n303 cases\na dichotomous outcome, disease (yes or no for heart disease)\n7 other categorical predictors\n6 numeric predictors\n2 missing values for thal, which is categorical\n4 missing values for ca, which is numeric\n\n\ndata_all |&gt; skim_all()\n\n\nData summary\n\n\nName\ndata_all\n\n\nNumber of rows\n303\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n8\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\ntop_counts\n\n\n\n\nsex\n0\n1.00\n2\nmal: 206, fem: 97\n\n\ncp\n0\n1.00\n3\nnon: 230, aty: 50, typ: 23\n\n\nfbs\n0\n1.00\n2\nno: 258, yes: 45\n\n\nrest_ecg\n0\n1.00\n2\nabn: 152, nor: 151\n\n\nexer_ang\n0\n1.00\n2\nno: 204, yes: 99\n\n\nexer_st_slope\n0\n1.00\n3\nups: 142, fla: 140, dow: 21\n\n\nthal\n2\n0.99\n3\nnor: 166, rev: 117, fix: 18\n\n\ndisease\n0\n1.00\n2\nno: 164, yes: 139\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\nage\n0\n1.00\n54.44\n9.04\n29\n48.0\n56.0\n61.0\n77.0\n-0.21\n-0.55\n\n\nrest_bp\n0\n1.00\n131.69\n17.60\n94\n120.0\n130.0\n140.0\n200.0\n0.70\n0.82\n\n\nchol\n0\n1.00\n246.69\n51.78\n126\n211.0\n241.0\n275.0\n564.0\n1.12\n4.35\n\n\nmax_hr\n0\n1.00\n149.61\n22.88\n71\n133.5\n153.0\n166.0\n202.0\n-0.53\n-0.09\n\n\nexer_st_depress\n0\n1.00\n1.04\n1.16\n0\n0.0\n0.8\n1.6\n6.2\n1.26\n1.50\n\n\nca\n4\n0.99\n0.67\n0.94\n0\n0.0\n0.0\n1.0\n3.0\n1.18\n0.21\n\n\n\n\n\n\nWe will be fitting a logistic regression with all of the predictors for the first half of this unit\n\nLets set up a recipe for feature engineering with this statistical algorithm\n\nImpute missing data for all numeric predictors using median imputation\nImpute missing data for all nominal predictors using the modal value\nDummy code all nominal predictors\n\n\nrec_lr &lt;- recipe(disease ~ ., data = data_all) |&gt; \n  step_impute_median(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt;   \n  step_dummy(all_nominal_predictors()) \n\n\nThe order of steps in a recipe matter\n\nWhile your project’s needs may vary, here is a suggested order of potential steps that should work for most problems according to tidy models folks:\n\n[Convert character to factor] (we do this outside our recipe as part of cleaning)\nImpute\nIndividual transformations for skewness and other issues\nDiscretize (if needed and if you have no other choice)\nCreate dummy variables\nCreate interactions\nNormalization steps (center, scale, range, etc)\nMultivariate transformation (e.g. PCA, spatial sign, etc)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#the-single-validation-test-set-approach",
    "href": "005_resampling.html#the-single-validation-test-set-approach",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.4 The single validation (test) set approach",
    "text": "5.4 The single validation (test) set approach\nTo date, you have essentially learned how to do the single validation set approach (although we haven’t called it that)\n\nWith this approach, we would take our full n = 303 and:\n\nSplit into one training set and one held-out set\nFit a model in our training set\nUse this trained model to predict scores in held-out set\nCalculate a performance metric (e.g., accuracy, rmse) based on predicted and observed scores in the held-out set\n\n\nIf our goal was to evaluate the expected performance of a single model configuration in new data\n\nWe called this held-out set a test set\nWe would report this performance metric from the held-out test set as our estimate of the performance of our model in new data\n\n\nIf our goal was to select the best model configuration among many candidate configurations\n\nWe called this held-out set a validation set\nWe would use this performance metric from the held-out validation set to select the best model configuration\n\n\nWe call this the single validation set approach but that single held-out set can be either a validation or test set depending on our goals\n\nIf you need to BOTH select a best model configuration AND evaluate that best model configuration, you would need both a validation and a test set.\n\nWe have been doing the single validation set approach all along but we will provide one more example now (with a 50/50 split) to transition the code we are using to a more general workflow that will accommodate our more complicated resampling approaches\n\nIn the first half of this unit, we will focus on assessing the performance of a single model configuration\n\nLogistic regression algorithm\nNo hyperparameters\nFeatures based on all available predictors\n\nWe will call the held-out set a test set and use it to evaluate the expected future performance of this single configuration\n\nPreviously:\n\nWe would fit the model configuration in training and then made predictions for observations in the held-out test set in separate steps\nWe did this in separate steps so you could better understand the process\nI will show you that first again as a baseline\n\nThen:\n\nWe will now do these tasks in one step using \\(validation\\_split()\\)\nI will show you this combined approach second\nThis latter approach will be an example for how we code this for our more complicated resampling approaches\n\n\n\nLet’s do a 50/50 split, stratified on our outcome, disease\n\n\nset.seed(19690127)\n\nsplits &lt;- data_all |&gt; \n  initial_split(prop = 0.5, strata = \"disease\")\n\ndata_trn &lt;- analysis(splits)\ndata_trn |&gt;  nrow()\n\n[1] 151\n\ndata_test &lt;- assessment(splits)\ndata_test |&gt; nrow()\n\n[1] 152\n\n\n\n\nMake features for train and test (skim them on your own time!)\n\n\nrec_prep &lt;- rec_lr |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_test &lt;- rec_prep |&gt; \n  bake(data_test)\n\n\n\nFit model in train\n\n\nfit_lr &lt;-\n  logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(disease ~ ., data = feat_trn)\n\n\n\nEvaluate model in test\n\n\naccuracy_vec(feat_test$disease, predict(fit_lr, feat_test, type = \"class\")$.pred_class)\n\n[1] 0.8355263\n\n\n\nNow lets do this in a new and more efficient workflow\n\nWe still start by settting up a splits object\nNote use of splits_validate() rather than initial_split()\nWe will use a variety of functions at this step depending on how we decide to handle resampling\n\n\nset.seed(19690127)\nsplits_validate &lt;- data_all |&gt; \n  validation_split(prop = 0.5, strata = \"disease\")\n\nWarning: `validation_split()` was deprecated in rsample 1.2.0.\nℹ Please use `initial_validation_split()` instead.\n\n\n\nNow we can fit our model configuration in our training set(s) and calculate performance metric(s) in the held-out sets using fit_resamples()\n\nYou can and should read more about this function\nTakes algorithm (broad category, engine, and mode if needed), recipe, and splits as inputs\nSpecify the (set of) metrics we want to use to estimate for our model configuration\nDon’t need to explicitly create feature matrices for held-in and held-out sets.\n\nBut also don’t see these feature matrices\nMay still want to create and skim them as a check?\n\n\n\nfits_lr &lt;-\n  logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit_resamples(preprocessor = rec_lr, resamples = splits_validate, \n                 metrics = metric_set(accuracy))\n\n\nThe object (we will call it fits_) that is returned in NOT a model using our model figuration (what we got using fit(), which we called fit_)\n\nInstead, it contains the performance metrics for the configuration, estimated by\n\nFitting the model configuration in the held in set(s) and then\nPredicting into the held-out sets\n\n\nWe pull these performance estimates out of the fits object using collect_metrics()\n\nThere is one performance estimate\nIt is for our model configurations performance in test set\nIt is an estimate of how well our model configuration will work with new data\nIt matches what we got previously when doing this manually\n\n\nfits_lr |&gt; \n  collect_metrics(summarize = FALSE)\n\n# A tibble: 1 × 5\n  id         .metric  .estimator .estimate .config             \n  &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 validation accuracy binary         0.836 Preprocessor1_Model1\n\n\n\n\n\n\n\n\n\nQuestion: How many participants were used to fit the model that we used to estimate the performance of our model configuration?\n\n\n\n\n\n\n\nShow Answer\nThe model configuration was fit in the training set. The training set had N = 151 participants.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: If we planned to implement this model (i.e., really use it in practice to predict heart disease in new patients) is this the best model we can develop or can we improve it?\n\n\n\n\n\n\n\nShow Answer\nThis model was trained with N = 151 but we have 303 participants.  If we trained\nthe same model configuration with all of our data, that model would be expected\nto performance better than the N-151 model.  \n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Why will the N = 303 model always be better (or at worst equivalent) to the model trained with N = 151.\n\n\n\n\n\n\n\nShow Answer\nIncreasing the sample size used to fit our model configuration will decrease\nmodel variance but not change model bias. This will produce overall lower \nerror.\n\nThis might not be true if the additional data were not similar quality to our\ntraining data but we know our validation/test set is similar because we did a\nrandom resample.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: So why did we not just fit this model configuration using N = 303 to start?\n\n\n\n\n\n\n\nShow Answer\nBecause then we would not have had any new data left to get an estimate of its performance in new data!\n\n\n\n\n\n\nIf you plan to actually use your model in the real world for prediction, you should always re-fit the best configuration using all available data!\n\n\n\n\n\n\nQuestion: But what does this mean about our estimate of the performance of this final model (fit with all available data) data when we get that estimate using a model that as fit with a smaller sample size (the sample size in our training set)?\n\n\n\n\n\n\n\nShow Answer\nOur estimate will likely be biased.  It will underestimate the true expected \npeformance of our final model.  We can think of it as a lower bound on that expected\nperformance.  The amount of bias will be a function of the difference between the \nsample size of the training set and the size the of full dataset.   If we want less \nbiased estimates, we want to allocate as much data as possible to the training set \nwhen estimating the performance of our final model configuration (but this will come \nwith other costs!)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Contrast the costs/benefits of a 50/50 vs. 80/20 split for train and test\n\n\n\n\n\n\n\nShow Answer\nUsing a training set with 80% of the sample will yield a less biased (under)\nestimate of the final (using all data) model performance than a training set with\n50% of the sample.  \n\nHowever, using a test set of 20% of the data will produce a \nmore variable (less precise) estimate of performance than the 50% test set.  \n\nThis  is another bias-variance trade off but now instead of talking about model\nperformance, we are seeing that we have to trade off bias and variance in our\nestimate of the model performance too!\n\n\n\n\n\nThis recognition of a bias-variance trade-off in our performance estimates is what motivates the more complicated resampling approaches we will now consider.\n\nIn our example, we plan to use this model for future predictions, so now lets fit it a final time using the full dataset\n\nWe do this manually\nNOTE: tidymodels has routines to do all of this including fitting final models (read more about “workflows”)\nWe do not use them in the course because they hide steps that are important for conceptual understanding\nWe do not use them in our lab because we break apart all of these steps to train models using high throughput computing\n\n\nMake a feature matrix for the full dataset\n\nWe are now using the full data set as our new training set so we prep and bake with the full dataset\n\nrec_prep &lt;- rec_lr |&gt; \n  prep(data_all)\n\nfeat_all &lt;- rec_prep |&gt; \n  bake(data_all)\n\n\nAnd then fit your model configuration\n\nfit_lr &lt;-\n  logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(disease ~ ., data = feat_all)\n\n\n\nfit_lr |&gt; tidy()\n\n# A tibble: 17 × 5\n   term                    estimate std.error statistic     p.value\n   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 (Intercept)             -5.06      2.79       -1.81  0.0704     \n 2 age                     -0.0185    0.0235     -0.787 0.431      \n 3 rest_bp                  0.0240    0.0109      2.20  0.0279     \n 4 chol                     0.00414   0.00376     1.10  0.271      \n 5 max_hr                  -0.0204    0.0104     -1.96  0.0497     \n 6 exer_st_depress          0.275     0.212       1.30  0.195      \n 7 ca                       1.32      0.263       5.00  0.000000566\n 8 sex_male                 1.43      0.489       2.92  0.00345    \n 9 cp_atyp_ang              1.05      0.752       1.40  0.162      \n10 cp_non_anginal           1.24      0.602       2.06  0.0396     \n11 fbs_yes                 -0.812     0.522      -1.55  0.120      \n12 rest_ecg_abnormal        0.469     0.364       1.29  0.197      \n13 exer_ang_yes             1.24      0.401       3.10  0.00195    \n14 exer_st_slope_flat       1.08      0.442       2.43  0.0150     \n15 exer_st_slope_downslope  0.468     0.819       0.572 0.568      \n16 thal_fixeddefect         0.262     0.759       0.345 0.730      \n17 thal_reversabledefect    1.41      0.397       3.54  0.000396   \n\n\nIf we need to predict disease in the future, this is the model we would use (with these parameter estimates)\n\nOur estimate of its future accuracy is based on our previous assessment using the held-in training set to fit the model configuration and the held-out test set to estimate its performance\n\nThis estimate should be considered a lower bound on its expected performance",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#leave-one-out-cross-validation",
    "href": "005_resampling.html#leave-one-out-cross-validation",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.5 Leave One Out Cross Validation",
    "text": "5.5 Leave One Out Cross Validation\nLet’s turn to a new resampling technique and start with some questions to motivate it\n\n\n\n\n\n\nQuestion: How could you use this single validation set approach to get the least biased estimate of model performance with your n = 303 dataset that would still allow you to estimate its performance in a held out test set?\n\n\n\n\n\n\n\nshow answer\nPut all but one case into the training set (i.e., leave only one case out in the\ntest set).  In our example, you would fit a model with n = 302  this model will \nhave essentially equivalent overfitting as n = 303 so it will not yield much bias \nwhen we use it to estimate the performance of the n = 303 model.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What will be the biggest problem with this approach?\n\n\n\n\n\n\n\nShow Answer\nYou will estimate performance with only n = 1 in the test set.  This means there \nwill be high variance in your performance estimate.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How might you reduce this problem?\n\n\n\n\n\n\n\nShow Answer\nRepeat this split between training and test n times so that there are n different\nsets of n = 1 test sets.  Then average the performance across all n of these test \nsets to get a more stable estimate of performance. Averaging is a good way to reduce \nthe variance of any estimate.  \n\nThis is leave one out cross-validation!\n\n\n\n\n\n\nComparisons across LOOCV and single validation set approaches\n\nThe performance estimate from LOOCV has less bias than the single validation set method (because the models that are used to estimate performance were fit with close to the full n of the final model that will be fit to all the data)\nLOOCV uses all observations in the held-out set at some point. This may yield less variance than single 20% or 50% validation set?\n\nbut…\n\nLOOCV can be computationally expensive (need to fit and evaluate the same model configuration n times).\n\nThis is a real problem when you are also working with a high number of model configurations (i.e., number fits = n * number of model configurations).\n\n\nLOOCV eventually uses all the data in the held-out set across the ‘n’ held-out sets.\n\nAveraging also helps reduce variance in the performance metric.\nHowever, averaging reduces variance to a greater degree when the performance measures being averaged are less related/more independent.\nThe n fitted models are very similar in LOOCV b/c they are each fit on almost the same data (each with n-1 observations)\n\n\n\nK-fold cross validation (next method) improves the variance of the average performance metric by averaging across more independent (less overlapping) training sets\n\nFor this reason, it is superior and (always?) preferred over LOOCV\nWe are not demonstrating LOOCV b/c we strongly prefer other methods (k-fold)\nStill important to understand it conceptually and its strengths/weaknesses\nIf you wanted to use this resampling approach, simply substitute loo_cv() for vfold_cv() in the next example",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#k-fold-cross-validation",
    "href": "005_resampling.html#k-fold-cross-validation",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.6 K-fold Cross Validation",
    "text": "5.6 K-fold Cross Validation\nK-fold cross validation\n\nDivide the observations into K equal size independent “folds” (each observation appears in only one fold)\nHold out 1 of these folds (1/Kth of the dataset) to use as a held-out set\nFit a model in the remaining K-1 folds\nRepeat until each of the folds has been held out once\nPerformance estimate is the average performance across the K held-out folds\n\n\n\nCommon values of K are 5 and 10\n\nNote that K is sometimes referred to as V in some fields/literatures (Don’t blame me!)\n\nVisualization of K-fold\n\n\nLet’s demonstrate the code for K-fold Cross-validation\n\nFirst, we split into 10 folds (default)\n\nrepeats = 1 (default; more on this in a bit)\nstratify on disease\n\n\n\nsplits_kfold &lt;- data_all |&gt; \n  vfold_cv(v = 10, repeats = 1, strata = \"disease\")\n\nsplits_kfold\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [272/31]&gt; Fold01\n 2 &lt;split [272/31]&gt; Fold02\n 3 &lt;split [272/31]&gt; Fold03\n 4 &lt;split [272/31]&gt; Fold04\n 5 &lt;split [273/30]&gt; Fold05\n 6 &lt;split [273/30]&gt; Fold06\n 7 &lt;split [273/30]&gt; Fold07\n 8 &lt;split [273/30]&gt; Fold08\n 9 &lt;split [273/30]&gt; Fold09\n10 &lt;split [274/29]&gt; Fold10\n\n\n\n\nFit model configuration in first 9 folds, evaluate in 10th fold. Repeat 9 more times for each additional held-out fold\n\nUse fit_resamples() as before\nStill no need to continue to remake features for held-in and held-out sets. Just provide splits and rec\nSet performance metrics with metric_set()\n\n\n\nfits_lr_kfold &lt;- \n  logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit_resamples(preprocessor = rec_lr, \n                resamples = splits_kfold, \n                metrics = metric_set(accuracy))\n\n\n\nThen, we review performance estimates in held out folds using collect_metrics()\n\nCan see performance in all folds using summarize = FALSE\nThe performance estimates are not all the same. This is what we mean when we talk about the variance in our performance estimates. You couldn’t see it before with only one held-out set but now that we have 10, it becomes more concrete. Ideally this variance is as low as possible.\n\n\n\nmetrics_kfold &lt;- collect_metrics(fits_lr_kfold, summarize = FALSE)\n\nmetrics_kfold |&gt; print_kbl()\n\n\n\n\n\n\nid\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nFold01\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nFold02\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nFold03\naccuracy\nbinary\n0.71\nPreprocessor1_Model1\n\n\nFold04\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nFold05\naccuracy\nbinary\n0.70\nPreprocessor1_Model1\n\n\nFold06\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nFold07\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nFold08\naccuracy\nbinary\n0.73\nPreprocessor1_Model1\n\n\nFold09\naccuracy\nbinary\n0.93\nPreprocessor1_Model1\n\n\nFold10\naccuracy\nbinary\n0.86\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\n\n\nCould plot this as a histogram to visualize this variance (i.e., the sampling distribution of performance estimates)\nWould be better if we had more folds (see repeats in a bit)\n\n\nmetrics_kfold |&gt; plot_hist(\".estimate\")\n\n\n\n\n\n\n\n\n\n\nCan see the average performance over folds along with its standard error using summarize = TRUE\nThis average will have lower variance (which is estimated by the standard error. Still not zero!)\n\n\ncollect_metrics(fits_lr_kfold, summarize = TRUE)\n\n# A tibble: 1 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.809    10  0.0242 Preprocessor1_Model1\n\n\n\nAs a last step, we still fit the final model as before using the full dataset\n\nWe already have the feature matrix for the full dataset (feat_all) from earlier\nOtherwise, remake it\n\n\nfit_lr &lt;-\n  logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(disease ~ ., data = feat_all)\n\n\n\nfit_lr |&gt; tidy()\n\n# A tibble: 17 × 5\n   term                    estimate std.error statistic     p.value\n   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 (Intercept)             -5.06      2.79       -1.81  0.0704     \n 2 age                     -0.0185    0.0235     -0.787 0.431      \n 3 rest_bp                  0.0240    0.0109      2.20  0.0279     \n 4 chol                     0.00414   0.00376     1.10  0.271      \n 5 max_hr                  -0.0204    0.0104     -1.96  0.0497     \n 6 exer_st_depress          0.275     0.212       1.30  0.195      \n 7 ca                       1.32      0.263       5.00  0.000000566\n 8 sex_male                 1.43      0.489       2.92  0.00345    \n 9 cp_atyp_ang              1.05      0.752       1.40  0.162      \n10 cp_non_anginal           1.24      0.602       2.06  0.0396     \n11 fbs_yes                 -0.812     0.522      -1.55  0.120      \n12 rest_ecg_abnormal        0.469     0.364       1.29  0.197      \n13 exer_ang_yes             1.24      0.401       3.10  0.00195    \n14 exer_st_slope_flat       1.08      0.442       2.43  0.0150     \n15 exer_st_slope_downslope  0.468     0.819       0.572 0.568      \n16 thal_fixeddefect         0.262     0.759       0.345 0.730      \n17 thal_reversabledefect    1.41      0.397       3.54  0.000396   \n\n\n\n\nIf we need to predict disease in the future, this is the fitted model we would use (with these parameter estimates)\n\nOur estimate of its future accuracy is 0.8090026 with a standard error of 0.0241847\n\nComparisons between K-fold vs. LOOCV and Single Validation set\n\nFor Bias:\n\nK-fold typically has less bias than the single validation set method\n\nE.g. 10-fold fits models with 9/10th of the data vs. 50% or 80%, etc\n\nK Fold has somewhat more bias than LOOCV because LOOCV uses n - 1 observations for fitting models\n\n\nFor Variance:\n\nK-fold has less variance than LOOCV\n\nLike LOOCV, it uses all observations in test at some point\nThe averaged models are more independent b/c models are fitted on less overlapping training sets\n\nK-fold has less variance than single validation set b/c it uses all data as test at some point (vs. a subset of held-out test data)\nK-fold is less computationally expensive than LOOCV (though more expensive than single validation set)\nK-fold is generally preferred over both of these other approaches\n\n\nK-fold is less computationally intensive BUT still can be costly. Particularly when you are getting performance estimates for multiple model configurations (more on that when we learn how to tune hyperparameters)\n\nSo you may want to start caching the fits_ object so you don’t have to recalculate it.\n\nHere is a demonstration\n\nFirst we set up an environment variable that we can flip between true/false to invalidate all our cached calculations\nPut this near the top of your code with other environment settings so its easy to find\n\n\nrerun_setting &lt;- TRUE \n\n\n\nNow use the cache_rds() function\nPut the resampling code inside of {}\nCached file will be saved in cached/ folder with filename fits_lr_kfold_HASH\nIf you change any code in the function, it will invalidate the cache and re-run the code\nHowever, it will not invalidate the cache if you change objects or data that affect this function but are outside it, e.g.,\n\nFix errors in data\nUpdate rec_lr\nUpdate splits_kfold\nBe VERY careful!\n\nYou can manually invalidate just the cache for just this code chunk by setting rerun = TRUE temporarily or you can change rerun_setting &lt;- TRUE at the top of your script to do fresh calculations for all of your cached code chunks\nYou can set rerun_settings &lt;- TRUE when you are done with your development to make sure everything is accurate (review output carefully for any changes!)\n\n\nfits_lr_kfold &lt;- cache_rds(\n  expr = {\n    logistic_reg() |&gt; \n    set_engine(\"glm\") |&gt; \n    fit_resamples(preprocessor = rec_lr, \n                  resamples = splits_kfold, \n                  metrics = metric_set(accuracy))\n  }, \n  dir = \"cache/005/\",\n  file = \"fits_lr_kfold\",\n  rerun = rerun_setting)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#repeated-k-fold-cross-validation",
    "href": "005_resampling.html#repeated-k-fold-cross-validation",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.7 Repeated K-fold Cross Validation",
    "text": "5.7 Repeated K-fold Cross Validation\nYou can repeat the K-fold procedure multiple times with new splits for a different mix of K folds each time\n\nTwo benefits:\n\nMore stable performance estimate (because averaged over more folds: repeats * K)\nMany more estimates of performance to characterize (SE; plot) of your performance estimate\n\n\n\nBut it is computationally expensive (depending on number of repeats)\n\nAn example of Repeated K-fold Cross-validation\n\nSplits with repeats = 10 (will do 10 different splits of 10-fold)\n\n\nset.seed(19690127)\nsplits_kfold10x &lt;- data_all |&gt; \n  vfold_cv(v = 10, repeats = 10, strata = \"disease\")\n\nsplits_kfold10x\n\n#  10-fold cross-validation repeated 10 times using stratification \n# A tibble: 100 × 3\n   splits           id       id2   \n   &lt;list&gt;           &lt;chr&gt;    &lt;chr&gt; \n 1 &lt;split [272/31]&gt; Repeat01 Fold01\n 2 &lt;split [272/31]&gt; Repeat01 Fold02\n 3 &lt;split [272/31]&gt; Repeat01 Fold03\n 4 &lt;split [272/31]&gt; Repeat01 Fold04\n 5 &lt;split [273/30]&gt; Repeat01 Fold05\n 6 &lt;split [273/30]&gt; Repeat01 Fold06\n 7 &lt;split [273/30]&gt; Repeat01 Fold07\n 8 &lt;split [273/30]&gt; Repeat01 Fold08\n 9 &lt;split [273/30]&gt; Repeat01 Fold09\n10 &lt;split [274/29]&gt; Repeat01 Fold10\n# ℹ 90 more rows\n\n\n\n\nEverything else is the same!\n\n\nfits_lr_kfold10x &lt;- cache_rds(\n  expr = {\n    logistic_reg() |&gt; \n      set_engine(\"glm\") |&gt; \n      fit_resamples(preprocessor = rec_lr, \n                    resamples = splits_kfold10x, \n                    metrics = metric_set(accuracy))\n  }, \n  dir = \"cache/005/\",\n  file = \"fits_lr_kfold10x\",\n  rerun = rerun_setting)\n\n\n\nIndividual estimates across 100 held-out folds\n\n\nmetrics_kfold10x &lt;- collect_metrics(fits_lr_kfold10x, summarize = FALSE)\n\nmetrics_kfold10x |&gt; print_kbl()\n\n\n\n\n\n\nid\nid2\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nRepeat01\nFold01\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat01\nFold02\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat01\nFold03\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat01\nFold04\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat01\nFold05\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat01\nFold06\naccuracy\nbinary\n0.73\nPreprocessor1_Model1\n\n\nRepeat01\nFold07\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nRepeat01\nFold08\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nRepeat01\nFold09\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nRepeat01\nFold10\naccuracy\nbinary\n0.86\nPreprocessor1_Model1\n\n\nRepeat02\nFold01\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat02\nFold02\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat02\nFold03\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat02\nFold04\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat02\nFold05\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat02\nFold06\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nRepeat02\nFold07\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat02\nFold08\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat02\nFold09\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nRepeat02\nFold10\naccuracy\nbinary\n0.79\nPreprocessor1_Model1\n\n\nRepeat03\nFold01\naccuracy\nbinary\n0.94\nPreprocessor1_Model1\n\n\nRepeat03\nFold02\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat03\nFold03\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat03\nFold04\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat03\nFold05\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nRepeat03\nFold06\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nRepeat03\nFold07\naccuracy\nbinary\n0.73\nPreprocessor1_Model1\n\n\nRepeat03\nFold08\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nRepeat03\nFold09\naccuracy\nbinary\n0.70\nPreprocessor1_Model1\n\n\nRepeat03\nFold10\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nRepeat04\nFold01\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat04\nFold02\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat04\nFold03\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat04\nFold04\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat04\nFold05\naccuracy\nbinary\n0.67\nPreprocessor1_Model1\n\n\nRepeat04\nFold06\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat04\nFold07\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat04\nFold08\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat04\nFold09\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nRepeat04\nFold10\naccuracy\nbinary\n0.86\nPreprocessor1_Model1\n\n\nRepeat05\nFold01\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nRepeat05\nFold02\naccuracy\nbinary\n0.74\nPreprocessor1_Model1\n\n\nRepeat05\nFold03\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat05\nFold04\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat05\nFold05\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat05\nFold06\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat05\nFold07\naccuracy\nbinary\n0.93\nPreprocessor1_Model1\n\n\nRepeat05\nFold08\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat05\nFold09\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nRepeat05\nFold10\naccuracy\nbinary\n0.79\nPreprocessor1_Model1\n\n\nRepeat06\nFold01\naccuracy\nbinary\n0.94\nPreprocessor1_Model1\n\n\nRepeat06\nFold02\naccuracy\nbinary\n0.74\nPreprocessor1_Model1\n\n\nRepeat06\nFold03\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat06\nFold04\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat06\nFold05\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat06\nFold06\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nRepeat06\nFold07\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nRepeat06\nFold08\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat06\nFold09\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat06\nFold10\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat07\nFold01\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nRepeat07\nFold02\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat07\nFold03\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat07\nFold04\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat07\nFold05\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat07\nFold06\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nRepeat07\nFold07\naccuracy\nbinary\n0.73\nPreprocessor1_Model1\n\n\nRepeat07\nFold08\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nRepeat07\nFold09\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat07\nFold10\naccuracy\nbinary\n0.69\nPreprocessor1_Model1\n\n\nRepeat08\nFold01\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat08\nFold02\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nRepeat08\nFold03\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nRepeat08\nFold04\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nRepeat08\nFold05\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat08\nFold06\naccuracy\nbinary\n0.93\nPreprocessor1_Model1\n\n\nRepeat08\nFold07\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat08\nFold08\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nRepeat08\nFold09\naccuracy\nbinary\n0.67\nPreprocessor1_Model1\n\n\nRepeat08\nFold10\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat09\nFold01\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nRepeat09\nFold02\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat09\nFold03\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nRepeat09\nFold04\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nRepeat09\nFold05\naccuracy\nbinary\n0.93\nPreprocessor1_Model1\n\n\nRepeat09\nFold06\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nRepeat09\nFold07\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat09\nFold08\naccuracy\nbinary\n0.73\nPreprocessor1_Model1\n\n\nRepeat09\nFold09\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nRepeat09\nFold10\naccuracy\nbinary\n0.72\nPreprocessor1_Model1\n\n\nRepeat10\nFold01\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat10\nFold02\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat10\nFold03\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nRepeat10\nFold04\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nRepeat10\nFold05\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat10\nFold06\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nRepeat10\nFold07\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nRepeat10\nFold08\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat10\nFold09\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nRepeat10\nFold10\naccuracy\nbinary\n0.76\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\n\n\nHistogram of those 100 individual estimates\n\n\nmetrics_kfold10x |&gt; plot_hist(\".estimate\", bins = 10)\n\n\n\n\n\n\n\n\n\nAverage performance estimated (and its SE) across the 100 held-out folds\n\ncollect_metrics(fits_lr_kfold10x, summarize = TRUE)\n\n# A tibble: 1 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.824   100 0.00604 Preprocessor1_Model1\n\n\n\n\nYou should also refit a final model in the full data at the end as before\nWe wont demonstrate that here\n\n\nComparisons between repeated K-fold and K-fold\n\nRepeated K-fold:\n\nHas same bias as K-fold (still fitting models with K-1 folds)\nHas all the benefits of single K-fold\nHas even more stable estimate of performance (mean over more folds/repeats)\nProvides more info about distribution for the performance estimate\nBut is more computationally expensive\n\n\n\nRepeated K-fold is preferred over K-fold to the degree possible based on computational limitations (parallel, N, p, statistical algorithm, # of model configurations)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#grouped-k-fold",
    "href": "005_resampling.html#grouped-k-fold",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.8 Grouped K-fold",
    "text": "5.8 Grouped K-fold\nWe have to be particularly careful with resampling methods when we have repeated observations for the same participant (or unit of analysis more generally)\n\nWe can often predict an individual’s own data better using some of their own data.\n\nIf our model will not ever encounter that individual again, this will optimistically bias our estimate of our models performance with new/future observations.\n\nWe can remove that bias by making sure that all observations from an individual are grouped together so that they always either held-in or held-out but never split across both.\n\nEasy to do a grouped K-fold by making splits using group_vfold_cv() and then proceeding as before with all other analyses/code\n\nset the group argument to the name of the variable that codes for subid or unit of analysis that is repeated.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#bootstrap-resampling",
    "href": "005_resampling.html#bootstrap-resampling",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.9 Bootstrap Resampling",
    "text": "5.9 Bootstrap Resampling\nA bootstrap sample is a random sample taken with replacement (i.e., same observations can be sampled multiple times within one bootstrap sample)\n\nIf you bootstrap a new sample of size n from a dataset with sample size n, approximately 63.2% of the original observations end up in the bootstrap sample\n\nThe remaining 36.8% of the observations are often called the “out of bag” (OOB) samples\n\nBootstrap Resampling\n\nCreates B bootstrap samples of size n = n from the original dataset\nFor any specific bootstrap (b)\n\nModel(s) are fit to the bootstrap sample\nModel performance is evaluated in the associated out of bag (held-out) samples\n\nThis is repeated B times such that you have B assessments of model performance\n\n\nAn example of Bootstrap resampling\n\nAgain, all that changes is how you form the splits/resamples\nYou will use \\(bootstraps()\\) to form the splits\nHere are 100 bootstraps stratified on disease\n\n\nset.seed(19690127)\nsplits_boot &lt;- data_all |&gt; \n  bootstraps(times = 100, strata = \"disease\") \n\nsplits_boot\n\n# Bootstrap sampling using stratification \n# A tibble: 100 × 2\n   splits            id          \n   &lt;list&gt;            &lt;chr&gt;       \n 1 &lt;split [303/115]&gt; Bootstrap001\n 2 &lt;split [303/123]&gt; Bootstrap002\n 3 &lt;split [303/105]&gt; Bootstrap003\n 4 &lt;split [303/115]&gt; Bootstrap004\n 5 &lt;split [303/114]&gt; Bootstrap005\n 6 &lt;split [303/115]&gt; Bootstrap006\n 7 &lt;split [303/113]&gt; Bootstrap007\n 8 &lt;split [303/95]&gt;  Bootstrap008\n 9 &lt;split [303/101]&gt; Bootstrap009\n10 &lt;split [303/115]&gt; Bootstrap010\n# ℹ 90 more rows\n\n\n\n\nEverything else is the same!\n\n\nfits_lr_boot &lt;- cache_rds(\n  expr = {\n    logistic_reg() |&gt; \n      set_engine(\"glm\") |&gt; \n      fit_resamples(preprocessor = rec_lr, \n                    resamples = splits_boot, \n                    metrics = metric_set(accuracy))\n\n  },\n  dir = \"cache/005/\",\n  file = \"fits_lr_boot\", \n  rerun = rerun_setting)\n\n\n\n100 individual performance estimates from the 100 OOB sets\n\n\nmetrics_boot &lt;- collect_metrics(fits_lr_boot, summarize = FALSE)\n\nmetrics_boot |&gt; print_kbl()\n\n\n\n\n\n\nid\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nBootstrap001\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap002\naccuracy\nbinary\n0.78\nPreprocessor1_Model1\n\n\nBootstrap003\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nBootstrap004\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nBootstrap005\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap006\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap007\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap008\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nBootstrap009\naccuracy\nbinary\n0.79\nPreprocessor1_Model1\n\n\nBootstrap010\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap011\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nBootstrap012\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap013\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap014\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap015\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap016\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nBootstrap017\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nBootstrap018\naccuracy\nbinary\n0.85\nPreprocessor1_Model1\n\n\nBootstrap019\naccuracy\nbinary\n0.86\nPreprocessor1_Model1\n\n\nBootstrap020\naccuracy\nbinary\n0.78\nPreprocessor1_Model1\n\n\nBootstrap021\naccuracy\nbinary\n0.74\nPreprocessor1_Model1\n\n\nBootstrap022\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap023\naccuracy\nbinary\n0.85\nPreprocessor1_Model1\n\n\nBootstrap024\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nBootstrap025\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap026\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap027\naccuracy\nbinary\n0.78\nPreprocessor1_Model1\n\n\nBootstrap028\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nBootstrap029\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap030\naccuracy\nbinary\n0.78\nPreprocessor1_Model1\n\n\nBootstrap031\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap032\naccuracy\nbinary\n0.85\nPreprocessor1_Model1\n\n\nBootstrap033\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap034\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap035\naccuracy\nbinary\n0.85\nPreprocessor1_Model1\n\n\nBootstrap036\naccuracy\nbinary\n0.78\nPreprocessor1_Model1\n\n\nBootstrap037\naccuracy\nbinary\n0.86\nPreprocessor1_Model1\n\n\nBootstrap038\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap039\naccuracy\nbinary\n0.78\nPreprocessor1_Model1\n\n\nBootstrap040\naccuracy\nbinary\n0.88\nPreprocessor1_Model1\n\n\nBootstrap041\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap042\naccuracy\nbinary\n0.79\nPreprocessor1_Model1\n\n\nBootstrap043\naccuracy\nbinary\n0.90\nPreprocessor1_Model1\n\n\nBootstrap044\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap045\naccuracy\nbinary\n0.85\nPreprocessor1_Model1\n\n\nBootstrap046\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nBootstrap047\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nBootstrap048\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap049\naccuracy\nbinary\n0.79\nPreprocessor1_Model1\n\n\nBootstrap050\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap051\naccuracy\nbinary\n0.75\nPreprocessor1_Model1\n\n\nBootstrap052\naccuracy\nbinary\n0.86\nPreprocessor1_Model1\n\n\nBootstrap053\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap054\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap055\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap056\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap057\naccuracy\nbinary\n0.79\nPreprocessor1_Model1\n\n\nBootstrap058\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap059\naccuracy\nbinary\n0.79\nPreprocessor1_Model1\n\n\nBootstrap060\naccuracy\nbinary\n0.85\nPreprocessor1_Model1\n\n\nBootstrap061\naccuracy\nbinary\n0.72\nPreprocessor1_Model1\n\n\nBootstrap062\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap063\naccuracy\nbinary\n0.86\nPreprocessor1_Model1\n\n\nBootstrap064\naccuracy\nbinary\n0.78\nPreprocessor1_Model1\n\n\nBootstrap065\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nBootstrap066\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nBootstrap067\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap068\naccuracy\nbinary\n0.85\nPreprocessor1_Model1\n\n\nBootstrap069\naccuracy\nbinary\n0.87\nPreprocessor1_Model1\n\n\nBootstrap070\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nBootstrap071\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap072\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap073\naccuracy\nbinary\n0.76\nPreprocessor1_Model1\n\n\nBootstrap074\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nBootstrap075\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nBootstrap076\naccuracy\nbinary\n0.89\nPreprocessor1_Model1\n\n\nBootstrap077\naccuracy\nbinary\n0.78\nPreprocessor1_Model1\n\n\nBootstrap078\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nBootstrap079\naccuracy\nbinary\n0.79\nPreprocessor1_Model1\n\n\nBootstrap080\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap081\naccuracy\nbinary\n0.88\nPreprocessor1_Model1\n\n\nBootstrap082\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap083\naccuracy\nbinary\n0.77\nPreprocessor1_Model1\n\n\nBootstrap084\naccuracy\nbinary\n0.79\nPreprocessor1_Model1\n\n\nBootstrap085\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap086\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap087\naccuracy\nbinary\n0.76\nPreprocessor1_Model1\n\n\nBootstrap088\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap089\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nBootstrap090\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nBootstrap091\naccuracy\nbinary\n0.85\nPreprocessor1_Model1\n\n\nBootstrap092\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\nBootstrap093\naccuracy\nbinary\n0.83\nPreprocessor1_Model1\n\n\nBootstrap094\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap095\naccuracy\nbinary\n0.82\nPreprocessor1_Model1\n\n\nBootstrap096\naccuracy\nbinary\n0.81\nPreprocessor1_Model1\n\n\nBootstrap097\naccuracy\nbinary\n0.84\nPreprocessor1_Model1\n\n\nBootstrap098\naccuracy\nbinary\n0.78\nPreprocessor1_Model1\n\n\nBootstrap099\naccuracy\nbinary\n0.75\nPreprocessor1_Model1\n\n\nBootstrap100\naccuracy\nbinary\n0.80\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\n\n\nHistogram of those 100 performance estimates\n\n\nmetrics_boot |&gt; plot_hist(\".estimate\", bins = 10)\n\n\n\n\n\n\n\n\n\n\nAverage performance over those 100 estimates\n\n\ncollect_metrics(fits_lr_boot, summarize = TRUE)\n\n# A tibble: 1 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.817   100 0.00338 Preprocessor1_Model1\n\n\n\nYou should also refit a final model in the full data at the end as before to get final single fitted model for later use\n\n\nRelevant comparisons, strengths/weaknesses for bootstrap for resampling\n\nThe bootstrap resampling performance estimate will have higher bias than K-fold using typical K values (bias equivalent to about K = 2)\nAlthough training sets have full n, they only include about 63% unique observations. These models under perform training sets with 80 - 90% unique observations\nWith smaller training set sizes, this bias is considered too high by some (Kuhn)\n\n\n\nThe bootstrap resampling performance estimate will have less variance than K-fold\nCompare SE of accuracy for 100 resamples using k-fold with repeats: 0.0060406 vs. bootstrap: 0.0033803\nWith 1000 bootstraps (and test sets with ~ 37% of n) can get a very precise estimate of held-out error\n\n\n\nCan also represent the variance of our held-out error (like repeated K-fold)\nUsed primarily for selecting among model configurations when you don’t care about bias and just want a precise selection metric\nUseful in explanation scenarios where you just need the “best” model\n“Inner loop” of nested cross validation (more on this later)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#using-resampling-to-select-best-model-configurations",
    "href": "005_resampling.html#using-resampling-to-select-best-model-configurations",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.10 Using Resampling to Select Best Model Configurations",
    "text": "5.10 Using Resampling to Select Best Model Configurations\nIn all of the previous examples, we have used various resampling methods only to evaluate the performance of a single model configuration in new data. In these instances, we were treating the held-out sets as test sets.\n\nResampling is also used to get held out performance estimates to select best model configurations.\n\n\nBest means the model configuration that performs the best in new data and therefore is closest to the true DGP for the data\n\nFor example, we might want to select among model configurations in an explanatory scenario to have a principled approach to determine the model configuration that best matches the true DGP (and would be best to test your hypotheses). e.g.,\n\nSelecting covariates to include\nDeciding on X transformations\nOutlier identification approach\nStatistical algorithm\n\n\nWe can simply get performance estimates for each configuration using one of the previously described resampling methods\n\nWe would call the held-out data (the single set, the folds, the OOB samples) a validation set\nWe select the model configuration with the best mean (or median?) across our resampled validation sets on the relevant performance metric.\n\n\nOne additional common scenario where you will do model selection across many model configurations is when “tuning” (i.e., selecting) the best values for hyperparameters for a statistical algorithm (e.g., k in KNN).\n\ntidymodels makes this easy and it follows a very similar workflow as earlier with a few changes\n\nWe will need to indicate which hyperparameters we plan to tune in the statistical algorithm\nWe need (or can) select values to consider for that hyperparameter (or we can let the tune package functions decide in some cases)\nWe will now use tune_grid() rather fit_resamples() to fit and evaluate the models configurations that differ with respect to their hyperparameters\nThis IS computationally costly. Now fitting and estimating performance for multiple configurations across multipe held-in/held-out sets\n\n\nLets use bootstrap resampling to select the best K for KNN applied to our heart disease dataset\n\nWe can use the same splits are established as before (splits_boot)\n\nWe need a slightly different recipe for KNN vs. logistic regression\n\nWe have to scale (lets range correct) the features\nNo need to do this to the dummy features. They are already range corrected\n\n\nrec_knn &lt;- recipe(disease ~ ., data = data_all) |&gt; \n  step_impute_median(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt;   \n  step_range(all_numeric()) |&gt; \n  step_dummy(all_nominal_predictors())\n\n\nThe fitting process is what is different\n\nWe set up a tibble with values of the hyperparameters to consider\nWe indicate which hyperparameters need to be tuned\n\n\nhyper_grid &lt;- expand.grid(neighbors = seq(1, 150, by = 3))\nhyper_grid\n\n   neighbors\n1          1\n2          4\n3          7\n4         10\n5         13\n6         16\n7         19\n8         22\n9         25\n10        28\n11        31\n12        34\n13        37\n14        40\n15        43\n16        46\n17        49\n18        52\n19        55\n20        58\n21        61\n22        64\n23        67\n24        70\n25        73\n26        76\n27        79\n28        82\n29        85\n30        88\n31        91\n32        94\n33        97\n34       100\n35       103\n36       106\n37       109\n38       112\n39       115\n40       118\n41       121\n42       124\n43       127\n44       130\n45       133\n46       136\n47       139\n48       142\n49       145\n50       148\n\n\n\n\nWhen model configurations differ by features or statistical algorithms, we have to use fit_resamples() multiple times to estimate performance of those different configurations\nHeld out performance estimates for model configurations that differ by hyperparameter values are obtained in one step using tune_grid()\nWe need to set grid =\n\n\nfits_knn_boot &lt;- cache_rds(\n  expr = {\n    nearest_neighbor(neighbors = tune()) |&gt; \n      set_engine(\"kknn\") |&gt; \n      set_mode(\"classification\") |&gt;\n      tune_grid(preprocessor = rec_knn, \n                resamples = splits_boot, \n                grid = hyper_grid,\n                metrics = metric_set(accuracy))\n\n  },\n  dir = \"cache/005/\",\n  file = \"fits_knn_boot\",\n  rerun = rerun_setting)\n\n\nReviewing performance of model configurations is similar to before but now with multiple configurations\n\nWe can see the average performance over folds along with its standard error using summarize = TRUE\nWe could see the performance of each configuration in EACH fold too, but there are lots of them (use summarize = FALSE)\n\n\ncollect_metrics(fits_knn_boot, summarize = TRUE)\n\n# A tibble: 50 × 7\n   neighbors .metric  .estimator  mean     n std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1         1 accuracy binary     0.749   100 0.00362 Preprocessor1_Model01\n 2         4 accuracy binary     0.749   100 0.00362 Preprocessor1_Model02\n 3         7 accuracy binary     0.771   100 0.00367 Preprocessor1_Model03\n 4        10 accuracy binary     0.782   100 0.00353 Preprocessor1_Model04\n 5        13 accuracy binary     0.787   100 0.00345 Preprocessor1_Model05\n 6        16 accuracy binary     0.794   100 0.00329 Preprocessor1_Model06\n 7        19 accuracy binary     0.798   100 0.00314 Preprocessor1_Model07\n 8        22 accuracy binary     0.800   100 0.00313 Preprocessor1_Model08\n 9        25 accuracy binary     0.802   100 0.00311 Preprocessor1_Model09\n10        28 accuracy binary     0.803   100 0.00306 Preprocessor1_Model10\n# ℹ 40 more rows\n\n\n\n\nWe can plot average performance by the values of the hyperparameter\n\n\ncollect_metrics(fits_knn_boot, summarize = TRUE) |&gt; \n  ggplot(aes(x = neighbors, y = mean)) +\n    geom_line()\n\n\n\n\n\n\n\n\nK (neighbors) is affecting the bias-variance trade-off. As K increases, model bias increases but model variance decreases. In most instances, model variance decreases faster than model bias increases. Therefore performance should increase and then peak at a good point along the bias-variance trade-off. Beyond this optimal value, performance should decrease again. You want to select a hyperparameter value that is associated with peak (or near peak) performance.\n\nThe simplest way to select among model configurations (e.g., hyperparameters) is to choose the model configuration with the best performance\n\nshow_best(fits_knn_boot, n = 10)\n\n# A tibble: 10 × 7\n   neighbors .metric  .estimator  mean     n std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       145 accuracy binary     0.819   100 0.00321 Preprocessor1_Model49\n 2       148 accuracy binary     0.819   100 0.00323 Preprocessor1_Model50\n 3       142 accuracy binary     0.819   100 0.00324 Preprocessor1_Model48\n 4       124 accuracy binary     0.818   100 0.00310 Preprocessor1_Model42\n 5       121 accuracy binary     0.818   100 0.00309 Preprocessor1_Model41\n 6       136 accuracy binary     0.818   100 0.00323 Preprocessor1_Model46\n 7       139 accuracy binary     0.818   100 0.00323 Preprocessor1_Model47\n 8       133 accuracy binary     0.818   100 0.00319 Preprocessor1_Model45\n 9       130 accuracy binary     0.818   100 0.00316 Preprocessor1_Model44\n10       127 accuracy binary     0.818   100 0.00314 Preprocessor1_Model43\n\nselect_best(fits_knn_boot)\n\n# A tibble: 1 × 2\n  neighbors .config              \n      &lt;dbl&gt; &lt;chr&gt;                \n1       145 Preprocessor1_Model49\n\n\n\nThe next most common is to choose the simplest (least flexible) model that has performance within one SE of the best performing configuration.\n\nUse select_by_one_std_err()\nSort performance results from least to most flexible (e.g., desc(neighbors))\n\n\nselect_by_one_std_err(fits_knn_boot, \n                      desc(neighbors))\n\n# A tibble: 1 × 9\n  neighbors .metric  .estimator  mean     n std_err .config               .best\n      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;\n1       148 accuracy binary     0.819   100 0.00323 Preprocessor1_Model50 0.819\n  .bound\n   &lt;dbl&gt;\n1  0.816\n\n\n\n\nWe should also refit a final model with the “best” hyperparameter using the full data as before\n\n\nrec_prep &lt;- rec_knn |&gt;   \n  prep(data_all)\n\nfeat_all &lt;- rec_prep |&gt; \n  bake(data_all)\n\n\n\nWe can use the select_*() from above to use this best hyperparameter in our specification of the algorithm\nNote that we now fit using all the data and switch to fit() rather than tune_grid()\n\n\nfit_knn_best &lt;-\n  nearest_neighbor(neighbors = select_best(fits_knn_boot)$neighbors) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;\n  fit(disease ~ ., data = feat_all)\n\n\n\nHowever, we can’t use the previous bootstrap resampling to evaluate this final/best model because we already used it to select this the best configuration\nWe need new, held-out data to evaluate it (a new test set!)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#resampling-for-both-model-selection-and-evaluation",
    "href": "005_resampling.html#resampling-for-both-model-selection-and-evaluation",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.11 Resampling for Both Model Selection and Evaluation",
    "text": "5.11 Resampling for Both Model Selection and Evaluation\nResampling methods can be used to get model performance estimates to select the best model configuration and/or evaluate that best model\n\nSo far we have done EITHER selection OR evaluation but not both together\nThe concepts to both select the best configuration and evaluation it are similar but it requires different (slightly more complicated) resampling than what we have done so far\n\n\n\nIf you use your held-out resamples to select the best model among a number of model configurations then the same held-out resamples cannot also be used to evaluate the performance of that same best model\nIf it is, the performance metric will have optimization bias. To the degree that there is any noise (i.e., variance) in the measurement of performance, selecting the best model configuration will capitalize on this noise.\nYou need to use one set of held out resamples (validation sets) to select the best model. Then you need a DIFFERENT set of held out resamples (test sets) to evaluate that best model.\n\n\nThere are two strategies for this:\n\nStrategy 1:\n\nFirst, hold out a test set for final/best model evaluation (using initial_split()).\n\nThen use one of the above resampling methods (single validation set approach, k-fold, or bootstrap) to select the best model configuration.\n\nBootstrap is likely best option b/c it is typically more precise (though biased)\n\nStrategy 2: Nested resampling. More on this in a moment\n\n\nOther Observations about Common Practices:\n\nSimple resampling methods with the full sample (and no held-out test set) to both select AND evaluate are still common\nFailure by some (even Kuhn) to appreciate the degree of optimization bias\nParticular problem in Psychology because of small n (high variance in our performance metric)?\nCan be fine if you just want to find the best model configuration but don’t need to evaluate its performance rigorously (i.e., you don’t really care that much about validity of your performance estimate of your final model and are fine with it being a bit optimistic)\n\n\n\n5.11.1 Bootstrap with Test Set\n\nFirst we divide our data into training and test using inital_split()\nNext, we use bootstrap resampling with the training set to split training into many held-in and held-out sets. We use these held-out (OOB) sets as validation sets to select the best model configuration based on mean/median performance across those sets.\nAfter we select the best model configuration using bootstrap resampling of the training set\n\nWe refit that model configuration in the FULL training set\nAnd we use that model to predict into the test set to evaluate it\n\nOf course, in the end, if you plan to use the model, you will refit this final model configuration to the FULL dataset but the performance estimate for that model will come from test set on th previous step (there are no more data to estimate new performance)\n\n\n\nUse initial_split() for first train/test split\n\n\nset.seed(123456)\nsplits_test &lt;- data_all |&gt; \n  initial_split(prop = 2/3, strata = \"disease\")\n\ndata_trn &lt;- splits_test |&gt; \n  analysis()\n\ndata_test &lt;- splits_test |&gt; \n  assessment()\n\n\n\nUse training set for model selection via resampling (in this case bootstrap)\nNo need for another seed\n\n\nsplits_boot_trn &lt;- data_trn |&gt; \n  bootstraps(times = 100, strata = \"disease\") \n\n\nUse the same grid of hyperparameters we set up earlier (hyper_grid)\n\n\n\nFit model configurations that vary by K in all 100 bootstrap samples\nMake predictions and calculate accuracy for these fitted models in 100 OOB (validation) sets\n\n\nfits_knn_boot_trn &lt;- cache_rds(\n  expr = {\n    nearest_neighbor(neighbors = tune()) |&gt; \n      set_engine(\"kknn\") |&gt; \n      set_mode(\"classification\") |&gt;\n      tune_grid(preprocessor = rec_knn, \n                resamples = splits_boot_trn, \n                grid = hyper_grid, \n                metrics = metric_set(accuracy))\n  },\n  dir = \"cache/005/\",\n  file = \"fits_knn_boot_trn\",\n  rerun = rerun_setting)\n\n\n\nSelect the best model configuration (best k)\n\nK = 97 is the best model configuration determined by bootstrap resampling\nBUT this is NOT the correct estimate of its performance in new data\nWe compared 50 model configurations (values of k). This performance estimate may have some optimization bias (though 50 model configurations is really not THAT many)\n\n\n\nshow_best(fits_knn_boot_trn, n = 10)\n\n# A tibble: 10 × 7\n   neighbors .metric  .estimator  mean     n std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        91 accuracy binary     0.815   100 0.00309 Preprocessor1_Model31\n 2        97 accuracy binary     0.815   100 0.00305 Preprocessor1_Model33\n 3       100 accuracy binary     0.815   100 0.00302 Preprocessor1_Model34\n 4       124 accuracy binary     0.815   100 0.00299 Preprocessor1_Model42\n 5       121 accuracy binary     0.815   100 0.00299 Preprocessor1_Model41\n 6        88 accuracy binary     0.815   100 0.00306 Preprocessor1_Model30\n 7        79 accuracy binary     0.815   100 0.00300 Preprocessor1_Model27\n 8        94 accuracy binary     0.815   100 0.00302 Preprocessor1_Model32\n 9        85 accuracy binary     0.814   100 0.00299 Preprocessor1_Model29\n10        73 accuracy binary     0.814   100 0.00309 Preprocessor1_Model25\n\n\n\n\nshow exact means\n\n\nshow_best(fits_knn_boot_trn, n = 10)$mean\n\n [1] 0.8149497 0.8148387 0.8147184 0.8147127 0.8146859 0.8146851 0.8145969\n [8] 0.8145515 0.8144602 0.8144509\n\n\n\nselect best\n\n\nselect_best(fits_knn_boot_trn)\n\n# A tibble: 1 × 2\n  neighbors .config              \n      &lt;dbl&gt; &lt;chr&gt;                \n1        91 Preprocessor1_Model31\n\n\n\n\nFit the k = 97 model configuration in the full training set\n\n\nrec_prep &lt;- rec_knn |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_test &lt;- rec_prep |&gt; \n  bake(data_test)\n\nfit_knn_best &lt;-\n  nearest_neighbor(neighbors = select_best(fits_knn_boot_trn)$neighbors) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;\n  fit(disease ~ ., data = feat_trn)\n\n\n\nUse that fitted model to predict into test set\n\n\naccuracy_vec(feat_test$disease, predict(fit_knn_best, feat_test)$.pred_class)\n\n[1] 0.8333333\n\n\n\nOur best estimate of how accurate a model with k = 91 would be in new data is 0.8333333.\n\nHowever, it was only fit with n = 203 (the training set).\n\nIf we truly want the best model, we should now train once again with all of our data.\n\nThis model would likely perform even better b/c it has &gt; n.\n\nHowever, we have no more new data to evaluate it!\nThere is no perfect performance estimate!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#nested-resampling",
    "href": "005_resampling.html#nested-resampling",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.12 Nested Resampling",
    "text": "5.12 Nested Resampling\nAnd now the final, mind-blowing extension!!!!!\n\nThe bootstrap resampling + test set approach to simultaneously select and evaluate models is commonly used\n\nHowever, it suffers from the same problems as the single train, valdition, test set approach when it comes to evaluating the performance of the final best model\n\nIt only uses a single small held out test set. In this case, 1/3 of the total sample size.\n\nThis will yield a high variance/imprecise estimate of model performance\n\nIt also yields a biased estimate of model performance\n\nThe model we evaluated in test was fit to only the training data which was only 2/3 of total sample size\nYet our true final model is trained with the full dataset\nWe are likely underestimating its true performance\n\n\n\nNested resampling offers an improvement with respect to these two issues\n\nNested resampling involves two loops\n\nThe inner loop is used for model selection\nThe outer loop is used for model evaluation\n\n\n\nNested resampling is VERY CONFUSING at first (like the first year you use it!)\n\nNested resampling isn’t fully supported by tidymodels as of yet. You have to do some coding to iterate over the outer loop\n\nApplication of nested resampling is outside the scope of this course but you should understand it conceptually. For further reading on the implementation of this method, see an example provided by the tidymodels folks.\n\nA ‘simple’ example using bootstrap for inner loop and 10-fold CV for outer loop\n\nDivide the full sample into 10 folds\nIterate through those 10 folds as follows (this is the outer loop)\n\nHold out fold 1\nUse folds 2-10 to do the inner loop bootstrap\n\nBootstrap these folds B times\nFit models in B bootstrap samples\nCalculate selection performance metrics in B out-of-bag samples\nAverage the B bootstrapped selection performance metrics for each model configuration\nSelect the best model configuration using this average bootstrapped selection performance metric\n\nUse best model configuration from folds 2-10 to make predictions for the held-out fold 1 to get the first (of ten) evaluation performance metrics\nRepeat for held out fold 2 - 10\n\nAverage the 10 evaluation performance metrics. This is the expected performance of a best model configuration (selected by B bootstraps) in new data. [You still don’t know what configuration you should use because you have 10 ‘best’ model configurations]\nDo B bootstraps with the full sample to select the best model configuration\nFit this best model configuration to the full data\n\n\nNested resampling evaluates a fitting and selection process not a specific model configuration!\n\nYou therefore need to select a final model configuration using same resampling with full data\nYou then need to fit that new model configuration to the full data\nThat was the last two steps on the previous page\n\n\n\n\n\n\n\n\nQuestion: Why bootstrap on inner loop and k-fold on outer loop?\n\n\n\n\n\n\n\nShow Answer\nThe inner loop is used for selecting models.  Bootstrap yields low variance performance\nestimates (but they are biased).  We want low variance to select best model\nconfiguration.  K-fold is a good method for less biased performance estimates.  \nWe want less bias in our final evaluation of our best model.  You can do repeated\nK-fold in the outer loop to both reduce its variance and give you a sense of the\nperformance sampling distribution. BUT VERY COMPUTATIONALLY INTENSIVE",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#data-exploration-with-advanced-resampling-methods",
    "href": "005_resampling.html#data-exploration-with-advanced-resampling-methods",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.13 Data Exploration with Advanced Resampling Methods",
    "text": "5.13 Data Exploration with Advanced Resampling Methods\nFinal words on resampling:\n\nIterative methods (K-fold, bootstrap) are superior to single validation set approach wrt bias-variance trade-off in performance measurement\nK-Fold resampling should be used if you looking for a performance estimate of a single model configuration\nBootstrap resampling should be used if you are looking only to choose among model configurations but don’t need an independent assessment of that final model\nBootstrap resampling + Test set or Nested Resampling should be used when you plan to both select among model configurations AND evaluate the best model\n\nIn scenarios where you will not have one test set but will eventually use all the data as test ast some point (i.e., k-fold for evaluation of a single model configuration or Nested CV)\n\nThink carefully about how you do EDA\nResampling reduces overfitting in the model selection process\nCan use eyeball sample (10-20% of full data) with little impact on final performance measure",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "005_resampling.html#discussion",
    "href": "005_resampling.html#discussion",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.14 Discussion",
    "text": "5.14 Discussion\n\n5.14.1 Announcements\n\nStarting Unit 6 (Regularization/Penalized models) at end of class today; All set!\nUnit 7 is Mid-term Exam unit\n\nNo readings, lecture or quiz\nApplication assignment as exam (due at normal application assignment day/time; Weds, March 6th at 8pm)\nConceptual exam during discussion meeting (Thursday March 7th at 11 am)\n\nRoom assignments; 121 next week but staying in 338; Thanks for the feedback!\nEDA with new datasets (and the last application assignment)\n\nFeedback\n\nconcepts in color?, code visually distinct!!\ntables for contrasts vs. text. - will try for this year but can’t promise (still working on terms)\nreading papers using the methods - have one selected will consider more once we know enough!\nother ways to submit questions linked to lecture slides??\ndirect links to slides format?\ncaptions? Help me!\n\n\n\n\n5.14.2 Bias and Variance\n\nGeneral definitions - need to think about repeated estimation of something (\\(\\hat{f}\\), \\(\\hat{DGP}\\); \\(\\hat{accuracy}\\), \\(\\hat{rmse}\\))\nExamples for our models (\\(\\hat{f}\\), \\(\\hat{DGP}\\))\nExamples for our performance estimates (\\(\\hat{accuracy}\\), \\(\\hat{rmse}\\))\n\n\n\n\n5.14.3 Describe process to use resampling to get a performance estimate to evaluate of a single model configuration\n\nWhich method?\nUse held-in/held-out terminology in addition to train/test\n\nDiscuss the bias and variance of the performance estimate of this configuration. Consider implications of:\n\nThe method\nsize of the held-in data\nsize of held out data\n\n\n\n\n\n5.14.4 Describe process to use resampling to get performance estimates to select a best model configuration among many (explanatory setting?)\n\nWhen done (what are you selecting among)?\nWhich method?\nUse held-in/held-out terminology in addition to train/val\nDiscuss the bias and variance of the performance estimate of this configuration\nDiscuss the bias and variance of the performance estimate used to select the best configuration. Consider implications of:\n\nThe method\nsize of the held-in data\nsize of held out data\n\nWhat are the implications if you use that same performance estimate to evaluate that best model configuration (i.e., estimate its performance in new data)?\n\n\n\n\n5.14.5 Describe process to use resampling (other than nested) to get performance estimates to both select best configuration and evaluate it in new data\n\nWhich method?\nDescribe how to do it using held-in/held-out terminology in addition to train/val/test\nDiscuss the bias and variance of the performance estimate used to select the best configuration. Consider implications of:\n\nThe method\nsize of the held-in data\nsize of held out data\n\nDiscuss the bias and variance of the performance estimate used to evaluate that best configuration. Consider implications of:\n\nThe method\nsize of the held-in data\nsize of held out data\n\n\n\n\n\n5.14.6 Describe process to use Nested CV to get performance estimates to both select best configuration and evaluate it in new data\n\nWhen used?\nDescribe how to do it using held-in/held-out terminology in addition to train/val/test\nDiscuss the bias and variance of the performance estimate used to select the best configuration. Consider implications of:\n\nThe method\nsize of the held-in data\nsize of held out data\n\nDiscuss the bias and variance of the performance estimate used to evaluate that best configuration. Consider implications of:\n\nThe method\nsize of the held-in data\nsize of held out data\n\n\n\n\n\n5.14.7 Methods to EITHER select best model configuration (among many) OR evaluate a single best model configuration\nWhat are pros, cons, and when to use?\n\nSingle validation (or test) set\nLOOCV\nK-fold\nRepeated k-fold\nBootstrap\nALSO: grouped k-fold\n\n\n\n\n5.14.8 Methods to BOTH select best model configuration and evaluate that best configuration\n\nWhat is optimization bias (its just a form of bias but given a fancy name due to its source)?\nCombine above method with held out test set\nNested CV (see tutorial)\n\n\n\n\n5.14.9 Discuss data leakage with resampling\n\nWhy is data leakage a problem?\nHow does noise factor into the problem?\nWhy does resampling substantially reduce data leakage?\nHow to use eyeball sample with single held out test set\n\n\n\n\n5.14.10 A final interesting question…..\nBiased estimate of model performance has troubled me in the last assignment. I did repeated k-fold cross validation with a KNN model where k = 5. The effective sample size was n ~ 800 during model selection. Then I refit the model with the full training data where n ~ 1000 and submitted my predictions for the test set. After some testing with one of the TAs, I figured that my submitted model with a larger N performed worse than the model evaluated with resamples, if I used the same K determined via resampling (number of nearest neighbors). My hunch was that the optimal K (number of nearest neighbors) was different in a model trained with n ~ 800 than a model trained with n ~ 1000. Is this a possible explanation? If so, would LOOCV better serve the purpose of selecting a K for the final KNN model (so that the training set for selecting K is very much the same as the training set used for the final model)?\n\nOn average, what do you expect about the performance estimate of your final model configuration in test vs. the performance estimates used to select the best model configuration?\nWhat if you used bootstrap to select best configuration?\n…but, what about choosing k from train and then using that k in a model fit to train and val?\n\nbest k,\nbest k within +- 1 SE (lower limit or higher limit)\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag.\n\n\nKuhn, Max, and Kjell Johnson. 2018. Applied Predictive Modeling. 1st ed. 2013, Corr. 2nd printing 2018 edition. New York: Springer.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Resampling Methods for Model Selection and Evaluation</span>"
    ]
  },
  {
    "objectID": "006_regularization.html",
    "href": "006_regularization.html",
    "title": "6  Regularization and Penalized Models",
    "section": "",
    "text": "6.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#overview-of-unit",
    "href": "006_regularization.html#overview-of-unit",
    "title": "6  Regularization and Penalized Models",
    "section": "",
    "text": "6.1.1 Learning Objectives\n\nSubsetting approaches: Forward, Backward, Best Subset (covered in reading only)\nCost and Loss functions\n\nWhat are they and how are they used\nWhat are the specific formulas for linear model, logistic regression, and variants of glmnet (ridge, LASSO, full elasticnet)\n\nWhat is regularization\n\nWhat are its benefits?\nWhat are its costs?\n\nHow does lambda affect bias-variance trade-off in glmnet\nWhat does alpha do?\nFeature engineering approaches for dimensionality reduction: PCA (covered in reading only)\nOther algorithms that do feature selection/dimensionality reduction: PCR and PLS (covered in reading only)\nContrasts of PCA, PCR, PLS, and glmnet/LASSO for dimensionality reduction (covered in reading only)\n\n\n\n\n6.1.2 Readings\n\nJames et al. (2023) Chapter 6, pp 225 - 267\n\nPost questions to the readings channel in Slack\n\n\n6.1.3 Lecture Videos\n\nLecture 1: An Introduction to Penalized/Regularized Algorithms ~ 15 mins\n[Lecture 2: Intuitions about Penalized Cost Functions and Regularization ~ 11 mins\nLecture 3: Ridge Regression ~ 9 mins\nLecture 4: LASSO ~ 8 mins\nLecture 5: The Elastic net ~ 4 mins\nLecture 6: Emprical Example - Many good predictors ~ 23 mins\nLecture 7: Emprical Example - Good and zero predictors ~ 9 mins\nLecture 8: Emprical Example - LASSO for covariate selection ~ 8 mins\n\nPost questions to the video-lectures channel in Slack\n\n\n\n6.1.4 Application Assignment and Quiz\n\ndata\ndata dictionary\nqmd shell\nsolution\n\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, February 28th",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#introduction-to-penalizedregularized-statistical-algorithms",
    "href": "006_regularization.html#introduction-to-penalizedregularized-statistical-algorithms",
    "title": "6  Regularization and Penalized Models",
    "section": "6.2 Introduction to Penalized/Regularized Statistical Algorithms",
    "text": "6.2 Introduction to Penalized/Regularized Statistical Algorithms\n\n6.2.1 Overview\nComplex (e.g., flexible) models increase the chance of overfitting to the training set. This leads to:\n\nPoor prediction\nBurdensome prediction models for implementation (need to measure lots of predictors)\nLow power to test hypothesis about predictor effects\n\n\n\nComplex models are difficult to interpret\n\nRegularization is technique that:\n\nReduces overfitting\nAllows for p &gt;&gt; n (!!!)\nMay yield more interpretable models (LASSO, Elastic Net)\nMay reduce implementation burden (LASSO, Elastic Net)\n\n\nRegularization does this by applying a penalty to the parametric model coefficients (parameter estimates)\n\nThis constrains/shrinks these coefficients to yield a simpler/less overfit model\nSome types of penalties shrink the coefficients to zero (feature selection)\n\n\n\nWe will consider three approaches to regularization\n\nL2 (Ridge)\nL1 (LASSO)\nElastic net\n\n\n\nThese approaches are available for both regression and classification problems and for a variety of parametric statistical algorithms\n\n\n\n6.2.2 Cost functions\nTo understand regularization, we need to first explicitly consider loss/cost functions for the parametric statistical models we have been using.\n\nA loss function quantifies the error between a single predicted and observed outcome within some statistical model.\nA cost function is simply the aggregate of the loss across all observations in the training sample.\n\n\n\nOptimization procedures (least squares, maximum likelihood, gradient descent) seek to determine a set of parameter estimates that minimize some specific cost function for the training sample.\n\nThe cost function for the linear model is the mean squared error (squared loss):\n\n\\(\\frac{1}{n}\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}\\)\nNo constraints or penalties are placed on the parameter estimates (\\(\\beta_k\\))\nThey can take on any values with the only goal to minimize the MSE in the training sample\n\n\nThe cost function for logistic regression is log loss:\n\n\\(\\frac{1}{n}\\sum_{i = 1}^{n} -Y_ilog(\\hat{Y_i}) - (1-Y_i)log(1-\\hat{Y_i})\\)\nwhere \\(Y_i\\) is coded 0,1 and \\(\\hat{Y_i}\\) is the predicted probability that Y = 1\nAgain, no constraints or penalties are placed on the parameter estimates (\\(\\beta_k\\))\nThey can take on any values with the only goal to minimize the sum of the log loss in the training sample",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#section",
    "href": "006_regularization.html#section",
    "title": "6  Regularization and Penalized Models",
    "section": "6.3 ",
    "text": "6.3",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#intuitions-about-penalized-cost-functions-and-regularization",
    "href": "006_regularization.html#intuitions-about-penalized-cost-functions-and-regularization",
    "title": "6  Regularization and Penalized Models",
    "section": "6.4 Intuitions about Penalized Cost Functions and Regularization",
    "text": "6.4 Intuitions about Penalized Cost Functions and Regularization\nThis is an example from a series of wonderfully clear lectures in a machine learning course by Andrew Ng in Coursera.\n\nRegularization: The Problem Of Overfitting\nRegularization: Cost Functions\n\n\nLets imagine a training set:\n\nHouse sale price predicted by house size\nTrue DGP is quadratic. Diminishing increase in sale price as size increases\nN = 5 in training set\n\n\n\n\n\n\nIf we fit a linear model with size as the only feature…\n\n\\(\\hat{sale\\_price_i} = \\beta_0 + \\beta_1 * size\\)\nIn this training set, we might get the model below (in blue)\nThis is a biased model (predicts too high for low and high house sizes; predicts too low for moderate size houses)\nIf we took this model to new data from the same quadratic DGP, it would clearly not predict very well\n\n\n\nLets consider the other extreme\n\nIf we fit a 4th order polynomial model using size…\n\\(\\hat{sale\\_price_i} = \\beta_0 + \\beta_1 * size + \\beta_2 * size^2 + \\beta_3 * size^3 + \\beta_4 * size^4\\)\nIn this training set, we would get the model below (in blue)\nThis is model is overfit to this training set. It would not predict well in new data from the same quadratic DGP\nAlso, the model would have high variance (if we estimated the parameters in another N = 5 training set, they would be very different)\n\n\n\nThis problem with overfitting and variance isn’t limited to polynomial regression.\n\nWe would have the same problem (perfect fit in training with poor fit in new val data) if we predicted housing prices with many features when the training N = 5. e.g.,\n\\(\\hat{sale\\_price_i} = \\beta_0 + \\beta_1 * size + \\beta_2 * year\\_built + \\beta_3 * num\\_garages + \\beta_4 * quality\\)\n\n\nObviously, the correct model to fit is a second order polynomial model with size\n\n\\(\\hat{sale\\_price_i} = \\beta_0 + \\beta_1 * size + \\beta_2 * size^2\\)\nBut we couldn’t know this with real data because we wouldn’t know the underlying DGP\nWhen we don’t know the underlying DGP, we need to be able to consider potentially complex models with many features in some way that diminishes the potential problem with overfitting/model variance\n\n\n\nWhat if we still fit a fourth order polynomial but changed the cost function to penalize the absolute value of \\(\\beta_3\\) and \\(\\beta_4\\) parameter estimates?\n\nTypical cost based on MSE/squared loss:\n\n\\(\\frac{1}{n}\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}\\)\n\n\n\nOur new cost function:\n\n\\([\\frac{1}{n}\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] +  [1000 * \\beta_3 + 1000 * \\beta_4]\\)\n\n\n\\([\\frac{1}{n}\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [1000 * \\beta_3 + 1000 * \\beta_4]\\)\n\nThe only way to make the value of this new cost function small is to make \\(\\beta_3\\) and \\(\\beta_4\\) small\nIf we made the penalty applied to \\(\\beta_3\\) and \\(\\beta_4\\) large (e.g., 1000 as above), we will end up with the parameter estimates for these two features at approximately 0.\nWith a sufficient penalty applied, their parameter estimates will only change from zero to the degree that these changes accounted for a large enough drop in MSE to offset this penalty in the overall aggregate cost function.\n\n\n\\([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + 1000 * \\beta_3 + 1000 * \\beta_4\\)\n\nWith this penalty in place, our final model might shift from the blue model to the pink model below. The pink model is mostly quadratic but with a few extra “wiggles” if \\(\\beta_3\\) and \\(\\beta_4\\) are not exactly 0.\n\n\n\nOf course, we don’t typically know in advance which parameter estimates to penalize.\n\nInstead, we apply some penalty to all the parameter estimates (except \\(\\beta_0\\))\nThis shrinks the parameter estimates for all the features to some degree\nHowever, features that do reduce MSE meaningfully will be “worth” including with non-zero parameter estimates\nYou can also control the shrinkage by controlling the size of the penalty\n\n\nIn general, regularization produces models that:\n\nAre simpler (e.g. smoother, smaller coefficients/parameter estimates)\nAre less prone to overfitting\nAllow for models with p &gt;&gt; n\nAre sometimes more interpretable (LASSO, Elastic Net)\n\n\n\nThese benefits are provided by the introduction of some bias into the parameter estimates\n\nThis allows for a bias-variance trade-off where some bias is introduced for a big reduction in variance of model fit\n\nWe will now consider three regularization approaches that introduce different types of penalties to shrink the parameter estimates\n\nL2 (Ridge)\nL1 (LASSO)\nElastic net\n\n\n\nThese approaches are available for both regression and classification problems and for a variety of parametric statistical algorithms\n\nA fourth common regularized classification model (also sometimes used for regression) is the support vector machine (not covered in class but commonly used as well and easy to understand with this foundation)\n\nEach of these approaches uses a different specific penalty, which has implications for how the model performs in different settings",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#ridge-regression",
    "href": "006_regularization.html#ridge-regression",
    "title": "6  Regularization and Penalized Models",
    "section": "6.5 Ridge Regression",
    "text": "6.5 Ridge Regression\nThe cost function for Ridge Regression is:\n\n\\(\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:])\\)\n\n\n\nIt has two components:\n\nInside the left brackets is the SSE from linear regression\nInside the right brackets is the Ridge penalty.\n\n\n\n\nThis penalty:\n\nIncludes the sum of the squared parameter estimates (excluding \\(\\beta_0\\)). Squaring removes the sign of these parameter estimates.\nThis sum is multiplied by \\(\\lambda\\), a hyperparameter in Ridge regression. Lambda allows us to tune the size of the penalty.\nThis is an application of the L2 norm (matrix algebra) to the vector of parameter estimates\n\n\n\\(\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:])\\)\n\n\n\n\n\n\n\n\nQuestion: What will happen to a Ridge regression model’s parameter estimates and its performance (i.e., its bias & variance) as lambda increases/decreases?\n\n\n\n\n\n\n\nShow Answer\nAs lambda increases, the model becomes less flexible b/c its parameter estimates \nbecome constrained/shrunk.  This will increase bias but decrease variance for model \nperformance.\n\n\n\n\n\n\n\\(\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:])\\)\n\n\n\n\n\n\n\n\nQuestion: What is the special case of Ridge regression when lambda = 0?\n\n\n\n\n\n\n\nShow Answer\nThe OLS regression is a special case where lambda = 0 (i.e., no penalty is applied).  \nThis is the most flexible. It is unbiased but with higher variance than for \nnon-zero values of lambda\n\n\n\n\n\n\nLets compare Ridge regression to OLS (ordinary least squares with squared loss cost function) linear regression\n\nRidge parameter estimates are biased but have lower variance (smaller SE) than OLS\nRidge may predict better in new data\n\nThis depends on the value of \\(\\lambda\\) selected and its impact on bias-variance trade-off in Ridge regression vs. OLS\nThere does exist a value of \\(\\lambda\\) for which Ridge predicts better than OLS in new data\n\nRidge regression (but not OLS) allows for p &gt; (or even &gt;&gt;) than n\nRidge regression (but not OLS) accommodates highly correlated (or even perfectly multi-collinear) features\nOLS (but not Ridge regression) is scale invariant\n\nYou should scale (mean and standard deviation correct) features for use with Ridge regression\n\n\n\n\\(\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:])\\)\n\n\n\n\n\n\n\n\nQuestion: Why does the scale of the features matter for Ridge regression?\n\n\n\n\n\n\n\nShow Answer\nFeatures with bigger SDs will have smaller parameter estimates.  Therefore they \nwill be less affected by the penalty.\n\n\n\n\n\n\n\nUnless the features are on the same scale to start, you should standardize them for all applications (regression and classification) of Ridge (and also LASSO and elastic net). You can handle this during feature engineering in the recipe.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#lasso-regression",
    "href": "006_regularization.html#lasso-regression",
    "title": "6  Regularization and Penalized Models",
    "section": "6.6 LASSO Regression",
    "text": "6.6 LASSO Regression\nLASSO is an acronym for Least Absolute Shrinkage and Selection Operator\n\nThe cost function for LASSO Regression is:\n\n\\(\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} |\\beta_j|\\:])\\)\n\n\n\nIt has two components:\n\nInside the left brackets is the SSE from linear regression\nInside the right brackets is the LASSO penalty.\n\n\n\n\nThis penalty:\n\nIncludes the sum of the absolute value of the parameter estimates (excluding \\(\\beta_0\\)). The absolute value removes the sign of these parameter estimates.\nThis sum is multiplied by \\(\\lambda\\), a hyperparameter in LASSO regression. Lambda allows us to tune the size of the penalty.\nThis is an application of the L1 norm to the vector of parameter estimates\n\n\n\n6.6.1 LASSO vs. Ridge Comparison\nWith respect to the parameter estimates:\n\nLASSO yields sparse solution (some parameter estimates set to exactly zero)\nRidge tends to retain all features (parameter estimates don’t get set to exactly zero)\nLASSO selects one feature among correlated group and sets others to zero\nRidge shrinks all parameter estimates for correlated features\n\n\n\nRidge tends to outperform LASSO wrt prediction in new data. There are cases where LASSO can predict better (most features have zero effect and only a few are non-zero) but even in those cases, Ridge is competitive.\n\n\n\n6.6.2 Advantages of LASSO\n\nDoes feature selection (sets parameter estimates to exactly 0)\n\nYields a sparse solution\nSparse model is more interpretable?\nSparse model is easier to implement? (fewer features included so don’t need to measure as many predictors)\n\nMore robust to outliers (similar to LAD vs. OLS)\nTends to do better when there are a small number of robust features and the others are close to zero or zero\n\n\n\n\n6.6.3 Advantages of Ridge\n\nComputationally superior (closed form solution vs. iterative; Only one solution to minimize the cost function)\nMore robust to measurement error in features (remember no measurement error is an assumption for unbiased estimates in OLS regression)\nTends to do better when there are many features with large (and comparable) effects (i.e., most features are related to the outcome)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#elastic-net-regression",
    "href": "006_regularization.html#elastic-net-regression",
    "title": "6  Regularization and Penalized Models",
    "section": "6.7 Elastic Net Regression",
    "text": "6.7 Elastic Net Regression\nThe Elastic Net blends the L1 and L2 penalties to obtain the benefits of each of those approaches.\n\nWe will use the implementation of the Elastic Net in glmnet in R.\n\n\nYou can also read additional introductory documentation for this package\n\nIn the Gaussian regression context, the Elastic Net cost function is:\n\n\\(\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda (\\alpha\\sum_{j = 1}^{p} |\\beta_j| + (1-\\alpha)\\sum_{j = 1}^{p} \\beta_j^{2})\\:])\\)\n\n\n\nThis model has two hyper-parameters\n\n\\(\\lambda\\) controls the degree of regularization as before\n\\(\\alpha\\) is a “mixing” parameter that blends the degree of L1 and L2 contributions to the aggregate penalty. (Proportion of LASSO penalty)\n\n\\(\\alpha\\) = 1 results in the LASSO model\n\\(\\alpha\\) = 0 results in the Ridge model\nIntermediate values for \\(\\alpha\\) blend these penalties together proportionally to include more or less LASSO penalty\n\n\n\nAs before (e.g., KNN), best values of \\(\\lambda\\) (and \\(\\alpha\\)) can be selected using resampling using tune_grid()\n\nThe grid needs to have crossed values of both penalty (\\(lambda\\)) and mixture (\\(alpha\\)) for glmnet\n\nCan use expand_grid()\nOnly penalty is needed in grid if fitting a Ridge or LASSO model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#empirical-example-1-many-good-but-correlated-predictors",
    "href": "006_regularization.html#empirical-example-1-many-good-but-correlated-predictors",
    "title": "6  Regularization and Penalized Models",
    "section": "6.8 Empirical Example 1: Many “good” but correlated predictors",
    "text": "6.8 Empirical Example 1: Many “good” but correlated predictors\nFor the first example, we will simulate data with:\n\nMany correlated predictors\nAll related to outcome\nGet a small training sample\nGet a big test sample (for more precise estimate of model performance)\n\n\nFirst we set the predictors for our simulation\n\nn_cases_trn &lt;- 100\nn_cases_test &lt;- 1000\nn_x &lt;- 20\ncovs_x &lt;- 50\nvars_x &lt;- 100\nb_x &lt;- rep(1, n_x) # one unit change in y for 1 unit change in x\ny_error &lt;- 100\n\n\nThen we draw samples from population\n\nset.seed(12345)\nmu &lt;- rep(0, n_x)  # means for all variables = 0\nsigma &lt;- matrix(covs_x, nrow = n_x, ncol = n_x)\ndiag(sigma) &lt;- vars_x  \nsigma\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]  100   50   50   50   50   50   50   50   50    50    50    50    50\n [2,]   50  100   50   50   50   50   50   50   50    50    50    50    50\n [3,]   50   50  100   50   50   50   50   50   50    50    50    50    50\n [4,]   50   50   50  100   50   50   50   50   50    50    50    50    50\n [5,]   50   50   50   50  100   50   50   50   50    50    50    50    50\n [6,]   50   50   50   50   50  100   50   50   50    50    50    50    50\n [7,]   50   50   50   50   50   50  100   50   50    50    50    50    50\n [8,]   50   50   50   50   50   50   50  100   50    50    50    50    50\n [9,]   50   50   50   50   50   50   50   50  100    50    50    50    50\n[10,]   50   50   50   50   50   50   50   50   50   100    50    50    50\n[11,]   50   50   50   50   50   50   50   50   50    50   100    50    50\n[12,]   50   50   50   50   50   50   50   50   50    50    50   100    50\n[13,]   50   50   50   50   50   50   50   50   50    50    50    50   100\n[14,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[15,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[16,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[17,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[18,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[19,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[20,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20]\n [1,]    50    50    50    50    50    50    50\n [2,]    50    50    50    50    50    50    50\n [3,]    50    50    50    50    50    50    50\n [4,]    50    50    50    50    50    50    50\n [5,]    50    50    50    50    50    50    50\n [6,]    50    50    50    50    50    50    50\n [7,]    50    50    50    50    50    50    50\n [8,]    50    50    50    50    50    50    50\n [9,]    50    50    50    50    50    50    50\n[10,]    50    50    50    50    50    50    50\n[11,]    50    50    50    50    50    50    50\n[12,]    50    50    50    50    50    50    50\n[13,]    50    50    50    50    50    50    50\n[14,]   100    50    50    50    50    50    50\n[15,]    50   100    50    50    50    50    50\n[16,]    50    50   100    50    50    50    50\n[17,]    50    50    50   100    50    50    50\n[18,]    50    50    50    50   100    50    50\n[19,]    50    50    50    50    50   100    50\n[20,]    50    50    50    50    50    50   100\n\nx &lt;- MASS::mvrnorm(n = n_cases_trn, mu, sigma) |&gt; \n  magrittr::set_colnames(str_c(\"x_\", 1:n_x)) |&gt;\n  as_tibble()\ndata_trn_1 &lt;- x |&gt; \n  mutate(y = rowSums(t(t(x)*b_x)) + rnorm(n_cases_trn, 0, y_error)) |&gt;  \n  glimpse()\n\nRows: 100\nColumns: 21\n$ x_1  &lt;dbl&gt; 0.9961431, 14.6304657, 4.7113834, 5.4731915, 2.2835857, -16.15454…\n$ x_2  &lt;dbl&gt; 8.6957591, 0.8347032, -0.5353713, -12.5511722, -1.0787145, -16.42…\n$ x_3  &lt;dbl&gt; 10.6129264, 20.4883323, -8.6503901, -1.5413989, -17.7641062, -14.…\n$ x_4  &lt;dbl&gt; 11.80601585, 11.29743568, 4.46220219, 0.79397084, -3.78878523, -2…\n$ x_5  &lt;dbl&gt; 18.835971, 6.095917, -3.272026, -1.839770, -1.610545, -6.040994, …\n$ x_6  &lt;dbl&gt; 2.223292893, 11.500104110, -0.457979676, -8.496314755, 7.87014122…\n$ x_7  &lt;dbl&gt; 10.5129546, 6.4060743, 12.0210560, -13.7151243, 11.1422588, -14.2…\n$ x_8  &lt;dbl&gt; 8.8124570, 9.6462680, 2.4037228, -0.2015028, 6.3737654, -3.064850…\n$ x_9  &lt;dbl&gt; -3.67513802, 3.26579725, -7.25050948, -12.31704216, -1.69613573, …\n$ x_10 &lt;dbl&gt; -2.38262537, -6.46882639, 1.83840881, 2.57893960, 6.40948837, -27…\n$ x_11 &lt;dbl&gt; 14.9730316, 15.6370590, -1.1323317, 6.5593694, 5.1039078, -7.5830…\n$ x_12 &lt;dbl&gt; -4.47755314, 0.04826209, 2.59168486, -4.26699855, 16.49056393, -1…\n$ x_13 &lt;dbl&gt; 8.5928287, -4.0292190, -14.4559758, 6.0387060, 2.3659570, -13.319…\n$ x_14 &lt;dbl&gt; -10.54664618, 3.85439473, -7.14415536, 5.64697664, 2.86171243, -1…\n$ x_15 &lt;dbl&gt; -5.9948683, 0.6097567, 4.8671687, 0.7012719, 14.8754506, 3.478677…\n$ x_16 &lt;dbl&gt; 7.9805572, 12.1442098, -2.2095452, -9.1514837, 10.0124739, -2.676…\n$ x_17 &lt;dbl&gt; -2.4674864, -13.6498178, 1.0131614, 3.7409088, 5.9397242, -10.243…\n$ x_18 &lt;dbl&gt; 9.9571661, 5.7690926, 5.3204675, -13.9289436, 14.6130884, -19.710…\n$ x_19 &lt;dbl&gt; -3.37220750, 1.23419418, -6.67125147, -4.95630437, 8.94440082, -5…\n$ x_20 &lt;dbl&gt; 3.7686080, 3.4971897, -3.2892748, -14.2852637, -1.5467975, -28.72…\n$ y    &lt;dbl&gt; 24.067076, 210.433707, -73.482134, 44.144652, 228.535603, -259.78…\n\nx &lt;- MASS::mvrnorm(n = n_cases_test, mu, sigma) |&gt; \n  magrittr::set_colnames(str_c(\"x_\", 1:n_x)) |&gt;\n  as_tibble() \ndata_test_1 &lt;- x |&gt; \n  mutate(y = rowSums(t(t(x)*b_x)) + rnorm(n_cases_test, 0, y_error))\n\n\nSet up a tibble to track model performance in train and test sets\n\nWe are using test to repeatedly to get rigorous held-out performance separate from model selection process.\n\nJust for our understanding\nWe would not choose a model configuration based on test set error\n\n\nerror_ex1 &lt;- tibble(model = character(), \n                    rmse_trn = numeric(), \n                    rmse_test = numeric()) |&gt; \n  glimpse()\n\nRows: 0\nColumns: 3\n$ model     &lt;chr&gt; \n$ rmse_trn  &lt;dbl&gt; \n$ rmse_test &lt;dbl&gt; \n\n\n\n\n6.8.1 Fit a standard (OLS) linear regression\nFit the linear model\n\nNo feature engineering needed. Can use raw predictors as features\nNo resampling needed b/c there are no hyperparameters\n\n\nfit_lm_1 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(y ~ ., data = data_trn_1)\n\nfit_lm_1 |&gt; \n  tidy() |&gt; \n  print(n = 21)\n\n# A tibble: 21 × 5\n   term        estimate std.error statistic p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   -5.22      12.0    -0.436   0.664 \n 2 x_1            1.22       1.40    0.869   0.388 \n 3 x_2           -1.42       1.62   -0.874   0.385 \n 4 x_3           -0.679      1.59   -0.427   0.671 \n 5 x_4            0.392      1.49    0.263   0.793 \n 6 x_5            4.20       1.81    2.33    0.0226\n 7 x_6            0.804      1.58    0.510   0.612 \n 8 x_7            0.953      1.50    0.634   0.528 \n 9 x_8            2.30       1.53    1.50    0.137 \n10 x_9            2.37       1.53    1.55    0.126 \n11 x_10           0.758      1.67    0.454   0.651 \n12 x_11           0.530      1.87    0.283   0.778 \n13 x_12           1.44       1.78    0.813   0.419 \n14 x_13           2.40       1.65    1.45    0.150 \n15 x_14           3.26       1.89    1.72    0.0889\n16 x_15           0.110      1.53    0.0719  0.943 \n17 x_16          -1.28       1.68   -0.761   0.449 \n18 x_17          -0.144      1.55   -0.0926  0.926 \n19 x_18           2.74       1.66    1.65    0.102 \n20 x_19           1.36       1.61    0.843   0.402 \n21 x_20          -1.49       1.67   -0.893   0.375 \n\n\n\nIrreducible error was set by y_error (100)\n\nOverfit to train\nMuch worse in test\n\n\nrmse_vec(truth = data_trn_1$y, \n         estimate = predict(fit_lm_1, data_trn_1)$.pred)\n\n[1] 91.86214\n\n\n\nrmse_vec(truth = data_test_1$y, \n         estimate = predict(fit_lm_1, data_test_1)$.pred)\n\n[1] 112.3208\n\n\n\n\n\n6.8.2 Fit LASSO\nLASSO, Ridge, and glmnet all need features on same scale to apply penalty consistently\n\nUse step_normalize(). This sets mean = 0, sd = 1 (NOTE: Bad name as it does NOT change shape of distribution!)\nCan use same recipe for LASSO, Ridge, and glmnet\nCan use same train and test feature matrices as well\n\n\nrec_1 &lt;- recipe(y ~ ., data = data_trn_1) |&gt; \n  step_normalize(all_predictors())\n\nrec_prep_1 &lt;- rec_1 |&gt; \n  prep(data_trn_1)\n\nfeat_trn_1 &lt;- rec_prep_1 |&gt; \n  bake(NULL)\n\nfeat_test_1 &lt;- rec_prep_1 |&gt; \n  bake(data_test_1)\n\n\nSet up splits for resampling for tuning hyperparameters\n\nUse bootstrap for more precise estimation (even if more biased). Good for selection\nCan use same bootstrap splits for LASSO, Ridge, and glmnet\n\n\nset.seed(20140102)\nsplits_boot_1 &lt;- data_trn_1 |&gt; \n   bootstraps(times = 100, strata = \"y\")  \n\n\nNow onto the LASSO….\nWe need to tune \\(\\lambda\\) (tidymodels calls this penalty)\n\n\\(\\alpha\\) = 1 (tidymodels calls this mixture)\nSet up grid with exponential values for penalty\nglmnet uses warm starts so can fit lots of values for \\(\\lambda\\) quickly\nCould also use cv.glmnet() directly in glmnet package to find good values. See get_lamdas() in fun_modeling.R\n\n\ngrid_lasso &lt;- expand_grid(penalty = exp(seq(-4, 4, length.out = 500)))\n\n\nfits_lasso_1 &lt;- xfun::cache_rds(\n  expr = {\n  linear_reg(penalty = tune(), \n               mixture = 1) |&gt; \n    set_engine(\"glmnet\") |&gt; \n    tune_grid(preprocessor = rec_1, \n              resamples = splits_boot_1, \n              grid = grid_lasso, \n              metrics = metric_set(rmse))\n\n   },\n   rerun = rerun_setting,\n   dir = \"cache/006/\",\n   file = \"fits_lasso_1\")\n\n\nEvaluate model performance in validation sets (OOB)\nMake sure that you have hit a clear minimum (bottom of U or at least an asymptote)\n\nplot_hyperparameters(fits_lasso_1, hp1 = \"penalty\", metric = \"rmse\")\n\n\n\n\n\n\n\n\n\nFit best configuration (i.e., best lambda) to full train set\n\nUse select_best()\nDon’t forget to indicate which column (\\(penalty\\))\n\n\nfit_lasso_1 &lt;-\n  linear_reg(penalty = select_best(fits_lasso_1)$penalty, \n             mixture = 1) |&gt;\n  set_engine(\"glmnet\") |&gt; \n  fit(y ~ ., data = feat_trn_1)\n\n\nWe can now use tidy() to look at the LASSO parameter estimates\n\ntidy() uses Matrix package, which has conflicts with tidyr. Load the package without those conflicting functions\n\n\nlibrary(Matrix, exclude = c(\"expand\", \"pack\", \"unpack\"))\n\n\nNow call tidy()\n\nNotice that LASSO sets some \\(\\beta\\) to 0 even though none are 0 in DGP\nLASSO is not great at reproducing the DGP!\n\n\nfit_lasso_1 |&gt; \n  tidy() |&gt; \n  print(n = 21)\n\nLoaded glmnet 4.1-8\n\n\n# A tibble: 21 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)    29.9     11.3\n 2 x_1             7.26    11.3\n 3 x_2             0       11.3\n 4 x_3             0       11.3\n 5 x_4             0       11.3\n 6 x_5            34.2     11.3\n 7 x_6             8.00    11.3\n 8 x_7             1.56    11.3\n 9 x_8            17.4     11.3\n10 x_9            20.9     11.3\n11 x_10            7.95    11.3\n12 x_11            3.49    11.3\n13 x_12            7.01    11.3\n14 x_13           20.8     11.3\n15 x_14           25.8     11.3\n16 x_15            4.85    11.3\n17 x_16            0       11.3\n18 x_17            0       11.3\n19 x_18           23.5     11.3\n20 x_19            9.56    11.3\n21 x_20            0       11.3\n\n\n\nIrreducible error was set by y_error (100)\n\nSomewhat overfit to train\nSomewhat better in test\n\n\n(error_ex1 &lt;- error_ex1 |&gt; \n  bind_rows(tibble(model = \"LASSO model\",                       \n                   rmse_trn = rmse_vec(truth = feat_trn_1$y, \n                                       estimate = predict(fit_lasso_1,\n                                                          feat_trn_1)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_1$y, \n                                        estimate = predict(fit_lasso_1,\n                                                           feat_test_1)$.pred))))\n\n# A tibble: 2 × 3\n  model        rmse_trn rmse_test\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 linear model     91.9      112.\n2 LASSO model      94.7      107.\n\n\n\n\n\n6.8.3 Fit Ridge\nFit Ridge algorithm\n\nTune \\(\\lambda\\) (penalty)\nMay need to experiment to get right range of values for lambda\n\\(\\alpha\\) = 0 (mixture)\nEvaluate model configurations in OOB validation sets &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\n\ngrid_ridge &lt;- expand_grid(penalty = exp(seq(-1, 7, length.out = 500)))\n\n\ngrid_ridge &lt;- expand_grid(penalty = exp(seq(-1, 7, length.out = 500)))\n\n\nfits_ridge_1 &lt;- xfun::cache_rds(\n  expr = {\n    linear_reg(penalty = tune(), \n               mixture = 0) |&gt; \n    set_engine(\"glmnet\") |&gt; \n    tune_grid(preprocessor = rec_1, \n              resamples = splits_boot_1, \n              grid = grid_ridge, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/006/\",\n  file = \"fits_ridge_1\")\n\n\nReview hyperparameter plot\n\nplot_hyperparameters(fits_ridge_1, hp1 = \"penalty\", metric = \"rmse\")\n\n\n\n\n\n\n\n\n\nFit best model configuration (i.e., best lambda) in full train set\n\nNotice that no \\(\\beta\\) are exactly 0\nWhy are parameter estimates not near 1 for LASSO and Ridge?\n\n\nfit_ridge_1 &lt;-\n  linear_reg(penalty = select_best(fits_ridge_1)$penalty, \n             mixture = 0) |&gt;\n  set_engine(\"glmnet\") |&gt; \n  fit(y ~ ., data = feat_trn_1)\n\nfit_ridge_1 |&gt; \n  tidy() |&gt; \n  print(n = 21)\n\n# A tibble: 21 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)    29.9     281.\n 2 x_1            10.0     281.\n 3 x_2             5.53    281.\n 4 x_3             5.84    281.\n 5 x_4             6.38    281.\n 6 x_5            14.4     281.\n 7 x_6             9.78    281.\n 8 x_7             5.80    281.\n 9 x_8            12.2     281.\n10 x_9            13.2     281.\n11 x_10           10.5     281.\n12 x_11           10.3     281.\n13 x_12            8.43    281.\n14 x_13           12.8     281.\n15 x_14           12.3     281.\n16 x_15           10.2     281.\n17 x_16            3.93    281.\n18 x_17            7.67    281.\n19 x_18           12.3     281.\n20 x_19           11.2     281.\n21 x_20            5.91    281.\n\n\n\nIrreducible error was set by y_error (100)\n\nMuch less overfit to train\nStill not bad in test\n\n\n(error_ex1 &lt;- error_ex1 |&gt; \n  bind_rows(tibble(model = \"Ridge model\",   \n                   rmse_trn = rmse_vec(truth = feat_trn_1$y, \n                                       estimate = predict(fit_ridge_1,\n                                                          feat_trn_1)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_1$y, \n                                        estimate = predict(fit_ridge_1,\n                                                           feat_test_1)$.pred))))\n\n# A tibble: 3 × 3\n  model        rmse_trn rmse_test\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 linear model     91.9      112.\n2 LASSO model      94.7      107.\n3 Ridge model      98.6      103.\n\n\n\n\n\n6.8.4 Fit glmnet\nNow we need to tune both\n\n\\(\\lambda\\) (penalty)\n\\(\\alpha\\) (mixture)\nTypical to only evaluate a small number of \\(alpha\\)\nWarm starts across \\(\\lambda\\)\n\n\ngrid_glmnet &lt;- expand_grid(penalty = exp(seq(-1, 7, length.out = 500)),\n                           mixture = seq(0, 1, length.out = 6))\n\n\nfits_glmnet_1 &lt;- xfun::cache_rds(\n  expr = {\n    linear_reg(penalty = tune(), \n               mixture = tune()) |&gt; \n    set_engine(\"glmnet\") |&gt; \n    tune_grid(preprocessor = rec_1, \n              resamples = splits_boot_1, \n              grid = grid_glmnet, \n              metrics = metric_set(rmse))\n  \n  },\n  rerun = rerun_setting,\n  dir = \"cache/006/\",\n  file = \"fits_glmnet_1\")\n\n\n\nplot_hyperparameters(fits_glmnet_1, hp1 = \"penalty\", hp2 = \"mixture\", metric = \"rmse\")\n\n\n\n\n\n\n\n\n\nFit best configuration in full train set\n\nCan use select_best() for both hyperparameters, separately\nRidge was best (but cool that glmnet could determine that empirically!)\n\n\nselect_best(fits_glmnet_1)\n\n# A tibble: 1 × 3\n  penalty mixture .config                \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  \n1    281.       0 Preprocessor1_Model0415\n\nfit_glmnet_1 &lt;-\n  linear_reg(penalty = select_best(fits_glmnet_1)$penalty, \n             mixture = select_best(fits_glmnet_1)$mixture) |&gt;\n  set_engine(\"glmnet\") |&gt; \n  fit(y ~ ., data = feat_trn_1)\n\n\n\nfit_glmnet_1 |&gt; \n  tidy() |&gt; \n  print(n = 21)\n\n# A tibble: 21 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)    29.9     281.\n 2 x_1            10.0     281.\n 3 x_2             5.53    281.\n 4 x_3             5.84    281.\n 5 x_4             6.38    281.\n 6 x_5            14.4     281.\n 7 x_6             9.78    281.\n 8 x_7             5.80    281.\n 9 x_8            12.2     281.\n10 x_9            13.2     281.\n11 x_10           10.5     281.\n12 x_11           10.3     281.\n13 x_12            8.43    281.\n14 x_13           12.8     281.\n15 x_14           12.3     281.\n16 x_15           10.2     281.\n17 x_16            3.93    281.\n18 x_17            7.67    281.\n19 x_18           12.3     281.\n20 x_19           11.2     281.\n21 x_20            5.91    281.\n\n\n\nA final comparison of training and test error for the four statistical algorithms\n\n(error_ex1 &lt;- error_ex1 |&gt; \n  bind_rows(tibble(model = \"glmnet model\",   \n                   rmse_trn = rmse_vec(truth = feat_trn_1$y, \n                                       estimate = predict(fit_glmnet_1,\n                                                          feat_trn_1)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_1$y, \n                                        estimate = predict(fit_glmnet_1,\n                                                           feat_test_1)$.pred))))\n\n# A tibble: 4 × 3\n  model        rmse_trn rmse_test\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 linear model     91.9      112.\n2 LASSO model      94.7      107.\n3 Ridge model      98.6      103.\n4 glmnet model     98.6      103.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#empirical-example-2-good-and-zero-predictors",
    "href": "006_regularization.html#empirical-example-2-good-and-zero-predictors",
    "title": "6  Regularization and Penalized Models",
    "section": "6.9 Empirical Example 2: Good and Zero Predictors",
    "text": "6.9 Empirical Example 2: Good and Zero Predictors\nFor the second example, we will simulate data with:\n\nTwo sets of correlated predictors\nFirst set related to outcome\nSecond set unrelated to outcome\nGet a small training sample\nGet a big test sample (for more precise estimates of performance of our model configurations)\n\n\nSet up simulation parameters\n\nn_cases_trn &lt;- 100\nn_cases_test &lt;- 1000\nn_x &lt;- 20\ncovs_x &lt;- 50 \nvars_x &lt;- 100\nb_x &lt;- rep(c(1,0), each = n_x / 2)\ny_error &lt;- 100\n\n\n\nmu &lt;- rep(0, n_x)  \n\nsigma &lt;- matrix(0, nrow = n_x, ncol = n_x)\nfor (i in 1:(n_x/2)){\n  for(j in 1:(n_x/2)){\n    sigma[i, j] &lt;- covs_x\n  }\n} \nfor (i in (n_x/2 + 1):n_x){\n  for(j in (n_x/2 + 1):n_x){\n    sigma[i, j] &lt;- covs_x\n  }\n} \n\ndiag(sigma) &lt;- vars_x  \n\n\nSimulate predictors and Y\n\nset.seed(2468)\n\nx &lt;- MASS::mvrnorm(n = n_cases_trn, mu, sigma) |&gt; \n  magrittr::set_colnames(str_c(\"x_\", 1:n_x)) |&gt;\n  as_tibble()\ndata_trn_2 &lt;- x |&gt; \n  mutate(y = rowSums(t(t(x)*b_x)) + rnorm(n_cases_trn, 0, y_error)) |&gt;  \n  glimpse()\n\nRows: 100\nColumns: 21\n$ x_1  &lt;dbl&gt; -1.4627996, -1.8160400, 2.9068102, -6.5569418, -8.0480270, 9.2138…\n$ x_2  &lt;dbl&gt; 17.7272165, 0.6310498, 9.7749301, -9.7193778, -4.3778964, 6.72976…\n$ x_3  &lt;dbl&gt; -0.5051625, -17.7689516, 6.3105699, -10.5572604, -3.9617496, 3.90…\n$ x_4  &lt;dbl&gt; 3.281062, 8.955012, 3.352868, -11.268461, -4.286181, 13.815268, -…\n$ x_5  &lt;dbl&gt; 8.35180401, 6.81665987, 0.06645588, -9.66213146, -8.85459170, 16.…\n$ x_6  &lt;dbl&gt; -5.9821809, 18.5962999, -0.8264266, 1.4451062, -9.3772084, 14.342…\n$ x_7  &lt;dbl&gt; -12.40624948, 4.28231426, 1.53437679, 5.47908047, -4.77878760, 15…\n$ x_8  &lt;dbl&gt; 3.479352, -7.562426, 16.716015, -4.841876, 1.927100, 27.236838, 1…\n$ x_9  &lt;dbl&gt; 1.102297, 1.638666, 10.573017, 5.936702, 2.084463, 11.768179, 5.7…\n$ x_10 &lt;dbl&gt; -6.6174049, 9.8993606, 5.6307484, 5.7061525, -12.3675238, 0.64541…\n$ x_11 &lt;dbl&gt; -0.797748528, -18.721814405, -3.992559581, -21.480685628, -4.0253…\n$ x_12 &lt;dbl&gt; 6.7928298, -7.3322948, -5.8933992, -13.0153958, 2.4618048, -12.34…\n$ x_13 &lt;dbl&gt; -5.68712456, -1.01785876, 7.86559179, -18.79422479, -3.31514615, …\n$ x_14 &lt;dbl&gt; -9.99575229, -16.35133901, -18.02068048, -26.14924286, 1.46020815…\n$ x_15 &lt;dbl&gt; -4.2672643, -4.4350928, -13.6207539, -15.1147495, -5.5577594, -13…\n$ x_16 &lt;dbl&gt; 8.5520238, -2.5639516, 5.5768023, -19.1732900, -4.1939131, 0.7558…\n$ x_17 &lt;dbl&gt; 4.93829278, -14.17190979, -10.23808120, -8.90013599, -9.33297489,…\n$ x_18 &lt;dbl&gt; -1.987745, -9.259699, -13.225873, -28.090077, -13.168306, -1.3565…\n$ x_19 &lt;dbl&gt; 1.5085562, -13.9721276, -5.6622726, -10.9398210, -22.5233973, -8.…\n$ x_20 &lt;dbl&gt; 1.2541564, -19.0101170, -1.6780834, -16.9207862, -7.2618287, 1.13…\n$ y    &lt;dbl&gt; -94.94590, 69.75956, 88.46711, 15.72162, 145.49463, 101.30513, 10…\n\nx &lt;- MASS::mvrnorm(n = n_cases_test, mu, sigma) |&gt; \n  magrittr::set_colnames(str_c(\"x_\", 1:n_x)) |&gt;\n  as_tibble()\ndata_test_2 &lt;- x |&gt; \n  mutate(y = rowSums(t(t(x)*b_x)) + rnorm(n_cases_test, 0, y_error)) |&gt;  \n  glimpse()\n\nRows: 1,000\nColumns: 21\n$ x_1  &lt;dbl&gt; -11.082333, 10.623468, -7.962704, -4.360526, 4.170808, -8.056227,…\n$ x_2  &lt;dbl&gt; 4.4892149, 6.5181854, 7.4099931, 5.1254868, 12.7502461, -9.275426…\n$ x_3  &lt;dbl&gt; -3.94113759, 2.65646872, -0.10913588, 13.39114437, 12.99616116, 3…\n$ x_4  &lt;dbl&gt; -17.3732360, 2.6196690, -6.8545899, 3.6877167, 9.5982646, 7.09623…\n$ x_5  &lt;dbl&gt; 2.1853683, 25.4130435, 0.2749666, 3.9995128, 16.5471387, 9.649271…\n$ x_6  &lt;dbl&gt; -13.8387545, -7.7514735, -1.0824170, -10.3723020, 13.3474819, -4.…\n$ x_7  &lt;dbl&gt; -8.1371882, 14.0102608, 2.6593081, 5.2537524, 0.2673897, -5.48200…\n$ x_8  &lt;dbl&gt; -22.8820028, 0.8089559, -2.8108121, 10.4672192, 10.9376332, 1.937…\n$ x_9  &lt;dbl&gt; -20.8906202, 13.3578359, 19.4734781, 0.7248788, 7.3258207, 14.565…\n$ x_10 &lt;dbl&gt; -18.2918851, 3.0034874, 5.3284673, -1.6321880, 5.1493291, 4.60337…\n$ x_11 &lt;dbl&gt; 18.672349, 11.091231, 14.868589, 9.865558, 4.726164, 14.227183, 1…\n$ x_12 &lt;dbl&gt; 5.5770933, 0.5463577, -9.0256983, 3.8787265, 9.8113720, -3.925492…\n$ x_13 &lt;dbl&gt; -6.996611892, 19.551699340, 15.284866989, -2.964783803, 3.1004588…\n$ x_14 &lt;dbl&gt; 21.031040, 13.320671, 18.770094, 10.911134, -0.382450, 18.983760,…\n$ x_15 &lt;dbl&gt; -1.06370853, 14.64546199, -2.46018710, 3.09542088, -0.52366758, 8…\n$ x_16 &lt;dbl&gt; -0.8450554, 17.3738247, 4.3254573, 12.7759481, 13.7785211, 13.078…\n$ x_17 &lt;dbl&gt; 4.6867098, -1.5470071, 2.1471326, -9.5172721, 11.7413577, 6.54228…\n$ x_18 &lt;dbl&gt; 6.3226155, 9.0624381, 0.5223122, 6.1315901, 5.0103132, 7.2802472,…\n$ x_19 &lt;dbl&gt; 7.6803428, -2.6084423, 4.2716484, 22.0685295, 0.4731131, 8.255899…\n$ x_20 &lt;dbl&gt; -5.4343168, -4.9070165, 13.5187641, 14.4286786, 8.4636535, 8.9575…\n$ y    &lt;dbl&gt; -83.529025, 83.571326, 86.073513, -95.654257, 79.326001, 114.5172…\n\n\n\nSet up a tibble to track model performance in train and test\n\nerror_ex2 &lt;- tibble(model = character(), rmse_trn = numeric(), rmse_test = numeric()) |&gt; \n  glimpse()\n\nRows: 0\nColumns: 3\n$ model     &lt;chr&gt; \n$ rmse_trn  &lt;dbl&gt; \n$ rmse_test &lt;dbl&gt; \n\n\n\n\n6.9.1 Fit a standard (OLS) linear regression\nFit and evaluate the linear model\n\nfit_lm_2 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(y ~ ., data = data_trn_2)\n\nfit_lm_2 |&gt; \n  tidy() |&gt; \n  print(n = 21)\n\n# A tibble: 21 × 5\n   term        estimate std.error statistic p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) -14.0        10.8    -1.30    0.198 \n 2 x_1          -0.372       1.45   -0.256   0.798 \n 3 x_2           3.54        1.39    2.55    0.0126\n 4 x_3           1.54        1.53    1.01    0.317 \n 5 x_4           2.90        1.42    2.05    0.0441\n 6 x_5           2.98        1.50    1.98    0.0509\n 7 x_6          -1.28        1.34   -0.956   0.342 \n 8 x_7           0.821       1.48    0.553   0.582 \n 9 x_8           1.38        1.26    1.10    0.276 \n10 x_9           0.280       1.25    0.225   0.823 \n11 x_10          0.0247      1.33    0.0186  0.985 \n12 x_11         -2.09        1.56   -1.34    0.185 \n13 x_12          1.71        1.47    1.16    0.248 \n14 x_13          1.33        1.42    0.936   0.352 \n15 x_14         -0.927       1.24   -0.747   0.457 \n16 x_15          2.99        1.53    1.96    0.0536\n17 x_16         -0.602       1.40   -0.431   0.668 \n18 x_17          1.78        1.28    1.39    0.168 \n19 x_18         -1.73        1.73   -1.00    0.321 \n20 x_19         -1.28        1.37   -0.938   0.351 \n21 x_20         -2.83        1.24   -2.28    0.0253\n\n\n\nIrreducible error was set by y_error (100)\n\nVery overfit to train\nVery poor performance in test\n\n\n(error_ex2 &lt;- error_ex2 |&gt; \n  bind_rows(tibble(model = \"linear model\",                       \n                   rmse_trn = rmse_vec(truth = data_trn_2$y, \n                                       estimate = predict(fit_lm_2,\n                                                          data_trn_2)$.pred),\n                   rmse_test = rmse_vec(truth = data_test_2$y, \n                                        estimate = predict(fit_lm_2,\n                                                           data_test_2)$.pred))))\n\n# A tibble: 1 × 3\n  model        rmse_trn rmse_test\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 linear model     83.4      117.\n\n\n\n\n\n6.9.2 Fit LASSO\nFor all glmnet algorithms, set up:\n\nRecipe\nFeature matrices\nBootstraps for model configuration selection (tuning)\n\n\nrec_2 &lt;- recipe(y ~ ., data = data_trn_2) |&gt; \n  step_normalize(all_predictors())\n\nrec_prep_2 &lt;- rec_2 |&gt; \n  prep(data_trn_2)\n\nfeat_trn_2 &lt;- rec_prep_2 |&gt; \n  bake(NULL)\n\nfeat_test_2 &lt;- rec_prep_2 |&gt; \n  bake(data_test_2)\n\nset.seed(20140102)\nsplits_boot_2 &lt;- data_trn_2 |&gt; \n   bootstraps(times = 100, strata = \"y\") \n\n\nTune \\(\\lambda\\) for LASSO\n\nWe can use sample penalty grids from earlier example because sample size and number of features hasnt changed so likely still good\n\n\nfits_lasso_2 &lt;- xfun::cache_rds(\n  expr = {\n    linear_reg(penalty = tune(), \n               mixture = 1) |&gt; \n    set_engine(\"glmnet\") |&gt; \n    tune_grid(preprocessor = rec_2, \n              resamples = splits_boot_2, \n              grid = grid_lasso, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/006/\",\n  file = \"fits_lasso_2\")\n\n\nPlot hyperparameters\n\nplot_hyperparameters(fits_lasso_2, hp1 = \"penalty\", metric = \"rmse\")\n\n\n\n\n\n\n\n\n\nFit best LASSO to full training set\n\nNotice the many \\(\\beta\\) = 0\nIt did set some of the “good” features to 0 as well\n\n\nfit_lasso_2 &lt;-\n  linear_reg(penalty = select_best(fits_lasso_2)$penalty, \n             mixture = 1) |&gt;\n  set_engine(\"glmnet\") |&gt; \n  fit(y ~ ., data = feat_trn_2)\n\nfit_lasso_2 |&gt; \n  tidy() |&gt; \n  print(n = 21)\n\n# A tibble: 21 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)    5.52     12.7\n 2 x_1            1.02     12.7\n 3 x_2           21.4      12.7\n 4 x_3            8.60     12.7\n 5 x_4           23.3      12.7\n 6 x_5           19.9      12.7\n 7 x_6            0        12.7\n 8 x_7            0        12.7\n 9 x_8           10.8      12.7\n10 x_9            0.589    12.7\n11 x_10           0        12.7\n12 x_11          -0.632    12.7\n13 x_12           0        12.7\n14 x_13           0        12.7\n15 x_14           0        12.7\n16 x_15           0        12.7\n17 x_16           0        12.7\n18 x_17           0        12.7\n19 x_18           0        12.7\n20 x_19           0        12.7\n21 x_20          -6.93     12.7\n\n\n\nIrreducible error was set by y_error (100)\n\nSomewhat overfit to train\nGood in val\n\n\n(error_ex2 &lt;- error_ex2 |&gt; \n  bind_rows(tibble(model = \"LASSO model\",                       \n                   rmse_trn = rmse_vec(truth = feat_trn_2$y, \n                                       estimate = predict(fit_lasso_2,\n                                                          feat_trn_2)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_2$y, \n                                        estimate = predict(fit_lasso_2,\n                                                           feat_test_2)$.pred))))\n\n# A tibble: 2 × 3\n  model        rmse_trn rmse_test\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 linear model     83.4      117.\n2 LASSO model      92.9      103.\n\n\n\n\n\n6.9.3 Fit Ridge\nTune \\(\\lambda\\) for Ridge\n\ncan use ridge penalty grid from example 1\n\n\nfits_ridge_2 &lt;- xfun::cache_rds(\n  expr = {\n    linear_reg(penalty = tune(), \n               mixture = 0) |&gt; \n    set_engine(\"glmnet\") |&gt; \n    tune_grid(preprocessor = rec_2, \n              resamples = splits_boot_2, \n              grid = grid_ridge, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/006/\",\n  file = \"fits_ridge_2\")\n\n\nPlot hyperparameters\n\nplot_hyperparameters(fits_ridge_2, hp1 = \"penalty\", metric = \"rmse\")\n\n\n\n\n\n\n\n\n\n\nplot_hyperparameters(fits_ridge_2, hp1 = \"penalty\", metric = \"rmse\")\n\n\n\n\n\n\n\n\n\nFit best Ridge to full training set\n\nNotice no \\(\\beta\\) = 0\n\n\nfit_ridge_2 &lt;-\n  linear_reg(penalty = select_best(fits_ridge_2)$penalty, \n             mixture = 0) |&gt;\n  set_engine(\"glmnet\") |&gt; \n  fit(y ~ ., data = feat_trn_2)\n\nfit_ridge_2 |&gt; \n  tidy() |&gt; \n  print(n = 21)\n\n# A tibble: 21 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)     5.52    132.\n 2 x_1             7.19    132.\n 3 x_2            14.7     132.\n 4 x_3             9.23    132.\n 5 x_4            14.6     132.\n 6 x_5            12.7     132.\n 7 x_6             4.12    132.\n 8 x_7             6.27    132.\n 9 x_8             9.75    132.\n10 x_9             5.83    132.\n11 x_10            5.18    132.\n12 x_11           -6.65    132.\n13 x_12            4.21    132.\n14 x_13            3.84    132.\n15 x_14           -2.49    132.\n16 x_15            5.80    132.\n17 x_16           -1.39    132.\n18 x_17            4.19    132.\n19 x_18           -5.07    132.\n20 x_19           -5.88    132.\n21 x_20           -8.57    132.\n\n\n\nIrreducible error was set by y_error (100)\n\nSomewhat overfit to train\nStill slightly better than LASSO in test\n\n\n(error_ex2 &lt;- error_ex2 |&gt; \n  bind_rows(tibble(model = \"Ridge model\",                       \n                   rmse_trn = rmse_vec(truth = feat_trn_2$y, \n                                       estimate = predict(fit_ridge_2,\n                                                          feat_trn_2)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_2$y, \n                                        estimate = predict(fit_ridge_2,\n                                                           feat_test_2)$.pred))))\n\n# A tibble: 3 × 3\n  model        rmse_trn rmse_test\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 linear model     83.4      117.\n2 LASSO model      92.9      103.\n3 Ridge model      90.6      102.\n\n\n\n\n\n6.9.4 Fit Complete glmnet\nTune \\(\\lambda\\) and \\(\\alpha\\) for glmnet\n\nfits_glmnet_2 &lt;- xfun::cache_rds(\n  expr = {\n    linear_reg(penalty = tune(), \n               mixture = tune()) |&gt; \n    set_engine(\"glmnet\") |&gt; \n    tune_grid(preprocessor = rec_2, \n              resamples = splits_boot_2, \n              grid = grid_glmnet, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/006/\",\n  file = \"fits_glmnet_2\")\n\n\nPlot hyperparameters\n\nplot_hyperparameters(fits_glmnet_2, hp1 = \"penalty\", hp2 = \"mixture\", metric = \"rmse\")\n\n\n\n\n\n\n\n\n\nFit Best glmnet in full train set\n\nStill Ridge (but won’t always be)\n\n\nselect_best(fits_glmnet_2)\n\n# A tibble: 1 × 3\n  penalty mixture .config                \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  \n1    132.       0 Preprocessor1_Model0368\n\nfit_glmnet_2 &lt;-\n  linear_reg(penalty = select_best(fits_glmnet_2)$penalty, \n             mixture = select_best(fits_glmnet_2)$mixture) |&gt;\n  set_engine(\"glmnet\") |&gt; \n  fit(y ~ ., data = feat_trn_2)\n\nfit_glmnet_2 |&gt; \n  tidy() |&gt; \n  print(n = 21)\n\n# A tibble: 21 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)     5.52    132.\n 2 x_1             7.19    132.\n 3 x_2            14.7     132.\n 4 x_3             9.23    132.\n 5 x_4            14.6     132.\n 6 x_5            12.7     132.\n 7 x_6             4.12    132.\n 8 x_7             6.27    132.\n 9 x_8             9.75    132.\n10 x_9             5.83    132.\n11 x_10            5.18    132.\n12 x_11           -6.65    132.\n13 x_12            4.21    132.\n14 x_13            3.84    132.\n15 x_14           -2.49    132.\n16 x_15            5.80    132.\n17 x_16           -1.39    132.\n18 x_17            4.19    132.\n19 x_18           -5.07    132.\n20 x_19           -5.88    132.\n21 x_20           -8.57    132.\n\n\n\nIrreducible error was set by y_error (100)\n\nSomewhat overfit to train\nStill not bad in validate\n\n\n(error_ex2 &lt;- error_ex2 |&gt; \n  bind_rows(tibble(model = \"glmnet model\",   \n                   rmse_trn = rmse_vec(truth = feat_trn_2$y, \n                                       estimate = predict(fit_glmnet_2,\n                                                          feat_trn_2)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_2$y, \n                                        estimate = predict(fit_glmnet_2,\n                                                           feat_test_2)$.pred))))\n\n# A tibble: 4 × 3\n  model        rmse_trn rmse_test\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 linear model     83.4      117.\n2 LASSO model      92.9      103.\n3 Ridge model      90.6      102.\n4 glmnet model     90.6      102.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#lasso-for-feature-e.g.-covariate-selection",
    "href": "006_regularization.html#lasso-for-feature-e.g.-covariate-selection",
    "title": "6  Regularization and Penalized Models",
    "section": "6.10 LASSO for Feature (e.g., Covariate) Selection?",
    "text": "6.10 LASSO for Feature (e.g., Covariate) Selection?\nLets consider a typical explanatory setting in Psychology\n\nA focal dichotomous IV (your experimental manipulation)\nA number of covariates (some good, some bad)\nA quantitative outcome (y)\nCovariates are uncorrelated with IV b/c IV is manipulated\n\nLet’s pretend the previous 20 xs were your covariates\n\nWhat are your options to test iv prior to this course?\n\nYou want to use covariates to increase power\nBUT you don’t know which covariates to use\n\nYou might use all of them\nOr you might use none of them (a clear lost opportunity)\nOr you might hack it by using those increase your focal IV effect (very bad!)\n\n\n\nNOW, We might use the feature selection characteristics for LASSO to select which covariates are included\n\nThere are two possibilities that occur to me\n\n\nUse LASSO to build best DGP for a covariates only model\n\n\n\n\nCould be more conservative (fewer covariates) by using within 1 SE of best performance but less flexible (i.e., will set more parameter estimates to 0)\nFollow up with a linear model (using \\(lm\\)), regressing y on \\(iv\\) and covariates from LASSO that are non-zero\n\n\nfit_lasso_2 |&gt; \n  tidy() |&gt; \n  print(n = 21)\n\n# A tibble: 21 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)    5.52     12.7\n 2 x_1            1.02     12.7\n 3 x_2           21.4      12.7\n 4 x_3            8.60     12.7\n 5 x_4           23.3      12.7\n 6 x_5           19.9      12.7\n 7 x_6            0        12.7\n 8 x_7            0        12.7\n 9 x_8           10.8      12.7\n10 x_9            0.589    12.7\n11 x_10           0        12.7\n12 x_11          -0.632    12.7\n13 x_12           0        12.7\n14 x_13           0        12.7\n15 x_14           0        12.7\n16 x_15           0        12.7\n17 x_16           0        12.7\n18 x_17           0        12.7\n19 x_18           0        12.7\n20 x_19           0        12.7\n21 x_20          -6.93     12.7\n\n\n\nYou clearly improved your best guess on covariates to include\nYou will regress y on iv and the 11 covariates with non-zero effects\n\n\n\nUse LASSO to build best DGP including iv and covariates but don’t penalize iv\n\n\n\n\nLook at penalty.factor = rep(1, nvars) argument in glmnet()\nCan fit LASSO with unbiased? estimate of iv\nNeed to bootstrap for SE for iv (next unit)\nOnly appropriate if IV is manipulated\n\n\n\nShould really conduct simulation study of both of these options (vs. all and no covariates).\n\nI want to\nWant to do the study with me?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#ridge-lasso-and-elastic-net-models-for-other-y-distributions",
    "href": "006_regularization.html#ridge-lasso-and-elastic-net-models-for-other-y-distributions",
    "title": "6  Regularization and Penalized Models",
    "section": "6.11 Ridge, LASSO, and Elastic net models for other Y distributions",
    "text": "6.11 Ridge, LASSO, and Elastic net models for other Y distributions\nThese penalties can be added to the cost functions of other generalized linear models to yield regularized/penalized versions of those models as well. For example\n\nL1 penalized (LASSO) logistic regression (w/ labels coded 0,1):\n\n\\(\\frac{1}{n}([\\:\\sum_{i = 1}^{n} -Y_ilog(\\hat{Y_i}) - (1-Y_i)log(1-\\hat{Y_i})\\:]\\:+\\:[\\:\\lambda\\sum_{j = 1}^{p} |\\beta_j|\\:]\\)\n\n\n\nFor L2 penalized (Ridge) logistic regression (w/ labels coded 0,1)\n\n\\(\\frac{1}{n}([\\:\\sum_{i = 1}^{n} -Y_ilog(\\hat{Y_i}) - (1-Y_i)log(1-\\hat{Y_i})\\:]\\:+\\:[\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:]\\)\n\n\n\nglmnet implements:\n\nfamily = c(\"gaussian\", \"binomial\", \"poisson\", \"multinomial\", \"cox\", \"mgaussian\")\nFull range of \\(\\alpha\\) to mix two types of penalties",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "006_regularization.html#discussion",
    "href": "006_regularization.html#discussion",
    "title": "6  Regularization and Penalized Models",
    "section": "6.12 Discussion",
    "text": "6.12 Discussion\n\n6.12.1 Announcements\n\nMidterm - application and conceptual parts\nCan we have a midterm review session?\n\n\n\n\n6.12.2 feature matrix vs. raw predictors\nWhen to use which and why could I use raw predictors in simulated data example from lectures\n\n\n\n6.12.3 Cost functions\nWhat is Cost function?\n\nLinear model\n\n\\(\\frac{1}{n}\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}\\)\n\n\n\nRidge (L2)\n\n\\(\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:])\\)\n\n\n\nLASSO (L1)\n\n\\(\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} |\\beta_j|\\:])\\)\n\n\n\nElastic Net\n\n\\(\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda (\\alpha\\sum_{j = 1}^{p} |\\beta_j| + (1-\\alpha)\\sum_{j = 1}^{p} \\beta_j^{2})\\:])\\)\n\n\n\n\n6.12.4 LASSO, Ridge, and elastic net questions\n\nThe pros and cons of Lasso and Ridge vs. Elastic Net\nI’m still a little confused as to why you would ever use Ridge or LASSO separately when you can just selectively use one or the other through elastic net. Wouldn’t it make sense to just always use elastic net and then change the penalty accordingly for when you wanted to use a Ridge or LASSO regression approach?\n\nPure LASSO\nmodel variance\ntwo self-report measure scales vs. items\n\nSome explanation why Lasso is more robust to outliers and ridge is more robust to measurement error would be appreciated.\nI’m not very clear why LASSO could limit some parameters to be zero but ridge regression cannot. Can we go through this a bit?\nHow do you know what numbers to start at for tuning lamba (like in the code below)? I think John mentioned he has a function to find these, but I’m wondering if there are any rules of thumb.\nHow do we know how “strong” of a penalty we need to be applying to our cost function? Does the reduction in variance increase as we increase the strength of the penalty?\nIs a decreased number of features always good or bad, or does it depend on the model/recipe\nCan you talk more about how to interpret the scale of the parameter estimates? In the lecture you said the following and I’m not quite sure what that means:\nI might be totally wrong but I wonder if we have to care about the multi-collinearity or high dimension on classification as well. Or this is only limited to regression and so we are solving with regularising only regression model?\n\n\n\n\n6.12.5 Stepwise questions\n\nCould you go over Forward, Backward, and Best Subset subsetting? I think I understand the algorithm they use, but I do not understand the penalty function they use for picking the “best” model. In the book, it looks like it uses R-squared to pick the best model, but wouldn’t the full model always have the greatest R-squared?\nIs there a reason why we do not discuss variable selection using subset methods?\nIs there specific cases when you would pick backwards or forwards selection, or is it up to the researcher?\nTraining vs. val/val error in stepwise approaches.\nUse of AIC, BIC, Cp, and adjusted R2 vs. cross-validation\n\n\n\n\n6.12.6 Explanatory goals\n\nCan LASSO be used for variable selection when engaging in cross sectional data analysis to identify which variables in a large set of Xs are important for a particular outcome?\nIn practice, what elements should be considered before selecting IVs and covariates?\n\n\n\n\n6.12.7 PCA\n\nhttps://setosa.io/ev/principal-component-analysis/\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regularization and Penalized Models</span>"
    ]
  },
  {
    "objectID": "007_midterm.html",
    "href": "007_midterm.html",
    "title": "7  Midterm Exam",
    "section": "",
    "text": "7.1 Applications (take-home) Exam\nThe Applications exam is due at 8 pm on Wednesday, March 6th.\nAs the name suggests, it will focus primarily applications. However, within the QMD file there are questions that also assess your understanding of what you are doing and why.\nYou should complete the exam as you have previously completed the application assignments:\nIn contrast to the application assignments, the TAs and I will not be able to answer substantive questions about the exam. However, if you need us to clarify what we are requesting you to do for any specific question or believe you have found an error in the exam, please post your question to the exam_quizzes channel in Slack and we will respond ASAP\nExam qmd shell\nData:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Midterm Exam</span>"
    ]
  },
  {
    "objectID": "007_midterm.html#applications-take-home-exam",
    "href": "007_midterm.html#applications-take-home-exam",
    "title": "7  Midterm Exam",
    "section": "",
    "text": "Download the datasets and rmd file below\nProvide the requested code in the empty code chunks that are included in the rmd file\nAnswer the questions posed outside the code chunks within the text sections of the rmd file, immediately after the question.\nWhen you are done, knit the file to html and upload this knit file through the Canvas\nThe exam is due on Wednesday, March 6th, at 8pm\n\n\n\n\n\ndataset 1; data dictionary\ndataset 2\ndataset 3",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Midterm Exam</span>"
    ]
  },
  {
    "objectID": "007_midterm.html#conceptual-exam",
    "href": "007_midterm.html#conceptual-exam",
    "title": "7  Midterm Exam",
    "section": "7.2 Conceptual Exam",
    "text": "7.2 Conceptual Exam\nThe conceptual exam will be held in class during the Discussion section on Thursday, March 7th.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Midterm Exam</span>"
    ]
  },
  {
    "objectID": "008_advanced_performance_metrics.html",
    "href": "008_advanced_performance_metrics.html",
    "title": "8  Advanced Performance Metrics",
    "section": "",
    "text": "8.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Performance Metrics</span>"
    ]
  },
  {
    "objectID": "008_advanced_performance_metrics.html#overview-of-unit",
    "href": "008_advanced_performance_metrics.html#overview-of-unit",
    "title": "8  Advanced Performance Metrics",
    "section": "",
    "text": "8.1.1 Learning Objectives\n\nUnderstand costs and benefits of accuracy\nUse of a confusion matrix\nUnderstand costs and benefits of other performance metrics\nThe ROC curve and area under the curve\nModel selection using other performance metrics\nHow to address class imbalance\n\nSelection of performance metric\nSelection of classification threshold\nSampling and resampling approaches\n\n\n\n\n\n8.1.2 Readings\n\nKuhn and Johnson (2018) Chapter 11, pp 247-266\nKuhn and Johnson (2018) Chapter 16, pp 419-435\nWyant et al, in press\n\nPost questions to the readings channel in Slack\n\n\n8.1.3 Lecture Videos\n\nLecture 1: Unit Introduction ~ 15 mins\nLecture 2: The Confusion Matrix, Part 1~ 33 mins\nLecture 3: The Confusion Matrix, Part 2~ 11 mins\nLecture 4: The Receiver Operating Characteristic (ROC) Curve ~ 25 mins\nLecture 5: Selecting Model Configurations with Other Metrics ~ 10 mins\nLecture 6: Addressing Class Imbalance ~ 24 mins\n\nPost questions to the video-lectures channel in Slack\n\n\n\n8.1.4 Coding Assignment\n\ndata\nqmd shell\nsolution\n\nPost questions to application-assignments Slack channel\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, March 13th",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Performance Metrics</span>"
    ]
  },
  {
    "objectID": "008_advanced_performance_metrics.html#introduction",
    "href": "008_advanced_performance_metrics.html#introduction",
    "title": "8  Advanced Performance Metrics",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nIn this unit, we will again use the Cleveland heart disease dataset.\n\nHowever, I have modified this to make the outcome unbalanced such that Yes represents approximately 10% of the observations\n\nNow that we will calculate performance metrics beyond accuracy, the order of the levels of our outcome variable(disease) matters. We will make sure that the positive class (event of interest; in this case yes for disease) is the first level.\n\nFirst, lets open and skim the raw data\n\ndata_all &lt;- read_csv(here::here(path_data, \"cleveland_unbalanced.csv\"), \n                     col_names = FALSE, na = \"?\",col_types = cols()) |&gt; \n  rename(age = X1,\n         sex = X2,\n         cp = X3,\n         rest_bp = X4,\n         chol = X5,\n         fbs = X6,\n         rest_ecg = X7,\n         exer_max_hr = X8,\n         exer_ang = X9,\n         exer_st_depress = X10,\n         exer_st_slope = X11,\n         ca = X12,\n         thal = X13,\n         disease = X14) \n\ndata_all |&gt; skim_some()\n\n\nData summary\n\n\nName\ndata_all\n\n\nNumber of rows\n1281\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\nage\n0\n1.00\n29\n77.0\n\n\nsex\n0\n1.00\n0\n1.0\n\n\ncp\n0\n1.00\n1\n4.0\n\n\nrest_bp\n0\n1.00\n94\n200.0\n\n\nchol\n0\n1.00\n126\n564.0\n\n\nfbs\n0\n1.00\n0\n1.0\n\n\nrest_ecg\n0\n1.00\n0\n2.0\n\n\nexer_max_hr\n0\n1.00\n71\n202.0\n\n\nexer_ang\n0\n1.00\n0\n1.0\n\n\nexer_st_depress\n0\n1.00\n0\n6.2\n\n\nexer_st_slope\n0\n1.00\n1\n3.0\n\n\nca\n22\n0.98\n0\n3.0\n\n\nthal\n8\n0.99\n3\n7.0\n\n\ndisease\n0\n1.00\n0\n4.0\n\n\n\n\n\n\nCode categorical variables as factors with meaningful text labels (and no spaces)\n\nNOTE the use of disease = fct_relevel (disease, \"yes\") to set yes as positive class (first level) for disease\n\n\ndata_all &lt;- data_all |&gt; \n  mutate(disease = factor(disease, levels = 0:4, \n                          labels = c(\"no\", \"yes\", \"yes\", \"yes\", \"yes\")),\n         disease = fct_relevel (disease, \"yes\"),\n         sex = factor(sex,  levels = c(0, 1), labels = c(\"female\", \"male\")),\n         fbs = factor(fbs, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_ang = factor(exer_ang, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_st_slope = factor(exer_st_slope, levels = 1:3, \n                                labels = c(\"upslope\", \"flat\", \"downslope\")),\n         cp = factor(cp, levels = 1:4, \n                     labels = c(\"typ_ang\", \"atyp_ang\", \"non_anginal\", \"non_anginal\")),\n         rest_ecg = factor(rest_ecg, levels = 0:2, \n                           labels = c(\"normal\", \"wave_abn\", \"ventric_hypertrophy\")),\n         thal = factor(thal, levels = c(3, 6, 7), \n                       labels = c(\"normal\", \"fixeddefect\", \"reversabledefect\")))\n\ndata_all |&gt; skim_some()\n\n\nData summary\n\n\nName\ndata_all\n\n\nNumber of rows\n1281\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n8\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsex\n0\n1.00\nFALSE\n2\nmal: 752, fem: 529\n\n\ncp\n0\n1.00\nFALSE\n3\nnon: 872, aty: 296, typ: 113\n\n\nfbs\n0\n1.00\nFALSE\n2\nno: 1104, yes: 177\n\n\nrest_ecg\n0\n1.00\nFALSE\n3\nnor: 721, ven: 550, wav: 10\n\n\nexer_ang\n0\n1.00\nFALSE\n2\nno: 1044, yes: 237\n\n\nexer_st_slope\n0\n1.00\nFALSE\n3\nups: 778, fla: 434, dow: 69\n\n\nthal\n8\n0.99\nFALSE\n3\nnor: 940, rev: 285, fix: 48\n\n\ndisease\n0\n1.00\nFALSE\n2\nno: 1142, yes: 139\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\nage\n0\n1.00\n29\n77.0\n\n\nrest_bp\n0\n1.00\n94\n200.0\n\n\nchol\n0\n1.00\n126\n564.0\n\n\nexer_max_hr\n0\n1.00\n71\n202.0\n\n\nexer_st_depress\n0\n1.00\n0\n6.2\n\n\nca\n22\n0.98\n0\n3.0\n\n\n\n\n\n\nDisease is now unbalanced\n\ndata_all |&gt; tab(disease)\n\n# A tibble: 2 × 3\n  disease     n  prop\n  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;\n1 yes       139 0.109\n2 no       1142 0.891\n\n\n\nFor this example, we will evaluate our final model using a held out test set\n\nset.seed(20140102)\nsplits_test &lt;- data_all |&gt; \n  initial_split(prop = 2/3, strata = \"disease\")\n\ndata_trn &lt;- splits_test |&gt; \n  analysis()\n\ndata_test &lt;- splits_test |&gt; \n  assessment()\n\n\nWe will be fitting a penalized logistic regression again (using glmnet)\nWe will do only basic feature engineering for this algorithm and to handle missing data\n\nrec &lt;- recipe(disease ~ ., data = data_trn) |&gt; \n  step_impute_median(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt;   \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_normalize(all_predictors())\n\n\nWe tune/select best hyperparameter values via bootstrap resampling with the training data\n\nget bootstrap splits\nmake grid of hyperparameter values\n\n\nsplits_boot &lt;- data_trn |&gt; \n  bootstraps(times = 100, strata = \"disease\")  \n\ngrid_glmnet &lt;- expand_grid(penalty = exp(seq(-8, 3, length.out = 200)),\n                           mixture = seq(0, 1, length.out = 6))\n\n\n\nfits_glmnet &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      set_mode(\"classification\") |&gt; \n      tune_grid(preprocessor = rec, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(accuracy))\n  },\n  dir = \"cache/008/\",\n  file = \"fits_glmnet\",\n  rerun = rerun_setting)\n\n\nReview hyperparameter plot and best values for hyperparameters\n\nplot_hyperparameters(fits_glmnet, hp1 = \"penalty\", hp2 = \"mixture\", \n                     metric = \"accuracy\", log_hp1 = TRUE)\n\n\n\n\n\n\n\n\n\nshow_best(fits_glmnet, n = 1)\n\n# A tibble: 1 × 8\n   penalty mixture .metric  .estimator  mean     n std_err\n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 0.000583       1 accuracy binary     0.929   100 0.00108\n  .config                \n  &lt;chr&gt;                  \n1 Preprocessor1_Model1011\n\n\n\nLet’s fit this best model configuration to all of our training data and evaluate it in test\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfit_glmnet &lt;-   \n  logistic_reg(penalty = select_best(fits_glmnet)$penalty, \n               mixture = select_best(fits_glmnet)$mixture) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(disease ~ ., data = feat_trn)\n\n\nAnd evaluate it by predicting into test\n\nfeat_test &lt;- rec_prep |&gt; \n  bake(data_test)\n\n(model_accuracy &lt;- accuracy_vec(feat_test$disease, predict(fit_glmnet, feat_test)$.pred_class))\n\n[1] 0.9299065\n\n\n\nAccuracy is an attractive measure because it is:\n\nIntuitive and widely understood\nNaturally extends to multi-class scenarios\nThese are not trivial advantages in research or application\n\n\n\nHowever, accuracy has at least three problems in some situations\n\nIf the outcome is unbalanced, it can be misleading\n\nHigh performance from simply predicting the majority (vs. minority) class for all observations\nNeed to anchor evaluation of accuracy to baseline performance based on the majority case percentage\n\nIf the outcome is unbalanced, selecting among model configurations with accuracy can be biased toward configurations that predict the majority class because that will yield high accuracy by itself even without any signal from the predictors\nRegardless of outcome distribution, it considers false positives (FP) and false negatives (FN) equivalent in their costs\n\nThis is often not the case\n\n\n\nOutcome distributions :\n\nMay start to be considered unbalanced at ratios of 1:5 (20% of cases in the infrequent class)\nIn many real life applications (e.g., fraud detection), imbalance ratios ranging from 1:1000 up to 1:5000 are not atypical\n\nWhen working with unbalanced datasets:\n\nThe class or classes with many observations are called the major or majority class(es)\nThe class with few observations (and there is typically just one) is called the minor or minority class.\n\n\n\nIn our example, the majority class is negative (no) for heart disease and the minority class is positive (yes) for heart disease\n\n\n\n\n\n\n\nQuestion: In our example, our model’s accuracy in test seemed high but was it really performing as well as this seems?\n\n\n\n\n\n\n\nShow Answer\nAlthough this test accuracy seems high, this is somewhat misleading.  A model that \nsimply labeled everyone as negative for heart disease would achieve almost as high \naccuracy in our test data\n\n\n\n\n\n\n\n(model_accuracy &lt;- accuracy_vec(feat_test$disease, \n                                predict(fit_glmnet, feat_test)$.pred_class))\n\n[1] 0.9299065\n\nfeat_test |&gt; tab(disease)\n\n# A tibble: 2 × 3\n  disease     n  prop\n  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;\n1 yes        47 0.110\n2 no        381 0.890\n\n\n\n\n\n\n\n\n\nQuestion: Perhaps more importantly, are the costs of false positives (screening someone as positive when they do not have heart disease) and false negatives (screening someone as negative when they do have heart disease) comparable for a preliminary screening method?\n\n\n\n\n\n\n\nShow Answer\nProbably not.  A false positive might mean that we do more testing that is unnecessary \nand later find out they do not have heart disease.   This comes with some monetary \ncost and also likely some distress for the patient.   However, a false negative \nmeans we send the patient home thinking they are healthy and may suffer a heart \nattack or other bad outcome.  That seems worse if this is only a preliminary screen.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Performance Metrics</span>"
    ]
  },
  {
    "objectID": "008_advanced_performance_metrics.html#the-confusion-matrix-and-related-performance-metrics",
    "href": "008_advanced_performance_metrics.html#the-confusion-matrix-and-related-performance-metrics",
    "title": "8  Advanced Performance Metrics",
    "section": "8.3 The Confusion Matrix and Related Performance Metrics",
    "text": "8.3 The Confusion Matrix and Related Performance Metrics\nAt a minimum, it seems important to consider these issues explicitly but accuracy is not sufficiently informative.\n\nThe first step to this more careful assessment is to construct a confusion matrix\n\n\n\n\n\nGround Truth\n\n\n\n\n\nPrediction\nPositive\nNegative\n\n\nPositive\nTP\nFP\n\n\nNegative\nFN\nTN\n\n\n\nDefinitions:\n\nTP: True positive\nTN: True negative\nFP: False positive (Type 1 error/false alarm)\nFN: False negative (Type 2 error/miss)\n\n\n\nPerfect classifier has all the observations on the diagonal from top left to bottom right\n\nThe two types of errors (on the other diagonal) may have different costs\n\nWe can now begin to consider these costs\n\nLets look at the confusion matrix associated with our model’s performance in test\n\nWe will use conf_mat() to calculate the confusion matrix\nThere does NOT (yet?) seem to be a vector version (i.e., conf_mat_vec())\nTherefore, we have to build a tibble of truth and estimate to pass into conf_mat()\nIt is best to assign the result to an object (e.g., cm) because we will use it for a few different tasks\n\n\ncm &lt;- tibble(truth = feat_test$disease,\n             estimate = predict(fit_glmnet, feat_test)$.pred_class) |&gt; \n  conf_mat(truth, estimate)\n\n\nLet’s display the matrix\n\nThe columns are sorted based on the true value for the observation (i.e., ground truth)\n\nIn this case, that is the patients’ true disease status\nWe can see it is unbalanced with most of the cases in the “no” column\n\nThe rows are sorted by our model’s predictions\nAs we noted above, correct predictions fall on the top/left- bottom/right diagonal\n\n\ncm\n\n          Truth\nPrediction yes  no\n       yes  30  13\n       no   17 368\n\n\n\nTidy model’s makes it easy to visualize this matrix in one of two types of plots\n\nmosaic (the default)\n\n\nautoplot(cm)\n\n\n\n\n\n\n\n\n\nheatmap\n\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\nRegardless of the plot, you can now begin to see the issues with our model\n\nIt seems accurate for patients that do NOT have heart disease\n\n368/381 correct\n\nIt is not very accurate for patients that DO have heart disease\n\n30/47 correct\n\nThis differential performance was masked by our global accuracy measure because overall accuracy was weighted heavily toward accuracy for patients without heart disease given their much higher numbers\n\n\nWe can use this confusion matrix as the starting point for MANY other common metrics and methods for evaluating the performance of a classification model. In many instances, the metrics come in pairs that are relevant for FP and FN errors\n\nSensitivity & Specificity\nPositive and Negative Predictive Value (PPV, NPV)\nPrecision and Recall\n\n\nThere are also some single metric approaches (like accuracy) that may be preferred when the outcome is unbalanced\n\nBalanced accuracy\nF1 (and other F-scores)\nKappa\n\n\nThere are also graphical approaches are based on either sensitivity/specificity or precision/recall. These are:\n\nThe Receiver Operating Characteristic (ROC) curve\nThe Precision/Recall Curve (not covered further in this unit)\nEach of these curves also yields a single metric that represents the area under the curve\n\n\nThe best metric/method is a function both of your intended use and the class distributions\n\nAs noted, accuracy is widely understood but can be misleading when class distributions are highly unbalanced\nSensitivity/Specificity are common in literatures that consider diagnosis (clinical psychology/psychiatry, medicine)\nPositive/Negative predictive value are key to consider with sensitivity/specificity when classes are unbalanced\nROC curve and its auROC metric provide nice summary visualization and metric when considering classification thresholds other than 50% (also common when classes are unbalanced or types of errors matter)\n\n\nHere are definitions of many of the most common metrics linked to the confusion matrix\n\n\n\n\nGround Truth\n\n\n\n\n\nPrediction\nPositive\nNegative\n\n\nPositive\nTP\nFP\n\n\nNegative\nFN\nTN\n\n\n\n\\(Accuracy = \\frac{TN + TP}{TN + TP + FN + FP}\\)\n\\(Sensitivity\\:(Recall) = \\frac{TP}{TP + FN}\\)\n\\(Specificity = \\frac{TN}{TN + FP}\\)\n\\(Positive\\:Predictive\\:Value\\:(Precision) = \\frac{TP}{TP + FP}\\)\n\\(Negative\\:Predictive\\:Value = \\frac{TN}{TN + FN}\\)\n\\(Balanced\\:accuracy = \\frac{Sensitivity + Specificity}{2}\\)\n\\(F_\\beta\\) score:\n\n\\(F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}\\) (most common; harmonic mean of precision and recall)\n\\(F_\\beta = (1 + \\beta^2) * \\frac{Precision * Recall}{(\\beta^2*Precision) + Recall}\\)\n\\(F_\\beta\\) was derived so that it measures the effectiveness of a classifier for someone who assigns \\(\\beta\\) times as much importance to recall as precision\n\n\nIt is easy to get any of these performance metrics using summary() on the confusion matrix\n\nMany of the statistics generated are based on an understanding of which level is the positive level.\n\nTidymodels (yardstick to be precise) will default to consider the first level the positive level.\n\nIf this is not true, some statistics (e.g., sensitivity, specificity) will be incorrect (i.e., swapped).\nYou can override this default by setting the following parameter inside any function that is affected by the order of the classes” event_level = \"second\"\n\n\nHere is summary in action!\n\ncm |&gt; summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.930\n 2 kap                  binary         0.628\n 3 sens                 binary         0.638\n 4 spec                 binary         0.966\n 5 ppv                  binary         0.698\n 6 npv                  binary         0.956\n 7 mcc                  binary         0.628\n 8 j_index              binary         0.604\n 9 bal_accuracy         binary         0.802\n10 detection_prevalence binary         0.100\n11 precision            binary         0.698\n12 recall               binary         0.638\n13 f_meas               binary         0.667\n\n\n\nLet’s consider some of what these metrics are telling us about our classifier by looking at the metrics and a confusion matrix plot\n\nLet’s start with sensitivity and specificity and their arithmetic mean (balanced accuracy)\n\nThese are column specific accuracies\nFocus is on truth (columns)\nFocuses a priori on two types of patients that may walk into the clinic to use our classifier\n\n\ncm |&gt; \n  summary() |&gt; \n  filter(.metric == \"sens\" | .metric == \"spec\" | .metric == \"bal_accuracy\") |&gt; \n  select(-.estimator)\n\n# A tibble: 3 × 2\n  .metric      .estimate\n  &lt;chr&gt;            &lt;dbl&gt;\n1 sens             0.638\n2 spec             0.966\n3 bal_accuracy     0.802\n\n\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can you link the numbers in the confusion matrix on the previous slide to Sensitivity and Specificity metrics\n\n\n\n\n\n\n\nShow Answer\nSensivity is \"accuracy\" for the positive cases (in this instance, those with \ndisease = yes).  Sensitivity = 30 / (17 + 30)\n\nSpecificity is \"accuracy\" for the negative cases (disease = no).  \nSpecificity = 368 / (368 + 13)\n\n\n\n\n\n\nNow let’s consider positive predictive value and negative predictive value\n\nThese are row specific accuracies\nFocus is on model predictions (rows)\nFocuses on the utility of the information/screening result provided from our classifier\nNot typically reported alone but instead in combo with sensitivity/specificity and prevalence (see next pages)\nMosaic plot is better visualization for sensitivity/specificity (though I also like the numbers). Not that useful for PPV/NPV\nUse heatmap?\n\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\ncm |&gt; \n  summary() |&gt;\n  filter(.metric == \"ppv\" | .metric == \"npv\") |&gt; \n  select(-.estimator)\n\n# A tibble: 2 × 2\n  .metric .estimate\n  &lt;chr&gt;       &lt;dbl&gt;\n1 ppv         0.698\n2 npv         0.956\n\n\n\n\n\n\n\n\n\nQuestion: Can you link the numbers in the confusion matrix on the previous slide to PPV and NPV metrics\n\n\n\n\n\n\n\nShow Answer\nPPV is \"accuracy\" for the positive predictions (in this instance, when the \nmodel predicts yes. \nPPV = 30 / (30 + 13)\n\nNPV is \"accuracy\" for the negative predictions (disease = no).  \nNPV = 368 / (368 + 17)\n\n\n\n\n\n\n\nPPV and NPV are influenced by both sensitivity and specificity BUT ALSO prevalence.\n\n\nThis becomes important in unbalanced settings where prevalence of classes is not equal\n\nYour classifier’s PPV will be lower, even with good sensitivity and specificity if the prevalence of the positive class is low\n\nConversely, your classifier’s NPV will be lower, even with good sensitivity and specificity, if the prevalence of the negative class is low.\n\n\n\nPrevalence also can vary by testing setting\n\n\nTests for many genetic disorders have very good sensitivity and specificity but their PPV (and NPV) vary widely by setting/patients tested\n\nTest for multiple endocrine neoplasia type 2 (MEN2) based on mutations in RET\n\nSensitivity = 98%\nSpecificity = 99.9%\n\n\nMENS2 has prevalence of 1/30,000 in general population. If using the test in the general population with 3 million people:\n\n100 will have the disease\n2,999,900 will not have the disease\nColumn accuracies (sensitivity and specificity) are high\nPPV will be very BAD; 98/(3000 + 98) = 3.2%\nThough NPV will be very near perfect! 2996900 / (2996900 + 2)\n\n\n\n\n\nGround Truth\n\n\n\n\n\nPrediction\nPositive\nNegative\n\n\nPositive\n98\n3000\n\n\nNegative\n2\n2996900\n\n\n\n\nHowever, MENS2 prevalence is high (1/5) among patients who present in a clinic with medullary thyroid carcinoma. If we only used the test among 3 million of these patients\n\n600,000 will have the disease\n2,400,000 will NOT have the disease (still unbalanced by but much less)\nColumn accuracies (sensitivity and specificity) remain the same (98% and 99.9%)\n\nThese are properties of the test/classifier\n\nPPV is now much better; 588,000 / (2400 + 588,000) = 99.6%\n\n\n\n\n\nGround Truth\n\n\n\n\n\nPrediction\nPositive\nNegative\n\n\nPositive\n588000\n2400\n\n\nNegative\n12000\n2397600\n\n\n\n\nNow think about “accuracy” of any specific COVID test\n\nIt dismayed me to see talk of accuracy\n\nThe cost of the two types of errors is different!\n\nOccasionally, there was talk of sensitivity and specificity\nThere was rarely/never discussion of PPV and NPV, which is what matters most when you are given your test result\n\n\n\n\n\n\n\n\nQuestion: How would the PPV and NPV change when we moved from testing only people with symptoms who presented at the hospital to testing everyone (e.g., all college students)?\n\n\n\n\n\n\n\nShow Answer\nRelatively speaking, when testing someone with obvious COVID symptoms PPV would \nbe high but NPV could be low.  Conversely, for our students PPV is likely low but \nNPV is likely high\n\n\n\n\n\n\nIn some instances, it may be more useful to focus on precision and recall rather than sensitivity and specificity. The F1 measure is the harmonic mean of precision and recall\n\nThis is a row and column accuracy\nRecall (sensitivity) focuses on how many true positive cases will we correctly identify\nPrecision (PPV) focuses on how accurate the prediction of “positive” will be (prevalence dependent)\nThis keeps the focus on positive cases\n\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\ncm |&gt; \n  summary() |&gt; \n  filter(.metric == \"precision\" | .metric == \"recall\" | .metric == \"f_meas\") |&gt; \n  select(-.estimator)\n\n# A tibble: 3 × 2\n  .metric   .estimate\n  &lt;chr&gt;         &lt;dbl&gt;\n1 precision     0.698\n2 recall        0.638\n3 f_meas        0.667\n\n\n\n\n\n\n\n\n\nQuestion: Can you link the numbers in the confusion matrix on the previous slide to Recall (Sensitivity) and Precision (PPV) metrics\n\n\n\n\n\n\n\nShow Answer\nRecall/sensitivity is \"accuracy\" for the positive cases (in this instance, patients \nwith heart disease).\n30 / (30 + 17)\n\nPrecision/PPV is \"accuracy\" for the positive predictions (when model predicts yes).  \n30 / (30 + 13)\n\n\n\n\n\n\n\\(F1\\) is the harmonic mean of Recall and Precision\n\nHarmonic means are used with rates (see more detail about the Pythagorean means, if interested)\n\\(F1\\) is an unweighted harmonic mean\n\n\\(F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}\\)\nor using the more general formula for harmonic means: \\(F1 = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}\\)\n\n\\(F_\\beta\\) is a weighted version where \\(\\beta\\) is the relative weighting of recall to precision\n\nTwo commonly used values for \\(\\beta\\) are 2, which weighs recall twice as much than precision, and 0.5, which weighs precision twice as much as recall\n\\(F_\\beta = (1 + \\beta^2) * \\frac{Precision * Recall}{(\\beta^2*Precision) + Recall}\\)\n\n\n\ncm |&gt; \n  summary() |&gt; \n  filter(.metric == \"precision\" | .metric == \"recall\" | .metric == \"f_meas\") |&gt; \n  select(-.estimator)\n\n# A tibble: 3 × 2\n  .metric   .estimate\n  &lt;chr&gt;         &lt;dbl&gt;\n1 precision     0.698\n2 recall        0.638\n3 f_meas        0.667\n\n\n\nCohen’s Kappa is a bit more complicated to calculate and understand\n\nThere is a great explanation of kappa on stack overflow\nWikipedia also has a very detailed definition and explanation as well (though not in the context of machine learning)\nCompares observed accuracy to expected accuracy (random chance)\n\n\\(Kappa = \\frac{observed\\:accuracy - expected\\:accuracy}{1 - expected\\:accuracy}\\)\n\nWhen outcome is unbalanced, some agreement/accuracy (relationship between model predictions and reference/ground truth) is expected\nKappa adjusts for this\n\n\n\nKappa is essentially the proportional increase in accuracy above the accuracy expected by the base rates of the reference and classifier\n\nTo calculate the expected accuracy, we need to consider the probabilities of reference and classifier prediction both being positive (and both being negative) by chance given the base rates of these classes for the reference and classifier.\n\n\n\n\nGround Truth\n\n\n\n\n\nPrediction\nPositive\nNegative\n\n\nPositive\nTP\nFP\n\n\nNegative\nFN\nTN\n\n\n\n\\(P_{positive} = \\frac{FN + TP}{TN + FN + FP + TP} * \\frac{FP + TP}{TN + FN + FP + TP}\\)\n\n\\(P_{negative} = \\frac{TN + FP}{TN + FN + FP + TP} * \\frac{TN + FN}{TN + FN + FP + TP}\\)\n\n\\(P_{expected} = P_{positive} + P_{negative}\\)\n\nIn our example\n\ncm\n\n          Truth\nPrediction yes  no\n       yes  30  13\n       no   17 368\n\n\n\\(P_{positive} = \\frac{17 + 30}{428} * \\frac{13 + 30}{428} = 0.01103262\\)\n\n\\(P_{negative} = \\frac{368 + 13}{428} * \\frac{368 + 17}{428} = 0.8007522\\)\n\n\\(P_{expected} = 0.01103262 + 0.8007522 = 0.8117848\\)\n\n\\(Actual Accuracy = 0.9299065\\)\n\n\\(Kappa = \\frac{observed\\:accuracy - expected\\:accuracy}{1 - expected\\:accuracy}\\)\n\n\\(Kappa = \\frac{0.9299065 - 0.8117848}{1 - 0.8117848} = 0.627588\\)\n\nsummary(cm) |&gt; \n  filter(.metric == \"kap\") |&gt; \n  pull(.estimate)\n\n[1] 0.6275886\n\n\n\nKappa Rules of Thumb w/ a big grain of salt…….\n\nKappa &lt; 0: No agreement\nKappa between 0.00 and 0.20: Slight agreement\nKappa between 0.21 and 0.40: Fair agreement\nKappa between 0.41 and 0.60: Moderate agreement\nKappa between 0.61 and 0.80: Substantial agreement\nKappa between 0.81 and 1.00: Almost perfect agreement.\n\nSee Landis and Koch (1977) (PDF)for more details",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Performance Metrics</span>"
    ]
  },
  {
    "objectID": "008_advanced_performance_metrics.html#the-receiver-operating-characteristic-curve",
    "href": "008_advanced_performance_metrics.html#the-receiver-operating-characteristic-curve",
    "title": "8  Advanced Performance Metrics",
    "section": "8.4 The Receiver Operating Characteristic Curve",
    "text": "8.4 The Receiver Operating Characteristic Curve\nLet’s return now to consider sensitivity and specificity again\nRemember that our classifier is estimating the probability of an observation being in the positive class.\nWe dichotomize this probability when we formally make a class prediction\n\nIf the probability &gt; 50%, we classify the observation as positive\nIf the probability &lt;= 50%, we classify the observation as negative\n\n\n\n\n\n\n\n\nQuestion: How can we improve sensitivity?\n\n\n\n\n\n\n\nShow Answer\nWe can use a more liberal/lower classification threshold for saying someone has \nheart disease.  \n\nFor example, rather than requiring a 50% probability to classify as yes for heart disease, \nwe could lower to 20% for the classification threshold\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What will the consequences of this change be?\n\n\n\n\n\n\n\nShow Answer\nFirst, the Bayes classifier threshold of 50% produces the highest overall accuracy \nso accuracy will generally (though not always) drop when you shift from 50%.  \n\nIf we think about this change as it applies to the columns of our confusion matrix, \nwe will now catch more of the yes (fewer false negatives/misses), so sensitivity \nwill go up.  This was the goal of the lower threshold.  However, we will also end up \nwith more false positives so specificity will drop.  If you consider the rows of \nthe confusion matrix, we will have more false positives so the PPV will drop.  \nHowever, we will have fewer false negatives so the NPV will increase.  Whether \nthese trade-offs are worth it are a function of the cost of different types of errors \nand how much you gain and lose with regard to each type of performance metric \n(ROC can inform this; more in a moment)\n\n\n\n\n\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\nPreviously, we simply used predict(fit_glmnet, feat_test)$.pred_class.\n\n\n$.pred_class dichotomized at 50% by default\n\nThis is the classification threshold to use with predicted probabilities that will produce the best overall accuracy (e.g., Bayes classifier)\nHowever, we can use a different threshold to increase sensitivity or specificity\nThis comes at a cost to the other characteristic (its a trade-off)\n\nLower threshold increases sensitivity but decreases specificity\nHigher threshold increases specificity but decreases sensitivity\n\n\n\nIt is relatively easy to make a new confusion matrix and get new performance metrics with a different classification threshold\n\nMake a tibble with truth and predicted probabilities\n\n\npreds &lt;- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet, feat_test, type = \"prob\")$.pred_yes)\n\npreds\n\n# A tibble: 428 × 2\n   truth    prob\n   &lt;fct&gt;   &lt;dbl&gt;\n 1 no    0.0178 \n 2 no    0.119  \n 3 no    0.0204 \n 4 no    0.00269\n 5 no    0.0105 \n 6 no    0.00157\n 7 no    0.00574\n 8 no    0.0414 \n 9 no    0.00897\n10 no    0.104  \n# ℹ 418 more rows\n\n\n\n\nUse this to get class estimates at any threshold we want\nHere we threshold at 20%\n\n\npreds &lt;- preds |&gt; \n  mutate(estimate_20 = if_else(prob &lt;= .20, \"no\", \"yes\"),\n         estimate_20 = factor(estimate_20, levels = c(\"yes\", \"no\"))) |&gt; \n  print()\n\n# A tibble: 428 × 3\n   truth    prob estimate_20\n   &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;      \n 1 no    0.0178  no         \n 2 no    0.119   no         \n 3 no    0.0204  no         \n 4 no    0.00269 no         \n 5 no    0.0105  no         \n 6 no    0.00157 no         \n 7 no    0.00574 no         \n 8 no    0.0414  no         \n 9 no    0.00897 no         \n10 no    0.104   no         \n# ℹ 418 more rows\n\n\n\nWe can now make a confusion matrix for this new set of truth and estimates using the 20% threshold\n\ncm_20 &lt;- preds |&gt; \n  conf_mat(truth = truth, estimate = estimate_20)\n\ncm_20\n\n          Truth\nPrediction yes  no\n       yes  35  37\n       no   12 344\n\n\nLet’s compare to 50% (original)\n\ncm\n\n          Truth\nPrediction yes  no\n       yes  30  13\n       no   17 368\n\n\n\nAnd let’s compare these two thresholds on a subset of our numeric metrics\n\n20% threshold\n\n\ncm_20 |&gt; \n  summary() |&gt; \n  filter(.metric %in% c(\"accuracy\", \"sens\", \"spec\", \"ppv\", \"npv\")) |&gt; \n  select(-.estimator)\n\n# A tibble: 5 × 2\n  .metric  .estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 accuracy     0.886\n2 sens         0.745\n3 spec         0.903\n4 ppv          0.486\n5 npv          0.966\n\n\n\n50% threshold\n\n\ncm |&gt; \n  summary() |&gt; \n  filter(.metric %in% c(\"accuracy\", \"sens\", \"spec\", \"ppv\", \"npv\")) |&gt; \n  select(-.estimator)\n\n# A tibble: 5 × 2\n  .metric  .estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 accuracy     0.930\n2 sens         0.638\n3 spec         0.966\n4 ppv          0.698\n5 npv          0.956\n\n\nDo the changes on each of these metrics make sense to you? If not, please review these previous slides again!\n\nYou can begin to visualize the classifier performance by threshold simply by plotting histograms of the predicted positive class probabilities, separately for the true positive and negative classes\n\nLet’s look at our classifier\nIdeally, the probabilities are mostly low for the true negative class (“no”) and mostly high for the true positive class (“yes”)\nYou can imagine how any specific probability cut point would affect specificity (apply cut to the left panel) or sensitivity (apply cut to the right panel)\n\n\nggplot(data = preds, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What do you think about its performance? What insights does this plot generate?\n\n\n\n\n\n\n\nShow Answer\n1. You can see that we can likely drop the threshold to somewhere about 25% without \ndecreasing the specificity too much.  This will allow you to detect more positive cases.  \n\n2.  Our model seems to be able to predict negative cases well.  They mostly have low \nprobabilities.   However, you can see its poor performance with positive cases.  \nThey are spread pretty evenly across the full range of probabilities.  We likely \ndo not have enough positive cases in our training data\n\n\n\n\n\n\nThe Receiver Operating Characteristics (ROC) curve for a classifier provides a more formal method to visualize the trade-offs between sensitivity and specificity across all possible thresholds for classification.\nLets look at this in our example\n\nWe need columns for truth and probabilities of the positive class for each observation\nWe need to specify the positive class\nReturns tibble with data to plot an ROC curve\n\n\nroc_plot &lt;- \n  tibble(truth = feat_test$disease,\n         prob = predict(fit_glmnet, feat_test, type = \"prob\")$.pred_yes) |&gt; \n  roc_curve(prob, truth = truth)\n\nroc_plot\n\n# A tibble: 202 × 3\n    .threshold specificity sensitivity\n         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf            0                 1\n 2    0.000663     0                 1\n 3    0.000847     0.00787           1\n 4    0.00102      0.0131            1\n 5    0.00157      0.0210            1\n 6    0.00209      0.0262            1\n 7    0.00222      0.0315            1\n 8    0.00226      0.0341            1\n 9    0.00226      0.0394            1\n10    0.00263      0.0446            1\n# ℹ 192 more rows\n\n\n\nWe can autoplot() this\n\nautoplot(roc_plot)\n\n\n\n\n\n\n\n\n\nOr we can customize a plot passing the data intoggplot()\n\nNot doing anything fancy here\nConsider this a shell for you to build on if you want more than autoplot() provides\n\n\nroc_plot |&gt;\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_equal() +\n  labs(x = \"1 - Specificity (FPR)\",\n       y = \"Sensitivity (TPR)\")\n\n\n\n\n\n\n\n\n\nWhen evaluating an ROC curve:\n\nA random classifier would have a diagonal curve from bottom-left to top-right (the dotted line)\nA perfect classifier would reach up to top-left corner\n\nSensitivity = 1 (true positive rate)\n1 - Specificity = 0 (false positive rate)\n\n\n\nThe ROC Curve is not only a useful method to visualize classier performance across thresholds\n\nThe area under the ROC curve (auROC) is an attractive performance metric\n\nRanges from 1.0 (perfect) down to approximately 0.5 (random classifier)\n\nIf the auROC was consistently less than 0.5, then the predictions could simply be inverted\nValues between .70 and .80 are considered fair\nValues between .80 and .90 are considered good\nValues above .90 are considered excellent\nThese are very rough, and to my eye, the exact cuts and labels are somewhat arbitrary\n\n\n\n\nauROC is the probability that the classifier will rank/predict a randomly selected true positive observation higher than a randomly selected true negative observation\nAlternatively, it can be thought of as the average sensitivity across all decision thresholds\nauROC summarizes performance (sensitivity vs. specificity trade-off) across all possible thresholds\nauROC is not affected by class imbalances in contrast to many other metrics\n\n\nIt is easy to get the auROC for the ROC in tidymodels using roc_auc()\n\nAs with calculating the ROC curve, we need\n\ntruth\npredicted probabilities for the positive class\nto specify the event_level (default is first)\n\n\ntibble(truth = feat_test$disease,\n       prob = predict(fit_glmnet, feat_test, type = \"prob\")$.pred_yes) |&gt; \n  roc_auc(prob, truth = truth)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.898",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Performance Metrics</span>"
    ]
  },
  {
    "objectID": "008_advanced_performance_metrics.html#using-alternative-performance-metrics-for-model-selection",
    "href": "008_advanced_performance_metrics.html#using-alternative-performance-metrics-for-model-selection",
    "title": "8  Advanced Performance Metrics",
    "section": "8.5 Using Alternative Performance Metrics for Model Selection",
    "text": "8.5 Using Alternative Performance Metrics for Model Selection\nYou can select the best model configuration using resampling with performance metrics other than accuracy\n\nAggregate measures are typically your best choice (except potentially with high imbalance - more on this later)\n\nFor classification:\n\nAccuracy\nBalanced accuracy\n\\(F1\\)\nArea under ROC Curve (auROC)\nKappa\n\nFor regression\n\nRMSE\n\\(R^2\\)\nMAE (mean absolute error)\n\n\n\n\n\nYou typically need to use a single metric for selection among model configurations\n\nYou should generally use the performance metric that is the best aligned with your problem:\n\n\n\nIn classification\n\nDo you care about types of errors or just overall error rate\nIs the outcome relatively balanced or unbalanced\nWhat metric will be clearest to your audience\n\n\n\nIn regression\n\nDo you want to weight big and small errors the same\nWhat metric will be clearest to your audience (though all of these are pretty clear. There are more complicated regression metrics)\n\n\n\nAlthough you will use one metric to select the best configuration, you can evaluate/characterize the performance of your final model with as many metrics as you like\n\n\nYou should recognize the differences between the cost function for the algorithm and the performance metric:\n\nCost function is fundamental to the definition of the algorithm\nCost function is minimized to determine parameter estimates in parametric models\nPerformance metric is independent of algorithm\nPerformance metric is used to select and evaluate model configurations\n\n\nSometimes they can be the same metric (e.g., RMSE)\n\nBUT, this is not required\n\n\nWith tidymodels, it is easy to select hyperparameters or select among model configurations more generally using one of many different performance metrics\n\nWe will still use either tune_grid() or fit_resamples()\nWe will simply specify a different performance metric inside of metric_set()\nIf we only measure one performance metric, we can use defaults with show_best()\n\n\nHere is an example of measuring roc_auc() but you can use any performance function from the yardstick package\n\nfits_glmnet_auc &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      set_mode(\"classification\") |&gt; \n      tune_grid(preprocessor = rec, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(roc_auc))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_auc\")\n\n\nAnd now use show_best(), with best defined by auROC\n\nshow_best(fits_glmnet_auc, n = 5)\n\n# A tibble: 5 × 8\n  penalty mixture .metric .estimator  mean     n std_err .config                \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                  \n1  0.147        0 roc_auc binary     0.905   100 0.00222 Preprocessor1_Model0111\n2  0.0756       0 roc_auc binary     0.905   100 0.00220 Preprocessor1_Model0099\n3  0.155        0 roc_auc binary     0.905   100 0.00222 Preprocessor1_Model0112\n4  0.139        0 roc_auc binary     0.905   100 0.00222 Preprocessor1_Model0110\n5  0.124        0 roc_auc binary     0.905   100 0.00222 Preprocessor1_Model0108\n\n\n\nYou can also measure multiple metrics during resampling but you will need to select the best configuration using only one\n\nsee metric_set() for the measurement of multiple metrics,\nsee show_best() for the use of metric to indicate which metric to use for selection\n\n\nfits_glmnet_many &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      set_mode(\"classification\") |&gt; \n      tune_grid(preprocessor = rec, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(roc_auc, accuracy, sens, spec, bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_many\")\n\n\nBut now we need to indicate how to define best\n\nWe will define it based on balanced accuracy\n\nshow_best(fits_glmnet_many, metric = \"bal_accuracy\", n = 5)\n\n# A tibble: 5 × 8\n   penalty mixture .metric      .estimator  mean     n std_err\n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 0.000335     1   bal_accuracy binary     0.745   100 0.00378\n2 0.000355     1   bal_accuracy binary     0.745   100 0.00378\n3 0.000375     0.8 bal_accuracy binary     0.745   100 0.00378\n4 0.000396     0.8 bal_accuracy binary     0.745   100 0.00378\n5 0.000335     0.8 bal_accuracy binary     0.745   100 0.00379\n  .config                \n  &lt;chr&gt;                  \n1 Preprocessor1_Model1001\n2 Preprocessor1_Model1002\n3 Preprocessor1_Model0803\n4 Preprocessor1_Model0804\n5 Preprocessor1_Model0801\n\n\n\n\nAnd here based on auROC (same as before)\n\nshow_best(fits_glmnet_many, metric = \"roc_auc\", n = 5)\n\n# A tibble: 5 × 8\n  penalty mixture .metric .estimator  mean     n std_err .config                \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                  \n1  0.147        0 roc_auc binary     0.905   100 0.00222 Preprocessor1_Model0111\n2  0.0756       0 roc_auc binary     0.905   100 0.00220 Preprocessor1_Model0099\n3  0.155        0 roc_auc binary     0.905   100 0.00222 Preprocessor1_Model0112\n4  0.139        0 roc_auc binary     0.905   100 0.00222 Preprocessor1_Model0110\n5  0.124        0 roc_auc binary     0.905   100 0.00222 Preprocessor1_Model0108\n\n\n\nOf course, you can easily calculate any performance metric from yardstick package in test to evaluate your final model\n\nUsing conf_mat() and summary() as above (but not for auROC)\nUsing *_vec() functions; for example:\n\naccuracy_vec()\nroc_auc_vec()\nsens_vec(); spec_vec()\n\nPiping a tibble that contain truth, and estimate or probability into appropriate function\n\naccuracy()\nroc_auc()\nsens(); spec()\n\n\n\nFit best model using auROC\n\nfit_glmnet_auc &lt;-   \n  logistic_reg(penalty = select_best(fits_glmnet_many, metric = \"roc_auc\")$penalty, \n               mixture = select_best(fits_glmnet_many, metric = \"roc_auc\")$mixture) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(disease ~ ., data = feat_trn)\n\nand get all metrics\n\ncm_auc &lt;- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_auc, feat_test)$.pred_class) |&gt; \n  conf_mat(truth, estimate)\n\ncm_auc |&gt; \n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.928 \n 2 kap                  binary        0.479 \n 3 sens                 binary        0.340 \n 4 spec                 binary        1     \n 5 ppv                  binary        1     \n 6 npv                  binary        0.925 \n 7 mcc                  binary        0.561 \n 8 j_index              binary        0.340 \n 9 bal_accuracy         binary        0.670 \n10 detection_prevalence binary        0.0374\n11 precision            binary        1     \n12 recall               binary        0.340 \n13 f_meas               binary        0.508 \n\n\n\nFit best model using balanced accuracy\n\nfit_glmnet_bal &lt;-   \n  logistic_reg(penalty = select_best(fits_glmnet_many, metric = \"bal_accuracy\")$penalty, \n               mixture = select_best(fits_glmnet_many, metric = \"bal_accuracy\")$mixture) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(disease ~ ., data = feat_trn)\n\nand still get all metrics (different because best model configuration is different)\n\ncm_bal &lt;- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_bal, feat_test)$.pred_class) |&gt; \n  conf_mat(truth, estimate)\n\ncm_bal |&gt; \n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.930\n 2 kap                  binary         0.628\n 3 sens                 binary         0.638\n 4 spec                 binary         0.966\n 5 ppv                  binary         0.698\n 6 npv                  binary         0.956\n 7 mcc                  binary         0.628\n 8 j_index              binary         0.604\n 9 bal_accuracy         binary         0.802\n10 detection_prevalence binary         0.100\n11 precision            binary         0.698\n12 recall               binary         0.638\n13 f_meas               binary         0.667",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Performance Metrics</span>"
    ]
  },
  {
    "objectID": "008_advanced_performance_metrics.html#advanced-methods-for-class-imbalances",
    "href": "008_advanced_performance_metrics.html#advanced-methods-for-class-imbalances",
    "title": "8  Advanced Performance Metrics",
    "section": "8.6 Advanced Methods for Class Imbalances",
    "text": "8.6 Advanced Methods for Class Imbalances\nWhen there is a high degree of class imbalance:\n\nIt is often difficult to build models that predict the minority class well\nThis will yield low sensitivity if the positive class is the minority class (as in our example)\nThis will yield low specificity if the negative class is the minority class\nEach of these issues may be a problem depending on the costs of FP and FN\n\n\nLet’s see this at play again in our model\n\nUsing the 50% threshold we have low sensitivity but good specificity\n\n\nautoplot(cm)\n\n\n\n\n\n\n\n\n\ncm |&gt; \n  summary() |&gt; \n  filter(.metric == \"sens\" | .metric == \"spec\") |&gt; \n  select(-.estimator)\n\n# A tibble: 2 × 2\n  .metric .estimate\n  &lt;chr&gt;       &lt;dbl&gt;\n1 sens        0.638\n2 spec        0.966\n\n\n\n\nWe do much better with probabilities for negative (majority) vs. positive (minority) class\nWe can see that we will not affect specificity much by lowering the threshold for positive classification to 20-25%\nBUT, we really need to do do better with the distribution of probabilities for observations that are positive (yes)\n\n\nggplot(data = preds, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n\n\n\n\n\n\n\n\n\nWhat can we do when we have imbalanced outcomes?\n\nWe will consider:\n\nChanges to the classification/decision threshold that trade-off sensitivity vs. specificity for a fitted model (already discussed)\nChanges to performance metric for selecting the best model configuration (already demonstrated)\nSampling/Resampling methods that will affect the balance of the outcome in the training data to fit models that are better with the minority class (new)\n\n\n\n8.6.1 Classification (Decision) Threshold\nWe have already seen an example of how the classification threshold (the probability at which we split between predicting a case as positive vs. negative) affects sensitivity vs. specificity\nDecreasing the threshold (probability) for classifying a case as positive will:\n\nIncrease sensitivity and decrease specificity\nThis will decrease FN but increase FP\nThis may be useful if the positive class is the minority class\nThe ROC curve is a useful display to provide this information about your classifier\n\nCurve can be colored to show the threshold\n\nThe separate histograms by positive vs. negative can also be useful as well\nIf you want to use your data to select the best threshold, you will need yet another set of data to make this selection\n\nCan’t make the selection in training b/c those probabilities are overfit\nCan’t make the selection of threshold in test and then also use the same test data to evaluate that model!\n\n\n\n\n\n8.6.2 Performance Metric Considerations\nWhen you are choosing a performance metric for selecting your best model configuration, you should choose a performance metric that it best aligned with the nature of the performance you seek\n\nIf you want just good overall accuracy, accuracy may be a good metric\nIf the outcome is unbalanced, and you care about the types of errors, you might want\n\nBalanced accuracy (average of sensitive and specificity)\nOnly sensitivity or specificity by itself (recommended by Kuhn)\nauROC\nAn F measure (harmonic mean of sensitivity and PPV)\nMay need to think carefully about what is most important to you\n\n\n\n\nEarlier, we saw that we got better sensitivity when we used balanced accuracy rather than accuracy to tune our hyperparameters\n\n\n\n8.6.3 Sampling and Resampling to Address Class Imbalance\nWe can address issues of class imbalance either a priori or post-hoc with respect to data collection\n\nA priori method would be to over-sample to get more of the minority class into your training set\nUse targeted recruiting\nCan be very costly or impossible in many instances\nIf possible, this can be much better than the resampling approach below\nPost hoc, we can employ a variety of resampling procedures that are designed to make the training data more balanced\n\nWe can up-sample the minority class\nWe can down-sample the majority class\nWe can synthesize new minority class observations e.g, SMOTE\n\nFor both a priori sampling or post-hoc resampling strategies, it is important that your test set is not manipulated. It should represent the expected distribution for the outcome, unaltered\n\n\n\n\n8.6.4 Up-sampling\n\nWe resample minority class observations with replacement within our training set to increase the number of total observations of the minority class in the training set.\nThis simply duplicates existing minority class observations\nOur test (or validation) set(s) should NOT be resampled. This is handled well by step_upsample()\n\n\nLet’s apply this in our example\n\nUp-sampling is part of feature engineering recipe\nNeed to specify the outcome (disease)\nCan set over_ratio to values other than 1 if desired\nMakes sense to do this after missing data imputation and dummy coding\nMakes sense to do this before normalizing features\nThese steps are in the themis package rather than recipes (can use namespace or load full library)\n\n\nrec_up &lt;- recipe(disease ~ ., data = data_trn) |&gt; \n  step_impute_median(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt;   \n  step_dummy(all_nominal_predictors()) |&gt; \n  themis::step_upsample(disease, over_ratio = 1) |&gt; \n  step_normalize(all_numeric_predictors())\n\n\nNeed to re-tune model b/c the sample size has changed\n\nNeed to refit the final model to all of train\n\nfits_glmnet_up &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      set_mode(\"classification\") |&gt; \n      tune_grid(preprocessor = rec_up, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fit_glmnet_up\")\n\n\nReview hyperparameter plot\n\nplot_hyperparameters(fits_glmnet_up, hp1 = \"penalty\", hp2 = \"mixture\", metric = \"bal_accuracy\", log_hp1 = TRUE)\n\n\n\n\n\n\n\n\n\nLet’s fit this best model configuration to all of our training data and evaluate it in test\n\nNote the use of NULL for new_data\nstep_upsample() is only applied to training/held-in but not held-out (truly new) data\nbake() knows to use the training data provided during prep() if we specify NULL\nCould have done this all along for baking training. Its sometimes faster but previously same result. Now its necessary!\n\n\nrec_up_prep &lt;- rec_up |&gt; \n  prep(data_trn)\n\nfeat_trn_up &lt;- rec_up_prep |&gt; \n  bake(new_data = NULL)\n\n\nNotice that disease is now balanced in the training data!\n\nfeat_trn_up |&gt; \n  skim_all()\n\n\nData summary\n\n\nName\nfeat_trn_up\n\n\nNumber of rows\n1522\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\ntop_counts\n\n\n\n\ndisease\n0\n1\n2\nyes: 761, no: 761\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\nage\n0\n1\n0\n1\n-2.82\n-0.70\n0.08\n0.75\n2.42\n-0.31\n-0.53\n\n\nrest_bp\n0\n1\n0\n1\n-2.19\n-0.73\n-0.17\n0.40\n3.32\n0.55\n0.40\n\n\nchol\n0\n1\n0\n1\n-2.36\n-0.72\n-0.08\n0.64\n6.11\n1.03\n3.51\n\n\nexer_max_hr\n0\n1\n0\n1\n-2.63\n-0.74\n0.11\n0.71\n2.25\n-0.49\n-0.23\n\n\nexer_st_depress\n0\n1\n0\n1\n-0.94\n-0.94\n-0.23\n0.49\n4.05\n1.07\n0.84\n\n\nca\n0\n1\n0\n1\n-0.74\n-0.74\n-0.74\n0.35\n2.53\n1.16\n0.25\n\n\nsex_male\n0\n1\n0\n1\n-1.49\n-1.49\n0.67\n0.67\n0.67\n-0.82\n-1.33\n\n\ncp_atyp_ang\n0\n1\n0\n1\n-0.45\n-0.45\n-0.45\n-0.45\n2.24\n1.79\n1.21\n\n\ncp_non_anginal\n0\n1\n0\n1\n-1.76\n0.57\n0.57\n0.57\n0.57\n-1.20\n-0.57\n\n\nfbs_yes\n0\n1\n0\n1\n-0.42\n-0.42\n-0.42\n-0.42\n2.40\n1.98\n1.93\n\n\nrest_ecg_wave_abn\n0\n1\n0\n1\n-0.12\n-0.12\n-0.12\n-0.12\n8.45\n8.33\n67.40\n\n\nrest_ecg_ventric_hypertrophy\n0\n1\n0\n1\n-0.96\n-0.96\n-0.96\n1.04\n1.04\n0.08\n-1.99\n\n\nexer_ang_yes\n0\n1\n0\n1\n-0.68\n-0.68\n-0.68\n1.48\n1.48\n0.80\n-1.37\n\n\nexer_st_slope_flat\n0\n1\n0\n1\n-0.97\n-0.97\n-0.97\n1.03\n1.03\n0.05\n-2.00\n\n\nexer_st_slope_downslope\n0\n1\n0\n1\n-0.23\n-0.23\n-0.23\n-0.23\n4.36\n4.13\n15.06\n\n\nthal_fixeddefect\n0\n1\n0\n1\n-0.20\n-0.20\n-0.20\n-0.20\n5.02\n4.82\n21.25\n\n\nthal_reversabledefect\n0\n1\n0\n1\n-0.82\n-0.82\n-0.82\n1.22\n1.22\n0.40\n-1.84\n\n\n\n\n\n\n\nfit_glmnet_up &lt;-   \n  logistic_reg(penalty = select_best(fits_glmnet_up)$penalty, \n               mixture = select_best(fits_glmnet_up)$mixture) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(disease ~ ., data = feat_trn_up)\n\n\n\n\n\n\n\nTo evaluate this model, we now need test features too\n\n\n- IMPORTANT: Test is NOT up-sampled - bake it as new data!\n\n\n::: {.cell}\n\n\n```{.r .cell-code} feat_test &lt;- rec_up_prep |&gt; bake(data_test)\n\n\nfeat_test |&gt; skim_all() ```\n\n\n::: {.cell-output-display}\n\n\nTable: Data summary\n\n\n| | | |:————————|:———| |Name |feat_test | |Number of rows |428 | |Number of columns |18 | |_______________________ | | |Column type frequency: | | |factor |1 | |numeric |17 | |________________________ | | |Group variables |None |\n\n\nVariable type: factor\n\n\n|skim_variable | n_missing| complete_rate| n_unique|top_counts | |:————-|———:|————-:|——–:|:—————-| |disease | 0| 1| 2|no: 381, yes: 47 |\n\n\nVariable type: numeric\n\n\n|skim_variable | n_missing| complete_rate| mean| sd| p0| p25| p50| p75| p100| skew| kurtosis| |:—————————-|———:|————-:|—–:|—-:|—–:|—–:|—–:|—–:|—-:|—–:|——–:| |age | 0| 1| -0.07| 1.06| -2.26| -1.03| -0.14| 0.75| 2.53| 0.14| -0.75| |rest_bp | 0| 1| -0.20| 0.91| -2.19| -0.73| -0.17| 0.40| 3.78| 0.69| 1.13| |chol | 0| 1| -0.10| 1.02| -2.36| -0.76| -0.27| 0.37| 6.11| 1.99| 8.98| |exer_max_hr | 0| 1| 0.22| 0.89| -3.35| -0.23| 0.37| 0.88| 1.91| -0.73| 0.32| |exer_st_depress | 0| 1| -0.26| 0.89| -0.94| -0.94| -0.58| 0.15| 4.59| 1.69| 3.29| |ca | 0| 1| -0.33| 0.82| -0.74| -0.74| -0.74| -0.74| 2.53| 2.05| 3.44| |sex_male | 0| 1| -0.20| 1.06| -1.49| -1.49| 0.67| 0.67| 0.67| -0.39| -1.85| |cp_atyp_ang | 0| 1| 0.09| 1.08| -0.45| -0.45| -0.45| -0.45| 2.24| 1.49| 0.21| |cp_non_anginal | 0| 1| -0.11| 1.06| -1.76| -1.76| 0.57| 0.57| 0.57| -0.92| -1.15| |fbs_yes | 0| 1| -0.05| 0.95| -0.42| -0.42| -0.42| -0.42| 2.40| 2.18| 2.77| |rest_ecg_wave_abn | 0| 1| 0.00| 1.01| -0.12| -0.12| -0.12| -0.12| 8.45| 8.24| 66.02| |rest_ecg_ventric_hypertrophy | 0| 1| -0.08| 0.99| -0.96| -0.96| -0.96| 1.04| 1.04| 0.23| -1.95| |exer_ang_yes | 0| 1| -0.22| 0.88| -0.68| -0.68| -0.68| -0.68| 1.48| 1.40| -0.04| |exer_st_slope_flat | 0| 1| -0.23| 0.97| -0.97| -0.97| -0.97| 1.03| 1.03| 0.53| -1.72| |exer_st_slope_downslope | 0| 1| 0.10| 1.19| -0.23| -0.23| -0.23| -0.23| 4.36| 3.29| 8.83| |thal_fixeddefect | 0| 1| 0.01| 1.02| -0.20| -0.20| -0.20| -0.20| 5.02| 4.70| 20.11| |thal_reversabledefect | 0| 1| -0.32| 0.88| -0.82| -0.82| -0.82| -0.82| 1.22| 1.17| -0.64|\n\n\n::: :::\n\n\n\nLet’s see how this model performs in test\n\ncm_up &lt;- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_up, feat_test)$.pred_class) |&gt; \n  conf_mat(truth, estimate)\n\ncm_up |&gt; \n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.808\n 2 kap                  binary         0.393\n 3 sens                 binary         0.830\n 4 spec                 binary         0.806\n 5 ppv                  binary         0.345\n 6 npv                  binary         0.975\n 7 mcc                  binary         0.451\n 8 j_index              binary         0.636\n 9 bal_accuracy         binary         0.818\n10 detection_prevalence binary         0.264\n11 precision            binary         0.345\n12 recall               binary         0.830\n13 f_meas               binary         0.487\n\n\n\n\npreds_up &lt;- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet_up, feat_test, type = \"prob\")$.pred_yes)\n\nggplot(data = preds_up, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n\n\n\n\n\n\n\n\n\n\n\n8.6.5 Down-sampling\nWe resample majority class observations within our training set to decrease/match the number of total observations of the minority class in the training set.\n\nThis selects a subset of the majority class\nOur test (or validation) set(s) should NOT be resampled. This is handled well by step_downsample()\n\n\nDown-sampling is part of feature engineering recipe\n\nNeed to specify the outcome (disease)\nCan set under_ratio to values other than 1 if desired\nMakes sense to do this after missing data imputation and dummy coding\nMakes sense to do this before normalizing features\n\n\nrec_down &lt;- recipe(disease ~ ., data = data_trn) |&gt; \n  step_impute_median(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt;   \n  step_dummy(all_nominal_predictors()) |&gt; \n  themis::step_downsample(disease, under_ratio = 1) |&gt; \n  step_normalize(all_numeric_predictors())\n\n\nNeed to re-tune model b/c the sample size has changed\n\nNeed to refit the final model to all of train\n\nfits_glmnet_down &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      set_mode(\"classification\") |&gt; \n      tune_grid(preprocessor = rec_down, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_down\")\n\n\nReview hyperparameters\n\nplot_hyperparameters(fits_glmnet_down, hp1 = \"penalty\", hp2 = \"mixture\", \n                     metric = \"bal_accuracy\", log_hp1 = TRUE)\n\n\n\n\n\n\n\n\n\nLet’s fit this best model configuration to all of our training data and evaluate it in test\n\nNote the use of NULL again when getting features for training data. Very important!\nstep_downsample() is not applied to baked data (See its default for skip = TRUE argument)\nNOTE: sample size and ratio of yes/now for disease!\n\n\nrec_down_prep &lt;- rec_down |&gt; \n  prep(data_trn)\n\nfeat_trn_down &lt;- rec_down_prep |&gt; \n  bake(new_data = NULL)\n\nfeat_trn_down |&gt; skim_some()\n\n\nData summary\n\n\nName\nfeat_trn_down\n\n\nNumber of rows\n184\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndisease\n0\n1\nFALSE\n2\nyes: 92, no: 92\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\nage\n0\n1\n-2.85\n2.16\n\n\nrest_bp\n0\n1\n-1.89\n3.50\n\n\nchol\n0\n1\n-2.52\n3.34\n\n\nexer_max_hr\n0\n1\n-2.56\n2.16\n\n\nexer_st_depress\n0\n1\n-0.87\n4.24\n\n\nca\n0\n1\n-0.74\n2.46\n\n\nsex_male\n0\n1\n-1.55\n0.64\n\n\ncp_atyp_ang\n0\n1\n-0.47\n2.09\n\n\ncp_non_anginal\n0\n1\n-1.78\n0.56\n\n\nfbs_yes\n0\n1\n-0.47\n2.09\n\n\nrest_ecg_wave_abn\n0\n1\n-0.10\n9.51\n\n\nrest_ecg_ventric_hypertrophy\n0\n1\n-0.99\n1.01\n\n\nexer_ang_yes\n0\n1\n-0.64\n1.55\n\n\nexer_st_slope_flat\n0\n1\n-0.84\n1.19\n\n\nexer_st_slope_downslope\n0\n1\n-0.21\n4.68\n\n\nthal_fixeddefect\n0\n1\n-0.17\n5.97\n\n\nthal_reversabledefect\n0\n1\n-0.79\n1.26\n\n\n\n\n\n\nNow fit the model to these downsampled training data\n\nfit_glmnet_down &lt;-   \n  logistic_reg(penalty = select_best(fits_glmnet_down)$penalty, \n               mixture = select_best(fits_glmnet_down)$mixture) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(disease ~ ., data = feat_trn_down)\n\n\nLet’s see how this model performs in test\n\nFirst we need test features\nIMPORTANT: Test is NOT down-sampled\n\n\nfeat_test &lt;- rec_down_prep |&gt; \n  bake(data_test)\n\nfeat_test |&gt; skim_some()\n\n\nData summary\n\n\nName\nfeat_test\n\n\nNumber of rows\n428\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndisease\n0\n1\nFALSE\n2\nno: 381, yes: 47\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\nage\n0\n1\n-2.29\n2.49\n\n\nrest_bp\n0\n1\n-2.25\n3.97\n\n\nchol\n0\n1\n-2.52\n6.30\n\n\nexer_max_hr\n0\n1\n-3.27\n1.83\n\n\nexer_st_depress\n0\n1\n-0.87\n4.78\n\n\nca\n0\n1\n-0.74\n2.46\n\n\nsex_male\n0\n1\n-1.55\n0.64\n\n\ncp_atyp_ang\n0\n1\n-0.47\n2.09\n\n\ncp_non_anginal\n0\n1\n-1.78\n0.56\n\n\nfbs_yes\n0\n1\n-0.47\n2.09\n\n\nrest_ecg_wave_abn\n0\n1\n-0.10\n9.51\n\n\nrest_ecg_ventric_hypertrophy\n0\n1\n-0.99\n1.01\n\n\nexer_ang_yes\n0\n1\n-0.64\n1.55\n\n\nexer_st_slope_flat\n0\n1\n-0.84\n1.19\n\n\nexer_st_slope_downslope\n0\n1\n-0.21\n4.68\n\n\nthal_fixeddefect\n0\n1\n-0.17\n5.97\n\n\nthal_reversabledefect\n0\n1\n-0.79\n1.26\n\n\n\n\n\n\nGet metrics in test\n\ncm_down &lt;- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_down, feat_test)$.pred_class) |&gt; \n  conf_mat(truth, estimate)\n\ncm_down |&gt; \n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.780\n 2 kap                  binary         0.357\n 3 sens                 binary         0.851\n 4 spec                 binary         0.772\n 5 ppv                  binary         0.315\n 6 npv                  binary         0.977\n 7 mcc                  binary         0.426\n 8 j_index              binary         0.623\n 9 bal_accuracy         binary         0.811\n10 detection_prevalence binary         0.297\n11 precision            binary         0.315\n12 recall               binary         0.851\n13 f_meas               binary         0.460\n\n\n\nAnd plot faceted probabilites\n\npreds_down &lt;- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet_down, feat_test, type = \"prob\")$.pred_yes)\n\nggplot(data = preds_down, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n\n\n\n\n\n\n\n\n\n\n\n8.6.6 SMOTE\nA third approach to resampling is called the synthetic minority over-sampling technique (SMOTE)\n\nTo up-sample the minority class, SMOTE synthesizes new observations.\n\nTo do this, an observation is randomly selected from the minority class.\n\nThis observation’s K-nearest neighbors (KNNs) are then determined.\nThe new synthetic observation retains the outcome but a random combination of the predictors values from the randomly selected observation and its neighbors.\n\n\nThis is easily implemented by recipe in tidymodels using step_smote()\n\nNeed to specify the outcome (disease)\nCan set over_ratio to values other than 1 (default) if desired\nCan set neighbors to values other than 5 (default) if desired\nMakes sense to do this after missing data imputation and dummy coding\nOther features will need to be scaled/range-corrected prior to use (for distance)\nMakes sense to do this before normalizing features for glmnet, etc\n\n\nrec_smote &lt;- recipe(disease ~ ., data = data_trn) |&gt; \n  step_impute_median(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt;   \n  step_dummy(all_nominal_predictors()) |&gt;\n  step_range(all_predictors()) |&gt; \n  themis::step_smote(disease, over_ratio = 1, neighbors = 5) |&gt; \n  step_normalize(all_numeric_predictors())\n\n\nNeed to re-tune model b/c the sample size has changed\n\nNeed to refit the final model to all of train\n\nfits_glmnet_smote &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      set_mode(\"classification\") |&gt; \n      tune_grid(preprocessor = rec_smote, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_smote\")\n\n\nReview hyperparameters\n\nplot_hyperparameters(fits_glmnet_smote, hp1 = \"penalty\", hp2 = \"mixture\", \n                     metric = \"bal_accuracy\", log_hp1 = TRUE)\n\n\n\n\n\n\n\n\n\nLet’s fit this best model configuration to all of our training data and evaluate it in test\n\nNote the use of NULL (once again!) for new_data\nstep_smote() is not applied to new data\nNOTE: sample size and outcome balance\n\n\nrec_smote_prep &lt;- rec_smote |&gt; \n  prep(data_trn)\n\nfeat_trn_smote &lt;- rec_smote_prep |&gt; \n  bake(NULL)\n\nfeat_trn_smote |&gt; skim_some()\n\n\nData summary\n\n\nName\nfeat_trn_smote\n\n\nNumber of rows\n1522\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndisease\n0\n1\nFALSE\n2\nyes: 761, no: 761\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\nage\n0\n1\n-2.97\n2.57\n\n\nrest_bp\n0\n1\n-2.38\n3.72\n\n\nchol\n0\n1\n-2.51\n6.44\n\n\nexer_max_hr\n0\n1\n-2.84\n2.42\n\n\nexer_st_depress\n0\n1\n-1.00\n4.71\n\n\nca\n0\n1\n-0.78\n2.73\n\n\nsex_male\n0\n1\n-1.60\n0.64\n\n\ncp_atyp_ang\n0\n1\n-0.45\n2.29\n\n\ncp_non_anginal\n0\n1\n-1.81\n0.57\n\n\nfbs_yes\n0\n1\n-0.42\n2.52\n\n\nrest_ecg_wave_abn\n0\n1\n-0.11\n11.37\n\n\nrest_ecg_ventric_hypertrophy\n0\n1\n-0.98\n1.05\n\n\nexer_ang_yes\n0\n1\n-0.68\n1.50\n\n\nexer_st_slope_flat\n0\n1\n-0.97\n1.06\n\n\nexer_st_slope_downslope\n0\n1\n-0.24\n4.71\n\n\nthal_fixeddefect\n0\n1\n-0.19\n5.75\n\n\nthal_reversabledefect\n0\n1\n-0.89\n1.18\n\n\n\n\n\n\nFit model to smote sampled training data\n\nfit_glmnet_smote &lt;-   \n  logistic_reg(penalty = select_best(fits_glmnet_smote)$penalty, \n               mixture = select_best(fits_glmnet_smote)$mixture) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(disease ~ ., data = feat_trn_smote)\n\n\nLet’s see how this model performs in test\n\nIMPORTANT: Test is NOT SMOTE up-sampled\n\n\nfeat_test &lt;- rec_smote_prep |&gt; \n  bake(data_test)\n\ncm_smote &lt;- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_smote, feat_test)$.pred_class) |&gt; \n  conf_mat(truth, estimate)\n\ncm_smote |&gt; \n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.811\n 2 kap                  binary         0.390\n 3 sens                 binary         0.809\n 4 spec                 binary         0.811\n 5 ppv                  binary         0.345\n 6 npv                  binary         0.972\n 7 mcc                  binary         0.443\n 8 j_index              binary         0.620\n 9 bal_accuracy         binary         0.810\n10 detection_prevalence binary         0.257\n11 precision            binary         0.345\n12 recall               binary         0.809\n13 f_meas               binary         0.484\n\n\n\nPlot faceted predicted probabilites\n\npreds_smote &lt;- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet_smote, feat_test, type = \"prob\")$.pred_yes)\n\nggplot(data = preds_smote, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Performance Metrics</span>"
    ]
  },
  {
    "objectID": "008_advanced_performance_metrics.html#discussion",
    "href": "008_advanced_performance_metrics.html#discussion",
    "title": "8  Advanced Performance Metrics",
    "section": "8.7 Discussion",
    "text": "8.7 Discussion\n\n8.7.1 Announcements\n\nExams by the end of the weekend\nConceptual exam often curved a bit\nLectures released tonight and by end of tomorrow\nInstalling keras package - stay tuned\n\n\n\n\n8.7.2 General\n\nI’m not understanding the difference between tuning the model to create a fit object, and then creating a separate fit object to evaluate it using the test data. I know this is fundamental but it somehow has slipped through my understanding.\n\nWhat is “tuning”\nHow do we tune hyperparameters and then evaluate best model config? Using valiation + test. Using 10-fold + test\n\n\n\n\n\n8.7.3 Confusion matrix\n\nNYTimes article\n\nnew blood test for colon cancer\n53,000 die per year (more details)\ntest results: detected 87%, FPR was 10%\nHow well work for 55- 59 yo\nWhat if applied to 30 year old\n\nMore generally\n\ndefine accuracy, sens, spec, ppv, npv, bal_accuracy\nCost of FP and FN?\nimpact of decision threshold?\n\n\n\n\n\n8.7.4 ROC\n\nAxes and various names/representations\nWhy is top right perfect performance\nthresholds\ninterpretation of auROC and the diagonal line\nCan we go over some more concrete (real-world) examples of when it would be a good idea to use a different threshold for classification?\n\nI would like to understand more about adjusting decision thresholds across a few more application contexts. Also, are there any techniques to optimize decision thresholds? Or is it trial and error.\n\n\n\nlibrary(tidyverse)\ntheme_set(theme_classic()) \ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true\")\n\nℹ SHA-1 hash of file is \"a58e57da996d1b70bb9a5b58241325d6fd78890f\"\n\npath_models &lt;- format_path(\"studydata/risk/models/ema\")\npreds_hour&lt;- read_rds(file.path(path_models, \n                               \"outer_preds_1hour_0_v5_nested_main.rds\"))\nroc_hour &lt;- preds_hour|&gt; \n  yardstick::roc_curve(prob_beta, truth = label)\n\nOption 1 - TPR vs. FPR\n\nsensible axes\nLess common terms\n\n\nroc_hour |&gt; \n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n    geom_path(linewidth = 1.25) +\n    geom_abline(lty = 3) +\n    coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n    labs(x = \"False Positive Rate (FPR)\",\n        y = \"True Positive Rate (TPR)\") +\n  scale_x_continuous(breaks = seq(0,1,.25))\n\n\n\n\n\n\n\n\nOption 2 - Sensitivity vs. 1 - Specificity\n\nMore common terms\n1 - Specificity is confusing!\n\n\nroc_hour |&gt; \n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n    geom_path(linewidth = 1.25) +\n    geom_abline(lty = 3) +\n    coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n    labs(x = \"(1 - Specificity)\",\n        y = \"Sensitivity\") +\n  scale_x_continuous(breaks = seq(0,1,.25))\n\n\n\n\n\n\n\n\nOption 3\n\nMore common terms\nSensible (though reversed) axis\nMy preferred format\n\n\nroc_hour |&gt; \n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n    geom_path(linewidth = 1.25) +\n    geom_abline(lty = 3) +\n    coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +\n    labs(x = \"Specificity\",\n        y = \"Sensitivity\") +\n  scale_x_continuous(breaks = seq(0,1,.25),\n    labels = sprintf(\"%.2f\", seq(1,0,-.25)))\n\n\n\n\n\n\n\n\n\n\n\n8.7.5 Resampling\n\nsimilarities and differences\nresampling held-in but not held-out\nIs there a limit on how much you should up/down-sample or SMOTE? In other words, is there a threshold of imbalance at which it’s not helpful to use one of those techniques, since you are just putting in “fake” data? (Or is “fake data” a bad/incorrect way to think about it?)\n\n\n\n\n8.7.6 Kappa\n\nexpected accuracy\n\ngiven majority class proportion\nby chance given base rates\n\nratio of improvement above expected accuracy\n\n\n\n\n\nKuhn, Max, and Kjell Johnson. 2018. Applied Predictive Modeling. 1st ed. 2013, Corr. 2nd printing 2018 edition. New York: Springer.\n\n\nLandis, J. R., and G. G. Koch. 1977. “The Measurement of Observer Agreement for Categorical Data.” Biometrics 33 (1): 159–74.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Performance Metrics</span>"
    ]
  },
  {
    "objectID": "009_random_forest.html",
    "href": "009_random_forest.html",
    "title": "9  Advanced Models: Decision Trees, Bagging Trees, and Random Forest",
    "section": "",
    "text": "9.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Models: Decision Trees, Bagging Trees, and Random Forest</span>"
    ]
  },
  {
    "objectID": "009_random_forest.html#overview-of-unit",
    "href": "009_random_forest.html#overview-of-unit",
    "title": "9  Advanced Models: Decision Trees, Bagging Trees, and Random Forest",
    "section": "",
    "text": "9.1.1 Learning Objectives\n\nDecision trees\nBagged trees\nHow to bag models and the benefits\nRandom Forest\nHow Random Forest extends bagged trees\nFeature interpretation with decision tree plots\n\n\n\n\n9.1.2 Readings\n\nJames et al. (2023) Chapter 8, Tree Based Methods; pp 327 - 352\n\nIn addition, much of the content from this unit has been drawn from four chapters in a book called Hands On Machine Learning In R. It is a great book and I used it heavily (and at times verbatim) b/c it is quite clear in its coverage of these algorithms. If you want more depth, you might read chapters 9-12 from this book as a supplement to this unit in our course.\nPost questions to the readings channel in Slack\n\n\n\n9.1.3 Lecture Videos\n\nLecture 1: Decision Trees ~ 30 mins\nLecture 2: Decision Trees in Ames ~ 20 mins\nLecture 3: Bagged Treesi ~ 10 mins\nLecture 4: Bagged Trees in Ames ~ 6 mins\nLecture 5: Random Forest ~ 16 mins\n\n\nPost questions to the video-lectures channel in Slack\n\n\n9.1.4 Coding Assignment\n\ndata\nqmd shell\nsolution\n\nPost questions to application-assignments Slack channel\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, March 20th",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Models: Decision Trees, Bagging Trees, and Random Forest</span>"
    ]
  },
  {
    "objectID": "009_random_forest.html#decision-trees",
    "href": "009_random_forest.html#decision-trees",
    "title": "9  Advanced Models: Decision Trees, Bagging Trees, and Random Forest",
    "section": "9.2 Decision Trees",
    "text": "9.2 Decision Trees\nTree-based statistical algorithms:\n\nAre a class of flexible, nonparametric algorithms\nWork by partitioning the feature space into a number of smaller non-overlapping regions with similar responses by using a set of splitting rules\nMake predictions by assigning a single prediction to each of these regions\nCan produce simple rules that are easy to interpret and visualize with tree diagrams\nTypically lack in predictive performance compared to other common algorithms\nServe as base learners for more powerful ensemble approaches\n\n\nIn figure 8.1 from James et al. (2023), they display a simple tree to predict log(salary) using years in the major league and hits from the previous year\n\nThis tree only has a depth of two (there are only two levels of splits)\n\nyears &lt; 4.5\nhits &lt; 117.5\n\n\nThis results in three regions\n\nyears &lt; 4.5\nyears &gt;= 4.5 & hits &lt; 117.5\nyears &gt;= 4.5 & hits &gt;= 117.5\n\n\n\nA single value for salary is predicted for each of these three regions\n\nDecision trees are very interpretable. How we make decisions?\n\n\nYou can see these regions more clearly in the two-dimensional feature space displayed in figure 8.2\n\nNotice how even with a limited tree depth of 2, we can already get a complex partitioning of the feature space.\n\nDecision trees can encode complex decision boundaries (and even more complex than this as tree depth increases)\n\n\nThere are many methodologies for constructing decision trees but the most well-known is the classification and regression tree (CART) algorithm proposed in Breiman (1984)\n\nThis algorithm is implemented in the rpart package and this is the engine we will use in tidymodels for decision trees (and bagged trees - a more advance ensemble method)\nThe decision tree partitions the training data into homogeneous subgroups (i.e., groups with similar response values)\nThese subgroups are called nodes\nThe nodes are formed recursively using binary partitions by asking simple yes-or-no questions about each feature (e.g., are years in major league &lt; 4.5?)\nThis is done a number of times until a suitable stopping criteria is satisfied, e.g.,\n\na maximum depth of the tree is reached\nminimum number of remaining observations is available in a node\n\nAfter all the partitioning has been done, the model predicts a single value for each region\n\nmean response among all observations in the region for regression problems\nmajority vote among all observations in the region for classification problems\nprobabilities (for classification) can be obtained using the proportion of each class within the region\n\n\n\nThe bottom left panel in Figure 8.3 shows a slightly more complicated tree with depth = 3 for two arbitrary predictors (x1 and x2)\n\nThe right column shows a representation of the regions formed by this tree (top) and a 3D representation that includes predictions for y (bottom)\n\nThe top left panel displays a set of regions that is NOT possible using binary recursive splitting. This makes the point that there are some patterns in the data that cannot be accommodated well by decision trees\n\n\nFigure 8.4 shows a slightly more complicated decision tree for the hitters dataset with tree depth = 4. With respect to terminology:\n\nAs noted earlier, each of the subgroups are called nodes\nThe first “subgroup”” at the top of the tree is called the root node. This node contains all of the training data.\nThe final subgroups at the bottom of the tree are called the terminal nodes or leaves (the “tree” is upside down)\nEvery subgroup in between is referred to as an internal node.\nThe connections between nodes are called branches\n\n\n\nThis tree also highlights another key point. The same features can be used for splitting repeatedly throughout the tree. \n\nCART uses binary recursive partitioning\n\nRecursive simply means that each split (or rule) depends on the the splits above it:\n\nThe algorithm first identifies the “best” feature to partition the observations in the root node into one of two new regions (i.e., new nodes that will be on the left and right branches leading from the root node.)\n\nFor regression problems, the “best” feature (and the rule using that feature) is the feature that maximizes the reduction in SSE\nFor classification problems, the split is selected to maximize the reduction in cross-entropy or the Gini index. These are measures of impurity (and we want to get homogeneous nodes so we minimize them)\n\nThe splitting process is then repeated on each of the two new nodes (hence the name binary recursive partitioning).\nThis process is continued until a suitable stopping criterion is reached.\n\n\nThe final depth of the tree is what affects the bias/variance trade-off for this algorithm\n\nDeep trees (with smaller and smaller sized nodes) will have lower and lower bias but can become overfit to the training data\nShallow trees may be overly biased (underfit)\n\n\nThere are two primary approaches to achieve the optimal balance in the bias-variance trade-off\n\nEarly stopping\nPruning\n\n\nWith early stopping:\n\nWe explicitly stop the growth of the tree early based on a stopping rule. The two most common are:\n\nA maximum tree depth is reached\nThe node has too few cases to be considered for further splits\n\nThese two stopping criteria can be implemented independently of each other but they do interact\nThey should ideally be tuned via the cross-validation approaches we have learned\n\n\nWith pruning, we let the tree grow large (max depth = 30 on 32-bit machines) and then prune it back to an optimal size:\n\nTo do this, we apply a penalty (\\(\\lambda\\) * # of terminal nodes) to the cost function/impurity index (analogous to the L1/LASSO penalty). This penalty is also referred to as the cost complexity parameter\nBig values for cost complexity will result in less complex trees. Small values will result in deeper, more complex trees\nCost complexity can be tuned by our standard cross-validation approaches by itself or in combination with the previous two hyper parameters\n\n\nFeature engineering for decision trees can be simpler than with other algorithms because there are very few pre-processing requirements:\n\nMonotonic transformations (e.g., power transformations) are not required to meet algorithm assumptions (in contrast to many parametric models). These transformations only shift the location of the optimal split points.\nOutliers typically do not bias the results as much since the binary partitioning simply looks for a single location to make a split within the distribution of each feature.\nThe algorithm will handle non-linear effects of features and interactions natively\nCategorical predictors do not need pre-processing to convert to numeric (e.g., dummy coding).\nFor unordered categorical features with more than two levels, the classes are ordered based on the outcome\n\nFor regression problems, the mean of the response is used\nFor classification problems, the proportion of the positive outcome class is used.\nThis means that aggregating response levels is not necessary\n\nMost decision tree implementations (including the rpart engine) can easily handle missing values in the features and do not require imputation. In rpart, this is handled by using surrogate splits.\n\n\n\nIt is important to note that feature engineering (e.g., alternative strategies for missing data, categorical level aggregation) may still improve performance, but this algorithm does not have the same pre-processing requirements we have seen previously and will work fairly well “out of the box”.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Models: Decision Trees, Bagging Trees, and Random Forest</span>"
    ]
  },
  {
    "objectID": "009_random_forest.html#decision-trees-in-ames",
    "href": "009_random_forest.html#decision-trees-in-ames",
    "title": "9  Advanced Models: Decision Trees, Bagging Trees, and Random Forest",
    "section": "9.3 Decision Trees in Ames",
    "text": "9.3 Decision Trees in Ames\nLet’s see this algorithm in action\n\nWe will explore the decision tree algorithm (and ensemble approaches using it) with the Ames Housing Prices database\nParallel processing is VERY useful for ensemble approaches because they can be computationally costly\n\n\nRead the Ames dataset\n\nAll predictors\nSet factors\nSome tidying of variable names and responses\n\n\ndata_trn &lt;- read_csv(here::here(path_data, \"ames_raw_class.csv\"),\n              col_types = cols()) |&gt;   \n  janitor::clean_names(case = \"snake\") |&gt; \n  mutate(exter_qual = replace_na(exter_qual, \"none\"),\n         bsmt_qual = replace_na(bsmt_qual, \"none\"),\n         kitchen_qual = replace_na(kitchen_qual, \"none\"),\n         garage_qual = replace_na(garage_qual, \"none\"),\n         fireplace_qu = replace_na(fireplace_qu, \"none\"),\n         alley = replace_na(alley, \"none\"),\n         bsmt_cond = replace_na(bsmt_cond, \"none\"),\n         bsmt_exposure = replace_na(bsmt_exposure, \"none\"),\n         bsmt_fin_type_1 = replace_na(bsmt_fin_type_1, \"none\"),\n         bsmt_fin_type_2 = replace_na(bsmt_fin_type_2, \"none\"),\n         garage_type = replace_na(garage_type, \"none\"),\n         garage_finish = replace_na(garage_finish, \"none\"),\n         garage_cond = replace_na(garage_cond, \"none\"),\n         pool_qc = replace_na(pool_qc, \"none\"),\n         fence = replace_na(fence, \"none\"),\n         misc_feature = replace_na(misc_feature, \"none\"))  |&gt; \n  mutate(across(where(is.character), factor)) |&gt; \n  mutate(across(where(is.factor), tidy_responses)) |&gt; \n  mutate(mo_sold = factor(mo_sold, \n                          levels = 1:12,\n                          labels = c(\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\",\n                                     \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"))) |&gt; \n  mutate(ms_zoning = fct_recode(ms_zoning,\n                                res_low = \"rl\",\n                                res_med = \"rm\",\n                                res_high = \"rh\",\n                                float = \"fv\",\n                                agri = \"a_agr\",\n                                indus = \"i_all\",\n                                commer = \"c_all\"),\n         bldg_type = fct_recode(bldg_type,\n                                one_fam = \"1fam\",\n                                two_fam = \"2fmcon\",\n                                town_end = \"twnhse\",\n                                town_inside = \"twnhs\")) |&gt;\n  select(-pid)\n\n\nAnd take a quick look\n\ndata_trn |&gt; skim_some()\n\n\nData summary\n\n\nName\ndata_trn\n\n\nNumber of rows\n1955\n\n\nNumber of columns\n80\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n45\n\n\nnumeric\n35\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nms_sub_class\n0\n1.00\nFALSE\n16\n020: 730, 060: 388, 050: 208, 120: 122\n\n\nms_zoning\n0\n1.00\nFALSE\n7\nres: 1530, res: 297, flo: 91, com: 19\n\n\nstreet\n0\n1.00\nFALSE\n2\npav: 1946, grv: 9\n\n\nalley\n0\n1.00\nFALSE\n3\nnon: 1821, grv: 86, pav: 48\n\n\nlot_shape\n0\n1.00\nFALSE\n4\nreg: 1258, ir1: 636, ir2: 49, ir3: 12\n\n\nland_contour\n0\n1.00\nFALSE\n4\nlvl: 1769, hls: 75, bnk: 72, low: 39\n\n\nutilities\n0\n1.00\nFALSE\n2\nall: 1953, nos: 2\n\n\nlot_config\n0\n1.00\nFALSE\n5\nins: 1454, cor: 328, cul: 114, fr2: 55\n\n\nland_slope\n0\n1.00\nFALSE\n3\ngtl: 1864, mod: 78, sev: 13\n\n\nneighborhood\n0\n1.00\nFALSE\n28\nnam: 299, col: 174, old: 161, edw: 135\n\n\ncondition_1\n0\n1.00\nFALSE\n9\nnor: 1693, fee: 114, art: 54, rra: 31\n\n\ncondition_2\n0\n1.00\nFALSE\n6\nnor: 1938, fee: 6, art: 4, pos: 3\n\n\nbldg_type\n0\n1.00\nFALSE\n5\none: 1631, tow: 145, dup: 77, tow: 64\n\n\nhouse_style\n0\n1.00\nFALSE\n8\n1st: 989, 2st: 580, 1_5: 224, slv: 79\n\n\nroof_style\n0\n1.00\nFALSE\n6\ngab: 1557, hip: 362, gam: 16, fla: 9\n\n\nroof_matl\n0\n1.00\nFALSE\n7\ncom: 1929, tar: 11, wds: 8, wds: 4\n\n\nexterior_1st\n0\n1.00\nFALSE\n15\nvin: 671, hdb: 301, met: 298, wd_: 283\n\n\nexterior_2nd\n0\n1.00\nFALSE\n17\nvin: 662, met: 295, hdb: 279, wd_: 267\n\n\nmas_vnr_type\n17\n0.99\nFALSE\n5\nnon: 1167, brk: 581, sto: 171, brk: 18\n\n\nexter_qual\n0\n1.00\nFALSE\n4\nta: 1215, gd: 651, ex: 63, fa: 26\n\n\nexter_cond\n0\n1.00\nFALSE\n5\nta: 1707, gd: 195, fa: 42, ex: 8\n\n\nfoundation\n0\n1.00\nFALSE\n6\npco: 865, cbl: 849, brk: 198, sla: 33\n\n\nbsmt_qual\n0\n1.00\nFALSE\n5\nta: 861, gd: 808, ex: 167, fa: 62\n\n\nbsmt_cond\n0\n1.00\nFALSE\n6\nta: 1739, gd: 85, fa: 69, non: 57\n\n\nbsmt_exposure\n0\n1.00\nFALSE\n5\nno: 1271, av: 274, gd: 183, mn: 168\n\n\nbsmt_fin_type_1\n0\n1.00\nFALSE\n7\nunf: 576, glq: 535, alq: 294, rec: 202\n\n\nbsmt_fin_type_2\n0\n1.00\nFALSE\n7\nunf: 1655, rec: 75, lwq: 69, non: 57\n\n\nheating\n0\n1.00\nFALSE\n6\ngas: 1920, gas: 20, gra: 8, wal: 5\n\n\nheating_qc\n0\n1.00\nFALSE\n5\nex: 979, ta: 590, gd: 324, fa: 60\n\n\ncentral_air\n0\n1.00\nFALSE\n2\ny: 1821, n: 134\n\n\nelectrical\n1\n1.00\nFALSE\n5\nsbr: 1792, fus: 125, fus: 29, fus: 7\n\n\nkitchen_qual\n0\n1.00\nFALSE\n5\nta: 1011, gd: 765, ex: 126, fa: 52\n\n\nfunctional\n0\n1.00\nFALSE\n8\ntyp: 1822, min: 48, min: 41, mod: 23\n\n\nfireplace_qu\n0\n1.00\nFALSE\n6\nnon: 960, gd: 481, ta: 407, fa: 44\n\n\ngarage_type\n0\n1.00\nFALSE\n7\natt: 1161, det: 521, bui: 123, non: 107\n\n\ngarage_finish\n0\n1.00\nFALSE\n4\nunf: 826, rfn: 547, fin: 473, non: 109\n\n\ngarage_qual\n0\n1.00\nFALSE\n6\nta: 1745, non: 109, fa: 79, gd: 16\n\n\ngarage_cond\n0\n1.00\nFALSE\n6\nta: 1778, non: 109, fa: 46, gd: 12\n\n\npaved_drive\n0\n1.00\nFALSE\n3\ny: 1775, n: 139, p: 41\n\n\npool_qc\n0\n1.00\nFALSE\n5\nnon: 1945, ex: 3, gd: 3, fa: 2\n\n\nfence\n0\n1.00\nFALSE\n5\nnon: 1599, mnp: 215, gdw: 70, gdp: 61\n\n\nmisc_feature\n0\n1.00\nFALSE\n5\nnon: 1887, she: 62, oth: 3, gar: 2\n\n\nmo_sold\n0\n1.00\nFALSE\n12\njun: 333, jul: 298, may: 273, apr: 177\n\n\nsale_type\n0\n1.00\nFALSE\n10\nwd: 1695, new: 158, cod: 57, con: 16\n\n\nsale_condition\n0\n1.00\nFALSE\n6\nnor: 1616, par: 161, abn: 120, fam: 30\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\nlot_frontage\n319\n0.84\n21\n313\n\n\nlot_area\n0\n1.00\n1476\n215245\n\n\noverall_qual\n0\n1.00\n1\n10\n\n\noverall_cond\n0\n1.00\n1\n9\n\n\nyear_built\n0\n1.00\n1875\n2010\n\n\nyear_remod_add\n0\n1.00\n1950\n2010\n\n\nmas_vnr_area\n17\n0.99\n0\n1600\n\n\nbsmt_fin_sf_1\n1\n1.00\n0\n5644\n\n\nbsmt_fin_sf_2\n1\n1.00\n0\n1526\n\n\nbsmt_unf_sf\n1\n1.00\n0\n2153\n\n\ntotal_bsmt_sf\n1\n1.00\n0\n6110\n\n\nx1st_flr_sf\n0\n1.00\n372\n4692\n\n\nx2nd_flr_sf\n0\n1.00\n0\n2065\n\n\nlow_qual_fin_sf\n0\n1.00\n0\n1064\n\n\ngr_liv_area\n0\n1.00\n438\n5642\n\n\nbsmt_full_bath\n1\n1.00\n0\n3\n\n\nbsmt_half_bath\n1\n1.00\n0\n2\n\n\nfull_bath\n0\n1.00\n0\n4\n\n\nhalf_bath\n0\n1.00\n0\n2\n\n\nbedroom_abv_gr\n0\n1.00\n0\n8\n\n\nkitchen_abv_gr\n0\n1.00\n0\n3\n\n\ntot_rms_abv_grd\n0\n1.00\n3\n14\n\n\nfireplaces\n0\n1.00\n0\n3\n\n\ngarage_yr_blt\n109\n0.94\n1896\n2010\n\n\ngarage_cars\n1\n1.00\n0\n4\n\n\ngarage_area\n1\n1.00\n0\n1488\n\n\nwood_deck_sf\n0\n1.00\n0\n870\n\n\nopen_porch_sf\n0\n1.00\n0\n742\n\n\nenclosed_porch\n0\n1.00\n0\n552\n\n\nx3ssn_porch\n0\n1.00\n0\n508\n\n\nscreen_porch\n0\n1.00\n0\n576\n\n\npool_area\n0\n1.00\n0\n738\n\n\nmisc_val\n0\n1.00\n0\n12500\n\n\nyr_sold\n0\n1.00\n2006\n2010\n\n\nsale_price\n0\n1.00\n12789\n745000\n\n\n\n\n\n\nA basic recipe for Decision Tree approaches\n\nEasy to do because the algorithm is very flexible\nWill handle non-linear relationships and interactions natively\nDummy coding not needed (and generally not recommended) for factors\nNot even very important to consider frequency of response categories\nNot even necessary to convert character to factor (but I do to make it easy to do further feature engineering if desired)\nBe careful with categorical variables which are coded with numbers\n\nThey will be coded as numeric by R and treated as numeric by R\nIf they are ordered (overall_qual), this a priori order would be respected by rpart so no worries\nIf they are unordered, this will force an order on the levels rather than allowing rpart to determine an order based on the outcome in your training data.\n\nNotice the missing data for features (rpart will handle it with surrogates)\n\n\nrec &lt;- recipe(sale_price ~ ., data = data_trn)\n \nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(NULL)\n\nfeat_trn |&gt; skim_some()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n1955\n\n\nNumber of columns\n80\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n45\n\n\nnumeric\n35\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nms_sub_class\n0\n1.00\nFALSE\n16\n020: 730, 060: 388, 050: 208, 120: 122\n\n\nms_zoning\n0\n1.00\nFALSE\n7\nres: 1530, res: 297, flo: 91, com: 19\n\n\nstreet\n0\n1.00\nFALSE\n2\npav: 1946, grv: 9\n\n\nalley\n0\n1.00\nFALSE\n3\nnon: 1821, grv: 86, pav: 48\n\n\nlot_shape\n0\n1.00\nFALSE\n4\nreg: 1258, ir1: 636, ir2: 49, ir3: 12\n\n\nland_contour\n0\n1.00\nFALSE\n4\nlvl: 1769, hls: 75, bnk: 72, low: 39\n\n\nutilities\n0\n1.00\nFALSE\n2\nall: 1953, nos: 2\n\n\nlot_config\n0\n1.00\nFALSE\n5\nins: 1454, cor: 328, cul: 114, fr2: 55\n\n\nland_slope\n0\n1.00\nFALSE\n3\ngtl: 1864, mod: 78, sev: 13\n\n\nneighborhood\n0\n1.00\nFALSE\n28\nnam: 299, col: 174, old: 161, edw: 135\n\n\ncondition_1\n0\n1.00\nFALSE\n9\nnor: 1693, fee: 114, art: 54, rra: 31\n\n\ncondition_2\n0\n1.00\nFALSE\n6\nnor: 1938, fee: 6, art: 4, pos: 3\n\n\nbldg_type\n0\n1.00\nFALSE\n5\none: 1631, tow: 145, dup: 77, tow: 64\n\n\nhouse_style\n0\n1.00\nFALSE\n8\n1st: 989, 2st: 580, 1_5: 224, slv: 79\n\n\nroof_style\n0\n1.00\nFALSE\n6\ngab: 1557, hip: 362, gam: 16, fla: 9\n\n\nroof_matl\n0\n1.00\nFALSE\n7\ncom: 1929, tar: 11, wds: 8, wds: 4\n\n\nexterior_1st\n0\n1.00\nFALSE\n15\nvin: 671, hdb: 301, met: 298, wd_: 283\n\n\nexterior_2nd\n0\n1.00\nFALSE\n17\nvin: 662, met: 295, hdb: 279, wd_: 267\n\n\nmas_vnr_type\n17\n0.99\nFALSE\n5\nnon: 1167, brk: 581, sto: 171, brk: 18\n\n\nexter_qual\n0\n1.00\nFALSE\n4\nta: 1215, gd: 651, ex: 63, fa: 26\n\n\nexter_cond\n0\n1.00\nFALSE\n5\nta: 1707, gd: 195, fa: 42, ex: 8\n\n\nfoundation\n0\n1.00\nFALSE\n6\npco: 865, cbl: 849, brk: 198, sla: 33\n\n\nbsmt_qual\n0\n1.00\nFALSE\n5\nta: 861, gd: 808, ex: 167, fa: 62\n\n\nbsmt_cond\n0\n1.00\nFALSE\n6\nta: 1739, gd: 85, fa: 69, non: 57\n\n\nbsmt_exposure\n0\n1.00\nFALSE\n5\nno: 1271, av: 274, gd: 183, mn: 168\n\n\nbsmt_fin_type_1\n0\n1.00\nFALSE\n7\nunf: 576, glq: 535, alq: 294, rec: 202\n\n\nbsmt_fin_type_2\n0\n1.00\nFALSE\n7\nunf: 1655, rec: 75, lwq: 69, non: 57\n\n\nheating\n0\n1.00\nFALSE\n6\ngas: 1920, gas: 20, gra: 8, wal: 5\n\n\nheating_qc\n0\n1.00\nFALSE\n5\nex: 979, ta: 590, gd: 324, fa: 60\n\n\ncentral_air\n0\n1.00\nFALSE\n2\ny: 1821, n: 134\n\n\nelectrical\n1\n1.00\nFALSE\n5\nsbr: 1792, fus: 125, fus: 29, fus: 7\n\n\nkitchen_qual\n0\n1.00\nFALSE\n5\nta: 1011, gd: 765, ex: 126, fa: 52\n\n\nfunctional\n0\n1.00\nFALSE\n8\ntyp: 1822, min: 48, min: 41, mod: 23\n\n\nfireplace_qu\n0\n1.00\nFALSE\n6\nnon: 960, gd: 481, ta: 407, fa: 44\n\n\ngarage_type\n0\n1.00\nFALSE\n7\natt: 1161, det: 521, bui: 123, non: 107\n\n\ngarage_finish\n0\n1.00\nFALSE\n4\nunf: 826, rfn: 547, fin: 473, non: 109\n\n\ngarage_qual\n0\n1.00\nFALSE\n6\nta: 1745, non: 109, fa: 79, gd: 16\n\n\ngarage_cond\n0\n1.00\nFALSE\n6\nta: 1778, non: 109, fa: 46, gd: 12\n\n\npaved_drive\n0\n1.00\nFALSE\n3\ny: 1775, n: 139, p: 41\n\n\npool_qc\n0\n1.00\nFALSE\n5\nnon: 1945, ex: 3, gd: 3, fa: 2\n\n\nfence\n0\n1.00\nFALSE\n5\nnon: 1599, mnp: 215, gdw: 70, gdp: 61\n\n\nmisc_feature\n0\n1.00\nFALSE\n5\nnon: 1887, she: 62, oth: 3, gar: 2\n\n\nmo_sold\n0\n1.00\nFALSE\n12\njun: 333, jul: 298, may: 273, apr: 177\n\n\nsale_type\n0\n1.00\nFALSE\n10\nwd: 1695, new: 158, cod: 57, con: 16\n\n\nsale_condition\n0\n1.00\nFALSE\n6\nnor: 1616, par: 161, abn: 120, fam: 30\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\nlot_frontage\n319\n0.84\n21\n313\n\n\nlot_area\n0\n1.00\n1476\n215245\n\n\noverall_qual\n0\n1.00\n1\n10\n\n\noverall_cond\n0\n1.00\n1\n9\n\n\nyear_built\n0\n1.00\n1875\n2010\n\n\nyear_remod_add\n0\n1.00\n1950\n2010\n\n\nmas_vnr_area\n17\n0.99\n0\n1600\n\n\nbsmt_fin_sf_1\n1\n1.00\n0\n5644\n\n\nbsmt_fin_sf_2\n1\n1.00\n0\n1526\n\n\nbsmt_unf_sf\n1\n1.00\n0\n2153\n\n\ntotal_bsmt_sf\n1\n1.00\n0\n6110\n\n\nx1st_flr_sf\n0\n1.00\n372\n4692\n\n\nx2nd_flr_sf\n0\n1.00\n0\n2065\n\n\nlow_qual_fin_sf\n0\n1.00\n0\n1064\n\n\ngr_liv_area\n0\n1.00\n438\n5642\n\n\nbsmt_full_bath\n1\n1.00\n0\n3\n\n\nbsmt_half_bath\n1\n1.00\n0\n2\n\n\nfull_bath\n0\n1.00\n0\n4\n\n\nhalf_bath\n0\n1.00\n0\n2\n\n\nbedroom_abv_gr\n0\n1.00\n0\n8\n\n\nkitchen_abv_gr\n0\n1.00\n0\n3\n\n\ntot_rms_abv_grd\n0\n1.00\n3\n14\n\n\nfireplaces\n0\n1.00\n0\n3\n\n\ngarage_yr_blt\n109\n0.94\n1896\n2010\n\n\ngarage_cars\n1\n1.00\n0\n4\n\n\ngarage_area\n1\n1.00\n0\n1488\n\n\nwood_deck_sf\n0\n1.00\n0\n870\n\n\nopen_porch_sf\n0\n1.00\n0\n742\n\n\nenclosed_porch\n0\n1.00\n0\n552\n\n\nx3ssn_porch\n0\n1.00\n0\n508\n\n\nscreen_porch\n0\n1.00\n0\n576\n\n\npool_area\n0\n1.00\n0\n738\n\n\nmisc_val\n0\n1.00\n0\n12500\n\n\nyr_sold\n0\n1.00\n2006\n2010\n\n\nsale_price\n0\n1.00\n12789\n745000\n\n\n\n\n\n\nFit a simple decision tree\n\nUse rpart engine\ntree_depth = 3\nmin_n = 2 and cost_complexity = 0 removes impact of those hyperparameters\nmodel = TRUE if you want to plot the decision tree with rplot.plot() from the rpart.plot package\n\n\nfit_tree_ex1 &lt;-   \n  decision_tree(tree_depth = 3, min_n = 2, cost_complexity = 0) |&gt;\n  set_engine(\"rpart\", model = TRUE) |&gt;\n  set_mode(\"regression\") |&gt;  \n  fit(sale_price ~ garage_cars + garage_area + overall_qual + kitchen_qual +\n                  bsmt_qual, data = feat_trn)\n\n\nLet’s plot the decision tree using rpart.plot for a package with the same name. No need to load the full package\n\nEasy to understand how the model makes predictions\n\n\nfit_tree_ex1$fit |&gt; rpart.plot::rpart.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How can we determine how well this model predicts sale_price?\n\n\n\n\n\n\n\nShow Answer\nWe need held out data. We could do a validation split, k-fold, or bootstraps.  \nK-fold may be preferred b/c it provides a less biased estimate of model performance.  \nI will use bootstrap cross-validation instead because I will later use these same \nsplits to also choose among hyperparameter values.]\n\n\n\n\n\n\nUsing only 10 bootstraps to save time. Use more in your work!\n\nset.seed(20140102)\nsplits_boot &lt;- data_trn |&gt; \n  bootstraps(times = 10, strata = \"sale_price\")\n\n\nNow let’s evaluate its performance in our OOB held-out sets\n\nfits_tree_ex1 &lt;- cache_rds(\n  expr = {\n  decision_tree(tree_depth = 3, min_n = 2, cost_complexity = 0) |&gt;\n    set_engine(\"rpart\") |&gt;\n    set_mode(\"regression\") |&gt; \n    fit_resamples(preprocessor = rec, \n              resamples = splits_boot,  \n              metrics = metric_set(rmse))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/009/\",\n  file = \"fits_tree_ex1\")\n\n\nNot that great (remember unit 3 with only a subset of predictors did better than this)\n\nfits_tree_ex1 |&gt; collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   42771.   100    271. Preprocessor1_Model1\n\n\n\nWe want to allow a deeper tree to reduce bias\n\nBut will also need to consider pruning tree to prevent overfitting\n\nThis is the bias-variance trade-off again. To find the sweet stop we can tune\n\ntree_depth\nmin_n\ncost_complexity\n\n\nSet up a tuning grid\n\nCan use tidymodels to determine possibly good values to start\nStill need to evaluate\nMay still need to adjust\n\n\ncost_complexity()\n\nCost-Complexity Parameter (quantitative)\nTransformer: log-10 [1e-100, Inf]\nRange (transformed scale): [-10, -1]\n\ntree_depth()\n\nTree Depth (quantitative)\nRange: [1, 15]\n\nmin_n()\n\nMinimal Node Size (quantitative)\nRange: [2, 40]\n\n\n\nWe can use these function with dials::grid_regular() to get a tuning grid\n\nWith levels = 4 we get 64 combinations of values (4 x 4 x 4)\n\ngrid_tree &lt;- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 4)\n\ngrid_tree\n\n# A tibble: 64 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1    0.0000000001          1     2\n 2    0.0000001             1     2\n 3    0.0001                1     2\n 4    0.1                   1     2\n 5    0.0000000001          5     2\n 6    0.0000001             5     2\n 7    0.0001                5     2\n 8    0.1                   5     2\n 9    0.0000000001         10     2\n10    0.0000001            10     2\n# ℹ 54 more rows\n\n\n\nNow we can use the bootstrap as intended to select best values of hyperparameters (i.e., tune them)\n\nfits_tree &lt;- cache_rds(\n  expr = {\n    decision_tree(cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune()) |&gt;\n    set_engine(\"rpart\") |&gt;\n    set_mode(\"regression\") |&gt; \n    tune_grid(preprocessor = rec, \n              resamples = splits_boot, \n              grid = grid_tree, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/009\",\n  file = \"fits_tree\")\n\n\nCan use autoplot() to view performance by hyperparameter values\n\n# autoplot(fits_tree)\n\n\nThe best values for some of the hyperparameters (tree depth and min n) are at their edges so we might consider extending their range and training again. I will skip this here to save time.\n\nThis model as good as our other models in unit 3 (see OOB cross-validated error above). It was easy to fit with all the predictors. It might get even a little better for we further tuned the hyperparameters. However, we can still do better with a more advanced algorithm based on decision trees.\n\nshow_best(fits_tree)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator   mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1    0.0001               15    40 rmse    standard   37151.   100    219.\n2    0.0000000001         10    40 rmse    standard   37154.   100    220.\n3    0.0000001            10    40 rmse    standard   37154.   100    220.\n4    0.0000000001         15    40 rmse    standard   37154.   100    219.\n5    0.0000001            15    40 rmse    standard   37154.   100    219.\n  .config              \n  &lt;chr&gt;                \n1 Preprocessor1_Model63\n2 Preprocessor1_Model57\n3 Preprocessor1_Model58\n4 Preprocessor1_Model61\n5 Preprocessor1_Model62\n\n\n\nLet’s still fit this tree to all the training data and understand it a bit better\n\nfit_tree &lt;-   \n  decision_tree(cost_complexity = select_best(fits_tree)$cost_complexity,\n                tree_depth = select_best(fits_tree)$tree_depth,\n                min_n = select_best(fits_tree)$min_n) |&gt;\n  set_engine(\"rpart\", model = TRUE) |&gt;\n  set_mode(\"regression\") |&gt;  \n  fit(sale_price ~ ., data = feat_trn)\n\n\nStill interpretable but need bigger, higher res plot\n\nfit_tree$fit |&gt; rpart.plot::rpart.plot()\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting\n\n\n\n\n\n\n\n\n\n\nEven though decision trees themselves are relatively poor at prediction, these ideas will be key when we consider more advanced ensemble approaches\n\nEnsemble approaches aggregate multiple models to improve prediction. Our first ensemble approach is bagging.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Models: Decision Trees, Bagging Trees, and Random Forest</span>"
    ]
  },
  {
    "objectID": "009_random_forest.html#bagging",
    "href": "009_random_forest.html#bagging",
    "title": "9  Advanced Models: Decision Trees, Bagging Trees, and Random Forest",
    "section": "9.4 Bagging",
    "text": "9.4 Bagging\nBootstrap aggregating (bagging) prediction models involve:\n\nFitting multiple versions of a prediction model\nCombining (or ensembling) them into an aggregated prediction\nYou can begin to learn more about bagging in the original paper that proposed the technique\n\n\nThe specific steps are:\n\nB bootstraps of the original training data are created [NOTE: This is a new use for bootstrapping! More on that in a moment]\nThe model configuration (either regression or classification algorithm with a specific set of features and hyperparameters) is fit to each bootstrap sample\nThese individual fitted models are called the base learners\nFinal predictions are made by aggregating the predictions across all of the individual base learners\n\nFor regression, this can be the average of base learner predictions\nFor classification, it can be either the average of estimated class probabilities or majority class vote across individual base learners\n\n\n\nBagging effectively reduces the variance of an individual base learner\n\nHowever, bagging does not always improve on an individual base learner\n\nBagging works especially well for flexible, high variance base learners (based on statistical algorithms or other characteristics of the problem)\nThese base learners can become overfit to their training data\nTherefore base learners will produce different predictions across each bootstrap sample of the training data\nThe aggregate predictions across base learners will be lower variance\nWith respect to statistical algorithms that you know, decision trees are high variance (KNN also)\nIn contrast, bagging a linear model (with low P to N ratio) would likely not improve much upon the base learners’ performance\n\n\nBagging takes advantage of the “wisdom of the crowd” effect (Surowiecki, 2005)\n\nAggregation of information across large diverse groups often produces decisions that are better than any single member of the group\nRegis Philbin once stated that the Ask the Audience lifeline on Who Wants to be a Millionaire is right 95% of the time\nWith more diverse group members and perspectives (i.e., high variance learners), we get better aggregated predictions\n\n\nWith decision trees, optimal performance is often found by bagging 50-500 base learner trees\n\nData sets that have a few strong features typically require fewer trees\nData sets with lots of noise or multiple strong features may need more\nUsing too many trees will NOT lead to overfitting, just no further benefit in variance reduction\nHowever, too many trees will increase computational costs (particularly if you are also using “an outer loop” of resampling to select among configurations or to evaluate the model)\n\n\nBagging uses bootstrap resampling for yet another goal. We can use bootstrapping\n\nFor cross-validation to assess performance model configuration(s)\n\nSelect among model model configurations\nEvaluate a final model configuration (if we don’t have independent test set)\n\nFor estimating standard errors and confidence intervals of statistics (no longer part of this course - see Appendix)\nAnd now for building multiple base learners whose aggregate predictions are lower variance than any of the individual base learners\n\n\nWhen bagging, we can (and typically will):\n\nUse an “outer loop” of bootstrap resampling to select among model configurations\nWhile using an “inner loop” of bootstrapping to fit multiple base learners for any specific configuration\nThis inner loop is often opaque to users (more on this in a moment), happening under the hood in the algorithm (e.g., Random Forest) or the implementation of the code (bagged_tree())",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Models: Decision Trees, Bagging Trees, and Random Forest</span>"
    ]
  },
  {
    "objectID": "009_random_forest.html#bagging-trees-in-ames",
    "href": "009_random_forest.html#bagging-trees-in-ames",
    "title": "9  Advanced Models: Decision Trees, Bagging Trees, and Random Forest",
    "section": "9.5 Bagging Trees in Ames",
    "text": "9.5 Bagging Trees in Ames\nWe will now use bag_tree() rather than decision_tree() from the baguette package. Not part of minimal tidymodels libraries so we will need to load this package\n\nlibrary(baguette)\n\n\nWe can still tune the same hyperparameters. We are now just creating many rpart decision trees and aggregating their predictions. We will START with the recommended values by tidymodels\n\nWe can use the same recipe\n\nWe will use the same splits to tune hyperparameters\n\nNow we need times = 20 to fit 20 models (inner loop of bootstrapping) to each of the 10 bootstraps (outer loop; set earlier) of the training data.\n\nKeeping this low to reduce computational costs.\n\nYou will likely want more bootstrapped models to reduce final model variance and more boostraps resamples to get a lower variance performance estimate to select best hyperparameters\n\n\nfits_bagged &lt;- cache_rds(\n  expr = {\n    bag_tree(cost_complexity = tune(),\n           tree_depth = tune(),\n           min_n = tune()) |&gt;\n    set_engine(\"rpart\", times = 20) |&gt;\n    set_mode(\"regression\") |&gt; \n    tune_grid(preprocessor = rec, \n              resamples = splits_boot, \n              grid = grid_tree, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/009/\",\n  file = \"fits_bagged\")\n\n\nCan still use autoplot() to view performance by hyperparameter values\n\n# autoplot(fits_bagged)\n\n\nThe best values for all hyperparameters are at their edges.\n\nLooks like we could have fit a more complex model.\n\nMakes sense because we are relying on bagging to reduce model variance so we can accommodate base learners that are more overfit.\n\n\nshow_best(fits_bagged)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator   mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1    0.0000001            15     2 rmse    standard   27777.    10    537.\n2    0.0001               15     2 rmse    standard   27801.    10    644.\n3    0.0001               15    14 rmse    standard   27870.    10    748.\n4    0.0000000001         10    14 rmse    standard   27908.    10    777.\n5    0.0000000001         10     2 rmse    standard   27910.    10    609.\n  .config              \n  &lt;chr&gt;                \n1 Preprocessor1_Model14\n2 Preprocessor1_Model15\n3 Preprocessor1_Model31\n4 Preprocessor1_Model25\n5 Preprocessor1_Model09\n\n\n\nSwitching to expand_grid() to manually select\n\ngrid_tree_bagged &lt;- expand_grid(cost_complexity = c(10^-10, 10^-7, 10^-04, 10^-01), \n                                tree_depth = c(10, 15, 20, 30), \n                                min_n = c(2, 14, 27, 40))\n\ngrid_tree_bagged\n\n# A tibble: 64 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1    0.0000000001         10     2\n 2    0.0000000001         10    14\n 3    0.0000000001         10    27\n 4    0.0000000001         10    40\n 5    0.0000000001         15     2\n 6    0.0000000001         15    14\n 7    0.0000000001         15    27\n 8    0.0000000001         15    40\n 9    0.0000000001         20     2\n10    0.0000000001         20    14\n# ℹ 54 more rows\n\n\n\n\nfits_bagged_2 &lt;- cache_rds(\n  expr = {\n    bag_tree(cost_complexity = tune(),\n           tree_depth = tune(),\n           min_n = tune()) |&gt;\n    set_engine(\"rpart\", times = 20) |&gt;\n    set_mode(\"regression\") |&gt; \n    tune_grid(preprocessor = rec, \n              resamples = splits_boot, \n              grid = grid_tree_bagged, \n              metrics = metric_set(rmse))\n  },\n  rerun = rerun_setting, \n  dir = \"cache/009/\",\n  file = \"fits_bagged_2\")\n\n\nReview hyperparameter plot\n\n# autoplot(fits_bagged_2)\n\n\nThis looks better. And look at that BIG improvement in OOB cross-validated RMSE\n\nshow_best(fits_bagged_2)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator   mean     n std_err\n            &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1    0.0001               15     2 rmse    standard   27575.    10    640.\n2    0.0000000001         15     2 rmse    standard   27673.    10    635.\n3    0.0000001            30     2 rmse    standard   27724.    10    662.\n4    0.0000001            20     2 rmse    standard   27726.    10    713.\n5    0.0000000001         20     2 rmse    standard   27758.    10    625.\n  .config              \n  &lt;chr&gt;                \n1 Preprocessor1_Model37\n2 Preprocessor1_Model05\n3 Preprocessor1_Model29\n4 Preprocessor1_Model25\n5 Preprocessor1_Model09\n\n\nBUT, we can do better still…..",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Models: Decision Trees, Bagging Trees, and Random Forest</span>"
    ]
  },
  {
    "objectID": "009_random_forest.html#random-forest",
    "href": "009_random_forest.html#random-forest",
    "title": "9  Advanced Models: Decision Trees, Bagging Trees, and Random Forest",
    "section": "9.6 Random Forest",
    "text": "9.6 Random Forest\nRandom forests are a modification of bagged decision trees that build a large collection of de-correlated trees to further improve predictive performance.\n\nThey are a very popular “out-of-the-box” or “off-the-shelf” statistical algorithm that predicts well\nMany modern implementations of random forests exist; however, Breiman’s algorithm (Breiman 2001) has largely become the standard procedure.\nWe will use the ranger engine implementation of this algorithm\n\n\nRandom forests build on decision trees (its base learners) and bagging to reduce final model variance.\n\nSimply bagging trees is not optimal to reduce final model variance\n\nThe trees in bagging are not completely independent of each other since all the original features are considered at every split of every tree.\nTrees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to any underlying strong relationships\nThe trees are correlated (not as diverse a set of base learners)\n\n\nRandom forests help to reduce tree correlation by injecting more randomness into the tree-growing process\n\nMore specifically, while growing a decision tree during the bagging process, random forests perform split-variable randomization where each time a split is to be performed, the search for the best split variable is limited to a random subset of mtry of the original p features.\n\nBecause the algorithm randomly selects a bootstrap sample to train on and a random sample of features to use at each split, a more diverse set of trees is produced which tends to lessen tree correlation beyond bagged trees and often dramatically increases predictive power.\n\nThere are three primary hyper-parameters to consider tuning in random forests within tidymodels\n\nmtry\ntrees\nmin_n\n\n\nmtry\n\nThe number of features to randomly select for splitting on each split\nSelection of value for mtry balances low tree correlation with reasonable predictive strength\nGood starting values for mtry are \\(\\frac{p}{3}\\) for regression and \\(\\sqrt{p}\\) for classification\nWhen there are fewer relevant features (e.g., noisy data) a higher value may be needed to make it more likely to select those features with the strongest signal.\nWhen there are many relevant features, a lower value might perform better\nDefault in ranger is \\(\\sqrt{p}\\)\n\n\ntrees\n\nThe number of bootstrap resamples of the training data to fit decision tree base learners\nThe number of trees needs to be sufficiently large to stabilize the error rate.\nA good rule of thumb is to start with 10 times the number of features\nYou may need to adjust based on values for mtry and min_n\nMore trees provide more robust and stable error estimates and variable importance measures\nMore trees == more computational cost\nDefault in ranger is 500\n\n\nmin_n\n\nMinimum number of observations in a new node rather than min to split\nNote that this is different than its definition for decision trees and bagged trees\nYou can consider the defaults (above) as a starting point\nIf your data has many noisy predictors and higher mtry values are performing best, then performance may improve by increasing node size (i.e., decreasing tree depth and complexity).\nIf computation time is a concern then you can often decrease run time substantially by increasing the node size and have only marginal impacts to your error estimate\nDefault in ranger is 1 for classification and 5 for regression\n\n\n\n9.6.1 Random Forest in Ames\nLet’s see how de-correlated the base learner trees improves their aggregate performance\n\nWe will need a new recipe for Random Forest\n\nRandom Forest works well out of the box with little feature engineering\nIt is still aggregating decision trees in bootstrap resamples of the training data\nHowever, the Random Forest algorithm does not natively handling missing data. We need to handle missing data manually during feature engineering. We will impute\n\n\nrec_rf &lt;- recipe(sale_price ~ ., data = data_trn) |&gt; \n  step_impute_median(all_numeric()) |&gt; \n  step_impute_mode(all_nominal()) \n\nrec_rf_prep &lt;- rec_rf |&gt;\n  prep(data_trn)\n\nfeat_trn_rf &lt;- rec_rf_prep |&gt; \n  bake(NULL)\n\n\nA quick look at features\n\nfeat_trn_rf |&gt; skim_some()\n\n\nData summary\n\n\nName\nfeat_trn_rf\n\n\nNumber of rows\n1955\n\n\nNumber of columns\n80\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n45\n\n\nnumeric\n35\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nms_sub_class\n0\n1\nFALSE\n16\n020: 730, 060: 388, 050: 208, 120: 122\n\n\nms_zoning\n0\n1\nFALSE\n7\nres: 1530, res: 297, flo: 91, com: 19\n\n\nstreet\n0\n1\nFALSE\n2\npav: 1946, grv: 9\n\n\nalley\n0\n1\nFALSE\n3\nnon: 1821, grv: 86, pav: 48\n\n\nlot_shape\n0\n1\nFALSE\n4\nreg: 1258, ir1: 636, ir2: 49, ir3: 12\n\n\nland_contour\n0\n1\nFALSE\n4\nlvl: 1769, hls: 75, bnk: 72, low: 39\n\n\nutilities\n0\n1\nFALSE\n2\nall: 1953, nos: 2\n\n\nlot_config\n0\n1\nFALSE\n5\nins: 1454, cor: 328, cul: 114, fr2: 55\n\n\nland_slope\n0\n1\nFALSE\n3\ngtl: 1864, mod: 78, sev: 13\n\n\nneighborhood\n0\n1\nFALSE\n28\nnam: 299, col: 174, old: 161, edw: 135\n\n\ncondition_1\n0\n1\nFALSE\n9\nnor: 1693, fee: 114, art: 54, rra: 31\n\n\ncondition_2\n0\n1\nFALSE\n6\nnor: 1938, fee: 6, art: 4, pos: 3\n\n\nbldg_type\n0\n1\nFALSE\n5\none: 1631, tow: 145, dup: 77, tow: 64\n\n\nhouse_style\n0\n1\nFALSE\n8\n1st: 989, 2st: 580, 1_5: 224, slv: 79\n\n\nroof_style\n0\n1\nFALSE\n6\ngab: 1557, hip: 362, gam: 16, fla: 9\n\n\nroof_matl\n0\n1\nFALSE\n7\ncom: 1929, tar: 11, wds: 8, wds: 4\n\n\nexterior_1st\n0\n1\nFALSE\n15\nvin: 671, hdb: 301, met: 298, wd_: 283\n\n\nexterior_2nd\n0\n1\nFALSE\n17\nvin: 662, met: 295, hdb: 279, wd_: 267\n\n\nmas_vnr_type\n0\n1\nFALSE\n5\nnon: 1184, brk: 581, sto: 171, brk: 18\n\n\nexter_qual\n0\n1\nFALSE\n4\nta: 1215, gd: 651, ex: 63, fa: 26\n\n\nexter_cond\n0\n1\nFALSE\n5\nta: 1707, gd: 195, fa: 42, ex: 8\n\n\nfoundation\n0\n1\nFALSE\n6\npco: 865, cbl: 849, brk: 198, sla: 33\n\n\nbsmt_qual\n0\n1\nFALSE\n5\nta: 861, gd: 808, ex: 167, fa: 62\n\n\nbsmt_cond\n0\n1\nFALSE\n6\nta: 1739, gd: 85, fa: 69, non: 57\n\n\nbsmt_exposure\n0\n1\nFALSE\n5\nno: 1271, av: 274, gd: 183, mn: 168\n\n\nbsmt_fin_type_1\n0\n1\nFALSE\n7\nunf: 576, glq: 535, alq: 294, rec: 202\n\n\nbsmt_fin_type_2\n0\n1\nFALSE\n7\nunf: 1655, rec: 75, lwq: 69, non: 57\n\n\nheating\n0\n1\nFALSE\n6\ngas: 1920, gas: 20, gra: 8, wal: 5\n\n\nheating_qc\n0\n1\nFALSE\n5\nex: 979, ta: 590, gd: 324, fa: 60\n\n\ncentral_air\n0\n1\nFALSE\n2\ny: 1821, n: 134\n\n\nelectrical\n0\n1\nFALSE\n5\nsbr: 1793, fus: 125, fus: 29, fus: 7\n\n\nkitchen_qual\n0\n1\nFALSE\n5\nta: 1011, gd: 765, ex: 126, fa: 52\n\n\nfunctional\n0\n1\nFALSE\n8\ntyp: 1822, min: 48, min: 41, mod: 23\n\n\nfireplace_qu\n0\n1\nFALSE\n6\nnon: 960, gd: 481, ta: 407, fa: 44\n\n\ngarage_type\n0\n1\nFALSE\n7\natt: 1161, det: 521, bui: 123, non: 107\n\n\ngarage_finish\n0\n1\nFALSE\n4\nunf: 826, rfn: 547, fin: 473, non: 109\n\n\ngarage_qual\n0\n1\nFALSE\n6\nta: 1745, non: 109, fa: 79, gd: 16\n\n\ngarage_cond\n0\n1\nFALSE\n6\nta: 1778, non: 109, fa: 46, gd: 12\n\n\npaved_drive\n0\n1\nFALSE\n3\ny: 1775, n: 139, p: 41\n\n\npool_qc\n0\n1\nFALSE\n5\nnon: 1945, ex: 3, gd: 3, fa: 2\n\n\nfence\n0\n1\nFALSE\n5\nnon: 1599, mnp: 215, gdw: 70, gdp: 61\n\n\nmisc_feature\n0\n1\nFALSE\n5\nnon: 1887, she: 62, oth: 3, gar: 2\n\n\nmo_sold\n0\n1\nFALSE\n12\njun: 333, jul: 298, may: 273, apr: 177\n\n\nsale_type\n0\n1\nFALSE\n10\nwd: 1695, new: 158, cod: 57, con: 16\n\n\nsale_condition\n0\n1\nFALSE\n6\nnor: 1616, par: 161, abn: 120, fam: 30\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\nlot_frontage\n0\n1\n21\n313\n\n\nlot_area\n0\n1\n1476\n215245\n\n\noverall_qual\n0\n1\n1\n10\n\n\noverall_cond\n0\n1\n1\n9\n\n\nyear_built\n0\n1\n1875\n2010\n\n\nyear_remod_add\n0\n1\n1950\n2010\n\n\nmas_vnr_area\n0\n1\n0\n1600\n\n\nbsmt_fin_sf_1\n0\n1\n0\n5644\n\n\nbsmt_fin_sf_2\n0\n1\n0\n1526\n\n\nbsmt_unf_sf\n0\n1\n0\n2153\n\n\ntotal_bsmt_sf\n0\n1\n0\n6110\n\n\nx1st_flr_sf\n0\n1\n372\n4692\n\n\nx2nd_flr_sf\n0\n1\n0\n2065\n\n\nlow_qual_fin_sf\n0\n1\n0\n1064\n\n\ngr_liv_area\n0\n1\n438\n5642\n\n\nbsmt_full_bath\n0\n1\n0\n3\n\n\nbsmt_half_bath\n0\n1\n0\n2\n\n\nfull_bath\n0\n1\n0\n4\n\n\nhalf_bath\n0\n1\n0\n2\n\n\nbedroom_abv_gr\n0\n1\n0\n8\n\n\nkitchen_abv_gr\n0\n1\n0\n3\n\n\ntot_rms_abv_grd\n0\n1\n3\n14\n\n\nfireplaces\n0\n1\n0\n3\n\n\ngarage_yr_blt\n0\n1\n1896\n2010\n\n\ngarage_cars\n0\n1\n0\n4\n\n\ngarage_area\n0\n1\n0\n1488\n\n\nwood_deck_sf\n0\n1\n0\n870\n\n\nopen_porch_sf\n0\n1\n0\n742\n\n\nenclosed_porch\n0\n1\n0\n552\n\n\nx3ssn_porch\n0\n1\n0\n508\n\n\nscreen_porch\n0\n1\n0\n576\n\n\npool_area\n0\n1\n0\n738\n\n\nmisc_val\n0\n1\n0\n12500\n\n\nyr_sold\n0\n1\n2006\n2010\n\n\nsale_price\n0\n1\n12789\n745000\n\n\n\n\n\n\nWe will need a tuning grid for the hyperparameters\n\nI played around a bit with values for trees, mtry, and min_n until I arrived at values that produced a floor for RMSE\n\ngrid_rf &lt;- expand_grid(trees = c(250, 500, 750, 1000), \n                       mtry = c(5, 10, 20, 25), \n                       min_n = c(1, 2, 5, 10))\n\n\nLet’s now fit the model configurations defined by the grid_rf using tune_grid()\nranger gives you a lot of additional control by changing defaults in set_engine().\n\nWe will mostly use defaults\nYou should explore if you want to get the best performance from your models\nsee ?ranger\nDefaults for splitting rules are gini for classification and variance for regression. These are appropriate\nWe will explicitly specify respect.unordered.factors = \"order\" as recommended. Could consider respect.unordered.factors = \"partition\"\nWe will set oob.error = FALSE. TRUE would allow for OOB performance estimate using OOB for each bootstrap for each base learner. We calculate this ourselves using tune_grid()\nWe will set seed = to generate reproducible bootstraps\n\n\nfits_rf &lt;-cache_rds(\n  expr = {\n    rand_forest(trees = tune(),\n              mtry = tune(),\n              min_n = tune()) |&gt;\n    set_engine(\"ranger\",\n               respect.unordered.factors = \"order\",\n               oob.error = FALSE,\n               seed = 102030) |&gt;\n    set_mode(\"regression\") |&gt; \n    tune_grid(preprocessor = rec_rf, \n              resamples = splits_boot, \n              grid = grid_rf, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/009/\",\n  file = \"fits_rf\")\n\n\nWe used these plots to confirm that we had selected good combinations of hyperparameters to tune and that the best hyperparameters are inside the range of values considered (or at their objective edge)\n\n# autoplot(fits_rf)\n\n\nBut more importantly, look at the additional reduction in OOB RMSE for Random Forest relative to bagged trees!\n\nshow_best(fits_rf)\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator   mean     n std_err\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1    20   750     1 rmse    standard   24979.    10    710.\n2    20   750     2 rmse    standard   24980.    10    711.\n3    20  1000     1 rmse    standard   24981.    10    713.\n4    20  1000     2 rmse    standard   24991.    10    716.\n5    20   500     1 rmse    standard   24997.    10    717.\n  .config              \n  &lt;chr&gt;                \n1 Preprocessor1_Model41\n2 Preprocessor1_Model42\n3 Preprocessor1_Model57\n4 Preprocessor1_Model58\n5 Preprocessor1_Model25\n\n\n\nLet’s fit the best model configuration to all the training data\n\nWe will use the same seed and other arguments as before\nWe could now use this final model to predict into our Ames test set (but we will skip that step) to get a better estimate of true performance with new data.\n\n\nfit_rf &lt;-   \n  rand_forest(trees = select_best(fits_rf)$trees,\n                mtry = select_best(fits_rf)$mtry,\n                min_n = select_best(fits_rf)$min_n) |&gt;\n  set_engine(\"ranger\", \n             respect.unordered.factors = \"order\", \n             oob.error = FALSE,\n             seed = 102030) |&gt;\n  set_mode(\"regression\") |&gt;  \n  fit(sale_price ~ ., data = feat_trn_rf)\n\n\nIn conclusion:\n\nRandom Forest is a great out of the box statistical algorithm for both classification and regression\nWe see no compelling reason to use bagged trees because Random Forest has all (except missing data handling) the benefits of bagged trees plus better prediction b/c of the de-correlated base learners\nIn some instances, we might use a decision tree if we wanted an interpretable tree as a method to understand rule based relationships between our features and our outcome.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Models: Decision Trees, Bagging Trees, and Random Forest</span>"
    ]
  },
  {
    "objectID": "009_random_forest.html#discussion-notes",
    "href": "009_random_forest.html#discussion-notes",
    "title": "9  Advanced Models: Decision Trees, Bagging Trees, and Random Forest",
    "section": "9.7 Discussion Notes",
    "text": "9.7 Discussion Notes\n\n9.7.1 Announcements\n\nExam answer review\nExam grading questions\nQuiz performance and updates (alpha, what would you NEVER do)\nInstalling keras package\n\n\n\n\n9.7.2 Trees\n\nWhen we split the data in a tree by a variable, how the threshold is specified? Is the algorithm determining it internally? (It looks like so, but I wonder if we could/should/might decide threshold manully sometime?) How it automatically decides the thresholds: is the tree calculating the accuracy(or other metrics) value at each node for each possible threshold decided by each variable?\nCan you explain more on gini and entropy? What are the costs and benefits of them, and how can we determine which to use?\nWhy isn’t feature engineering required as extensively when it comes to decision trees?\n\nout of box statistical algorithmsi\n\nHow does CART and RF handle categorical variables? What about other transformations of predictors (normalize, scale, combine)? non-linear?\nWhat determines the depth of trees that we set in decision trees and random forest?\nCan we compare the advantages or situations when we should choose trees and random forests?\nHow do trees handle missing data? Do we ever want to impute missing data in our recipes?\nCan you give more examples of how to interpret the decision tree graph?\n\n\n\n\n9.7.3 Bagging\n\nWhen and Why does bagging improve model performance\nBy bagging the decision trees, what are we trading off for less variance? Is our model more biased in some way? Or are we sacrificing the accuracy of where trees splits for less model variance?\n\nother examples when we don’t trade off bias/variance of some estimate (one for models, one for performance estimates?)\n\nCan you explain more on why we should de-correlate the bagged trees? Do we not need to de-correlate if we are not using bagging?\n\n\n\n\n9.7.4 Random Forest\n\nI think it would be useful to go over how the hyperparameter values for random forests were chosen in the web book example. Specifically, I don’t understand how the range of values was determined and how I can do this on my own.\nWhen we tune mtry in random forest, how can we determine the best range to tune on? Do we need to find p/3 or p^(1/2) and use that as a midpoint and test a range by hand? Is there no default range for mtry?\nWhen tuning grid for hyperparameters, is there sort of an “intuitive” way to select the values for trees, mtry and min_n?\n\n\n\n\n9.7.5 Boosting\n\nI don’t understand the boosting process very well, especially mathematically, I don’t understand how to “change the outcome of the subsequent trees as the residual of the original tree”. And is the residual here referring the residual of the latest tree? (Because in regression model we want to predict the value and in classification model we try to figure out what the predictors point to, and residual as an outcome is somehow seemingly not included in these two types?) Could we go through boosting tree and Bayesian tree method quickly?\n\n\n\n\n9.7.6 Relative Costs/Benefits of Algorithms\n\nCan we compare the advantages or situations when we should choose trees and random forests?\nFor which types of data or questions is it best to use boosting vs. regular bagging?\nDiscuss the advantages and limitations of decision trees compared to other machine learning algorithms.\nDo we just try to juggle their strengths and weaknesses and land on the one that best aligns with the purpose of our study?\nCan you discuss comparison among decision tree vs. bagging vs. random forest in terms of bias variance tradeoff?\n\n\n\n\n9.7.7 Other Issues\n\nWhy did we use bootstrapping in this assignment when k-fold may be a better estimator?\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Models: Decision Trees, Bagging Trees, and Random Forest</span>"
    ]
  },
  {
    "objectID": "010_neural_networks.html",
    "href": "010_neural_networks.html",
    "title": "10  Advanced Models: Neural Networks",
    "section": "",
    "text": "10.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Models: Neural Networks</span>"
    ]
  },
  {
    "objectID": "010_neural_networks.html#overview-of-unit",
    "href": "010_neural_networks.html#overview-of-unit",
    "title": "10  Advanced Models: Neural Networks",
    "section": "",
    "text": "10.1.1 Learning Objectives\n\nWhat are neural networks\nTypes of neural networks\nNeural network architecture\n\nlayers and units\nweights and biases\nactivation functions\ncost functions\noptimization\n\nepochs\nbatches\nlearning rate\n\n\nHow to fit 3 layer MLPs in tidymodels using Keras\n\n\n\n\n10.1.2 Readings\n\nNeural Networks and Deep Learning, Chapter 1: Using neural networks to recognize handwritten digits\n\nPost questions to the readings channel in Slack\n\n\n10.1.3 Lecture Videos\n\nLecture 1: But what is a Neural Network? ~ 19 mins\nLecture 2: Gradient descent, how neural networks learn ~ 21 mins\nLecture 3: What is backpropagation really doing? ~ 13 mins\nOptional Lecture 4: Backpropagation calculus ~ 10 mins\nLecture 5: Introduction and the MNIST dataset ~ 14 mins\nLecture 6: Fitting neural networks in tidymodels with Keras ~ 18 mins\nLecture 7: Addressing overfitting - Intro and L2 ~ 5 mins\nLecture 8: Addressing overfitting - Dropout ~ 4 mins\nLecture 9: Addressing overfitting - Early stopping ~ 8 mins\nLecture 10: Selecting model configurations and final remarks ~ 8 mins\n\nPost questions to the video-lectures channel in Slack\n\n\n\n10.1.4 Coding Assignment\n\nwine quality datasets: training; test\nqmd shell\nsolution\n\nPost questions to application-assignments Slack channel\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, April 3rd",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Models: Neural Networks</span>"
    ]
  },
  {
    "objectID": "010_neural_networks.html#introduction-to-nerual-networks-with-keras-in-r",
    "href": "010_neural_networks.html#introduction-to-nerual-networks-with-keras-in-r",
    "title": "10  Advanced Models: Neural Networks",
    "section": "10.2 Introduction to Nerual Networks with Keras in R",
    "text": "10.2 Introduction to Nerual Networks with Keras in R\nWe will be using the keras engine to fit our neural networks in R.\nThe keras package provides an R Interface to the Keras API in Python.\n\nFrom the website:\n\nKeras is a high-level neural networks API developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\nKeras has the following key features:\n\nAllows the same code to run on CPU or on GPU, seamlessly.\nUser-friendly API - which makes it easy to quickly prototype deep learning models.\nBuilt-in support for basic multi-layer perceptrons, convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\nSupports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.\n\n\n\nKeras is actually a wrapper around an even more extensive open source platform, TensorFlow, which has also been ported to the R environment\n\nTensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\nTensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google’s Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research\n\n\nIf you are serious about focusing primarily or exclusively on neural networks, you will probably work directly within Keras in R or Python. However, tidymodels gives us access to 3 layer (single hidden layer) MLP neural networks through the keras engine. This allows us to fit simple (but still powerful) neural networks using all the tools (and code/syntax) that you already know. Yay!\n\nIf you plan to use Keras directly in R, you might start with this book. I’ve actually found it useful even in thinking about how to interface with Keras through tidymodels.\n\nGetting tidymodels configured to use the keras engine can take a little bit of upfront effort.\n\nWe provide an appendix to guide you through this process\n\nIf you havent already set this up, please do so immediately so that you can reach out to us for support if you need it\n\nOnce you have completed this one-time installation, you can now use the keras engine through tidymodels like any other engine. No need to do anything different from your normal tidymodeling workflow.\n\nYou should also know that Keras is configured to use GPUs rather than CPU (GPUs allow for highly parallel fitting of neural networks).\n\nHowever, it works fine with just a CPU as well.\n\nIt will generate some errors to tell you that you aren’t set up with a GPU (and then it will tell you to ignore those error messages).\nThis is an instance where you can ignore the messages!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Models: Neural Networks</span>"
    ]
  },
  {
    "objectID": "010_neural_networks.html#setting-up-our-environment",
    "href": "010_neural_networks.html#setting-up-our-environment",
    "title": "10  Advanced Models: Neural Networks",
    "section": "10.3 Setting up our Environment",
    "text": "10.3 Setting up our Environment\nNow lets start fresh\n\nWe load our normal environment including source files, parallel processing and cache support if we plan to use it (code not displayed)\nkeras will work with R without loading it or other packages (beyond what we always load). However, there will be some function conflicts.\n\nSo we will load keras and exclude the conflict\nWe also need to load magrittr and exclude two of its conflicting functions\n\n\n\nlibrary(keras, exclude = \"get_weights\")\nlibrary(magrittr, exclude = c(\"set_names\", \"extract\"))",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Models: Neural Networks</span>"
    ]
  },
  {
    "objectID": "010_neural_networks.html#the-mnist-dataset",
    "href": "010_neural_networks.html#the-mnist-dataset",
    "title": "10  Advanced Models: Neural Networks",
    "section": "10.4 The MNIST dataset",
    "text": "10.4 The MNIST dataset\nThe MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training and testing in the field of machine learning.\n\nIt consists of two sets:\n\nThere are 60,000 images from 250 people in train\nThere are 10,000 images from a different 250 people in test (from different people than in train)\n\n\nEach observation in the datasets represent a single image and its label\n\nEach image is a 28 X 28 grid of pixels = 784 predictors (x1 - x784)\nEach label is the actual value (0-9; y). We will treat it as categorical because we are trying to identify each number “category”, predicting a label of “4” when the image is a “5” is just as bad as predicting “9”\n\n\nLet’s start by reading train and test sets\n\ndata_trn &lt;- read_csv(here::here(path_data, \"mnist_train.csv.gz\"),\n                     col_types = cols()) |&gt; \n  mutate(y = factor(y, levels = 0:9, labels = 0:9))\ndata_trn |&gt; dim()\n\n[1] 60000   785\n\ndata_test &lt;- read_csv(here::here(path_data, \"mnist_test.csv\"),\n                      col_types = cols()) |&gt; \n    mutate(y = factor(y, levels = 0:9, labels = 0:9))\ndata_test |&gt; dim()\n\n[1] 10000   785\n\n\n\nHere is some very basic info on the outcome distribution\n\nin train\n\n\ndata_trn |&gt; tab(y)\n\n# A tibble: 10 × 3\n   y         n   prop\n   &lt;fct&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 0      5923 0.0987\n 2 1      6742 0.112 \n 3 2      5958 0.0993\n 4 3      6131 0.102 \n 5 4      5842 0.0974\n 6 5      5421 0.0904\n 7 6      5918 0.0986\n 8 7      6265 0.104 \n 9 8      5851 0.0975\n10 9      5949 0.0992\n\n\n\ndata_trn |&gt; plot_bar(\"y\")\n\n\n\n\n\n\n\n\n\n\nin test\n\n\ndata_test|&gt; tab(y)\n\n# A tibble: 10 × 3\n   y         n   prop\n   &lt;fct&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 0       980 0.098 \n 2 1      1135 0.114 \n 3 2      1032 0.103 \n 4 3      1010 0.101 \n 5 4       982 0.0982\n 6 5       892 0.0892\n 7 6       958 0.0958\n 8 7      1028 0.103 \n 9 8       974 0.0974\n10 9      1009 0.101 \n\n\n\ndata_test |&gt; plot_bar(\"y\")\n\n\n\n\n\n\n\n\n\nLet’s look at some of the images. We will need a function to display these images. We will use as.cimg() from the imager package\n\ndisplay_image &lt;- function(data){\n  message(\"Displaying: \", data$y)\n  \n  data |&gt; \n    select(-y) |&gt; \n    unlist(use.names = FALSE) |&gt; \n    imager::as.cimg(x = 28, y = 28) |&gt; \n    plot(axes = FALSE)\n}\n\n\nObservations 1, 3, 10, and 100 in training set\n\ndata_trn |&gt; \n  slice(1) |&gt; \n  display_image()\n\nDisplaying: 5\n\n\n\n\n\n\n\n\n\n\n\ndata_trn |&gt; \n  slice(3) |&gt; \n  display_image()\n\nDisplaying: 4\n\n\n\n\n\n\n\n\n\n\n\ndata_trn |&gt; \n  slice(10) |&gt; \n  display_image()\n\nDisplaying: 4\n\n\n\n\n\n\n\n\n\n\n\ndata_trn |&gt; \n  slice(100) |&gt; \n  display_image()\n\nDisplaying: 1\n\n\n\n\n\n\n\n\n\n\nAnd here is the first observation in test set\n\ndata_test |&gt; \n  slice(1) |&gt; \n  display_image()\n\nDisplaying: 7\n\n\n\n\n\n\n\n\n\n\nLet’s understand the individual predictors a bit more\n\nEach predictor is a pixel in the 28 X 28 grid for the image\nPixel intensity is coded for intensity in the range from 0 (black) to 255 (white)\nFirst 28 variables are the top row of 28 pixels\nNext 28 variables are the second row of 28 pixels\nThere are 28 rows of 28 predictors total (784 predictors)\n\n\n\nLets understand this by changing values for individual predictors\nHere is the third image again\n\nWhat will happen to the image if I change the value of predictor x25 to 255\n\ndata_trn |&gt; \n  slice(3) |&gt; \n  display_image()\n\nDisplaying: 4\n\n\n\n\n\n\n\n\n\n\n\nChange the x25 to 255\n\n\ndata_trn |&gt; \n  slice(3) |&gt;\n  mutate(x25 = 255) |&gt; \n  display_image()\n\nDisplaying: 4\n\n\n\n\n\n\n\n\n\n\n\nWhat will happen to the image if I change the value of predictor x29 to 255\n\n\nChange the x29 to 255\n\n\ndata_trn |&gt; \n  slice(3) |&gt;\n  mutate(x29 = 255) |&gt; \n  display_image()\n\nDisplaying: 4\n\n\n\n\n\n\n\n\n\nWhat will happen to the image if I change the value of predictor x784 to 255\n\n\nChange the x784 to 255\n\n\ndata_trn |&gt; \n  slice(3) |&gt;\n  mutate(x784 = 255) |&gt; \n  display_image()\n\nDisplaying: 4",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Models: Neural Networks</span>"
    ]
  },
  {
    "objectID": "010_neural_networks.html#fitting-neural-networks",
    "href": "010_neural_networks.html#fitting-neural-networks",
    "title": "10  Advanced Models: Neural Networks",
    "section": "10.5 Fitting Neural Networks",
    "text": "10.5 Fitting Neural Networks\nLet’s train some models to understand some basics about neural networks and the use of Keras within tidymodels\n\nWe will fit some configurations in the full training set and evaluate their performance in test\nWe are NOT using test to select among configurations (it wouldn’t be a true test set then) but only for instructional purposes.\nWe will start with an absolute minimal recipe and mostly defaults for the statistical algorithm\nWe will build up to more complex (and better) configurations\nWe will end with a demonstration of the use of the single validation set approach to select among model configurations\n\n\nLet’s start with a minimal recipe\n\n10 level categorical outcome as factor\nWill be used to establish 10 output neurons\n\n\nrec_min &lt;- \n  recipe(y ~ ., data = data_trn)\n\n\nHere are feature matrices for train and test using this recipe\n\nrec_min_prep &lt;- rec_min |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_min_prep |&gt; \n  bake(NULL)\n\nfeat_test &lt;-rec_min_prep |&gt; \n  bake(data_test)\n\n\nAnd let’s use a mostly out of the box (defaults) 3 layer (1 hidden layer) using Keras engine\n\nDefaults:\n\nhidden units = 5\npenalty = 0\ndropout = 0\nactivation = “softmax” for hidden units layer\nepochs = 20\nseeds = sample.int(10^5, size = 3)\n\n\nThe default activation for the hidden units when using Keras through tidymodels is softmax not sigmoid as per the basic models discussed in the book and lectures.\n\nThe activation for the output layer will always be softmax for classification problems when using Keras through tidymodels\n\nThis is likely a good choice\nIt provides scores that function like probabilities for each categorical response\n\nThe activation for the output layer will always be ‘linear’ for regression problems.\n\nAlso a generally good choice\n\nThe hidden units can have a variety of different activation functions\n\nlinear, softmax, relu, and elu through tidymodels\nAdditional activation functions (and many other “dials”) are available in Keras directly\n\n\n\nWe will adjust seeds from the start\n\nThere are a number of points in the fitting process where random numbers needed by Keras\n\ninitializing weights for hidden and output layers\nselecting units for dropout\nselecting batches within epochs\n\ntidymodels lets us provide three seeds to make the first two bullet points more reproducible.\n\n\nThere seems to still be some randomness across runs due to batch selection (and possibly other opaque steps)\n\nset.seed(1234567)\nfit_seeds &lt;- sample.int(10^5, size = 3)  # c(87591, 536, 27860)\n\n\nWe will also set verbose = 0 for now\n\nThis turns off messages and plots about epoch level performance\nAt this point, verbose would only report performance in the training data, which isn’t that informative\nWe will turn it on later when we learn how to get performance in a validation set\nNonetheless, you might still turn it on if you just want feedback on how long it will take for the fit to complete.\n\n\nLet’s fit this first model configuration in training set\n\nverbose = 0\nseeds = fit_seeds\n\n\nfit_1 &lt;-\n    mlp() |&gt;\n    set_mode(\"classification\") |&gt; \n    set_engine(\"keras\", \n               verbose = 0, \n               seeds = fit_seeds) |&gt;\n    fit(y ~ ., data = feat_trn)\n\nNOTE: The first model fit with Keras in each new session will generate those warnings/errors about GPU. You can ignore them.\n\nHere is this model’s performance in test\n\nIt’s not that great (What would you expect by chance?)\n\naccuracy_vec(feat_test$y, predict(fit_1, feat_test)$.pred_class)\n\n313/313 - 1s - 529ms/epoch - 2ms/step\n\n\n[1] 0.2063\n\n\n\nTheoretically, the scale of the inputs should not matter\n\nHOWEVER, gradient descent works better with inputs on the same scale\n\nWe will also want inputs with the same variance if we later apply L2 regularization to our models\n\nThere is a lot of discussion about how best to scale inputs\nBest if the input means are near zero\nBest if variances are comparable\n\n\nWe could:\n\nUse step_normalize() [Bad choice of function names by tidymodel folks; standardize vs. normalize]\nUse step_range()\nBook range corrected based on known true range (/ 255)\n\nWe will use step_normalize()\n\n\n\nrec_scaled_wrong &lt;- \n  recipe(y ~ ., data = data_trn) |&gt;\n  step_normalize(all_predictors())\n\nThis is wrong! Luckily we glimpsed our feature matrix (not displayed here)\n\n\n\n\n\n\nQuestion: What went wrong and what should we do?\n\n\n\n\n\n\n\nShow Answer\nMany of the features have zero variance b/c they are black for ALL of the \nimages (e.g., top rows of pixels.  We can not scale a predictor with zero variance \nb/c when we divide by the SD = 0, we get NaN).  At a minimum, we should remove \nzero variance predictors in training from training and test\n\n\n\n\n\n\nFor example\n\ndata_trn$x1 |&gt; sd()\n\n[1] 0\n\n\n\nLet’s remove zero variance predictors before we scale\n\nTo be clear, zero variance features are NOT a problem for neural networks (though clearly they won’t help either).\nBut they WILL definitely cause problems for some scaling transformations.\n\n\nrec_scaled &lt;- \n  recipe(y ~ ., data = data_trn) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\n\nWe now have 717 (+ y) features rather than 28 * 28 = 784 features\n\nrec_scaled_prep &lt;- rec_scaled |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_scaled_prep |&gt; \n  bake(NULL)\n\ndim(feat_trn)\n\n[1] 60000   718\n\n\n\nLet’s also make the feature matrix for test. This will exclude features that were zero variance in train and scale them by their mean and sd in train\n\nfeat_test &lt;- rec_scaled_prep |&gt; \n  bake(data_test)\n\ndim(feat_test)\n\n[1] 10000   718\n\n\n\nLet’s fit and evaluate this new feature set with no other changes to the model configuration\n\nfit_2 &lt;-\n  mlp() |&gt;\n  set_mode(\"classification\") |&gt; \n  set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |&gt;\n  fit(y ~ ., data = feat_trn)\n\n\nThat helped a LOT\nStill could be better though (but it always impresses me! ;-)\n\n\naccuracy_vec(feat_test$y, predict(fit_2, feat_test)$.pred_class)\n\n313/313 - 1s - 513ms/epoch - 2ms/step\n\n\n[1] 0.4958\n\n\n\nThere are many other recommendations about feature engineering to improve the inputs\nThese include:\n\nNormalize (and here I mean true normalization; e.g., step_BoxCox(), step_YeoJohnson())\nDe-correlate (e.g., step_pca() but retain all features?)\n\nYou can see some discussion of these issues here and here to get you started. The paper linked in the stack overflow response is also a useful starting point.\n\nSome preliminary modeling EDA on my part suggested these additional considerations didn’t have major impact on the performance of our models with this dataset so we will stick with just scaling the features.\n\nIt is not surprising that a model configuration with only one hidden layer and 5 units isn’t sufficient for this complex task\nLet’s try 30 units (cheating based on the book chapter!! ;-)\n\nfit_5units &lt;- mlp(hidden_units = 30) |&gt;\n    set_mode(\"classification\") |&gt; \n    set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |&gt;\n    fit(y ~ ., data = feat_trn)\n\n\n\nBingo! Much, much better!\nWe could see if even more units works better still but I won’t follow that through here for sake of simplicity\n\n\naccuracy_vec(feat_test$y, predict(fit_5units, feat_test)$.pred_class)\n\n313/313 - 1s - 554ms/epoch - 2ms/step\n\n\n[1] 0.9386\n\n\n\nThe Three Blue 1 Brown videos had a brief discussion of the relu activation function.\nLet’s see how to use other activation functions and if this one helps.\n\nfit_relu &lt;- mlp(hidden_units = 30, activation = \"relu\") |&gt;\n  set_mode(\"classification\") |&gt; \n  set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |&gt;\n  fit(y ~ ., data = feat_trn)\n\n\naccuracy_vec(feat_test$y, predict(fit_relu, feat_test)$.pred_class)\n\n313/313 - 0s - 414ms/epoch - 1ms/step\n\n\n[1] 0.9647",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Models: Neural Networks</span>"
    ]
  },
  {
    "objectID": "010_neural_networks.html#dealing-with-overfitting",
    "href": "010_neural_networks.html#dealing-with-overfitting",
    "title": "10  Advanced Models: Neural Networks",
    "section": "10.6 Dealing with Overfitting",
    "text": "10.6 Dealing with Overfitting\nAs you might imagine, given the number of weights to be fit in even a modest neural network (our 30 hidden unit network has 21,850 parameters to estimate), it is easy to become overfit\n\n21,540 for hidden layer (717 * 30 weight + 30 biases)\n310 for output layer (30 * 10 weights, + 10 biases)\n\n\nfit_relu\n\nparsnip model object\n\nModel: \"sequential_3\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_6 (Dense)                    (None, 30)                      21540       \n dense_7 (Dense)                    (None, 10)                      310         \n================================================================================\nTotal params: 21850 (85.35 KB)\nTrainable params: 21850 (85.35 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\nThis will be an even bigger problem if you aren’t using “big” data\n\nThere are a number of different methods available to reduce potential overfitting\n\nSimplify the network architecture (fewer units, fewer layers)\nL2 regularization\nDropout\nEarly stopping or monitoring validation error to prevent too many epochs\n\n\n\n10.6.1 Regularization or Weight Decay\nL2 regularization is implemented in essentially the same fashion as you have seen it previously (e.g., glmnet)\nThe cost function is expanded to include a penalty based on the sum of the squared weights multiplied by \\(\\lambda\\).\nIn the tidymodels implementation of Keras:\n\n\\(\\lambda\\) is called penalty and is set and/or (ideally) tuned via the penalty argument in mlp()\nCommon values for the L2 penalty to tune a neural network are often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc.\npenalty = 0 (the default) means no L2 regularization\nKeras implements other penalties (L1, and a mixture) but not currently through tidymodels\nHere is a starting point for more reading on regularization in neural networks\n\n\nLet’s set penalty = .0001.\n\nfit_penalty &lt;- mlp(hidden_units = 30, activation = \"relu\", penalty = .0001) |&gt;\n  set_mode(\"classification\") |&gt; \n  set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |&gt;\n  fit(y ~ ., data = feat_trn)\n\n\n\nLooks like there is not much benefit to regularization for this network.\n\nWould likely provide much greater benefit in smaller N contexts or with more complicated model architectures (more hidden units, more hidden unit layers).\n\n\naccuracy_vec(feat_test$y, predict(fit_penalty, feat_test)$.pred_class)\n\n313/313 - 0s - 355ms/epoch - 1ms/step\n\n\n[1] 0.9661\n\n\n\n\n\n10.6.2 Dropout\nDropout is a second technique to minimize overfitting.\n\nHere is a clear description of dropout from a blog post on the Machine Learning Mastery:\n\nDropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\nAs a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features providing some specialization. Neighboring neurons come to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data.\nYou can imagine that if neurons are randomly dropped out of the network during training, that other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network.\nThe effect is that the network becomes less sensitive to the specific weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overfit the training data.\n\n\nFor further reading, you might start with the 2014 paper by Srivastava, et al that proposed the technique.\n\nIn tidymodels, you can set or tune the amount of dropout via the dropout argument in mlp()\n\nSrivastava, et al suggest starting with values around .5.\n\nYou might consider a range between .1 and .5\ndroppout = 0 (the default) means no dropout\nIn tidymodels implementation of Keras, you can use a non-zero penalty or dropout but not both\n\n\nLet’s try dropout = .1.\n\nfit_dropout &lt;- mlp(hidden_units = 30, activation = \"relu\", dropout = .1) |&gt;\n  set_mode(\"classification\") |&gt;  \n  set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |&gt;\n  fit(y ~ ., data = feat_trn)\n\n\n\nLooks like there may be a little benefit but not substantial.\n\nWould likely provide much greater benefit in smaller N contexts or with more complicated model architectures (more hidden units, more hidden unit layers).\n\n\naccuracy_vec(feat_test$y, predict(fit_dropout, feat_test)$.pred_class)\n\n313/313 - 1s - 515ms/epoch - 2ms/step\n\n\n[1] 0.9653\n\n\n\n\n\n10.6.3 Number of Epochs and Early Stopping\nNow that we have a model that is working well, lets return to the issue of number of epochs\n\nToo many epochs can lead to overfitting\nToo many epochs also just slow things down (not a bit deal if using GPU or overnight but still…..)\nToo few epochs can lead to under-fitting (which also produces poor performance)\nThe default of epochs = 20 is a reasonable starting point for a network with one hidden layer but may not work for all situations\n\n\nMonitoring training error (loss, accuracy) is not ideal b/c it will tend to always decrease\n\nThis is what you would get if you set verbose = 1\n\n\n\nValidation error is what you need to monitor\n\nValidation error will increase when the model becomes overfit to training\nWe can have Keras hold back some portion of the training data for validation\n\nvalidation_split = 1/6\nWe pass it in as an optional argument in set_engine()\nWe can use this to monitor validation error rather than training error by epoch.\nYou can fit an exploratory model with epochs = 50 to review the plot\nThis can allow us to determine an appropriate value for epochs\n\n\n\nLet’s see this in action in the best model configuration without regularization or dropout\nNOTE:\n\nepochs = 50\nverbose = 1\nmetrics = c(\"accuracy\")\nvalidation_split = 1/6\nYou will see message updates and a plot that tracks training and validation loss and accuracy across epochs\nThis is not rendered into my slides but the plot and messages are pretty clear\nYou can use this information to choose appropriate values for epoch\nval_accuracy had plateau and val_loss had started to creep up by 10 epochs.\n\n\nfit_epochs50 &lt;- mlp(hidden_units = 30, activation = \"relu\", epochs = 50) |&gt;\n  set_mode(\"classification\") |&gt;  \n  set_engine(\"keras\", verbose = 1, seeds = fit_seeds, \n             metrics = c(\"accuracy\"), \n             validation_split = 1/6) |&gt;\n  fit(y ~ ., data = feat_trn)\n\n\nIn some instances, it may be that we want to do more than simply look at epoch performance plots during modeling EDA\n\nWe can instead set the number of epochs to be high but use an early stopping callback to end the training early at an optimal time\n\nCallbacks allow us to interrupt training.\n\nThere are many types of callbacks in Keras\nWe will only discuss callback_early_stopping()\nWe set up callbacks in a list\nWe pass them in as an optional argument in set_engine() using callbacks =\nNotice the arguments for callback_early_stopping()\nWe also must provide validation error. Here we set validation_split = 1/6\nThis is a method to tune or select best number of epochs.\n\nI haven’t yet figured out where the epochs at termination are saved so need to watch the feedback. It was 35 epochs here\nAs always, we could next refit to the full training set after we have determined the optimal number of epochs. We won’t do that here.\n\n\n\nThis fit stopped at 15 epochs\n\ncallback_list &lt;- list(keras::callback_early_stopping(monitor = \"val_loss\", \n                                                     min_delta = 0, \n                                                     patience = 10))\n\n\nfit_early &lt;- mlp(hidden_units = 30, activation = \"relu\", epochs = 200) |&gt;\n  set_mode(\"classification\") |&gt; \n  set_engine(\"keras\", verbose = 1,\n             seeds = fit_seeds, \n             metrics = c(\"accuracy\" ), \n             validation_split = 1/6,\n             callbacks = callback_list) |&gt;\n  fit(y ~ ., data = feat_trn)\n\n\n\naccuracy_vec(feat_test$y, predict(fit_early, feat_test)$.pred_class)\n\n313/313 - 0s - 430ms/epoch - 1ms/step\n\n\n[1] 0.9642\n\n\n\nCoding sidebar: You can see many of the optional arguments you can set for Keras in the help here.\n\n\nAnd you can see more info about callback_early_stopping() here",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Models: Neural Networks</span>"
    ]
  },
  {
    "objectID": "010_neural_networks.html#using-resampling-to-select-best-model-configuration",
    "href": "010_neural_networks.html#using-resampling-to-select-best-model-configuration",
    "title": "10  Advanced Models: Neural Networks",
    "section": "10.7 Using Resampling to Select Best Model Configuration",
    "text": "10.7 Using Resampling to Select Best Model Configuration\nDeveloping a good network artchitecture and considering feature enginnering options involves experimentation\n\nThis is what Keras is designed to do\ntidymodels allows this too\nWe need to evaluate configurations with a valid method to evaluate performance\n\nvalidation split metric\nk-fold metric\nbootstrap method\nEach can be paired with fit_resamples() or tune_grid()\n\nWe need to be systematic\n\ntune_grid() helps with this too\nrecipes can be tuned as well (outside the scope of this course)\n\n\n\nHere is an example where we can select among many model configurations that differ across multiple network characteristics\n\nEvaluate with validation split accuracy\nSample size is relatively big so we have 10,000 validation set observations. Should offer a low variance performance estimate\nK-fold and bootstrap would still be better but big computation costs (too big for this web book but could be done in real life!)\n\n\nIts really just our normal workflow at this point\n\nGet splits (validation splits in this example)\n\n\nset.seed(102030)\nsplits_validation &lt;-\n  data_trn |&gt; \n  validation_split(prop = 5/6)\n\n\n\nSet up grid of hyperparameter values\n\n\ngrid_keras &lt;- expand_grid(hidden_units = c(5, 10, 20, 30, 50, 100), \n                          penalty = c(.00001, .0001, .01, .1))\n\n\n\nUse tune_grid() to fit models in training and predict into validation set for each combination of hyperparameter values\n\n\nfits_nn &lt;- cache_rds(\n  expr = {\n    mlp(hidden_units = tune(), penalty = tune(), activation = \"relu\") |&gt;\n    set_mode(\"classification\") |&gt; \n    # setting to verbose = 1 to track progress.  Training error not that useful\n    set_engine(\"keras\", verbose = 1, seeds = fit_seeds) |&gt;  \n    tune_grid(preprocessor = rec_scaled, \n                  grid = grid_keras,\n                  resamples = splits_validation,\n                  metrics = metric_set(accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/010/\",\n  file = \"fits_nn\")\n\n\n\nFind model configuration with best performance in the held-out validation set\n\n\nshow_best(fits_nn)\n\n# A tibble: 5 × 8\n  hidden_units penalty .metric  .estimator  mean     n std_err\n         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1          100 0.00001 accuracy multiclass 0.973     1      NA\n2          100 0.0001  accuracy multiclass 0.970     1      NA\n3           50 0.0001  accuracy multiclass 0.967     1      NA\n4           50 0.00001 accuracy multiclass 0.966     1      NA\n5           30 0.00001 accuracy multiclass 0.962     1      NA\n  .config              \n  &lt;chr&gt;                \n1 Preprocessor1_Model21\n2 Preprocessor1_Model22\n3 Preprocessor1_Model18\n4 Preprocessor1_Model17\n5 Preprocessor1_Model13",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Models: Neural Networks</span>"
    ]
  },
  {
    "objectID": "010_neural_networks.html#other-details",
    "href": "010_neural_networks.html#other-details",
    "title": "10  Advanced Models: Neural Networks",
    "section": "10.8 Other Details",
    "text": "10.8 Other Details\nWe can get a better sense of how tidymodels is interacting with Keras by looking at the function that is called\n\nmlp(hidden_units = 30, activation = \"relu\", dropout = .1) |&gt;\n  set_mode(\"classification\") |&gt;  \n  set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |&gt; \n  translate()\n\nSingle Layer Neural Network Model Specification (classification)\n\nMain Arguments:\n  hidden_units = 30\n  dropout = 0.1\n  activation = relu\n\nEngine-Specific Arguments:\n  verbose = 0\n  seeds = fit_seeds\n\nComputational engine: keras \n\nModel fit template:\nparsnip::keras_mlp(x = missing_arg(), y = missing_arg(), hidden_units = 30, \n    dropout = 0.1, activation = \"relu\", verbose = 0, seeds = fit_seeds)\n\n\n\nkeras_mlp() is a wrapper around the calls to Keras. Lets see what it does\n\nkeras_mlp\n\nfunction (x, y, hidden_units = 5, penalty = 0, dropout = 0, epochs = 20, \n    activation = \"softmax\", seeds = sample.int(10^5, size = 3), \n    ...) \n{\n    if (penalty &gt; 0 & dropout &gt; 0) {\n        rlang::abort(\"Please use either dropoput or weight decay.\", \n            call. = FALSE)\n    }\n    if (!is.matrix(x)) {\n        x &lt;- as.matrix(x)\n    }\n    if (is.character(y)) {\n        y &lt;- as.factor(y)\n    }\n    factor_y &lt;- is.factor(y)\n    if (factor_y) {\n        y &lt;- class2ind(y)\n    }\n    else {\n        if (isTRUE(ncol(y) &gt; 1)) {\n            y &lt;- as.matrix(y)\n        }\n        else {\n            y &lt;- matrix(y, ncol = 1)\n        }\n    }\n    model &lt;- keras::keras_model_sequential()\n    if (penalty &gt; 0) {\n        model %&gt;% keras::layer_dense(units = hidden_units, activation = activation, \n            input_shape = ncol(x), kernel_regularizer = keras::regularizer_l2(penalty), \n            kernel_initializer = keras::initializer_glorot_uniform(seed = seeds[1]))\n    }\n    else {\n        model %&gt;% keras::layer_dense(units = hidden_units, activation = activation, \n            input_shape = ncol(x), kernel_initializer = keras::initializer_glorot_uniform(seed = seeds[1]))\n    }\n    if (dropout &gt; 0) {\n        model %&gt;% keras::layer_dense(units = hidden_units, activation = activation, \n            input_shape = ncol(x), kernel_initializer = keras::initializer_glorot_uniform(seed = seeds[1])) %&gt;% \n            keras::layer_dropout(rate = dropout, seed = seeds[2])\n    }\n    if (factor_y) {\n        model &lt;- model %&gt;% keras::layer_dense(units = ncol(y), \n            activation = \"softmax\", kernel_initializer = keras::initializer_glorot_uniform(seed = seeds[3]))\n    }\n    else {\n        model &lt;- model %&gt;% keras::layer_dense(units = ncol(y), \n            activation = \"linear\", kernel_initializer = keras::initializer_glorot_uniform(seed = seeds[3]))\n    }\n    arg_values &lt;- parse_keras_args(...)\n    compile_call &lt;- expr(keras::compile(object = model))\n    if (!any(names(arg_values$compile) == \"loss\")) {\n        if (factor_y) {\n            compile_call$loss &lt;- \"binary_crossentropy\"\n        }\n        else {\n            compile_call$loss &lt;- \"mse\"\n        }\n    }\n    if (!any(names(arg_values$compile) == \"optimizer\")) {\n        compile_call$optimizer &lt;- \"adam\"\n    }\n    compile_call &lt;- rlang::call_modify(compile_call, !!!arg_values$compile)\n    model &lt;- eval_tidy(compile_call)\n    fit_call &lt;- expr(keras::fit(object = model))\n    fit_call$x &lt;- quote(x)\n    fit_call$y &lt;- quote(y)\n    fit_call$epochs &lt;- epochs\n    fit_call &lt;- rlang::call_modify(fit_call, !!!arg_values$fit)\n    history &lt;- eval_tidy(fit_call)\n    model$y_names &lt;- colnames(y)\n    model\n}\n&lt;bytecode: 0x58b8ad98f570&gt;\n&lt;environment: namespace:parsnip&gt;\n\n\nWe can see:\n\nHow the activation functions are setup\nThe choice of cost function: mse or binary_crossentropy\nThe optimizer: Adam\nHow the three seeds are being used\n\n\nFinally, you might have noticed that we never set a learning rate anywhere\nThe Adam optimizer is used instead of classic stochastic gradient descent. The authors of this optimizer state it is:\n\nStraightforward to implement.\nComputationally efficient.\nLittle memory requirements.\nInvariant to diagonal rescale of the gradients.\nWell suited for problems that are large in terms of data and/or parameters.\nAppropriate for non-stationary objectives.\nAppropriate for problems with very noisy/or sparse gradients.\nHyper-parameters have intuitive interpretation and typically require little tuning.\n\n\nYou can start additional reading about Adam here\nFor now, if you want another optimizer or much more control over your network architecture, you may need to work directly in Keras.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Models: Neural Networks</span>"
    ]
  },
  {
    "objectID": "010_neural_networks.html#discussion-notes",
    "href": "010_neural_networks.html#discussion-notes",
    "title": "10  Advanced Models: Neural Networks",
    "section": "10.9 Discussion Notes",
    "text": "10.9 Discussion Notes\n\n10.9.1 Announcements\n\nReadings for unit 11 ready\nVideos/slides will be delayed a bit (complete by early weekend)\n\n\n\n10.9.2 keras vs nnet\n\npower/flexibility vs. simplicity (but in tidymodels)\nnnet package\n\n\n\n10.9.3 Overview\n\nlayers\nunits\nweights, biases\nactivation functions\ngradient descent\nback propagation of errors\nepochs\nstochastic gradient descent (and Adam)\nbatches\ndeep learning\n\n\n\n10.9.4 Activation functions\n\nperceptron\nsigmoid\nsoftmax\nrelu\n\n\n\n10.9.5 Addressing overfitting\n\nL2 regularization\nDropout\n\n\n\n10.9.6 Autoencoders\n\nOverview/example\nCosts/benefits vs. PCA\n\nautoencoders accommodate non-linearity\nlots of variants of autoencoders to address different types of data (images, time-series)\nautoencoders are very flexible (lots of parameters) - overfitting\nautoencoders are computationally costly\nautoencoders require expertise to set up and train\nPCA may be more interpretable (linear combo of features)\n\n\n\n\n10.9.7 Comparison of statistical algorithms\n\nlinear regression (linear model)\nlogistic regression (from generalized linear model)\nglmnet (LASSO, Ridge)\nKNN\nLDA, QDA\ndecision trees\nbagged decision trees\nrandom forest\nMLPs (neural networks)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Models: Neural Networks</span>"
    ]
  },
  {
    "objectID": "011_explanation.html",
    "href": "011_explanation.html",
    "title": "11  Explanatory Approaches",
    "section": "",
    "text": "11.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#overview-of-unit",
    "href": "011_explanation.html#overview-of-unit",
    "title": "11  Explanatory Approaches",
    "section": "",
    "text": "11.1.1 Learning Objectives\n\nUse of feature ablation to statisticall compare model configurations\n\nFrequentist correlated t-test using CV\nBayesian estimation for model comparisons\n\nROPE\n\n\nFeature importance metrics for explanation\n\nModel specific vs. model agnostic approaches\nPermutation feature importance\nShapley values (SHAP)\n\nlocal importance\nglobal importance\n\n\nVisual approaches for explanation\n\nPartial Dependence plots\nAccumulated Local Effects (ALE) plots\n\n\n\n\n\n11.1.2 Readings\n\nBenavoli et al. (2017) paper: Read pages 1-9 that describe the correlated t-test and its limitations.\nKruschke (2018) paper: Describes Bayesian estimation and the ROPE (generally, not in the context of machine learning and model comparisons)\nMolnar (2023) Chapter 3 - Interpretability\nMolnar (2023) Chapter 6 - Model-Agnostic Methods\nMolnar (2023) Chapter 8 - Global Model Agnostic Methods: Read setions 8.1, 8.2, 8.3, and 8.5\nMolnar (2023) Chapter 9 - Local Model-Agnostic Methods: Read section 9.5\n\n\n\nPost questions to the readings channel in Slack",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#lecture-videos",
    "href": "011_explanation.html#lecture-videos",
    "title": "11  Explanatory Approaches",
    "section": "11.2 Lecture Videos",
    "text": "11.2 Lecture Videos\n\nIntroduction to Model Comparisons ~ 6 mins\nAn Empirical Example of Feature Ablation ~ 13 mins\nThe Nadeau & Bengio Correlated t-test for Model Comparisons ~ 9 mins\nBayesian Estimation for Model Comparisons ~ 28 mins\nIntroduction to Feature Importance and the DALEX package ~ 11 mins\nPermutation Feature Importance ~ 7 mins\nSHAP Feature Importance ~ 14 mins\nVisual Approaches to Understand Models ~ 11 mins\n\n\n\nPost questions to the video-lectures channel in Slack",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#application-assignment-and-quiz",
    "href": "011_explanation.html#application-assignment-and-quiz",
    "title": "11  Explanatory Approaches",
    "section": "11.3 Application Assignment and Quiz",
    "text": "11.3 Application Assignment and Quiz\n\ndata: student_perf_cln.csv\nassignment qmd\nsolution html\n\n\n\nPost questions to application-assignments Slack channel\n\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, April 10th",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#model-comparisons-feature-ablation",
    "href": "011_explanation.html#model-comparisons-feature-ablation",
    "title": "11  Explanatory Approaches",
    "section": "11.4 Model Comparisons & Feature Ablation",
    "text": "11.4 Model Comparisons & Feature Ablation\nIn 610/710, you learned to think about the tests of specific parameter estimates as model comparisons of models that did vs. did not include the specific feature(s) in the model\n\nModel comparisons can be used in a similar way for explanatory goals with machine learning\n\nThis can be done for a single feature (e.g., \\(x_3\\))\n\ncompact model: \\(y = b_0 + b_1*x_1 + b_2*x_2\\)\nfull (augmented) model: \\(y = b_0 + b_1*x_1 + b_2*x_2 + b_3*x_3\\)\nThe comparison of these two models is equivalent to the test of \\(H_0: b_3 = 0\\)\n\n\n\nThis can also involve sets of features if you hypothesis involves the effect of a set of features\n\nAll features that represent a categorical predictor\nSet of features that represent some broad construct (e.g., psychiatric illness represented by symptoms counts for all of the major psychiatric diagnoses)\n\n\n\nThis technique of comparing two nested models (i.e. feature set for the compact model is a subset of the feature set for the full/augmented model) is often called feature ablation in the machine learning world\n\nModel comparisons can also be done between model configurations that differ by characteristics other than their features (e.g., statistical algorithm)\n\nModel comparisons can be useful to determine the best available model configuration to use for a prediction goal\n\nIn some instances, it is OK to simply choose the descriptively better performing model configuration (e.g., better validation set or resampled performance estimate)\nHowever, if the descriptively better performing model has other disadvantages (e.g., more costly to implement) you might want to only use it if you had rigorously demonstrated that it likely better for all new data.\n\n\nIn this unit, we will learn two approaches to statistically compare models\n\nTraditional frequentist (NHST) approach using a variant of the t-test to accommodate correlated observations\nA Bayesian alternative to the t-test\n\n\n\nWe will compare model nested model configurations formed by feature ablation (i.e., full and compact models will differ by features included)\n\nHowever, nothing would be different when implementing these comparison methods if these model configurations different by other characteristics such as statistical algorithm",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#an-empirical-example-of-feature-ablation",
    "href": "011_explanation.html#an-empirical-example-of-feature-ablation",
    "title": "11  Explanatory Approaches",
    "section": "11.5 An Empirical Example of Feature Ablation",
    "text": "11.5 An Empirical Example of Feature Ablation\nThe context for our example will be the Cleveland heart disease dataset\n\nWe will imagine we have developed a new diagnostic screener for heart disease based on an exercise test protocol\n\nWe want to demonstrate the incremental improvement in our screening for heart disease using features from this test vs. other readily available characteristics about the patients\n\nOur exercise test protocol yields four scores (\\(exer\\_*\\)) that we use in combination to predict the probability of heart disease in the patient\n\nMax heart rate during the exercise test\nExperience of angina during the test\nSlope of the peak exercise ST segment (don’t ask me what that is! ;-)\nST depression induced by exercise test relative to rest\n\n\n\nWe also have many other demographic and physical characteristics that we want to “control” for when evaluating the performance of our test\n\nI use control in both its senses. It likely helps to have these covariates b/c they reduce error in the outcome\nI also want to demonstrate that our test has incremental predictive validity above these other characteristics which are already available for screening without my complicated test\n\n\nLet’s open the data set and do some basic cleaning\n\nNote there were some complex issues to deal with\nGood examples of code to resolve those issues\n\n\ndata_all &lt;- read_csv(here::here(path_data, \"cleveland.csv\"), col_names = FALSE, \n                     na = \"?\", col_types = cols()) |&gt; \n  rename(age = X1,\n         sex = X2,\n         cp = X3,\n         rest_bp = X4,\n         chol = X5,\n         fbs = X6,\n         rest_ecg = X7,\n         exer_max_hr = X8,\n         exer_ang = X9,\n         exer_st_depress = X10,\n         exer_st_slope = X11,\n         ca = X12,\n         thal = X13,\n         disease = X14) |&gt; \n  mutate(disease = fct(if_else(disease == 0, \"no\", \"yes\"),\n                       levels = c(\"yes\", \"no\")), # pos event first\n         sex = fct(if_else(sex == 0, \"female\", \"male\"), \n                   levels = c(\"female\", \"male\")),\n         fbs = fct(if_else(fbs == 0, \"normal\", \"elevated\"),\n                   levels = c(\"normal\", \"elevated\")),\n         exer_ang = fct(if_else(exer_ang == 0, \"no\", \"yes\"),\n                           levels = c(\"no\", \"yes\")),\n         exer_st_slope = fct_recode(as.character(exer_st_slope), \n                                       upslope = \"1\", \n                                       flat = \"2\",\n                                       downslope = \"3\"),\n         cp = fct_recode(as.character(cp), \n                            typ_ang = \"1\", \n                            atyp_ang = \"2\", \n                            non_anginal = \"3\", \n                            non_anginal = \"4\"),\n         rest_ecg = fct_recode(as.character(rest_ecg), \n                                  normal = \"0\", \n                                  wave_abn = \"1\", \n                                  ventric_hypertrophy = \"2\"),\n         thal = fct_recode(as.character(thal), \n                              normal = \"3\", \n                              fixeddefect = \"6\", \n                              reversabledefect = \"7\"))  \n\n\nSkim it to make sure we didnt break anything during our cleaning!\n\ndata_all |&gt; skim_some()\n\n\nData summary\n\n\nName\ndata_all\n\n\nNumber of rows\n303\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n8\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsex\n0\n1.00\nFALSE\n2\nmal: 206, fem: 97\n\n\ncp\n0\n1.00\nFALSE\n3\nnon: 230, aty: 50, typ: 23\n\n\nfbs\n0\n1.00\nFALSE\n2\nnor: 258, ele: 45\n\n\nrest_ecg\n0\n1.00\nFALSE\n3\nnor: 151, ven: 148, wav: 4\n\n\nexer_ang\n0\n1.00\nFALSE\n2\nno: 204, yes: 99\n\n\nexer_st_slope\n0\n1.00\nFALSE\n3\nups: 142, fla: 140, dow: 21\n\n\nthal\n2\n0.99\nFALSE\n3\nnor: 166, rev: 117, fix: 18\n\n\ndisease\n0\n1.00\nFALSE\n2\nno: 164, yes: 139\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\nage\n0\n1.00\n29\n77.0\n\n\nrest_bp\n0\n1.00\n94\n200.0\n\n\nchol\n0\n1.00\n126\n564.0\n\n\nexer_max_hr\n0\n1.00\n71\n202.0\n\n\nexer_st_depress\n0\n1.00\n0\n6.2\n\n\nca\n4\n0.99\n0\n3.0\n\n\n\n\n\n\nThe dataset is not that large and we have a decent number of features so we will build models using regularized logistic regression (glmnet)\n\nIt would be better to actually do some exploration to build the best compact model first but we will skip that part of the analysis here\n\nWe are using glmnet so we need to find the optimal set of hyperparameter values for this model configuration\n\nLet’s select/tune hyperparameters using 10 repeats of 10-fold CV (more on why this many in a few moments)\n\nset.seed(123456)\nsplits &lt;- data_all |&gt; \n  vfold_cv(v = 10, repeats = 10, strata = \"disease\")\n\n\n\nAnd here is a grid of hyperparameters to tune\n\ngrid_glmnet &lt;- expand_grid(penalty = exp(seq(-8, 2, length.out = 300)),\n                           mixture = c(0, .025, .05, .1, .2, .4, .6, .8, 1))\n\n\nHere is a feature engineering recipe for the full model with all features that is appropriate for glmnet\n\nrec_full &lt;- recipe(disease ~ ., data = data_all) |&gt; \n  step_impute_median(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt;   \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_normalize(all_predictors())\n\n\nNow we tune the model configuration\n\nfits_full &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(),\n                 mixture = tune()) |&gt; \n    set_engine(\"glmnet\") |&gt; \n    tune_grid(preprocessor = rec_full,\n              resamples = splits,\n              grid = grid_glmnet,\n              metrics = metric_set(accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/011/\",\n  file = \"fits_full\")\n\n\nLet’s check how the models perform (k-fold resampled accuracy) with various values for the hyperparameters\n\nHere is accuracy by log penalty (lambda) and mixture (alpha)\n\nfits_full |&gt; \n  plot_hyperparameters(hp1 = \"penalty\", hp2 = \"mixture\", \n                       metric = \"accuracy\", log_hp1 = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Does the glmnet (regularized logistic regresssion) outperform a simple logistic regression? How would these two algorithms compare?\n\n\n\n\n\n\n\nShow Answer\nRemember that the linear model is a special case of glmnet where the penalty = 0.  \nHere we are looking log penalty down to exp(-8) = 0.0003354626.  That is pretty \nclose to 0 so an approximation of how the standard logistic regression would \nperform.  glmnet is slightly more accurate with its optimal hyperparameter values\n\n\n\n\n\n\nHere we show the mean performance of the top 10 configurations\n\nWe use these performance estimates to select the top optimal values for the hyperparameters\n\nWe will choose the hyperparameter values from the configuration displayed in the first row of this table\n\ncollect_metrics(fits_full) |&gt; \n  arrange(desc(mean)) |&gt; \n  print(n = 10)\n\n# A tibble: 2,700 × 8\n   penalty mixture .metric  .estimator  mean     n std_err\n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1   0.929   0.05  accuracy binary     0.829   100 0.00610\n 2   1.17    0.025 accuracy binary     0.829   100 0.00607\n 3   0.509   0.1   accuracy binary     0.828   100 0.00618\n 4   0.544   0.1   accuracy binary     0.828   100 0.00614\n 5   0.622   0.1   accuracy binary     0.828   100 0.00618\n 6   0.665   0.1   accuracy binary     0.828   100 0.00615\n 7   0.526   0.1   accuracy binary     0.828   100 0.00611\n 8   0.688   0.1   accuracy binary     0.828   100 0.00623\n 9   0.869   0.05  accuracy binary     0.828   100 0.00623\n10   1.03    0.025 accuracy binary     0.828   100 0.00601\n   .config                \n   &lt;chr&gt;                  \n 1 Preprocessor1_Model0838\n 2 Preprocessor1_Model0545\n 3 Preprocessor1_Model1120\n 4 Preprocessor1_Model1122\n 5 Preprocessor1_Model1126\n 6 Preprocessor1_Model1128\n 7 Preprocessor1_Model1121\n 8 Preprocessor1_Model1129\n 9 Preprocessor1_Model0836\n10 Preprocessor1_Model0541\n# ℹ 2,690 more rows\n\n\n\nHere we are storing that best configuration in an object so that we could later fit that configuration to the full dataset\n\n\n\n(hp_best_full &lt;- select_best(fits_full, n = 1))\n\n# A tibble: 1 × 3\n  penalty mixture .config                \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  \n1   0.929    0.05 Preprocessor1_Model0838\n\n\n\n\n\n\n\n\n\nQuestion: Tell me about bias if we use this mean performance from the 100 held-out folds as an estimate of how well that model fit to the full N will perform\n\n\n\n\n\n\n\nShow Answer\nThere is some optimization bias in this performance estimate because we already\nused these 100 held-out folds to select the best configuration.  This will be a \nsecond use of those folds.  This will lead us to over-estimate the models true \nperformance in new data.  We would need a test set to remove this bias.\n\nThere is also some bias because we are using less that the full sample (k-1 folds) \nto estimate performance of a model that will eventually have N unique observations \nacross all k folds.  This will lead  us to underestimate the true performance of \nour model in new data. \n\n\n\n\n\n\nWhen we (eventually) compare the full and compact models, the presence of some bias may not be as important (and there will always be some bias anyway, because of the latter concern from the last slide).\n\nWe are focused on a relative comparison in performance across the two models so what is most important is that the bias is comparable for our assessments of the two models.\n\nIf we use validation data (e.g., our 100 held-out folds), there will be comparable optimization bias for both models so this may not be too problematic. We don’t need test data.\n\nBut before we go further, we need to have a compact model to compare to our full model\n\nHere is a recipe to feature engineer features associated with this compact model\n\nWe can start with the full recipe and add one more step to remove (i.e., ablate) the features we want to evaluate\nstep_rm(contains(\"exer_\")\nAll previous recipe steps remain the same\n\n\nrec_compact &lt;- rec_full |&gt; \n  step_rm(contains(\"exer_\"))\n\n\nWe need to select/tune hyperparameters for this new model\n\nIt has different complexity than the full model so it might need different (less?) regularization for optimal performance\n\nfits_compact &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(),\n                 mixture = tune()) |&gt; \n    set_engine(\"glmnet\") |&gt; \n    tune_grid(preprocessor = rec_compact,   # use recipe for compact model\n              resamples = splits,\n              grid = grid_glmnet,\n              metrics = metric_set(accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/011/\",\n  file = \"fits_compact\")\n\n\nConfirm that we found a good set of hyperparameters\n\nfits_compact |&gt; \n  plot_hyperparameters(hp1 = \"penalty\", hp2 = \"mixture\", metric = \"accuracy\", \n                       log_hp1 = TRUE)\n\n\n\n\n\n\n\n\n\nAnd here is our best configuration, for when we want to train the compact model on all the data at a later stage\n\n(hp_best_compact &lt;- select_best(fits_compact, n = 1))\n\n# A tibble: 1 × 3\n  penalty mixture .config                \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  \n1 0.00116   0.025 Preprocessor1_Model0338\n\n\n\nNow, here are accuracies for these two models assessed by 10 repeats of 10-fold\n\nThis is the mean performance for the best configuration\nThese means are based on 100 individual held-out folds\n\n\ncollect_metrics(fits_full) |&gt; \n  arrange(desc(mean)) |&gt; \n  slice(1)\n\n# A tibble: 1 × 8\n  penalty mixture .metric  .estimator  mean     n std_err\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1   0.929    0.05 accuracy binary     0.829   100 0.00610\n  .config                \n  &lt;chr&gt;                  \n1 Preprocessor1_Model0838\n\ncollect_metrics(fits_compact) |&gt; \n  arrange(desc(mean)) |&gt; \n  slice(1)\n\n# A tibble: 1 × 8\n  penalty mixture .metric  .estimator  mean     n std_err\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 0.00116   0.025 accuracy binary     0.795   100 0.00706\n  .config                \n  &lt;chr&gt;                  \n1 Preprocessor1_Model0338\n\n\n\nThe compact model is less accurate but…..\n\nA simple descriptive comparison is not sufficient to justify the use of a costly test\nWe need to be more confident that the test really improves screening in all possible held out samples from our dataset\nAnd by how much?\nHow can we compare these two models?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#nadeau-and-bengio-2003-correlated-t-test",
    "href": "011_explanation.html#nadeau-and-bengio-2003-correlated-t-test",
    "title": "11  Explanatory Approaches",
    "section": "11.6 Nadeau and Bengio (2003) Correlated t-test",
    "text": "11.6 Nadeau and Bengio (2003) Correlated t-test\nWe have 100 held-out accuracies for each model.\n\nCould we compare these?\n\nWell, we have the same 100 held-out samples (we used the same splits) for both compact and full models so these two sets of accuracies (for each of the two models) should be considered paired/repeated.\n\nNot a problem, we could use paired samples t-test\nEasiest to think about this paired test as testing if the differences in accuracy for each of the 100 held out sets == 0. That removes the lack of independence from using the sample 100 held-out sets twice\n\n\nBUT these 100 differences are still not independent\n\nEach have been estimated using models that were fit with overlapping observations (the held in sets were fit with many of the same observations for each of the k-1 held in folds)\nIf we ignore this violation and simply do a paired-samples t-test, we will have inflation of alpha\n\n\nNadeau and Bengio (2003) (see pdf) and Bouckaert (2003) (see pdf) have explored the degree of dependence among performance estimates using resampling.\n\n\nThis was originally done for repeated random train/test splits (e.g., 90/10 splits) but is now also used when doing repeated k-fold.\n\nThe classic paired t-test has the following formula\n\\(t = \\frac{\\overline{x} - 0}{\\sqrt{\\hat{\\sigma^2}*\\frac{1}{n}}}\\)\n\nThe standard error for the difference (denominator of the t-statistic formula) is too small\n\nNadeau and Benigo adjusted it by \\(\\frac{\\rho}{1-\\rho}\\) where \\(\\rho = \\frac{n_{test}}{n_{total}}\\) or equivalent \\(\\frac{1}{k}\\)\n\nThis adjustment yields:\n\\(t = \\frac{\\overline{x} - 0}{\\sqrt{\\hat{\\sigma^2}*(\\frac{1}{n} + \\frac{\\rho}{1-\\rho})}}\\)\n\nLet’s perform this correlated t-test to compare our compact and full models\n\nWe first need to extract the 100 held-out folds from fits_full and fits_compact\n\nWhen we have used collect_metrics() in the past, we always used the default for summarize (which is TRUE). This gave us average performance for each model configuration across all the held-out folds. But the individual folds for each configuration are in that object too\n\nOur best model configuration for the full model was\n\nhp_best_full\n\n# A tibble: 1 × 3\n  penalty mixture .config                \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  \n1   0.929    0.05 Preprocessor1_Model0838\n\nhp_best_full$.config\n\n[1] \"Preprocessor1_Model0838\"\n\n\nWe can see that label assigned to that specific configuration in the .config column. We can use that to pull out the 100 folds for that configuration\n\ncv_full &lt;- collect_metrics(fits_full, summarize = FALSE) |&gt; \n  filter(.config == hp_best_full$.config) |&gt; \n  pull(.estimate)\n\ncv_full |&gt; print()\n\n  [1] 0.8387097 0.9032258 0.8387097 0.8709677 0.8000000 0.7333333 0.8666667\n  [8] 0.7333333 0.8666667 0.8275862 0.8064516 0.8064516 0.7741935 0.8709677\n [15] 0.8666667 0.8666667 0.8000000 0.8000000 0.7666667 0.8965517 0.8064516\n [22] 0.8387097 0.8709677 0.8709677 0.9000000 0.9000000 0.9000000 0.6666667\n [29] 0.7666667 0.7586207 0.9354839 0.8064516 0.8709677 0.7419355 0.7666667\n [36] 0.9333333 0.7666667 0.7666667 0.9000000 0.7586207 0.7419355 0.8387097\n [43] 0.8709677 0.9354839 0.8333333 0.9000000 0.8333333 0.7333333 0.7666667\n [50] 0.8275862 0.8387097 0.9354839 0.8064516 0.7741935 0.8333333 0.8000000\n [57] 0.9000000 0.7666667 0.9000000 0.7931034 0.7741935 0.8387097 0.8387097\n [64] 0.9032258 0.9000000 0.8000000 0.8666667 0.7000000 0.8333333 0.8620690\n [71] 0.7741935 0.8387097 0.8387097 0.8064516 0.7000000 0.9000000 0.8333333\n [78] 0.8666667 0.8666667 0.8275862 0.8387097 0.8387097 0.8709677 0.8387097\n [85] 0.7333333 0.8000000 0.8000000 0.9333333 0.7666667 0.8620690 0.6774194\n [92] 0.7741935 0.9032258 0.8709677 0.8666667 0.9333333 0.8333333 0.8666667\n [99] 0.7666667 0.8275862\n\n\n\nAnd lets get the 100 folds for the compact model\n\nhp_best_compact\n\n# A tibble: 1 × 3\n  penalty mixture .config                \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  \n1 0.00116   0.025 Preprocessor1_Model0338\n\nhp_best_compact$.config\n\n[1] \"Preprocessor1_Model0338\"\n\n\n\ncv_compact &lt;- collect_metrics(fits_compact, summarize = FALSE) |&gt; \n  filter(.config == hp_best_compact$.config) |&gt; \n  pull(.estimate)\n\ncv_full |&gt; print()\n\n  [1] 0.8387097 0.9032258 0.8387097 0.8709677 0.8000000 0.7333333 0.8666667\n  [8] 0.7333333 0.8666667 0.8275862 0.8064516 0.8064516 0.7741935 0.8709677\n [15] 0.8666667 0.8666667 0.8000000 0.8000000 0.7666667 0.8965517 0.8064516\n [22] 0.8387097 0.8709677 0.8709677 0.9000000 0.9000000 0.9000000 0.6666667\n [29] 0.7666667 0.7586207 0.9354839 0.8064516 0.8709677 0.7419355 0.7666667\n [36] 0.9333333 0.7666667 0.7666667 0.9000000 0.7586207 0.7419355 0.8387097\n [43] 0.8709677 0.9354839 0.8333333 0.9000000 0.8333333 0.7333333 0.7666667\n [50] 0.8275862 0.8387097 0.9354839 0.8064516 0.7741935 0.8333333 0.8000000\n [57] 0.9000000 0.7666667 0.9000000 0.7931034 0.7741935 0.8387097 0.8387097\n [64] 0.9032258 0.9000000 0.8000000 0.8666667 0.7000000 0.8333333 0.8620690\n [71] 0.7741935 0.8387097 0.8387097 0.8064516 0.7000000 0.9000000 0.8333333\n [78] 0.8666667 0.8666667 0.8275862 0.8387097 0.8387097 0.8709677 0.8387097\n [85] 0.7333333 0.8000000 0.8000000 0.9333333 0.7666667 0.8620690 0.6774194\n [92] 0.7741935 0.9032258 0.8709677 0.8666667 0.9333333 0.8333333 0.8666667\n [99] 0.7666667 0.8275862\n\n\nNOTE: It is important that these are the SAME splits for both model configurations\n\nNow we can compare these 100 folds across the two models using the correlated t-test\n\nDefine a function for Nadeau and Bengio (2003) correlated t-test\n\n# included in fun_ml.R\nnb_correlated_t_test &lt;- function(cv_full, cv_compact, k = 10){\n\n    diffs &lt;- cv_full - cv_compact\n    n &lt;- length(diffs)\n    mean_diff &lt;- mean(diffs)\n    var_diffs &lt;- var(diffs)\n    proportion_test &lt;- 1 / k\n    proportion_train &lt;- 1 - proportion_test\n    correction &lt;- (1 / n) + (proportion_test / proportion_train)\n    se = sqrt(correction * var_diffs)\n\n    t = abs(mean_diff/se)\n    p_value &lt;- 2 * pt(t, n - 1, lower.tail = FALSE)\n    tibble(mean_diff = mean_diff, se = se, t = t, df = n - 1, p_value = p_value)\n}\n\n\nCalculate the t-test.\n\n\nIn this instance we likely want a one-tailed test (though of course, that should have been planned in advanced and ideally pre-registered!).\n\n\nMy function returns a two-tailed p-value so we should cut it in half.\n\nnb_correlated_t_test(cv_full, \n                     cv_compact, \n                     k = 10)\n\n# A tibble: 1 × 5\n  mean_diff     se     t    df p_value\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1    0.0333 0.0271  1.23    99   0.222\n\n\nThe improvement in prediction accuracy associated with the use of our exercise test protocol is not significant (p = 0.11, one-tailed).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#bayesian-estimation-for-model-comparisons",
    "href": "011_explanation.html#bayesian-estimation-for-model-comparisons",
    "title": "11  Explanatory Approaches",
    "section": "11.7 Bayesian estimation for model comparisons",
    "text": "11.7 Bayesian estimation for model comparisons\nBenavoli et al. (2017) critique the many shortcomings wrt the frequentist approach, and I must admit, I am mostly convinced\n\nNHST does not provide the probabilities of the null and alternative hypotheses.\n\nThat is what we want\nNHST gives us the probability of our data given the null\n\nNHST focuses on a point-wise comparison (no difference) that is almost never true.\nNHST yields no information about the null hypothesis (i.e., when we fail to reject)\nThe inference depends on the sampling and testing intention (think about Bonferonni correction)\n\n\nThey suggest to use Bayesian parameter estimation as alternative to the t-test. Bayesian estimation has now been included in tidymodels in the tidyposterior package using the perf_mod() function.\n\n\nYou can (and should!) read more about this implementation of Bayesian Estimation in the associated vignette AND by reading the help materials on perf_mod()\n\nUsing this approach, we will estimate the posterior probability for values associated with specific parameters of interest. For our goals, we will care about estimates of three parameters\n\nThe accuracy of the full model\nThe accuracy of the compact model\nThe difference in accuracies between these two models.\n\n\nWe want to determine the posterior probabilities associated with ranges of values for each of these three model performance parameters estimates. We can then use these posterior probability distributions to determine that probability that the accuracy of the full model is greater than the accuracy of the compact model.\nIn addition, we can also determine if the increased accuracy of the full model is meaningful (i.e., practically important).\nTo accomplish this latter goal, we will:\n\nSpecify a Region of Practical Equivalence (a better alternative to the point-wise null in NHST)\n\nI will define classifiers whose performance are within +-1% as equivalent (not meaningfully different from each other) for our example\n\nNot worth the effort if my test doesn’t improve screening accuracy by at least this\n\n\n\nTo estimate posterior probabilities for these three parameter estimates, we need to\n\nset prior probabilities for these parameter estimates. These should be broad/uninformative in most instances unless you have substantial prior information about credible values.\n\nCollect data on these estimates. This will be the same as before - the 100 estimates of accuracy using 10x10 fold CV for both the full and compact models.\n\n\n\nUsing these priors and these data, we can derive the posterior probabilities for our three performance estimates\n\nLets do this step by step. We will use the tidyposterior package. It in not included when we load tidymodels so we will load it now\n\nlibrary(tidyposterior)\n\n\nWe need to make a dataframe of our 100 performance estimates for the full and compact models. Here is the code to do this using our previous resamples of our models\n\nMake dataframes of the accuracies from the full model and the compact model\n\n\naccuracy_full &lt;- collect_metrics(fits_full, summarize = FALSE) |&gt; \n  filter(.config == hp_best_full$.config) |&gt;   # as before - the best config\n  select(id, id2, full = .estimate) |&gt; \n  print()\n\n# A tibble: 100 × 3\n   id       id2     full\n   &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;\n 1 Repeat01 Fold01 0.839\n 2 Repeat01 Fold02 0.903\n 3 Repeat01 Fold03 0.839\n 4 Repeat01 Fold04 0.871\n 5 Repeat01 Fold05 0.8  \n 6 Repeat01 Fold06 0.733\n 7 Repeat01 Fold07 0.867\n 8 Repeat01 Fold08 0.733\n 9 Repeat01 Fold09 0.867\n10 Repeat01 Fold10 0.828\n# ℹ 90 more rows\n\n\n\naccuracy_compact &lt;- collect_metrics(fits_compact, summarize = FALSE) |&gt; \n  filter(.config == hp_best_compact$.config) |&gt;   # as before - the best config\n  select(id, id2, compact = .estimate) |&gt; \n  print()\n\n# A tibble: 100 × 3\n   id       id2    compact\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;\n 1 Repeat01 Fold01   0.774\n 2 Repeat01 Fold02   0.806\n 3 Repeat01 Fold03   0.774\n 4 Repeat01 Fold04   0.806\n 5 Repeat01 Fold05   0.867\n 6 Repeat01 Fold06   0.7  \n 7 Repeat01 Fold07   0.733\n 8 Repeat01 Fold08   0.9  \n 9 Repeat01 Fold09   0.7  \n10 Repeat01 Fold10   0.759\n# ℹ 90 more rows\n\n\n\nNow we need to join these dataframes, matching on repeat and fold ids\n\nresamples &lt;- accuracy_full |&gt; \n  full_join(accuracy_compact, by = c(\"id\", \"id2\")) |&gt; \n  print()\n\n# A tibble: 100 × 4\n   id       id2     full compact\n   &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 Repeat01 Fold01 0.839   0.774\n 2 Repeat01 Fold02 0.903   0.806\n 3 Repeat01 Fold03 0.839   0.774\n 4 Repeat01 Fold04 0.871   0.806\n 5 Repeat01 Fold05 0.8     0.867\n 6 Repeat01 Fold06 0.733   0.7  \n 7 Repeat01 Fold07 0.867   0.733\n 8 Repeat01 Fold08 0.733   0.9  \n 9 Repeat01 Fold09 0.867   0.7  \n10 Repeat01 Fold10 0.828   0.759\n# ℹ 90 more rows\n\n\n\nNow we can use perf_mod() to derive the posterior probabilites for the accuracy of each of these two models\n\nWe need to specify a model with parameters in formula. Here we indicate that we have a multi-level model with repeated observation of accuracy across folds (id2) nested within repeats (id). This handles dependence associated with repeated observations of accuracy using similar models in k-fold cv.\nWe are interested in the intercept from this model listed in formula. The intercept value will represent the accuracy estimate for each model.\nThe default for perf_mod() will be to constrain the variances of the intercept parameter estimate to be the same across models. This may be fine for some performance metrics (e.g., rmse) but for binary accuracy the variance is dependent on the mean. Therefore we allow these variances to be different using hetero_var = TRUE\nIn some instances (e..g., rmse), we may want to allow the errors in our model to be something other than Gaussian (though this is often a reasonable assumption by the central limit theorem). You can change the family for the errors if needed. See vignette and help on perf_mod(). Here, we use the default Gaussian distribution.\nThis is an iterative process using a Markov chain Monte Carlo method (Hamilton Monte Carlo) so we need to set a seed (for reproducibility), and the number of iterations and chains (beyond the scope of this course to dive into this method). I provide default values for iter and chains because you may need to increase these in some instances for the method to converge on valid values. You can often address converge and other warnings by increasing iter, chains or adapt_delta. You can read more about these warnings and issues here, here, here, and here to start.\n\n\nHere is the code\n\nset.seed(101)\npp &lt;- cache_rds(\n  expr = {\n    perf_mod(resamples, \n            formula = statistic ~ model + (1 | id2/id),\n            # defaults but may require increases\n            iter = 2000, chains = 4,  \n            # for more Gaussian distribution of accuracy\n            transform = tidyposterior::logit_trans,\n            hetero_var = TRUE, # for accuracy\n            family = gaussian, # default but could change depending on DV\n            # increase adapt_delta (e.g., .99, .999) to \n            # fix divergent transitions\n            adapt_delta = .99)  \n  },\nrerun = rerun_setting,\ndir = \"cache/011/\",\nfile = \"pp\")\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000766 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 7.66 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 3.385 seconds (Warm-up)\nChain 1:                2.268 seconds (Sampling)\nChain 1:                5.653 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.46 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 4.391 seconds (Warm-up)\nChain 2:                2.2 seconds (Sampling)\nChain 2:                6.591 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4.3e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 3.781 seconds (Warm-up)\nChain 3:                2.245 seconds (Sampling)\nChain 3:                6.026 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3.8e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 3.611 seconds (Warm-up)\nChain 4:                3.844 seconds (Sampling)\nChain 4:                7.455 seconds (Total)\nChain 4: \n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\n\nIn contrast to the NHST approach, we now have what we really want - posterior probabilities. Lets look at them\nWe can view the posterior probability distributions using an autoplot method for perf_mod objects.\n\nThese density plots tell how probable various values are for the accuracy of each model\nThe probabilities associated with any region of the curve is equal to the area under that curve for that region. This will tell you the probability associated with that range of values for accuracy.\nYou can easily see in this instance that the probable values for accuracy are higher generally for full model than the compact model\n\n\npp |&gt; autoplot()\n\n\n\n\n\n\n\n\n\nYou will likely want to publish a figure showing these posterior probability distributions so you may want to fine tune the plots. Here are some code options using ggplot\nHere is the same density plots using ggplot so you can now edit to adjust as you like\n\npp |&gt; \n  tidy(seed = 123) |&gt; \n  mutate(model = fct_inorder(model)) |&gt;\n  ggplot() + \n  geom_density(aes(x = posterior, color = model) )\n\n\n\n\n\n\n\n\n\nWe are actually sampling from the posterior distribution so it might make more sense to display these as histograms rather than density plots\n\npp |&gt; \n  tidy(seed = 123) |&gt; \n  mutate(model = fct_inorder(model)) |&gt;\n  ggplot() + \n  geom_histogram(aes(x = posterior, fill = model), color = \"white\", alpha = 0.4,\n                 bins = 50, position = \"identity\") \n\n\n\n\n\n\n\n\n\nOr maybe you want to facet the histograms if the overlap is difficulty to view\n\npp |&gt; \n  tidy(seed = 123) |&gt; \n  mutate(model = fct_inorder(model)) |&gt;\n  ggplot(aes(x = posterior)) + \n  geom_histogram(color = \"white\", fill = \"blue\", bins = 30) + \n  facet_wrap(~ model, ncol = 1)\n\n\n\n\n\n\n\n\n\nWe can also calculate the 95% Higher Density Intervals (aka, 95% Credible Intervals; the Bayesian alternative to the 95% Confidence Intervals) for the accuracy of each model. This is the range of parameter estimate values that include 95% of the credible values. Kruschke described this in the assigned reading.\n\npp |&gt; tidy(seed = 123) |&gt; summary()\n\n# A tibble: 2 × 4\n  model    mean lower upper\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 compact 0.805 0.790 0.820\n2 full    0.838 0.825 0.850\n\n\n\nBut what we really want is derive the posterior probability for the difference in accuracy between the two models. This will let us determine credible values for the magnitude of the difference and determine if this difference is meaningful.\nWe said early that we would define a ROPE of +-.01 around zero. The models are only meaningful different if their accuracies differ by at least 1%\nLets visualize the posterior probability distribution for the difference along with this ROPE using the built in autoplot function\n\npp |&gt; contrast_models(seed = 4) |&gt; autoplot(size = .01)\n\n\n\n\n\n\n\n\n\nWe could make more pretty plots directly in ggplot\n\npp |&gt; \n  contrast_models(seed = 4) |&gt; \n  ggplot() +\n  geom_density(aes(x = difference), color = \"blue\")+\n  geom_vline(aes(xintercept = -.01), linetype = \"dashed\") + \n  geom_vline(aes(xintercept = .01), linetype = \"dashed\")\n\n\n\n\n\n\n\n\nor my preferred histogram\n\npp |&gt; \n  contrast_models(seed = 4) |&gt; \n  ggplot(aes(x = difference)) + \n  geom_histogram(bins = 50, color = \"white\", fill = \"blue\")+\n  geom_vline(aes(xintercept = -.01), linetype = \"dashed\") + \n  geom_vline(aes(xintercept = .01), linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\nBut perhaps most important, lets calculate the probability that the full model is more accurate than the compact model\n\nThe mean increase in accuracy is in the meancolumn\nThe 95% HDI is given by lower and upper\nThe probability that the full model is meaningfully higher than the compact model (i.e., what proportion of the credible values are above the ROPE) is in the prac_pos column.\n\n\npp |&gt; contrast_models(seed = 4) |&gt; summary(size = .01)\n\n# A tibble: 1 × 9\n  contrast        probability   mean  lower  upper  size pract_neg pract_equiv\n  &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 full vs compact           1 0.0324 0.0189 0.0458  0.01         0     0.00225\n  pract_pos\n      &lt;dbl&gt;\n1     0.998\n\n\nAlternatively, using the approach proposed by Kruschke (2018), you can conclude that the full model is meaningfully better than the compact model if the 95% HDI is fully above the ROPE. This is also true!\n\nFinally, in some instances, you may not want to use the ROPE.\n\nInstead, you may simply want the posterior probability that the full model performs better than the compact model.\n\nThis is probability is provided in the probability column of the table.\nYou can also set the size of the ROPE to 0 (though not necessary)\n\n\npp |&gt; contrast_models(seed = 4) |&gt; summary(size = 0)\n\n# A tibble: 1 × 9\n  contrast        probability   mean  lower  upper  size pract_neg pract_equiv\n  &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 full vs compact           1 0.0324 0.0189 0.0458     0        NA          NA\n  pract_pos\n      &lt;dbl&gt;\n1        NA",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#feature-importance",
    "href": "011_explanation.html#feature-importance",
    "title": "11  Explanatory Approaches",
    "section": "11.8 Feature Importance",
    "text": "11.8 Feature Importance\nThere as been increasing focus on improving the interpretability of machine learning models that we are using.\n\n\nThere are numerous reasons to want to better understand why our models make the predictions that they do.\n\nThe growing set of tools to interpret our models can help address our explanatory questions\nBut they can also help us find errors in our models\nAnd they can detect possible bias (we will focus explicitly on algorithmic bias in later units)\n\n\nFeature importance metrics are an important tool to better understand how our models work.\n\nThese metrics help us understand which features in our models contribute most to the predictions that the model makes.\n\n\nFor some models, interpretation and identification of important features is easy.\n\n\nFor example, if we standardize the features in a glm or glmnet model, we can interpret the absolute magnitude of the parameter estimates (i.e., the coefficients) as an index of the global (i.e., across all observations) importance of each feature.\n\nYou can use the vip package to extract these model-specific feature importance metrics, but you can often just get them directly from the model as well\nMore info on the use of vip package is available elsewhere\n\n\nBut for other models, we need different approaches.\n\n\nThere are many model-agnostic (i.e., can be used across all statistical algorithms) approaches to quantify the importance of a feature, but we will focus on two:\n\nPermutation Feature Importance\nShapley Values\n\n\nWe follow recommendations from the tidymodels folks and use the DALEX and DALEXtra packages for model agnostic approaches to feature importance.\n\nlibrary(DALEX, exclude= \"explain\")\n\nWelcome to DALEX (version: 2.4.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\n\nlibrary(DALEXtra)\n\n\nLets first get some coding issues accomplished before we dig into the details of the two feature importance metrics\n\nTo calculate these importance metrics, we will need access to the raw features and outcome.\n\nrec_full_prep &lt;- rec_full |&gt; \n  prep(data_all)\n\nfeat_full &lt;-  rec_full_prep |&gt; \n  bake(data_all)\n\n\nAnd we now will need to fit the full model trained on all the data\n\nfit_full &lt;- \n  logistic_reg(penalty = hp_best_full$penalty,\n               mixture = hp_best_full$mixture) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  fit(disease ~ ., data = feat_full)\n\n\nWe will need to have a df for the features (without the outcome) and a separate vector for the outcome\n\nfeatures are easy. Just select out the outcome\n\n\nx &lt;- feat_full |&gt; select(-disease)\n\n\nFor outcome, we need to convert to 0/1 (if classification), and then pull the vector out of the dataframe\n\n\ny &lt;- feat_full |&gt; \n  mutate(disease = if_else(disease == \"yes\", 1, 0)) |&gt; \n  pull(disease)\n\n\nWe also need a specific predictor function that will work with the DALEX package\nWe will write a custom function that “wraps” around our tidymodels predict() function\nDALEX needs:\n\nthe prediction function to have two parameters named model and newdata\nthe prediction function must return a vector of probabilites for the positive class for classification problems (for regression, it simply returns a vector of the predicted values for \\(y\\))\n\n\npredict_wrapper &lt;- function(model, newdata) {\n  predict(model, newdata, type = \"prob\") |&gt; \n    pull(.pred_yes)\n}\n\n\nWe will also need an explainer object based on our model and data\n\nThe explain_tidymodels() function in DALEXtra will create (and check) this object for us.\n\nexplain_full &lt;- explain_tidymodels(fit_full, # our model object \n                                   data = x, # df with features without outcome\n                                   y = y, # outcome vector\n                                   # our custom predictor function\n                                   predict_function = predict_wrapper)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  model_fit  (  default  )\n  -&gt; data              :  303  rows  17  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  303  values \n  -&gt; predict function  :  predict_function \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package parsnip , ver. 1.1.0.9004 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.2574049 , mean =  0.458745 , max =  0.7385297  \n  -&gt; residual function :  residual_function \n  -&gt; residuals         :  numerical, min =  0 , mean =  0 , max =  0  \n  A new explainer has been created!  \n\n\n\nFinally, we need to define a custom function for our performance metric as well\n\nIt needs to have two parameters: observed and predicted\nWe can create a wrapper function around accuracy_vec() to fit these needs\nFor accuracy, we need to transform the predicted probabilites from our prediction function to class predictions (e.g.. yes/no)\nAnd because we converted our labels to 0/1 in the outcome vector, we need to transform observed back to yes/no as well\n\n\naccuracy_wrapper &lt;- function(observed, predicted) {\n  observed &lt;- fct(if_else(observed == 1, \"yes\", \"no\"),\n                  levels = c(\"yes\", \"no\"))\n  predicted &lt;- fct(if_else(predicted &gt; .5, \"yes\", \"no\"), levels  = c(\"yes\", \"no\"))\n  accuracy_vec(observed, predicted)\n}\n\n\n\nWe are now ready to calculate feature importance metrics\n\n\n11.8.1 Permutation Feature Importance\nThe first model agnostic approach to calculating feature important is called Permutation Feature Importance\n\nThis approach is very straight forward. This approach says - if we want to calculate the importance of any specific feature, we can compare our performance metric using the original features to the performance metric we get if we permute (i.e., shuffle) the values for the feature we are evaluating.\n\n\nBy randomly shuffling the values for the feature, we break the relationship between that feature and the outcome so it no longer contributes to the predictions. If performance doesn’t change much, then that feature is not important. If performance goes down a lot, the feature is important.\n\nThe function can provide raw performance (will give us performance for the non-permuted model and then performance for the model with each feature permuted, one at a time)\ndifference performance measure, which is the difference between the permuted model and the non-permuted mode, separately for each feature\nratio performance measure, which is (\\(\\frac{permuted}{original}\\)), separately for each feature\n\n\nTo calculate accuracy after permuting each feature, we use model_parts(). We pass in\n\nour explainer object\nset the type (raw in this example)\nindicate our custom accuracy function\nset B to indicate number of permutations to perform\n\n\nset.seed(123456)\nimp_permute &lt;- model_parts(explain_full, \n                               type = \"raw\", \n                               loss_function = accuracy_wrapper,\n                               B = 100)\n\n\nLets look at what this function returns\n\nthe first row contains the accuracy for the full model (with no features permuted)\nlast row is a baseline models (performance with all features permuted)\nOther row show the accuracy of the model when that specific feature is permuted\n\n\nimp_permute\n\n                       variable mean_dropout_loss     label\n1                  _full_model_         0.8382838 model_fit\n2                            ca         0.7944224 model_fit\n3         thal_reversabledefect         0.8002640 model_fit\n4                  exer_ang_yes         0.8116172 model_fit\n5                   exer_max_hr         0.8161386 model_fit\n6               exer_st_depress         0.8189769 model_fit\n7            exer_st_slope_flat         0.8237294 model_fit\n8                cp_non_anginal         0.8249505 model_fit\n9                      sex_male         0.8304620 model_fit\n10                  cp_atyp_ang         0.8338284 model_fit\n11                      rest_bp         0.8351155 model_fit\n12                          age         0.8355116 model_fit\n13 rest_ecg_ventric_hypertrophy         0.8378878 model_fit\n14                         chol         0.8382838 model_fit\n15                 fbs_elevated         0.8382838 model_fit\n16            rest_ecg_wave_abn         0.8382838 model_fit\n17      exer_st_slope_downslope         0.8382838 model_fit\n18             thal_fixeddefect         0.8382838 model_fit\n19                   _baseline_         0.5082508 model_fit\n\n\n\nWe can use the built in plot function from DALEX to display this\n\nplot(imp_permute)\n\n\n\n\n\n\n\n\n\nOr we can plot it directly. Here is an example from the tidymodels folks\n\nfull_model &lt;- imp_permute |&gt;  \n    filter(variable == \"_full_model_\")\n  \nimp_permute |&gt; \n  filter(variable != \"_full_model_\",\n         variable != \"_baseline_\") |&gt; \n  mutate(variable = fct_reorder(variable, dropout_loss)) |&gt; \n  ggplot(aes(dropout_loss, variable)) +\n  geom_vline(data = full_model, aes(xintercept = dropout_loss),\n             linewidth = 1.4, lty = 2, alpha = 0.7) +\n  geom_boxplot(fill = \"#91CBD765\", alpha = 0.4) +\n  theme(legend.position = \"none\") +\n  labs(x = \"accuracy\", \n       y = NULL,  fill = NULL,  color = NULL)\n\n\n\n\n\n\n\n\n\nWe can also permute a set of features to quantify the contribution of the full set\n\nThis is what we would want for our example, were we want to know the contribution of the four features that represent our exercise test.\n\nTo do this, we pass in a list of vectors of the groups. Here we provide just one group that we name exer_test\n\nset.seed(123456)\nimp_permute_group &lt;- model_parts(explain_full, \n                               type = \"raw\", \n                               loss_function = accuracy_wrapper,\n                               B = 100,\n                               variable_groups = list(exer_test = \n                                                        c(\"exer_ang_yes\",\n                                                          \"exer_max_hr\",\n                                                          \"exer_st_depress\", \n                                                          \"exer_st_slope_downslope\")))\n\n\nThe results show that permuting these four features as a set drops accuracy from 0.838 to 0.738\n\nimp_permute_group\n\n      variable mean_dropout_loss     label\n1 _full_model_         0.8382838 model_fit\n2    exer_test         0.7128713 model_fit\n3   _baseline_         0.5145215 model_fit\n\n\n\n\n\n11.8.2 Shapley Values\nShapley values provide insight on the importance of any feature to the prediction for a single observation - often called local importance (vs. global importance as per the permutation feature importance measure above).\n\nShapley values can also be used to index global importance by averaging the local shapley values for a feature across all (or a random sample) of the observations.\n\nShapley values are derived from Coalition Game Theory.\n\n\nThey provide the average marginal contribution to prediction (for a single observation) of a feature value across all possible coalitions of features (combinations of sets of features from the null set to all other features).\n\nMolnar (2023) provides a detailed account of the theory behind these values and how they are calculated which I will not reproduce here.\n\nLets calculate Shapley Values for the first observation in our dataset\nTheir features values were\n\nobs_num &lt;- 1\nx1 &lt;- x |&gt; \n  slice(obs_num) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 17\n$ age                          &lt;dbl&gt; 0.9471596\n$ rest_bp                      &lt;dbl&gt; 0.756274\n$ chol                         &lt;dbl&gt; -0.2644628\n$ exer_max_hr                  &lt;dbl&gt; 0.01716893\n$ exer_st_depress              &lt;dbl&gt; 1.085542\n$ ca                           &lt;dbl&gt; -0.7099569\n$ sex_male                     &lt;dbl&gt; 0.6850692\n$ cp_atyp_ang                  &lt;dbl&gt; -0.44382\n$ cp_non_anginal               &lt;dbl&gt; -1.772085\n$ fbs_elevated                 &lt;dbl&gt; 2.390484\n$ rest_ecg_wave_abn            &lt;dbl&gt; -0.115472\n$ rest_ecg_ventric_hypertrophy &lt;dbl&gt; 1.021685\n$ exer_ang_yes                 &lt;dbl&gt; -0.69548\n$ exer_st_slope_flat           &lt;dbl&gt; -0.9252357\n$ exer_st_slope_downslope      &lt;dbl&gt; 3.658449\n$ thal_fixeddefect             &lt;dbl&gt; 3.972541\n$ thal_reversabledefect        &lt;dbl&gt; -0.7918057\n\n\n\nAnd we can get Shapley values using predict_parts()\n\nsv &lt;- predict_parts(explain_full, \n                    new_observation = x1,\n                    type = \"shap\",\n                    B = 25)\n\n\nThere is a built in plot function for shap values\n\nFor this first observation\n\nThe values for each feature are listed in the left margin\nBars to the right (e.g., sex_male) indicate that their feature value increases their probability of disease\nBars to the left indicate that their feature value decreases their probability of disease\n\n\nplot(sv)\n\n\n\n\n\n\n\n\n\nWe can use these Shapley values for the local importance of the features for each observation to calculate the global importance of these features.\n\n\nFeatures that have big absolute Shapley values on average across observation are more important. Let’s calculate this.\n\nFirst we need a function to get shapley values for each observation (along with the feature values for a nicer plot)\n\nget_shaps &lt;- function(df1){\n  predict_parts(explain_full, \n                new_observation = df1,\n                type = \"shap\",\n                B = 25) |&gt; \n    filter(B == 0) |&gt; \n    select(variable_name, variable_value, contribution) |&gt; \n    as_tibble()\n}\n\n\nAnd then we can map this function over observations to get the Shapley values for each observation\n\nlocal_shaps &lt;- cache_rds(\n  expr = {\n    x |&gt;\n      slice_sample(prop = 1/5) |&gt; # take random sample to reduce computation time\n      mutate(id = row_number()) |&gt;\n      nest(.by = id, .key = \"dfs\") |&gt;   # nest a dataframe for each observation\n      mutate(shaps = map(dfs, \\(df1) get_shaps(df1))) |&gt; \n      select(-dfs) |&gt;\n      unnest(shaps)\n  },\n  rerun = rerun_setting,\n  dir = \"cache/011/\",\n  file = \"local_shaps\")\n\n\nHere is what we get from applying the function over observations\n\nlocal_shaps |&gt; head()\n\n# A tibble: 6 × 4\n     id variable_name  variable_value contribution\n  &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;                 &lt;dbl&gt;\n1     1 age            1.058               0.00626\n2     1 ca             -0.71              -0.0229 \n3     1 chol           1.281               0      \n4     1 cp_atyp_ang    -0.4438             0.00308\n5     1 cp_non_anginal 0.5624              0.00598\n6     1 exer_ang_yes   -0.6955            -0.0190 \n\n\nProgramming note: This code demonstrates another nice R programming technique using nest() and unnest() in combination with map() and list-columns. For more info, see this chapter in Wickham, Çetinkaya-Rundel, and Grolemund (2023) and the vignette on nesting (vignette(\"nest\")).\n\nNow that we have Shapley values for all observations, we can calculate the mean absolute Shapley value across observations and plot it.\n\nAcross all observations, ca contributes to an average change of .06 from the mean predicted probability of disease.\nOne of the features from our exercise test, exer_ang_yes, contributes about .05 change from mean predicted probability of disease.\nThe other exer_ features are not far behind.\n\n\nlocal_shaps |&gt;\n  mutate(contribution = abs(contribution)) |&gt;\n  group_by(variable_name) |&gt;\n  summarize(mean_shap = mean(contribution)) |&gt;\n  arrange(desc(mean_shap)) |&gt;\n  mutate(variable_name = factor(variable_name),\n         variable_name = fct_reorder(variable_name, mean_shap)) |&gt;\n  ggplot(aes(x = variable_name, y = mean_shap)) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nFor a more advanced plot (a sina plot; not displayed here) we could superimpose the individual local Shapley values and color them based on the feature score.\n\n\nThis would allow us to show the direction of the relationship between the Shapley values and feature values.\n\n\nSee FIGURE 9.26 in Molnar (2023) for an example of this type of plot.\n\nShapley values are attractive relative to other approaches because\n\nThey have a solid theoretical basis\nSound statistical properties (Efficiency, Symmetry, Dummy and Additivity - see Molnar (2023))\nCan provided a unified perspective across both local and global importance.\n\n\nHowever, they can be VERY time consuming to calculate (particularly if you want to use them for global importance such that you need them for all/many observations).\nThere are computational shortcuts available but even those can be very time consuming in some instances (though XGBoost has a very fast implementation that we use regularly).\n(Note that for decision tree based algorithms SHAP provides a more computationally efficient way to estimate Shapley values - see section 9.6 in Molnar (2023) for more detail.)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#visual-approaches-to-understand-our-models",
    "href": "011_explanation.html#visual-approaches-to-understand-our-models",
    "title": "11  Explanatory Approaches",
    "section": "11.9 Visual Approaches to Understand our Models",
    "text": "11.9 Visual Approaches to Understand our Models\nWe can also learn about how our features are used to make predictions in our models using visual approaches.\n\n\nThere are two key plots that we can use:\n\nPartial Dependence (PD) Plots\nAccumulated Local Effects (ALE) Plots\n\n\n\n11.9.1 Partial Dependence (PD) Plots\nThe Partial dependence (PD) plot displays the marginal effect of a target feature or combination of features on the predictions from a model.\n\n\nIn essence, the prediction for any value of a target feature is the average prediction across cases if we set all cases to have that value for the target feature but their observed values for all other features.\n\nWe can use PD plots to understand whether the relationship between a target feature and the outcome is linear, monotonic, or more complex. It may also help us visualize and understand if interactions between features exist (if we make a PD plot for two target features).\n\nThe PD Plot is attractive because\n\nit is easy to understand (prediction for each feature value averaged across observed values for all other features)\nif the target feature is uncorrelated with all other features, its interpretation is clear, it is how the average prediction changes as the target features changes values.\nit is computationally easy to implement\nit has a causal (for the model, not the real world!) interpretation. This is what happens to the predciction if we manipulate the values of the target feature but hold all other features constant at their observed values.\n\n\nHowever:\n\nThe assumption that the target feature is not correlated with the other features is likely unrealistic in many/most instances\nThis plot (but also other plot methods) are limited to 1 - 2 features in combination.\nIt may hide effects when interactions exist\n\n\n\n\n11.9.2 Accumulated Local Effects (ALE) Plots\nIf the features are correlated, the partial dependence plot should not be used because the plots will otherwise be based on combinations of the target feature and other features that may never occur (given the feature correlations).\n\nMolnar describes how this problem of correlated features and unrealistic combinations of features can be solved by M-Plots that plot the average effect of a target feature using the conditional values on other features (i.e., only using realistic values for the other features based on their correlations with the target feature). Unfortunately, this too is sub-optimal because it will confound the effect of the target feature with the effects of the other features that are correlated with it.\n\nAccumulated Local Effects (ALE) plots also use conditional values of other features to solve the correlated features problem. However, ALE plots solve the confounding problem by calculating differences in predictions associated with changes in the target feature rather than average predictions for each value of that target feature. These differences hold the other features values (mostly) constant to remove their effects.\n\n\nALE plots are the preferred plot in situations where you expect your target feature to be correlated with other features (which is likely most situations.)\n\nWe will use the DALEX package again to make these PD and ALE plots.\n\nIt will require the explainer object we created earlier for feature importance\n\nOtherwise, the code is very straight-forward. Here we get the predicted values for an ALE plot to examine the effect of one of the features from our exercise test (exer_max_hr) on disease probabilities.\nIf we wanted a PD plot, we could simply substitute partial for accumulated\n\nale &lt;- model_profile(explainer = explain_full,\n                     type = \"accumulated\",\n                     variables = \"exer_max_hr\",\n                     N = NULL)  # to use full sample (default is 100)\n\n\nThere is a default plot function for these plot object (or you could use the data in the object to make your own ggplot)\n\nThe probability of disease decreases as max hr increases in the exercise test\n\nale |&gt; plot()",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#summary-and-closing-thoughts",
    "href": "011_explanation.html#summary-and-closing-thoughts",
    "title": "11  Explanatory Approaches",
    "section": "11.10 Summary and Closing Thoughts",
    "text": "11.10 Summary and Closing Thoughts\nWhen pursuing purely explanatory goals with machine learning methods, we can:\n\nUse resampling with the full dataset to determine appropriate model configuration\n\nBest statistical algorithm\nWhich covariates\nOther “researcher degrees of freedom” such as handling of outliers, transformations of predictors\n\nUse model comparisons (Frequentist or Bayesian) in combination with feature ablation to test effect of feature or set of features\nWe can use feature importance measures (permutation or Shapley) to understand the contributions that various features make to prediction for an observation (local) or across observations (global)\nWe can explore the shape (and magnitude?) of relationships using PD and ALE plots",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "011_explanation.html#discussion",
    "href": "011_explanation.html#discussion",
    "title": "11  Explanatory Approaches",
    "section": "11.11 Discussion",
    "text": "11.11 Discussion\n\n11.11.1 Neural Net Winners\n\n\n11.11.2 Permutation Test (quiz question 5 was updated so everyone got a point!)\nReal quick though what is permutation?\n\nIt involves shuffling (permuting) the data many times.\nAfter each permutation you refit the model and get a test-statistic (e.g., accuracy).\nEssentially any relationship between features and the outcome are gone. So the permuted test-statistic is what we should expect to get if our features calculated no signal.\nThen you would calculate the proportion of permutations where the test statistic is greater than or equal to your model performance estimate with the un-imputed data.\nIf this is below .05 your interpretation would be to reject NULL (i.e., your model is performing better than a model with no signal)\n\nExample from ChatGPT of when a permutation test can come in hand:\nImagine you have two groups of people, and you want to see if there’s a difference in their average heights. You measure everyone, and you find that, on average, Group A is taller than Group B. But you wonder: Is this difference just due to random chance, or is there something real going on?\nIt also doesn’t have any assumption requirements for your data\nBut…It is computationally expensive and there are often other more efficient methods for testing your model’s performance.\n\n\n11.11.3 Ice Breaker\nGet into pairs or small groups. Think about one of the data sets we have worked with in class or your own! Create a research question you might ask using each of the following explanatory methods.\n\nPermutation\nBayesian model comparison\nShapley values\n\n\n\n11.11.4 Question 5 from quiz\nLet’s say you want to build a model to predict which individuals will develop hypertension based on health data within patients’ medical records (e.g. family history, medical chart notes, genetic testing results). Which of the following claims about your model could be investigated using a permutation test?\n\nAddition of genetic data to our model does not significantly improve upon the accuracy of models using only family history and medical chart notes\nFamily history variables are more important for prediction of future hypertension than medical chart notes\nThe predictors from patients’ medical records are related to their hypertension and your model is capturing this relationship to some degree\nOur model performs significantly better than leading diagnostic screening questionnaires for prediction of future hypertension\n\nWhat method would we use to investigate the other claims?\n\n\n11.11.5 Question 3 from quiz\nImagine you want to determine if an elastic net logistic regression model (that you tuned on penalty and mixture) is performing better than would be expected by chance. Which situations would require you to include the model selection process inside of your permutation test? Select all that apply.\n\nModel selection was performed via k-fold cross validation, model evaluation was performed in an independent test set b. Model selection and evaluation was performed via bootstrapped resampling\nModel selection and evaluation was performed via nested resampling\nYou always need to include model selection processes inside of permutation tests\n\nWhy not the other methods?\n\n\n11.11.6 Bayesian Model Comparisons\nConceptually - could we go over posterior probabilities again - I was a bit confused on what this is and how that differ from other metrics that we use.\nPosterior probabilities are a way for us to estimate the likelihood of a parameter being a certain value. For example, the probability that are model is performing within a certain range (e.g., how probable various values are for the accuracy of each model).\nWhereas before we used resampling to obtain performance estimates of our model and stopped there. Now we can use these estimates as data to pass into perf_mod() an get a distribution of probabilities that the true performance estimate falls within a certain range.\nWhat do HDI values represent?\nThe range of parameter estimate values that include 95% of the credible values.\nLets look at an example figure.\nIs the probability that full model is better than compact model deemed as ‘p-value’ in Bayesian way? What’s the difference between using a ROPE to inspect on the result and using a p-value?\nhttps://jjcurtin.github.io/book_iaml/011_explanation.html#bayesian-estimation-for-model-comparisons\n\n\n11.11.7 Shapley Values\nI feel like I dont have a good grasp on Shapely values.What do they really mean and how are they interpreted\nShapley values give us a measure of relative feature importance (i.e., which features are contributing most to a model’s predictions compared to other features). For classification models, Shapley values represent the impact of each feature on the probability of the predicted class.\nThey are calculated by considering all the possible ways features can be ordered in a model. Then, for each possible ordering, the contribution of each feature is measured when an additional feature is added. Finally, the contribution of each feature is averaged across all possible orders. This is why it took so long to run in your homework assignment!\nSo, how do we know which features are most important? How might we report our findings about top features?\nWe look at the relative ordering of features and the ones with the highest Shapley value are the most important for our model’s predictions. We can use the Shapley value to indicate how much each feature contributes to the change in probability of our positive class being predicted.\nNote Shapley values are not inferential tests, they are descriptive!\nCan you talk more about Shapley values (SHAP), as well as local importance and global importance?\nAnyone want to volunteer to explain the difference?\nWhat is a real-life example of some data where we might care about feature importance?\nLets look at an example from John’s lab!\nAdditivity principle of Shapley values\nThe difference between permutation feature importance and shapley values.\nPermutation feature importance focuses on how shuffling a feature’s values affects the model’s overall performance (i.e., removing any association between the feature and outcome), while Shapley values focus on the marginal contribution of each feature to individual predictions.\nWhen using L1 and L2 penalties, does it use feature importance to decide which features to drop out/down?\nLasso and Ridge penalties shrink the coefficients of less important features, but does so by adding a penalty to the loss function. This is different than calculating feature importance with permutation or Shapley methods. Remember penalties are for reducing overfitting of a model, and indices of feature importance are for explaining the output of a model.\n\n\n\n\nBenavoli, Alessio, Giorgio Coraniy, Janez Demsar, and Marco Zaffalon. 2017. “Time for a Change: A Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis.” Journal of Machine Learning Research 18: 1–36.\n\n\nBouckaert, Remco R. 2003. “Choosing Between Two Learning Algorithms Based on Calibrated Tests.” In Proceedings of the Twentieth International Conference on International Conference on Machine Learning, 51–58. ICML’03. Washington, DC, USA: AAAI Press.\n\n\nKruschke, John K. 2018. “Rejecting or Accepting Parameter Values in Bayesian Estimation.” Advances in Methods and Practices in Psychological Science 1: 270–80.\n\n\nMolnar, Christoph. 2023. Intepretable Machine Learning: A Guide for Makiong Black Box MOdels Explainable. 2nd ed. https://christophm.github.io/interpretable-ml-book/.\n\n\nNadeau, Claude, and Yoshua Bengio. 2003. “Inference for the Generalization Error.” Machine Learning 52 (3): 239–81. https://doi.org/10.1023/A:1024068626366.\n\n\nWickham, Hadley, Çetinkaya-Rundel Mine, and Garrett Grolemund. 2023. R for Data Science: Visualize, Model, Transform, and Import Data. 2nd ed. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Explanatory Approaches</span>"
    ]
  },
  {
    "objectID": "012_nlp.html",
    "href": "012_nlp.html",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "",
    "text": "12.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#overview-of-unit",
    "href": "012_nlp.html#overview-of-unit",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "",
    "text": "12.1.1 Learning Objectives\n\nObjective 1\nObjective 2\n\n\n\n\n12.1.2 Readings\n\nHvitfeldt and Silge (2022) Chapter 2: Tokenization\nHvitfeldt and Silge (2022) Chapter 5: Word Embeddings\n\nNOTES: Please read the above chapters more with an eye toward concepts and issues rather than code. I will demonstrate a minimum set of functions to accomplish the NLP modeling tasks for this unit.\nAlso know that the entire Hvitfeldt and Silge (2022, book) is really mandatory reading. I would also strongly recommend this entire Silge and Robinson (2017) book. Both will be important references at a minimum.\nPost questions to the readings channel in Slack\n\n\n\n12.1.3 Lecture Videos\n\nLecture 1: General Text (Pre-) Processing - the stringr package ~ 9 mins\nLecture 2: General Text (Pre-) Processing - regular expressions ~ 13 mins\nLecture 3: The IMDB Reviews Dataset ~ 6 mins\nLecture 4: Tokenization- Part 1 ~ 27 mins\nLecture 5: Tokenization- Part 2 ~ 13 mins\nLecture 6: Stopwords ~ 12 mins\nLecture 7: Stemming ~12 mins\nLecture 8: Bag of Words ~19 mins\nLecture 9: NLP in Action - Part 1 ~ 17 mins\nLecture 10: NLP in Action - Part 2 ~ 20 mins\n\nPost questions to the video-lectures channel in Slack\n\n\n\n12.1.4 Application Assignment\n\ndata\nqmd shell\nGloVE embeddings for twitter\nsolution\n\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, April 17th",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#general-text-pre--processing",
    "href": "012_nlp.html#general-text-pre--processing",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "12.2 General Text (Pre-) Processing",
    "text": "12.2 General Text (Pre-) Processing\nIn order to work with text, we need to be able to manipulate text. We have two sets of tools to master:\n\nThe stringr package\nRegular expressions (regex)\n\n\n12.2.1 The stringr package\nThere are many functions in the stringr package that are very useful for searching and manipulating text.\n\nstringr is included in tidyverse\nI recommend keeping the stringr cheatsheet open whenever you are working with text until you learn these functions well.\n\n\nAll functions in stringr start with str_ and take a vector of strings as the first argument.\n\nHere is a simple vector of strings to use as an example\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nx &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\n\n\nLength of each string\n\n\nstr_length(x)\n\n[1] 3 5 5 5 4 9\n\n\n\nCollapse all strings in vector into one long string that is comma separated\n\n\nstr_c(x, collapse = \", \")\n\n[1] \"why, video, cross, extra, deal, authority\"\n\n\n\nGet substring based on position (start and end position)\n\n\nstr_sub(x, 1, 2)\n\n[1] \"wh\" \"vi\" \"cr\" \"ex\" \"de\" \"au\"\n\n\n\nMost stringr functions work with regular expressions, a concise language for describing patterns of text.\n\nFor example, the regular expression “[aeiou]” matches any single character that is a vowel.\n\nHere we use str_subset() to return the strings that contain vowels (doesnt include “why”)\n\n\nstr_subset(x, \"[aeiou]\")\n\n[1] \"video\"     \"cross\"     \"extra\"     \"deal\"      \"authority\"\n\n\n\nHere we count the vowels in each string\n\n\nstr_count(x, \"[aeiou]\") \n\n[1] 0 3 1 2 2 4\n\n\n\nThere are eight main verbs that work with patterns:\n1 str_detect(x, pattern) tells you if there is any match to the pattern in each string\n\n# x &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_detect(x, \"[aeiou]\")\n\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\n\n2 str_count(x, pattern) counts the number of patterns\n\n# x &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_count(x, \"[aeiou]\")\n\n[1] 0 3 1 2 2 4\n\n\n\n3 str_subset(x, pattern) extracts the matching components\n\n# x &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_subset(x, \"[aeiou]\")\n\n[1] \"video\"     \"cross\"     \"extra\"     \"deal\"      \"authority\"\n\n\n\n4 str_locate(x, pattern) gives the position of the match\n\n# x &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_locate(x, \"[aeiou]\")\n\n     start end\n[1,]    NA  NA\n[2,]     2   2\n[3,]     3   3\n[4,]     1   1\n[5,]     2   2\n[6,]     1   1\n\n\n\n5 str_extract(x, pattern) extracts the text of the match\n\n# x &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_extract(x, \"[aeiou]\")\n\n[1] NA  \"i\" \"o\" \"e\" \"e\" \"a\"\n\n\n\n6 str_match(x, pattern) extracts parts of the match defined by parentheses. In this case, the characters on either side of the vowel\n\n# x &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_match(x, \"(.)[aeiou](.)\")\n\n     [,1]  [,2] [,3]\n[1,] NA    NA   NA  \n[2,] \"vid\" \"v\"  \"d\" \n[3,] \"ros\" \"r\"  \"s\" \n[4,] NA    NA   NA  \n[5,] \"dea\" \"d\"  \"a\" \n[6,] \"aut\" \"a\"  \"t\" \n\n\n\n7 str_replace(x, pattern, replacement) replaces the matches with new text\n\n# x &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_replace(x, \"[aeiou]\", \"?\")\n\n[1] \"why\"       \"v?deo\"     \"cr?ss\"     \"?xtra\"     \"d?al\"      \"?uthority\"\n\n\n\n8 str_split(x, pattern) splits up a string into multiple pieces.\n\nstr_split(c(\"a,b\", \"c,d,e\"), \",\")\n\n[[1]]\n[1] \"a\" \"b\"\n\n[[2]]\n[1] \"c\" \"d\" \"e\"\n\n\n\n\n\n12.2.2 Regular Expressions\nRegular expressions are a way to specify or search for patterns of strings using a sequence of characters. By combining a selection of simple patterns, we can capture quite complicated strings.\n\nThe stringr package uses regular expressions extensively\n\nThe regular expressions are passed as the pattern = argument. Regular expressions can be used to detect, locate, or extract parts of a string.\n\nJulia Silge has put together a wonderful tutorial/primer on the use of regular expressions. After reading it, I finally had a solid grasp on them. Rather than grab sections, I will direct you to it (and review it live in our filmed lectures). She does it much better than I could!\n\nYou might consider installing the RegExplain package using devtools if you want more support working with regular expressions. They are powerful but they are complicated to learn initially\n\nThere is also a very helpful cheatsheet for regular expressions\n\nAnd finally, there is a great Wickham, Çetinkaya-Rundel, and Grolemund (2023) chapter on strings more generally, which covers both stringr and regex.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#the-imdb-dataset",
    "href": "012_nlp.html#the-imdb-dataset",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "12.3 The IMDB Dataset",
    "text": "12.3 The IMDB Dataset\nNow that we have a basic understanding of how to manipulation raw text, we can get set up for NLP and introduce a guiding example for this unit\nWe can start with our normal cast of characters RE packages, source, and settings (not displayed here)\nHowever, we will also install a few new ones that are specific to working with text.\n\nlibrary(tidytext)\nlibrary(textrecipes)  #step_- functions for NLP\nlibrary(SnowballC)  \nlibrary(stopwords)\n\n\nThe IMDB Reviews dataset is a classic NLP dataset that is used for sentiment analysis\n\nIt contains:\n\n25,000 movie reviews in train and test\nBalanced on positive and negative sentiment (labeled outcome)\nFor more info, see the website\n\n\nLet’s start by loading the dataset and adding an identifier for each review (i.e., document, doc_num)\n\ndata_trn &lt;- read_csv(here::here(path_data, \"imdb_trn.csv\"), \n                     show_col_types = FALSE) |&gt;\n  rowid_to_column(var = \"doc_num\") |&gt; \n  mutate(sentiment = fct(sentiment, levels = c(\"neg\", \"pos\"))) \n\ndata_trn  |&gt;\n  skim_some()\n\n\nData summary\n\n\nName\ndata_trn\n\n\nNumber of rows\n25000\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntext\n0\n1\n52\n13704\n0\n24904\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsentiment\n0\n1\nFALSE\n2\nneg: 12500, pos: 12500\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\np0\np100\n\n\n\n\ndoc_num\n0\n1\n1\n25000\n\n\n\n\n\n\nLet’s look at our outcome\n\ndata_trn |&gt; tab(sentiment)\n\n# A tibble: 2 × 3\n  sentiment     n  prop\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n1 neg       12500   0.5\n2 pos       12500   0.5\n\n\n\nTo get a better sense of the dataset, We can view first five negative reviews from the training set\n\ndata_trn |&gt; \n  filter(sentiment == \"neg\") |&gt; \n  slice(1:5) |&gt; \n  pull(text) |&gt; \n  print_kbl() \n\n\n\n\n\n\nx\n\n\n\n\nStory of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n\n\nAirport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...&lt;br /&gt;&lt;br /&gt;Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.&lt;br /&gt;&lt;br /&gt;The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.&lt;br /&gt;&lt;br /&gt;The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.&lt;br /&gt;&lt;br /&gt;Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\n\n\nThis film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.&lt;br /&gt;&lt;br /&gt;But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.&lt;br /&gt;&lt;br /&gt;I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself.\n\n\nSorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i'm no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying.\n\n\nWhen I was little my parents took me along to the theater to see Interiors. It was one of many movies I watched with my parents, but this was the only one we walked out of. Since then I had never seen Interiors until just recently, and I could have lived out the rest of my life without it. What a pretentious, ponderous, and painfully boring piece of 70's wine and cheese tripe. Woody Allen is one of my favorite directors but Interiors is by far the worst piece of crap of his career. In the unmistakable style of Ingmar Berman, Allen gives us a dark, angular, muted, insight in to the lives of a family wrought by the psychological damage caused by divorce, estrangement, career, love, non-love, halitosis, whatever. The film, intentionally, has no comic relief, no music, and is drenched in shadowy pathos. This film style can be best defined as expressionist in nature, using an improvisational method of dialogue to illicit a \"more pronounced depth of meaning and truth\". But Woody Allen is no Ingmar Bergman. The film is painfully slow and dull. But beyond that, I simply had no connection with or sympathy for any of the characters. Instead I felt only contempt for this parade of shuffling, whining, nicotine stained, martyrs in a perpetual quest for identity. Amid a backdrop of cosmopolitan affluence and baked Brie intelligentsia the story looms like a fart in the room. Everyone speaks in affected platitudes and elevated language between cigarettes. Everyone is \"lost\" and \"struggling\", desperate to find direction or understanding or whatever and it just goes on and on to the point where you just want to slap all of them. It's never about resolution, it's only about interminable introspective babble. It is nothing more than a psychological drama taken to an extreme beyond the audience's ability to connect. Woody Allen chose to make characters so immersed in themselves we feel left out. And for that reason I found this movie painfully self indulgent and spiritually draining. I see what he was going for but his insistence on promoting his message through Prozac prose and distorted film techniques jettisons it past the point of relevance. I highly recommend this one if you're feeling a little too happy and need something to remind you of death. Otherwise, let's just pretend this film never happened.\n\n\n\n\n\n\n\n\n\nand the first five positive reviews from the training set\n\ndata_trn |&gt; \n  filter(sentiment == \"pos\") |&gt; \n  slice(1:5) |&gt; \n  pull(text) |&gt; \n  print_kbl()\n\n\n\n\n\n\nx\n\n\n\n\nBromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n\n\nHomelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.&lt;br /&gt;&lt;br /&gt;But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.&lt;br /&gt;&lt;br /&gt;Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.&lt;br /&gt;&lt;br /&gt;While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.&lt;br /&gt;&lt;br /&gt;Or maybe this film will inspire you to help others.\n\n\nBrilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).\n\n\nThis is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not give a realistic view of homelessness (unlike, say, how Citizen Kane gave a realistic view of lounge singers, or Titanic gave a realistic view of Italians YOU IDIOTS). Many of the jokes fall flat. But still, this film is very lovable in a way many comedies are not, and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive. Its not The Fisher King, but its not crap, either. My only complaint is that Brooks should have cast someone else in the lead (I love Mel as a Director and Writer, not so much as a lead).\n\n\nThis is not the typical Mel Brooks film. It was much less slapstick than most of his movies and actually had a plot that was followable. Leslie Ann Warren made the movie, she is such a fantastic, under-rated actress. There were some moments that could have been fleshed out a bit more, and some scenes that could probably have been cut to make the room to do so, but all in all, this is worth the price to rent and see it. The acting was good overall, Brooks himself did a good job without his characteristic speaking to directly to the audience. Again, Warren was the best actor in the movie, but \"Fume\" and \"Sailor\" both played their parts well.\n\n\n\n\n\n\n\n\n\nYou need to spend a LOT of time reviewing the text before you begin to process it.\n\nI have NOT done this yet!\n\nMy models will be sub-optimal!",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#tokens",
    "href": "012_nlp.html#tokens",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "12.4 Tokens",
    "text": "12.4 Tokens\nMachine learning algorithms cannot work with raw text (documents) directly\n\nWe must feature engineer these documents to allow them to serve as input to statistical algorithms\n\nThe first step for most NLP feature engineering methods is to represent text (documents) as tokens (words, ngrams)\n\nGiven that tokenization is often one of our first steps for extracting features from text, it is important to consider carefully what happens during this step and its implications for your subsequent modeling\n\nIn tokenization, we take input documents (text strings) and a token type (a meaningful unit of text, such as a word) and split the document into pieces (tokens) that correspond to the type\n\nWe can tokenize text into a variety of token types:\n\ncharacters\nwords (most common - our focus; unigrams)\nsentences\nlines\nparagraphs\nn-grams (bigrams, trigrams)\n\n\nAn n-gram consists of a sequence of n items from a given sequence of text. Most often, it is a group of n words (bigrams, trigrams)\n\n\nn-grams retain word order which would otherwise be lost if we were just using words as the token type\n\n“I am not happy”\n\nTokenized by word, yields:\n\nI\nam\nnot\nhappy\n\n\n\nTokenized by 2-gram words:\n\nI am\nam not\nnot happy\n\n\nWe will be using tokenizer functions from the tokenizers package. Three in particular are:\n\ntokenize_words(x, lowercase = TRUE, stopwords = NULL, strip_punct = TRUE, strip_numeric = FALSE, simplify = FALSE)\ntokenize_ngrams(x, lowercase = TRUE, n = 3L, n_min = n, stopwords = character(), ngram_delim = \" \", simplify = FALSE)\ntokenize_regex(x, pattern = \"\\\\s+\", simplify = FALSE)\n\n\nHowever, we will be accessing these functions through wrappers:\n\ntidytext::unnest_tokens(tbl, output, input, token = \"words\", format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\"), to_lower = TRUE, drop = TRUE, collapse = NULL) for tidyverse data exploration of tokens within tibbles\ntextrecipes::step_tokenize() for tokenization in our recipes\n\n\nWord-level tokenization by tokenize_words() is done by finding word boundaries as follows:\n\nBreak at the start and end of text, unless the text is empty\nDo not break within CRLF (new line characters)\nOtherwise, break before and after new lines (including CR and LF)\nDo not break between most letters\nDo not break letters across certain punctuation\nDo not break within sequences of digits, or digits adjacent to letters (“3a,” or “A3”)\nDo not break within sequences, such as “3.2” or “3,456.789.”\nDo not break between Katakana\nDo not break from extenders\nDo not break within emoji zwj sequences\nDo not break within emoji flag sequences\nIgnore Format and Extend characters, except after sot, CR, LF, and new line\nKeep horizontal whitespace together\nOtherwise, break everywhere (including around ideographs, e.g., @, %, &gt;)\n\n\nLet’s start with using tokenize_words() to get a sense of how it works by default\n\nSplits on spaces\nConverts to lowercase by default (does it matter? MAYBE!!!)\nRetains apostrophes but drops other punctuation (,, ., !) and some symbols (e.g., - \\\\, @) by default. Does not drop _ (Do you need punctuation? !!!!)\nRetains numbers (by default) and decimals but drops + appended to 4+\nHas trouble with URLs and email address (do you need this?)\nOften, these issues may NOT matter\n\n\n\"Here is a sample document to tokenize.  How EXCITING (I _love_ it).  Sarah has spent 4 or 4.1 or 4P or 4+ or &gt;4 years developing her pre-processing and NLP skills.  You can learn more about tokenization here: https://smltar.com/tokenization.html or by emailing me at jjcurtin@wisc.edu\" |&gt; \n  tokenizers::tokenize_words()\n\n[[1]]\n [1] \"here\"              \"is\"                \"a\"                \n [4] \"sample\"            \"document\"          \"to\"               \n [7] \"tokenize\"          \"how\"               \"exciting\"         \n[10] \"i\"                 \"_love_\"            \"it\"               \n[13] \"sarah\"             \"has\"               \"spent\"            \n[16] \"4\"                 \"or\"                \"4.1\"              \n[19] \"or\"                \"4p\"                \"or\"               \n[22] \"4\"                 \"or\"                \"4\"                \n[25] \"years\"             \"developing\"        \"her\"              \n[28] \"pre\"               \"processing\"        \"and\"              \n[31] \"nlp\"               \"skills\"            \"you\"              \n[34] \"can\"               \"learn\"             \"more\"             \n[37] \"about\"             \"tokenization\"      \"here\"             \n[40] \"https\"             \"smltar.com\"        \"tokenization.html\"\n[43] \"or\"                \"by\"                \"emailing\"         \n[46] \"me\"                \"at\"                \"jjcurtin\"         \n[49] \"wisc.edu\"         \n\n\n\nSome of these behaviors can be altered from their defaults\n\nlowercase = TRUE\nstrip_punc = TRUE\nstrip_numeric = FALSE\n\n\nSome of these issues can be corrected by pre-processing the text\n\nstr_replace(\"jjcurtin@wisc.edu\", \"@\", \"_at_\")\n\n[1] \"jjcurtin_at_wisc.edu\"\n\n\n\nIf you need finer control, you can use tokenize_regex() and then do further processing with stringr functions and regex\n\nNow it may be easier to build up from here (e.g., ):\n\nstr_to_lower(word)\nstr_replace(word, \".$\", \"\")\n\n\n\"Here is a sample document to tokenize.  How EXCITING (I _love_ it).  Sarah has spent 4 or 4.1 or 4P or 4+ years developing her pre-processing and NLP skills.  You can learn more about tokenization here: https://smltar.com/tokenization.html or by emailing me at jjcurtin@wisc.edu\" |&gt; \n  tokenizers::tokenize_regex(pattern = \"\\\\s+\")\n\n[[1]]\n [1] \"Here\"                                \n [2] \"is\"                                  \n [3] \"a\"                                   \n [4] \"sample\"                              \n [5] \"document\"                            \n [6] \"to\"                                  \n [7] \"tokenize.\"                           \n [8] \"How\"                                 \n [9] \"EXCITING\"                            \n[10] \"(I\"                                  \n[11] \"_love_\"                              \n[12] \"it).\"                                \n[13] \"Sarah\"                               \n[14] \"has\"                                 \n[15] \"spent\"                               \n[16] \"4\"                                   \n[17] \"or\"                                  \n[18] \"4.1\"                                 \n[19] \"or\"                                  \n[20] \"4P\"                                  \n[21] \"or\"                                  \n[22] \"4+\"                                  \n[23] \"years\"                               \n[24] \"developing\"                          \n[25] \"her\"                                 \n[26] \"pre-processing\"                      \n[27] \"and\"                                 \n[28] \"NLP\"                                 \n[29] \"skills.\"                             \n[30] \"You\"                                 \n[31] \"can\"                                 \n[32] \"learn\"                               \n[33] \"more\"                                \n[34] \"about\"                               \n[35] \"tokenization\"                        \n[36] \"here:\"                               \n[37] \"https://smltar.com/tokenization.html\"\n[38] \"or\"                                  \n[39] \"by\"                                  \n[40] \"emailing\"                            \n[41] \"me\"                                  \n[42] \"at\"                                  \n[43] \"jjcurtin@wisc.edu\"                   \n\n\n\nYou can explore the tokens that will be formed using unnest_tokens() and basic tidyverse data wrangling using a tidied format of your documents as part of your EDA\n\nWe unnest to 1 token (word) per row (tidy format)\nWe keep track of doc_num (added earlier)\n\n\n\nHere, we tokenize the IMDB training set.\n\nUsing defaults\nCan change other default for tokenize_*() by passing into function via ...\nCan set drop = TRUE (default) to discard the original document column (text)\nIts pretty fast!\n\n\ntokens &lt;- data_trn |&gt; \n  unnest_tokens(word, text, token = \"words\", to_lower = TRUE, drop = FALSE) |&gt; \n  glimpse()\n\nRows: 5,935,548\nColumns: 4\n$ doc_num   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ sentiment &lt;fct&gt; neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, …\n$ text      &lt;chr&gt; \"Story of a man who has unnatural feelings for a pig. Starts…\n$ word      &lt;chr&gt; \"story\", \"of\", \"a\", \"man\", \"who\", \"has\", \"unnatural\", \"feeli…\n\n\n\n\nCoding sidebar: You can take a much deeper dive into tidyverse text processing in chapter 1 of Silge and Robinson (2017).\n\nLet’s get oriented by reviewing the tokens from the first document\n\nRaw form\n\n\ndata_trn$text[1]\n\n[1] \"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\"\n\n\n\nTokenized by word and tidied\n\n\ntokens |&gt; \n  filter(doc_num == 1) |&gt; \n  select(word) |&gt; \n  print(n = Inf)\n\n# A tibble: 112 × 1\n    word          \n    &lt;chr&gt;         \n  1 story         \n  2 of            \n  3 a             \n  4 man           \n  5 who           \n  6 has           \n  7 unnatural     \n  8 feelings      \n  9 for           \n 10 a             \n 11 pig           \n 12 starts        \n 13 out           \n 14 with          \n 15 a             \n 16 opening       \n 17 scene         \n 18 that          \n 19 is            \n 20 a             \n 21 terrific      \n 22 example       \n 23 of            \n 24 absurd        \n 25 comedy        \n 26 a             \n 27 formal        \n 28 orchestra     \n 29 audience      \n 30 is            \n 31 turned        \n 32 into          \n 33 an            \n 34 insane        \n 35 violent       \n 36 mob           \n 37 by            \n 38 the           \n 39 crazy         \n 40 chantings     \n 41 of            \n 42 it's          \n 43 singers       \n 44 unfortunately \n 45 it            \n 46 stays         \n 47 absurd        \n 48 the           \n 49 whole         \n 50 time          \n 51 with          \n 52 no            \n 53 general       \n 54 narrative     \n 55 eventually    \n 56 making        \n 57 it            \n 58 just          \n 59 too           \n 60 off           \n 61 putting       \n 62 even          \n 63 those         \n 64 from          \n 65 the           \n 66 era           \n 67 should        \n 68 be            \n 69 turned        \n 70 off           \n 71 the           \n 72 cryptic       \n 73 dialogue      \n 74 would         \n 75 make          \n 76 shakespeare   \n 77 seem          \n 78 easy          \n 79 to            \n 80 a             \n 81 third         \n 82 grader        \n 83 on            \n 84 a             \n 85 technical     \n 86 level         \n 87 it's          \n 88 better        \n 89 than          \n 90 you           \n 91 might         \n 92 think         \n 93 with          \n 94 some          \n 95 good          \n 96 cinematography\n 97 by            \n 98 future        \n 99 great         \n100 vilmos        \n101 zsigmond      \n102 future        \n103 stars         \n104 sally         \n105 kirkland      \n106 and           \n107 frederic      \n108 forrest       \n109 can           \n110 be            \n111 seen          \n112 briefly       \n\n\n\nConsidering all the tokens across all documents\n\nThere are almost 6 million words\n\n\nlength(tokens$word)\n\n[1] 5935548\n\n\n\nThe total unique vocabulary is around 85 thousand words\n\n\nlength(unique(tokens$word))\n\n[1] 85574\n\n\n\nWord frequency is VERY skewed\n\nThese are the counts for the most frequent 750 words\nthere are 84,000 additional infrequent words in the right tail not shown here!\n\n\ntokens |&gt;\n  count(word, sort = TRUE) |&gt; \n  slice(1:750) |&gt; \n  mutate(word = reorder(word, -n)) |&gt;\n  ggplot(aes(word, n)) +\n    geom_col() +\n    xlab(\"Words\") +\n    ylab(\"Raw Count\") +\n    theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())\n\n\n\n\n\n\n\n\n\nNow let’s review the 100 most common words\n\nWe SHOULD review MUCH deeper than this\nSome of our feature engineering approaches (e.g., BoW) will use the first 5K - 20K tokens\nSome of our feature engineering approaches (e.g., word embeddings) may use ALL tokens\nSome of these are likely not very informative (the, a, of, to, is). We will return to those words in a bit when we consider stopwords\nNotice br. Why is it so common?\n\n\ntokens |&gt;\n  count(word, sort = TRUE) |&gt;\n  print(n = 100)\n\n# A tibble: 85,574 × 2\n    word         n\n    &lt;chr&gt;    &lt;int&gt;\n  1 the     336179\n  2 and     164061\n  3 a       162738\n  4 of      145848\n  5 to      135695\n  6 is      107320\n  7 br      101871\n  8 in       93920\n  9 it       78874\n 10 i        76508\n 11 this     75814\n 12 that     69794\n 13 was      48189\n 14 as       46903\n 15 for      44321\n 16 with     44115\n 17 movie    43509\n 18 but      42531\n 19 film     39058\n 20 on       34185\n 21 not      30608\n 22 you      29886\n 23 are      29431\n 24 his      29352\n 25 have     27725\n 26 be       26947\n 27 he       26894\n 28 one      26502\n 29 all      23927\n 30 at       23500\n 31 by       22538\n 32 an       21550\n 33 they     21096\n 34 who      20604\n 35 so       20573\n 36 from     20488\n 37 like     20268\n 38 her      18399\n 39 or       17997\n 40 just     17764\n 41 about    17368\n 42 out      17099\n 43 it's     17094\n 44 has      16789\n 45 if       16746\n 46 some     15734\n 47 there    15671\n 48 what     15374\n 49 good     15110\n 50 more     14242\n 51 when     14161\n 52 very     14059\n 53 up       13283\n 54 no       12698\n 55 time     12691\n 56 even     12638\n 57 she      12624\n 58 my       12485\n 59 would    12236\n 60 which    12047\n 61 story    11918\n 62 only     11910\n 63 really   11734\n 64 see      11465\n 65 their    11376\n 66 had      11289\n 67 can      11144\n 68 were     10782\n 69 me       10745\n 70 well     10637\n 71 than      9920\n 72 we        9858\n 73 much      9750\n 74 bad       9292\n 75 been      9287\n 76 get       9279\n 77 will      9195\n 78 do        9159\n 79 also      9130\n 80 into      9109\n 81 people    9107\n 82 other     9083\n 83 first     9054\n 84 because   9045\n 85 great     9033\n 86 how       8870\n 87 him       8865\n 88 most      8775\n 89 don't     8445\n 90 made      8351\n 91 its       8156\n 92 then      8097\n 93 make      8018\n 94 way       8005\n 95 them      7954\n 96 too       7820\n 97 could     7745\n 98 any       7653\n 99 movies    7648\n100 after     7617\n# ℹ 85,474 more rows\n\n\n\nHere is the first document that has the br token in it.\n\nIt is html code for a line break.\n\ntokens |&gt; \n  filter(word == \"br\") |&gt; \n  slice(1) |&gt; \n  pull(text)\n\n[1] \"Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...&lt;br /&gt;&lt;br /&gt;Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.&lt;br /&gt;&lt;br /&gt;The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.&lt;br /&gt;&lt;br /&gt;The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.&lt;br /&gt;&lt;br /&gt;Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\"\n\n\n\nLet’s clean it in the raw documents and re-tokenize\nYou should always check your replacements CAREFULLY before doing them for unexpected matches and side-effects\n\ndata_trn &lt;- data_trn |&gt; \n  mutate(text = str_replace_all(text, \"&lt;br /&gt;&lt;br /&gt;\", \" \"))\n\ntokens &lt;- data_trn |&gt; \n  unnest_tokens(word, text, drop = FALSE)\n\n\n\nWe should continue to review MUCH deeper into the common tokens to detect other tokenization errors. I will not demonstrate that here.\n\nWe should also review the least common tokens\n\nWorth searching to make sure they haven’t resulted from tokenization errors\nCan use this to tune our pre-processing and tokenization\nBUT, not as important b/c these will be dropped by some of our feature engineering approaches (BoW) and may not present too much problem to others (embeddings)\n\n\ntokens |&gt;\n  count(word) |&gt; \n  arrange(n) |&gt; \n  print(n = 200)\n\n# A tibble: 85,574 × 2\n    word                            n\n    &lt;chr&gt;                       &lt;int&gt;\n  1 0.10                            1\n  2 0.48                            1\n  3 0.7                             1\n  4 0.79                            1\n  5 0.89                            1\n  6 00.01                           1\n  7 000                             1\n  8 0000000000001                   1\n  9 00015                           1\n 10 003830                          1\n 11 006                             1\n 12 0079                            1\n 13 0093638                         1\n 14 01pm                            1\n 15 020410                          1\n 16 029                             1\n 17 041                             1\n 18 050                             1\n 19 06th                            1\n 20 087                             1\n 21 089                             1\n 22 08th                            1\n 23 0f                              1\n 24 0ne                             1\n 25 0r                              1\n 26 0s                              1\n 27 1'40                            1\n 28 1,000,000,000,000               1\n 29 1,000.00                        1\n 30 1,000s                          1\n 31 1,2,3                           1\n 32 1,2,3,4,5                       1\n 33 1,2,3,5                         1\n 34 1,300                           1\n 35 1,400                           1\n 36 1,430                           1\n 37 1,500,000                       1\n 38 1,600                           1\n 39 1,65m                           1\n 40 1,700                           1\n 41 1,999,999                       1\n 42 1.000                           1\n 43 1.19                            1\n 44 1.30                            1\n 45 1.30am                          1\n 46 1.3516                          1\n 47 1.37                            1\n 48 1.47                            1\n 49 1.49                            1\n 50 1.4x                            1\n 51 1.60                            1\n 52 1.66                            1\n 53 1.78                            1\n 54 1.9                             1\n 55 1.95                            1\n 56 10,000,000                      1\n 57 10.000                          1\n 58 10.75                           1\n 59 10.95                           1\n 60 100.00                          1\n 61 1000000                         1\n 62 1000lb                          1\n 63 100b                            1\n 64 100k                            1\n 65 100m                            1\n 66 100mph                          1\n 67 100yards                        1\n 68 102nd                           1\n 69 1040                            1\n 70 1040a                           1\n 71 1040s                           1\n 72 1050                            1\n 73 105lbs                          1\n 74 106min                          1\n 75 10am                            1\n 76 10lines                         1\n 77 10mil                           1\n 78 10min                           1\n 79 10minutes                       1\n 80 10p.m                           1\n 81 10star                          1\n 82 10x's                           1\n 83 10yr                            1\n 84 11,2001                         1\n 85 11.00                           1\n 86 11001001                        1\n 87 1100ad                          1\n 88 1146                            1\n 89 11f                             1\n 90 11m                             1\n 91 12.000.000                      1\n 92 120,000.00                      1\n 93 1200f                           1\n 94 1201                            1\n 95 1202                            1\n 96 123,000,000                     1\n 97 12383499143743701               1\n 98 125,000                         1\n 99 125m                            1\n100 127                             1\n101 12hr                            1\n102 12mm                            1\n103 12s                             1\n104 13,15,16                        1\n105 13.00                           1\n106 1300                            1\n107 1318                            1\n108 135m                            1\n109 137                             1\n110 139                             1\n111 13k                             1\n112 14.00                           1\n113 14.99                           1\n114 140hp                           1\n115 1415                            1\n116 142                             1\n117 1454                            1\n118 1473                            1\n119 1492                            1\n120 14ieme                          1\n121 14yr                            1\n122 15,000,000                      1\n123 15.00                           1\n124 150,000                         1\n125 1500.00                         1\n126 150_worst_cases_of_nepotism     1\n127 150k                            1\n128 150m                            1\n129 151                             1\n130 152                             1\n131 153                             1\n132 1547                            1\n133 155                             1\n134 156                             1\n135 1561                            1\n136 1594                            1\n137 15mins                          1\n138 15minutes                       1\n139 16,000                          1\n140 16.9                            1\n141 16.97                           1\n142 1600s                           1\n143 160lbs                          1\n144 1610                            1\n145 163,000                         1\n146 164                             1\n147 165                             1\n148 166                             1\n149 1660s                           1\n150 16ieme                          1\n151 16k                             1\n152 16x9                            1\n153 16éme                           1\n154 17,000                          1\n155 17,2003                         1\n156 17.75                           1\n157 1700s                           1\n158 1701                            1\n159 171                             1\n160 175                             1\n161 177                             1\n162 1775                            1\n163 1790s                           1\n164 1794                            1\n165 17million                       1\n166 18,000,000                      1\n167 1800mph                         1\n168 1801                            1\n169 1805                            1\n170 1809                            1\n171 180d                            1\n172 1812                            1\n173 18137                           1\n174 1814                            1\n175 1832                            1\n176 1838                            1\n177 1844                            1\n178 1850ies                         1\n179 1852                            1\n180 1860s                           1\n181 1870                            1\n182 1871                            1\n183 1874                            1\n184 1875                            1\n185 188                             1\n186 1887                            1\n187 1889                            1\n188 188o                            1\n189 1893                            1\n190 18year                          1\n191 19,000,000                      1\n192 190                             1\n193 1904                            1\n194 1908                            1\n195 192                             1\n196 1920ies                         1\n197 1923                            1\n198 1930ies                         1\n199 193o's                          1\n200 194                             1\n# ℹ 85,374 more rows\n\n\n\nWhat is the deal with _*_?\n\ntokens |&gt; \n  filter(word == \"_anything_\") |&gt; \n  slice(1) |&gt; \n  pull(text)\n\n[1] \"I am shocked. Shocked and dismayed that the 428 of you IMDB users who voted before me have not given this film a rating of higher than 7. 7?!?? - that's a C!. If I could give FOBH a 20, I'd gladly do it. This film ranks high atop the pantheon of modern comedy, alongside Half Baked and Mallrats, as one of the most hilarious films of all time. If you know _anything_ about rap music - YOU MUST SEE THIS!! If you know nothing about rap music - learn something!, and then see this! Comparisons to 'Spinal Tap' fail to appreciate the inspired genius of this unique film. If you liked Bob Roberts, you'll love this. Watch it and vote it a 10!\"\n\n\n\ntokens |&gt; \n  filter(word == \"_love_\") |&gt; \n  slice(1) |&gt; \n  pull(text)\n\n[1] \"Before I start, I _love_ Eddie Izzard. I think he's one of the funniest stand-ups around today. Possibly that means I'm going into this with too high expectations, but I just didn't find Eddie funny in this outing. I think the main problem is Eddie is trying too hard to be Eddie. Everyone knows him as a completely irrelevant comic, and we all love him for it. But in Circle, he appears to be going more for irrelevant than funny, and completely lost me in places. Many of the topics he covers he has covered before - I even think I recognised a few recycled jokes in there. If you buy the DVD you'll find a behind-the-scenes look at Eddie's tour (interesting in places, but not very funny), and a French language version of one of his shows. Die-hards will enjoy seeing Eddie in a different language, but subtitled comedy isn't very funny. If you're a fan of Eddie you've either got this already or you're going to buy it whatever I say. If you're just passing through, buy Glorious or Dressed to Kill - you won't be disappointed. With Circle, you probably will.\"\n\n\n\nLet’s find all the tokens that start or end with _\n\nA few of these are even repeatedly used\n\ntokens |&gt; \n  filter(str_detect(word, \"^_\") | str_detect(word, \"_$\")) |&gt; \n  count(word, sort = TRUE) |&gt;\n  print(n = Inf)\n\n# A tibble: 130 × 2\n    word                                                                   n\n    &lt;chr&gt;                                                              &lt;int&gt;\n  1 _the                                                                   6\n  2 _a                                                                     5\n  3 thing_                                                                 4\n  4 ____                                                                   3\n  5 _atlantis_                                                             3\n  6 _is_                                                                   3\n  7 ______                                                                 2\n  8 _____________________________________                                  2\n  9 _bounce_                                                               2\n 10 _night                                                                 2\n 11 _not_                                                                  2\n 12 _plan                                                                  2\n 13 _real_                                                                 2\n 14 _waterdance_                                                           2\n 15 story_                                                                 2\n 16 9_                                                                     1\n 17 _____                                                                  1\n 18 _________                                                              1\n 19 ____________________________________                                   1\n 20 __________________________________________________________________     1\n 21 _absolute                                                              1\n 22 _am_                                                                   1\n 23 _and_                                                                  1\n 24 _angel_                                                                1\n 25 _annie_                                                                1\n 26 _any_                                                                  1\n 27 _anything_                                                             1\n 28 _apocalyptically                                                       1\n 29 _as                                                                    1\n 30 _atlantis                                                              1\n 31 _attack                                                                1\n 32 _before_                                                               1\n 33 _blair                                                                 1\n 34 _both_                                                                 1\n 35 _by                                                                    1\n 36 _can't_                                                                1\n 37 _cannon_                                                               1\n 38 _certainly_                                                            1\n 39 _could                                                                 1\n 40 _cruel                                                                 1\n 41 _dirty                                                                 1\n 42 _discuss_                                                              1\n 43 _discussing_                                                           1\n 44 _do_                                                                   1\n 45 _dr                                                                    1\n 46 _dying                                                                 1\n 47 _earned_                                                               1\n 48 _everything_                                                           1\n 49 _ex_executives                                                         1\n 50 _extremeley_                                                           1\n 51 _extremely_                                                            1\n 52 _film_                                                                 1\n 53 _get_                                                                  1\n 54 _have_                                                                 1\n 55 _i_                                                                    1\n 56 _innerly                                                               1\n 57 _inside_                                                               1\n 58 _inspire_                                                              1\n 59 _les                                                                   1\n 60 _love_                                                                 1\n 61 _magic_                                                                1\n 62 _much_                                                                 1\n 63 _mystery                                                               1\n 64 _napolean                                                              1\n 65 _new                                                                   1\n 66 _obviously_                                                            1\n 67 _other_                                                                1\n 68 _penetrate_                                                            1\n 69 _possible_                                                             1\n 70 _really_is_                                                            1\n 71 _shall                                                                 1\n 72 _shock                                                                 1\n 73 _so_                                                                   1\n 74 _so_much_                                                              1\n 75 _somewhere_                                                            1\n 76 _spiritited                                                            1\n 77 _starstruck_                                                           1\n 78 _strictly                                                              1\n 79 _sung_                                                                 1\n 80 _the_                                                                  1\n 81 _the_lost_empire_                                                      1\n 82 _there's_                                                              1\n 83 _they_                                                                 1\n 84 _think_                                                                1\n 85 _told_                                                                 1\n 86 _toy                                                                   1\n 87 _tried                                                                 1\n 88 _twice                                                                 1\n 89 _undertow_                                                             1\n 90 _very_                                                                 1\n 91 _voice_                                                                1\n 92 _want_                                                                 1\n 93 _we've                                                                 1\n 94 _well_                                                                 1\n 95 _whale_                                                                1\n 96 _wrong_                                                                1\n 97 _x                                                                     1\n 98 acteurs_                                                               1\n 99 apple_                                                                 1\n100 away_                                                                  1\n101 ballroom_                                                              1\n102 been_                                                                  1\n103 beginners_                                                             1\n104 brail_                                                                 1\n105 casablanca_                                                            1\n106 composer_                                                              1\n107 dancing_                                                               1\n108 dougray_scott_                                                         1\n109 dozen_                                                                 1\n110 dynamite_                                                              1\n111 eaters_                                                                1\n112 eyre_                                                                  1\n113 f___                                                                   1\n114 film_                                                                  1\n115 hard_                                                                  1\n116 men_                                                                   1\n117 night_                                                                 1\n118 opera_                                                                 1\n119 rehearsals_                                                            1\n120 shrews_                                                                1\n121 space_                                                                 1\n122 starts__                                                               1\n123 that_                                                                  1\n124 treatment_                                                             1\n125 watchmen_                                                              1\n126 what_the_bleep_                                                        1\n127 witch_                                                                 1\n128 words_                                                                 1\n129 you_                                                                   1\n130 zhivago_                                                               1\n\n\n\nNow we can clean the raw documents again. This works but there is probably a better regex using ^_ and _$\n\ndata_trn &lt;- data_trn |&gt; \n  mutate(text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n\n\nLet’s take another look and uncommon tokens\n\ndata_trn |&gt; \n  unnest_tokens(word, text, drop = FALSE) |&gt; \n  count(word) |&gt; \n  arrange(n) |&gt; \n  print(n = 200)\n\n# A tibble: 85,535 × 2\n    word                            n\n    &lt;chr&gt;                       &lt;int&gt;\n  1 0.10                            1\n  2 0.48                            1\n  3 0.7                             1\n  4 0.79                            1\n  5 0.89                            1\n  6 00.01                           1\n  7 000                             1\n  8 0000000000001                   1\n  9 00015                           1\n 10 003830                          1\n 11 006                             1\n 12 0079                            1\n 13 0093638                         1\n 14 01pm                            1\n 15 020410                          1\n 16 029                             1\n 17 041                             1\n 18 050                             1\n 19 06th                            1\n 20 087                             1\n 21 089                             1\n 22 08th                            1\n 23 0f                              1\n 24 0ne                             1\n 25 0r                              1\n 26 0s                              1\n 27 1'40                            1\n 28 1,000,000,000,000               1\n 29 1,000.00                        1\n 30 1,000s                          1\n 31 1,2,3                           1\n 32 1,2,3,4,5                       1\n 33 1,2,3,5                         1\n 34 1,300                           1\n 35 1,400                           1\n 36 1,430                           1\n 37 1,500,000                       1\n 38 1,600                           1\n 39 1,65m                           1\n 40 1,700                           1\n 41 1,999,999                       1\n 42 1.000                           1\n 43 1.19                            1\n 44 1.30                            1\n 45 1.30am                          1\n 46 1.3516                          1\n 47 1.37                            1\n 48 1.47                            1\n 49 1.49                            1\n 50 1.4x                            1\n 51 1.60                            1\n 52 1.66                            1\n 53 1.78                            1\n 54 1.9                             1\n 55 1.95                            1\n 56 10,000,000                      1\n 57 10.000                          1\n 58 10.75                           1\n 59 10.95                           1\n 60 100.00                          1\n 61 1000000                         1\n 62 1000lb                          1\n 63 100b                            1\n 64 100k                            1\n 65 100m                            1\n 66 100mph                          1\n 67 100yards                        1\n 68 102nd                           1\n 69 1040                            1\n 70 1040a                           1\n 71 1040s                           1\n 72 1050                            1\n 73 105lbs                          1\n 74 106min                          1\n 75 10am                            1\n 76 10lines                         1\n 77 10mil                           1\n 78 10min                           1\n 79 10minutes                       1\n 80 10p.m                           1\n 81 10star                          1\n 82 10x's                           1\n 83 10yr                            1\n 84 11,2001                         1\n 85 11.00                           1\n 86 11001001                        1\n 87 1100ad                          1\n 88 1146                            1\n 89 11f                             1\n 90 11m                             1\n 91 12.000.000                      1\n 92 120,000.00                      1\n 93 1200f                           1\n 94 1201                            1\n 95 1202                            1\n 96 123,000,000                     1\n 97 12383499143743701               1\n 98 125,000                         1\n 99 125m                            1\n100 127                             1\n101 12hr                            1\n102 12mm                            1\n103 12s                             1\n104 13,15,16                        1\n105 13.00                           1\n106 1300                            1\n107 1318                            1\n108 135m                            1\n109 137                             1\n110 139                             1\n111 13k                             1\n112 14.00                           1\n113 14.99                           1\n114 140hp                           1\n115 1415                            1\n116 142                             1\n117 1454                            1\n118 1473                            1\n119 1492                            1\n120 14ieme                          1\n121 14yr                            1\n122 15,000,000                      1\n123 15.00                           1\n124 150,000                         1\n125 1500.00                         1\n126 150_worst_cases_of_nepotism     1\n127 150k                            1\n128 150m                            1\n129 151                             1\n130 152                             1\n131 153                             1\n132 1547                            1\n133 155                             1\n134 156                             1\n135 1561                            1\n136 1594                            1\n137 15mins                          1\n138 15minutes                       1\n139 16,000                          1\n140 16.9                            1\n141 16.97                           1\n142 1600s                           1\n143 160lbs                          1\n144 1610                            1\n145 163,000                         1\n146 164                             1\n147 165                             1\n148 166                             1\n149 1660s                           1\n150 16ieme                          1\n151 16k                             1\n152 16x9                            1\n153 16éme                           1\n154 17,000                          1\n155 17,2003                         1\n156 17.75                           1\n157 1700s                           1\n158 1701                            1\n159 171                             1\n160 175                             1\n161 177                             1\n162 1775                            1\n163 1790s                           1\n164 1794                            1\n165 17million                       1\n166 18,000,000                      1\n167 1800mph                         1\n168 1801                            1\n169 1805                            1\n170 1809                            1\n171 180d                            1\n172 1812                            1\n173 18137                           1\n174 1814                            1\n175 1832                            1\n176 1838                            1\n177 1844                            1\n178 1850ies                         1\n179 1852                            1\n180 1860s                           1\n181 1870                            1\n182 1871                            1\n183 1874                            1\n184 1875                            1\n185 188                             1\n186 1887                            1\n187 1889                            1\n188 188o                            1\n189 1893                            1\n190 18year                          1\n191 19,000,000                      1\n192 190                             1\n193 1904                            1\n194 1908                            1\n195 192                             1\n196 1920ies                         1\n197 1923                            1\n198 1930ies                         1\n199 193o's                          1\n200 194                             1\n# ℹ 85,335 more rows\n\n\nLots of numbers. Probably? not that important for our classification problem.\n\n\nLet’s strip them for demonstration purposes at least using strip_numeric = TRUE\n\nThis is likey good for unigrams but wouldn’t be good/possible for bigrams (break sequence)\n\ndata_trn |&gt; \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE) |&gt;   \n  count(word) |&gt; \n  arrange(n) |&gt; \n  print(n = 200)\n\n# A tibble: 84,392 × 2\n    word                            n\n    &lt;chr&gt;                       &lt;int&gt;\n  1 01pm                            1\n  2 06th                            1\n  3 08th                            1\n  4 0f                              1\n  5 0ne                             1\n  6 0r                              1\n  7 0s                              1\n  8 1,000s                          1\n  9 1,65m                           1\n 10 1.30am                          1\n 11 1.4x                            1\n 12 1000lb                          1\n 13 100b                            1\n 14 100k                            1\n 15 100m                            1\n 16 100mph                          1\n 17 100yards                        1\n 18 102nd                           1\n 19 1040a                           1\n 20 1040s                           1\n 21 105lbs                          1\n 22 106min                          1\n 23 10am                            1\n 24 10lines                         1\n 25 10mil                           1\n 26 10min                           1\n 27 10minutes                       1\n 28 10p.m                           1\n 29 10star                          1\n 30 10x's                           1\n 31 10yr                            1\n 32 1100ad                          1\n 33 11f                             1\n 34 11m                             1\n 35 1200f                           1\n 36 125m                            1\n 37 12hr                            1\n 38 12mm                            1\n 39 12s                             1\n 40 135m                            1\n 41 13k                             1\n 42 140hp                           1\n 43 14ieme                          1\n 44 14yr                            1\n 45 150_worst_cases_of_nepotism     1\n 46 150k                            1\n 47 150m                            1\n 48 15mins                          1\n 49 15minutes                       1\n 50 1600s                           1\n 51 160lbs                          1\n 52 1660s                           1\n 53 16ieme                          1\n 54 16k                             1\n 55 16éme                           1\n 56 1700s                           1\n 57 1790s                           1\n 58 17million                       1\n 59 1800mph                         1\n 60 180d                            1\n 61 1850ies                         1\n 62 1860s                           1\n 63 188o                            1\n 64 18year                          1\n 65 1920ies                         1\n 66 1930ies                         1\n 67 193o's                          1\n 68 1949er                          1\n 69 1961s                           1\n 70 1970ies                         1\n 71 1970s.i                         1\n 72 197o                            1\n 73 1980ies                         1\n 74 1982s                           1\n 75 1983s                           1\n 76 19k                             1\n 77 19thc                           1\n 78 1and                            1\n 79 1h40m                           1\n 80 1million                        1\n 81 1min                            1\n 82 1mln                            1\n 83 1o                              1\n 84 1ton                            1\n 85 1tv.ru                          1\n 86 1ç                              1\n 87 2.00am                          1\n 88 2.5hrs                          1\n 89 2000ad                          1\n 90 2004s                           1\n 91 200ft                           1\n 92 200th                           1\n 93 20c                             1\n 94 20ft                            1\n 95 20k                             1\n 96 20m                             1\n 97 20mins                          1\n 98 20minutes                       1\n 99 20mn                            1\n100 20p                             1\n101 20perr                          1\n102 20s.what                        1\n103 20ties                          1\n104 20widow                         1\n105 20x                             1\n106 20year                          1\n107 20yrs                           1\n108 225mins                         1\n109 22d                             1\n110 230lbs                          1\n111 230mph                          1\n112 23d                             1\n113 24m30s                          1\n114 24years                         1\n115 25million                       1\n116 25mins                          1\n117 25s                             1\n118 25yrs                           1\n119 261k                            1\n120 2fast                           1\n121 2furious                        1\n122 2h                              1\n123 2hour                           1\n124 2hr                             1\n125 2in                             1\n126 2inch                           1\n127 2more                           1\n128 300ad                           1\n129 300c                            1\n130 300lbs                          1\n131 300mln                          1\n132 30am                            1\n133 30ish                           1\n134 30k                             1\n135 30lbs                           1\n136 30s.like                        1\n137 30something                     1\n138 30ties                          1\n139 32lb                            1\n140 32nd                            1\n141 330am                           1\n142 330mins                         1\n143 336th                           1\n144 33m                             1\n145 35c                             1\n146 35mins                          1\n147 35pm                            1\n148 39th                            1\n149 3bs                             1\n150 3dvd                            1\n151 3lbs                            1\n152 3m                              1\n153 3mins                           1\n154 3pm                             1\n155 3po's                           1\n156 3th                             1\n157 3who                            1\n158 4.5hrs                          1\n159 401k                            1\n160 40am                            1\n161 40min                           1\n162 40mph                           1\n163 442nd                           1\n164 44c                             1\n165 44yrs                           1\n166 45am                            1\n167 45min                           1\n168 45s                             1\n169 480m                            1\n170 480p                            1\n171 4cylinder                       1\n172 4d                              1\n173 4eva                            1\n174 4f                              1\n175 4h                              1\n176 4hrs                            1\n177 4o                              1\n178 4pm                             1\n179 4w                              1\n180 4ward                           1\n181 4x                              1\n182 5.50usd                         1\n183 500db                           1\n184 500lbs                          1\n185 50c                             1\n186 50ft                            1\n187 50ies                           1\n188 50ish                           1\n189 50k                             1\n190 50mins                          1\n191 51b                             1\n192 51st                            1\n193 52s                             1\n194 53m                             1\n195 540i                            1\n196 54th                            1\n197 57d                             1\n198 58th                            1\n199 5kph                            1\n200 5min                            1\n# ℹ 84,192 more rows\n\n\n\nThe tokenizer didn’t get rid of numbers connected to text\n\nHow do we want to handle these?\nCould add a space to make them two words?\nWe will leave them as is\n\n\nOther issues?\n\nMis-spellings\nRepeated letters for emphasize or effect?\nDid caps matter (default tokenization was to convert to lowercase)?\nDomain knowledge is useful here though multiple model configurations can be considered\nstrings of words that aren’t meaningful (“Welcome to Facebook”)\n\n\nIn the above workflow, we:\n\ntokenize\nreview tokens for issues\nclean raw text documents\nre-tokenize\n\n\n\nIn some instances, it may be easier to clean the token and then put them back together\n\ntokenize\nreview tokens for issues\nclean tokens\nrecreate document\ntokenize\n\n\nIf this latter workflow feels easier (i.e., easier to regex into a token than a document), we will need code to put the tokens back together into a document\n\nHere is an example using the first three documents (`slice(1:3)1) and no cleaning\n\nTokenize\n\n\ntokens &lt;- \n  data_trn |&gt;\n  slice(1:3) |&gt; \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE)\n\n\nNow we can do further cleaning with tokens\n\nINSERT CLEANING CHUNKS HERE\n\nThen put back together. Could also collapse into text_cln to retain original text column\n\n\ndata_trn_cln &lt;-\n  tokens |&gt;  \n  group_by(doc_num, sentiment) |&gt; \n  summarize(text = str_c(word, collapse = \" \"),\n            .groups = \"drop\") \n\n\nLets see what we have\n\nNow all lower case\nNo numbers, punctuation, etc.\nand any other cleaning we could have done with the tokens along the way….\n\n\ndata_trn_cln |&gt; \n  pull(text) |&gt;\n  print_kbl() \n\n\n\n\n\n\nx\n\n\n\n\nstory of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is turned into an insane violent mob by the crazy chantings of it's singers unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting even those from the era should be turned off the cryptic dialogue would make shakespeare seem easy to a third grader on a technical level it's better than you might think with some good cinematography by future great vilmos zsigmond future stars sally kirkland and frederic forrest can be seen briefly\n\n\nairport starts as a brand new luxury plane is loaded up with valuable paintings such belonging to rich businessman philip stevens james stewart who is flying them a bunch of vip's to his estate in preparation of it being opened to the public as a museum also on board is stevens daughter julie kathleen quinlan her son the luxury jetliner takes off as planned but mid air the plane is hi jacked by the co pilot chambers robert foxworth his two accomplice's banker monte markham wilson michael pataki who knock the passengers crew out with sleeping gas they plan to steal the valuable cargo land on a disused plane strip on an isolated island but while making his descent chambers almost hits an oil rig in the ocean loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the bermuda triangle with air in short supply water leaking in having flown over miles off course the problems mount for the survivor's as they await help with time fast running out also known under the slightly different tile airport this second sequel to the smash hit disaster thriller airport was directed by jerry jameson while once again like it's predecessors i can't say airport is any sort of forgotten classic it is entertaining although not necessarily for the right reasons out of the three airport films i have seen so far i actually liked this one the best just it has my favourite plot of the three with a nice mid air hi jacking then the crashing didn't he see the oil rig sinking of the maybe the makers were trying to cross the original airport with another popular disaster flick of the period the poseidon adventure submerged is where it stays until the end with a stark dilemma facing those trapped inside either suffocate when the air runs out or drown as the floods or if any of the doors are opened it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's dull dialogue lethargic set pieces a real lack of danger or suspense or tension means this is a missed opportunity while the rather sluggish plot keeps one entertained for odd minutes not that much happens after the plane sinks there's not as much urgency as i thought there should have been even when the navy become involved things don't pick up that much with a few shots of huge ships helicopters flying about but there's just something lacking here george kennedy as the jinxed airline worker joe patroni is back but only gets a couple of scenes barely even says anything preferring to just look worried in the background the home video theatrical version of airport run minutes while the us tv versions add an extra hour of footage including a new opening credits sequence many more scenes with george kennedy as patroni flashbacks to flesh out character's longer rescue scenes the discovery or another couple of dead bodies including the navigator while i would like to see this extra footage i am not sure i could sit through a near three hour cut of airport as expected the film has dated badly with horrible fashions interior design choices i will say no more other than the toy plane model effects aren't great either along with the other two airport sequels this takes pride of place in the razzie award's hall of shame although i can think of lots of worse films than this so i reckon that's a little harsh the action scenes are a little dull unfortunately the pace is slow not much excitement or tension is generated which is a shame as i reckon this could have been a pretty good film if made properly the production values are alright if nothing spectacular the acting isn't great two time oscar winner jack lemmon has said since it was a mistake to star in this one time oscar winner james stewart looks old frail also one time oscar winner lee grant looks drunk while sir christopher lee is given little to do there are plenty of other familiar faces to look out for too airport is the most disaster orientated of the three airport films so far i liked the ideas behind it even if they were a bit silly the production bland direction doesn't help though a film about a sunken plane just shouldn't be this boring or lethargic followed by the concorde airport\n\n\nthis film lacked something i couldn't put my finger on at first charisma on the part of the leading actress this inevitably translated to lack of chemistry when she shared the screen with her leading man even the romantic scenes came across as being merely the actors at play it could very well have been the director who miscalculated what he needed from the actors i just don't know but could it have been the screenplay just exactly who was the chef in love with he seemed more enamored of his culinary skills and restaurant and ultimately of himself and his youthful exploits than of anybody or anything else he never convinced me he was in love with the princess i was disappointed in this movie but don't forget it was nominated for an oscar so judge for yourself",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#stop-words",
    "href": "012_nlp.html#stop-words",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "12.5 Stop words",
    "text": "12.5 Stop words\nNot all words are equally informative or useful to our model depending on the nature of our problem\n\nVery common words often may carry little or no meaningful information\n\nThese words are called stop words\n\nIt is common advice and practice to remove stop words for various NLP tasks\n\nNotice some of the top most frequent words among our tokens from IMDB reviews\n\ndata_trn |&gt; \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE) |&gt; \n  count(word, sort = TRUE) |&gt; \n  print(n = 100)\n\n# A tibble: 84,392 × 2\n    word         n\n    &lt;chr&gt;    &lt;int&gt;\n  1 the     336185\n  2 and     164061\n  3 a       162743\n  4 of      145848\n  5 to      135695\n  6 is      107320\n  7 in       93920\n  8 it       78874\n  9 i        76508\n 10 this     75814\n 11 that     69794\n 12 was      48189\n 13 as       46904\n 14 for      44321\n 15 with     44115\n 16 movie    43509\n 17 but      42531\n 18 film     39058\n 19 on       34185\n 20 not      30608\n 21 you      29886\n 22 are      29431\n 23 his      29352\n 24 have     27725\n 25 be       26947\n 26 he       26894\n 27 one      26502\n 28 all      23927\n 29 at       23500\n 30 by       22539\n 31 an       21550\n 32 they     21096\n 33 who      20604\n 34 so       20573\n 35 from     20488\n 36 like     20268\n 37 her      18399\n 38 or       17997\n 39 just     17764\n 40 about    17368\n 41 out      17099\n 42 it's     17094\n 43 has      16789\n 44 if       16746\n 45 some     15734\n 46 there    15671\n 47 what     15374\n 48 good     15110\n 49 more     14242\n 50 when     14161\n 51 very     14059\n 52 up       13283\n 53 no       12698\n 54 time     12691\n 55 even     12638\n 56 she      12624\n 57 my       12485\n 58 would    12236\n 59 which    12047\n 60 story    11919\n 61 only     11910\n 62 really   11734\n 63 see      11465\n 64 their    11376\n 65 had      11289\n 66 can      11144\n 67 were     10782\n 68 me       10745\n 69 well     10637\n 70 than      9920\n 71 we        9858\n 72 much      9750\n 73 bad       9292\n 74 been      9287\n 75 get       9279\n 76 will      9195\n 77 do        9159\n 78 also      9130\n 79 into      9109\n 80 people    9107\n 81 other     9083\n 82 first     9054\n 83 because   9045\n 84 great     9033\n 85 how       8870\n 86 him       8865\n 87 most      8775\n 88 don't     8445\n 89 made      8351\n 90 its       8156\n 91 then      8097\n 92 make      8018\n 93 way       8005\n 94 them      7954\n 95 too       7820\n 96 could     7746\n 97 any       7653\n 98 movies    7648\n 99 after     7617\n100 think     7293\n# ℹ 84,292 more rows\n\n\n\nStop words can have different roles in a corpus (a set of documents)\n\nFor our purposes, we generally care about two different types of stop words:\n\nGlobal\nSubject-specific\n\n\n\nGlobal stop words almost always have very little value for our modeling goals\n\nThese are frequent words like “the”, “of” and “and” in English.\n\nIt is typically pretty safe to remove these and you can find them in pre-made lists of stop words (see below)\n\nSubject-specific stop words are words that are common and uninformative given the subject or context within which your text/documents were collected and your modeling goals.\n\n\nFor example, given our goal to classify movie reviews as positive or negative, subject-specific stop words might include:\n\nmovie\nfilm\nmovies\n\n\nWe likely we see others if we expand our review of commons words a bit more (which we should!)\n\ncharacter\nactor\nactress\ndirector\ncast\nscene\n\nThese are not general stop words but they will be common in this dataset and the **may* be uninformative RE our classification goal\n\nSubject-specific stop words may improve performance if you have the domain expertise to create a good list\n\nHOWEVER, you should think carefully about your goals and method. For example, if you are using bigrams rather than single word (unigram) tokens, you might retain words like actor or director because them may be informative in bigrams\n\nbad actor\ngreat director\n\n\n\nThough it might be sufficient to just retain bad and great\n\nThe stopwords package contains many lists of stopwords.\n\nWe can access these lists using through that package\nThose lists are also available with get_stopwords() in the tidytext package (my preference)\nget_stopwords() returns a tibble with two columns (see below)\n\n\nTwo commonly used stop word lists are:\n\nsnowball (175 words)\n\n\nstop_snowball &lt;- \n  get_stopwords(source = \"snowball\") |&gt; \n  print(n = 50)  # review the first 50 words\n\n# A tibble: 175 × 2\n   word       lexicon \n   &lt;chr&gt;      &lt;chr&gt;   \n 1 i          snowball\n 2 me         snowball\n 3 my         snowball\n 4 myself     snowball\n 5 we         snowball\n 6 our        snowball\n 7 ours       snowball\n 8 ourselves  snowball\n 9 you        snowball\n10 your       snowball\n11 yours      snowball\n12 yourself   snowball\n13 yourselves snowball\n14 he         snowball\n15 him        snowball\n16 his        snowball\n17 himself    snowball\n18 she        snowball\n19 her        snowball\n20 hers       snowball\n21 herself    snowball\n22 it         snowball\n23 its        snowball\n24 itself     snowball\n25 they       snowball\n26 them       snowball\n27 their      snowball\n28 theirs     snowball\n29 themselves snowball\n30 what       snowball\n31 which      snowball\n32 who        snowball\n33 whom       snowball\n34 this       snowball\n35 that       snowball\n36 these      snowball\n37 those      snowball\n38 am         snowball\n39 is         snowball\n40 are        snowball\n41 was        snowball\n42 were       snowball\n43 be         snowball\n44 been       snowball\n45 being      snowball\n46 have       snowball\n47 has        snowball\n48 had        snowball\n49 having     snowball\n50 do         snowball\n# ℹ 125 more rows\n\n\n\n\nsmart (571 words)\n\n\nstop_smart &lt;-\n  get_stopwords(source = \"smart\") |&gt; \n  print(n = 50)  # review the first 50 words\n\n# A tibble: 571 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           smart  \n 2 a's         smart  \n 3 able        smart  \n 4 about       smart  \n 5 above       smart  \n 6 according   smart  \n 7 accordingly smart  \n 8 across      smart  \n 9 actually    smart  \n10 after       smart  \n11 afterwards  smart  \n12 again       smart  \n13 against     smart  \n14 ain't       smart  \n15 all         smart  \n16 allow       smart  \n17 allows      smart  \n18 almost      smart  \n19 alone       smart  \n20 along       smart  \n21 already     smart  \n22 also        smart  \n23 although    smart  \n24 always      smart  \n25 am          smart  \n26 among       smart  \n27 amongst     smart  \n28 an          smart  \n29 and         smart  \n30 another     smart  \n31 any         smart  \n32 anybody     smart  \n33 anyhow      smart  \n34 anyone      smart  \n35 anything    smart  \n36 anyway      smart  \n37 anyways     smart  \n38 anywhere    smart  \n39 apart       smart  \n40 appear      smart  \n41 appreciate  smart  \n42 appropriate smart  \n43 are         smart  \n44 aren't      smart  \n45 around      smart  \n46 as          smart  \n47 aside       smart  \n48 ask         smart  \n49 asking      smart  \n50 associated  smart  \n# ℹ 521 more rows\n\n\n\nsmart is mostly a super-set of snowball except for these words which are only in snowball\n\nStop word lists aren’t perfect. Why does smart contain he's but not she's?\n\nsetdiff(pull(stop_snowball, word),\n        pull(stop_smart, word))\n\n [1] \"she's\"   \"he'd\"    \"she'd\"   \"he'll\"   \"she'll\"  \"shan't\"  \"mustn't\"\n [8] \"when's\"  \"why's\"   \"how's\"  \n\n\n\nIt is common and appropriate to start with a pre-made word list or set of lists and combine, add, and/or remove words based on your specific needs\n\nYou can add global words that you feel are missed\nYou can add subject specific words\nYou can remove global words that might be relevant to your problem\n\n\nIn the service of simplicity, we will use the union of the two previous pre-made global lists without any additional subject specific lists\n\nall_stops &lt;- union(pull(stop_snowball, word), pull(stop_smart, word))\n\n\nWe can remove stop words as part of tokenization using stopwords = all_stops\n\nLet’s see our two 100 tokens now\n\ndata_trn |&gt; \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE,\n                stopwords = all_stops) |&gt; \n  count(word) |&gt; \n  arrange(desc(n)) |&gt; \n  print(n = 100)\n\n# A tibble: 83,822 × 2\n    word            n\n    &lt;chr&gt;       &lt;int&gt;\n  1 movie       43509\n  2 film        39058\n  3 good        15110\n  4 time        12691\n  5 story       11919\n  6 bad          9292\n  7 people       9107\n  8 great        9033\n  9 made         8351\n 10 make         8018\n 11 movies       7648\n 12 characters   7142\n 13 watch        6959\n 14 films        6881\n 15 character    6701\n 16 plot         6563\n 17 life         6560\n 18 acting       6482\n 19 love         6421\n 20 show         6171\n 21 end          5640\n 22 man          5630\n 23 scene        5356\n 24 scenes       5206\n 25 back         4965\n 26 real         4734\n 27 watching     4597\n 28 years        4508\n 29 thing        4498\n 30 actors       4476\n 31 work         4368\n 32 funny        4278\n 33 makes        4204\n 34 director     4184\n 35 find         4129\n 36 part         4020\n 37 lot          3965\n 38 cast         3816\n 39 world        3698\n 40 things       3685\n 41 pretty       3663\n 42 young        3634\n 43 horror       3578\n 44 fact         3521\n 45 big          3471\n 46 long         3441\n 47 thought      3434\n 48 series       3410\n 49 give         3374\n 50 original     3358\n 51 action       3351\n 52 comedy       3230\n 53 times        3223\n 54 point        3218\n 55 role         3175\n 56 interesting  3125\n 57 family       3109\n 58 bit          3052\n 59 music        3045\n 60 script       3007\n 61 guy          2962\n 62 making       2960\n 63 feel         2947\n 64 minutes      2944\n 65 performance  2887\n 66 kind         2780\n 67 girl         2739\n 68 tv           2732\n 69 worst        2730\n 70 day          2711\n 71 fun          2690\n 72 hard         2666\n 73 woman        2651\n 74 played       2586\n 75 found        2571\n 76 screen       2474\n 77 set          2452\n 78 place        2403\n 79 book         2394\n 80 put          2379\n 81 ending       2351\n 82 money        2351\n 83 true         2329\n 84 sense        2320\n 85 reason       2316\n 86 actor        2312\n 87 shows        2304\n 88 dvd          2282\n 89 worth        2274\n 90 job          2270\n 91 year         2268\n 92 main         2264\n 93 watched      2235\n 94 play         2222\n 95 american     2217\n 96 plays        2214\n 97 effects      2196\n 98 takes        2192\n 99 beautiful    2176\n100 house        2171\n# ℹ 83,722 more rows\n\n\n\nWhat if we were doing bigrams instead?\n\ntoken = “ngrams”\nn = 2,\nn_min = 2\nNOTE: can’t strip numeric (would break the sequence of words)\nNOTE: There are more bigrams than unigrams (more features for BoW!)\n\n\ndata_trn |&gt; \n  unnest_tokens(word, text, \n                drop = FALSE,\n                token = \"ngrams\",\n                stopwords = all_stops, \n                n = 2,\n                n_min = 2) |&gt; \n  count(word) |&gt; \n  arrange(desc(n)) |&gt; \n  print(n = 100)\n\n# A tibble: 1,616,477 × 2\n    word                      n\n    &lt;chr&gt;                 &lt;int&gt;\n  1 special effects        1110\n  2 low budget              881\n  3 waste time              793\n  4 good movie              785\n  5 watch movie             695\n  6 movie made              693\n  7 sci fi                  647\n  8 years ago               631\n  9 real life               617\n 10 film made               588\n 11 movie good              588\n 12 movie movie             561\n 13 pretty good             557\n 14 bad movie               552\n 15 high school             545\n 16 watching movie          534\n 17 movie bad               512\n 18 main character          509\n 19 good film               487\n 20 great movie             473\n 21 horror movie            468\n 22 horror film             454\n 23 long time               448\n 24 make movie              445\n 25 film making             417\n 26 film good               412\n 27 worth watching          406\n 28 10 10                   404\n 29 movie great             388\n 30 bad acting              387\n 31 worst movie             386\n 32 black white             385\n 33 main characters         381\n 34 end movie               380\n 35 film film               368\n 36 takes place             360\n 37 great film              358\n 38 camera work             356\n 39 make sense              348\n 40 good job                347\n 41 story line              347\n 42 watch film              344\n 43 movie watch             343\n 44 character development   341\n 45 supporting cast         338\n 46 1 2                     334\n 47 love story              334\n 48 read book               332\n 49 bad guys                327\n 50 end film                320\n 51 8 10                    318\n 52 horror movies           318\n 53 make film               317\n 54 made movie              315\n 55 good thing              307\n 56 7 10                    305\n 57 world war               289\n 58 bad film                288\n 59 horror films            285\n 60 watched movie           285\n 61 thing movie             283\n 62 1 10                    280\n 63 part movie              278\n 64 watching film           277\n 65 bad guy                 274\n 66 4 10                    273\n 67 made film               272\n 68 rest cast               268\n 69 tv series               268\n 70 writer director         267\n 71 time movie              266\n 72 half hour               265\n 73 production values       265\n 74 film great              262\n 75 highly recommend        261\n 76 makes sense             256\n 77 martial arts            256\n 78 love movie              255\n 79 science fiction         255\n 80 acting bad              254\n 81 tv movie                253\n 82 recommend movie         251\n 83 3 10                    246\n 84 entire movie            245\n 85 film makers             245\n 86 9 10                    242\n 87 movie time              242\n 88 fun watch               238\n 89 kung fu                 238\n 90 film bad                235\n 91 good acting             235\n 92 true story              233\n 93 movie make              232\n 94 movies made             232\n 95 point view              232\n 96 film festival           231\n 97 great job               227\n 98 young woman             227\n 99 good story              225\n100 star wars               224\n# ℹ 1,616,377 more rows\n\n\n\n\nLooks like we are starting to get some signal",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#stemming",
    "href": "012_nlp.html#stemming",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "12.6 Stemming",
    "text": "12.6 Stemming\nDocuments often contain different versions of one base word\n\nWe refer to the common base as the stem\n\nOften, we may want to treat the different versions of the stem as the same token. This can reduce the total number of tokens that we need to use for features later which can lead to a better performing model\n\nFor example, do we need to distinguish between movie vs. movies or actor vs. actors or should we collapse those pairs into a single token?\n\nThere are many different algorithms that can stem words for us (i.e., collapse multiple versions into the same base). However, we will focus on only one here as an introduction to the concept and approach for stemming\n\nThis is the Porter method and a current implementation of it is available in the using wordStem() in the SnowballC package\n\nThe goal of stemming is to reduce the dimensionality (size) of our vocabulary\n\nWhenever we can combine words that “belong” together with respect to our goal, we may improve the performance of our model\n\nHowever, stemming is hard and it will also invariably combine words that shouldn’t be combined\n\nStemming is useful when it suceeds more than it fails or when it succees more with important words/tokens\n\nHere are examples of when it helps to reduce our vocabulary given our task\n\nwordStem(c(\"movie\", \"movies\"))\n\n[1] \"movi\" \"movi\"\n\nwordStem(c(\"actor\", \"actors\", \"actress\", \"actresses\"))\n\n[1] \"actor\"   \"actor\"   \"actress\" \"actress\"\n\nwordStem(c(\"wait\", \"waits\", \"waiting\", \"waited\"))\n\n[1] \"wait\" \"wait\" \"wait\" \"wait\"\n\nwordStem(c(\"play\", \"plays\", \"played\", \"playing\"))\n\n[1] \"plai\" \"plai\" \"plai\" \"plai\"\n\n\n\nSometimes it works partially, likely with still some benefit\n\nwordStem(c(\"go\", \"gone\", \"going\"))\n\n[1] \"go\"   \"gone\" \"go\"  \n\nwordStem(c(\"send\", \"sending\", \"sent\"))\n\n[1] \"send\" \"send\" \"sent\"\n\nwordStem(c(\"fishing\", \"fished\", \"fisher\"))\n\n[1] \"fish\"   \"fish\"   \"fisher\"\n\n\n\nBut it clearly makes salient errors too\n\nwordStem(c(\"university\", \"universal\", \"universe\"))\n\n[1] \"univers\" \"univers\" \"univers\"\n\nwordStem(c(\"is\", \"are\", \"was\"))\n\n[1] \"i\"  \"ar\" \"wa\"\n\nwordStem(c(\"he\", \"his\", \"him\"))\n\n[1] \"he\"  \"hi\"  \"him\"\n\nwordStem(c(\"like\", \"liking\", \"likely\"))\n\n[1] \"like\" \"like\" \"like\"\n\nwordStem(c(\"mean\", \"meaning\"))\n\n[1] \"mean\" \"mean\"\n\n\n\nOf course, the errors are more important if they are with words that contain predictive signal\n\nTherefore, we should look at how it works with our text\n\nTo stem our tokens if we only care about unigrams:\n\nWe first tokenize as before (including removal of stop words)\nThen we stem, for now using wordStem()\nWe are mutating the stemmed words into a new column, stem, so we can compare its effect\n\n\ntokens &lt;- \n  data_trn |&gt; \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE,\n                stopwords = all_stops) |&gt; \n  mutate(stem = wordStem(word)) |&gt; \n  select(word, stem)\n\n\nLet’s compare vocabulary size  \nStemming produced a sizable reduction in vocabulary size\n\nlength(unique(tokens$word))\n\n[1] 83822\n\nlength(unique(tokens$stem))\n\n[1] 60813\n\n\n\nLet’s compare frequencies of the top 100 words vs. stems\n\nHere are our normal (not stemmed words). Notice vocabulary size\n\n\nword_tokens &lt;-\n  tokens |&gt; \n  tab(word) |&gt; \n  arrange(desc(n)) |&gt; \n  slice(1:100) |&gt; \n  select(word, n_word = n)\n\nstem_tokens &lt;-\n  tokens |&gt; \n  tab(stem) |&gt; \n  arrange(desc(n)) |&gt; \n  slice(1:100) |&gt; \n  select(stem, n_stem = n)\n\nword_tokens |&gt; \n  bind_cols(stem_tokens)\n\n# A tibble: 100 × 4\n   word   n_word stem    n_stem\n   &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;    &lt;int&gt;\n 1 movie   43509 movi     51159\n 2 film    39058 film     47096\n 3 good    15110 time     16146\n 4 time    12691 good     15327\n 5 story   11919 make     15203\n 6 bad      9292 watch    13920\n 7 people   9107 charact  13844\n 8 great    9033 stori    13104\n 9 made     8351 scene    10563\n10 make     8018 show      9750\n# ℹ 90 more rows\n\n\n\nStemming is often routinely used as part of an NLP pipeline.\n\nOften without thought\nWe should consider carefully if it will help or hurt\nWe should likely formally evaluate it (via model configurations), sometimes without much comment about when it is helpful or not. We encourage you to think of stemming as a pre-processing step in text modeling, one that must be thought through and chosen (or not) with good judgment.\n\n\nIn this example, we focused on unigrams.\n\n\nIf we had wanted bigrams, we would have needed a different order of steps\n\nExtract words\nStem\nPut back together\nExtract bigrams\nRemove stop words\n\n\n\nThink carefully about what you are doing and what your goals are!\n\nYou can read more about stemming and related (more complicated but possibly more precise) procedure called lemmazation in a chapter from Hvitfeldt and Silge (2022)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#bag-of-words",
    "href": "012_nlp.html#bag-of-words",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "12.7 Bag of Words",
    "text": "12.7 Bag of Words\nNow that we understand how to tokenize our documents, we can begin to consider how to feature engineer using these tokens\n\nThe Bag-of-words (BoW) method\n\nProvides one way of representing tokens within text data when modeling text with machine learning algorithms.\nIs simple to understand and implement\nWorks well for problems such as document classification\n\n\nBoW is a representation of text that describes the occurrence of words within a document. It involves two things:\n\nA vocabulary of known “words” (I put words in quote because our tokens will sometimes be something other than a word)\nA measure of the occurrence or frequency of these known words.\n\n\n\nIt is called a “bag” of words because information about the order or structure of words in the document is discarded. BoW is only concerned with occurrence or frequency of known words in the document, not where in the document they occur.\n\nBoW assumes that documents that contain the same content are similar and that we can learn something about the document by its content alone.\n\nBoW approaches vary on two primary characteristics:\n\nWhat the token type is\n\nWord is most common\nAlso common to use bigrams, combinations of unigrams (words) and bigrams\nOther options exist (e.g.,trigram)\n\nHow occurrence frequent of the word/token is measured\n\nBinary (presence or absence)\nRaw count\nTerm frequency\nTerm frequency - inverse document frequency (tf-idf)\nand other less common options\n\n\n\nLets start with a very simple example\n\nTwo documents\nTokenization to words\nLowercase, strip punctuation, did not remove any stopwords, no stemming or lemmatization\nBinary measurement (1 = yes, 0 = no)\n\n\n\n\n\n\n\n\nDocument\ni\nloved\nthat\nmovie\nam\nso\nhappy\nwas\nnot\ngood\n\n\n\n\nI loved that movie! I am so so so happy.\n1\n1\n1\n1\n1\n1\n1\n0\n0\n0\n\n\nThat movie was not good.\n0\n0\n1\n1\n0\n0\n0\n1\n1\n1\n\n\n\n\n\n\n\n\n\nThis matrix is referred to as a Document-Term Matrix (DTM)\n\nRows are documents\nColumns are terms (tokens)\nCombination of all terms/tokens is called the vocabulary\nThe terms columns (or a subset) will be used as features in our statistical algorithm to predict some outcome (not displayed)\n\n\nYou will also see the use of raw counts for measurement of the cell value for each term\n\n\n\n\n\n\n\nDocument\ni\nloved\nthat\nmovie\nam\nso\nhappy\nwas\nnot\ngood\n\n\n\n\nI loved that movie! I am so so so happy.\n2\n1\n1\n1\n1\n3\n1\n0\n0\n0\n\n\nThat movie was not good.\n0\n0\n1\n1\n0\n0\n0\n1\n1\n1\n\n\n\n\n\n\n\n\nBoth binary and raw count measures are biased (increased) for longer documents\n\nThe bias based on document length motivates the use of term frequency\n\nTerm frequency is NOT a simple frequency count (that is raw count)\nInstead it is the raw count divided by the document length\nThis removes the bias based on document length\n\n\n## echo: false\n\ntibble::tribble(\n  ~Document, ~i, ~loved, ~that, ~movie, ~am, ~so, ~happy, ~was, ~not, ~good,\n  \"I loved that movie! I am so so so happy.\", .2, .1, .1, .1, .1, .3, .1, 0, 0, 0,\n  \"That movie was not good.\", 0, 0, .2, .2, 0, 0, 0, .2, .2, .2) |&gt;\n  print_kbl()\n\n\n\n\n\n\nDocument\ni\nloved\nthat\nmovie\nam\nso\nhappy\nwas\nnot\ngood\n\n\n\n\nI loved that movie! I am so so so happy.\n0.2\n0.1\n0.1\n0.1\n0.1\n0.3\n0.1\n0.0\n0.0\n0.0\n\n\nThat movie was not good.\n0.0\n0.0\n0.2\n0.2\n0.0\n0.0\n0.0\n0.2\n0.2\n0.2\n\n\n\n\n\n\n\n\n\nTerm frequency can be dominated by frequently occurring words that may not be as important to understand the document as are rarer but more domain specific words\n\nThis was the motivation for removing stopwords but stopword removal may not be sufficient\n\nTerm Frequency - Inverse Document Frequency (tf-idf) was developed to address this issue\n\nTF-IDF scales the term frequency by the inverse document frequency\n\nTerm frequency tells us that word is frequently used in the current document\nIDF indexes how rare the word is across documents (higer IDF == more rare)\n\n\n\nThis emphasizes words used in specific documents that are not commonly used otherwise\n\n\\(IDF = log(\\frac{total\\:number\\:of\\:documents}{documents\\:containing\\:the\\:word})\\)\n\nThis results in larger values for words that arent used in many documents. e.g.,\n\nfor a word that appears in only 1 of 1000 documents, idf = log(1000/1) = 3\nfor a word that appears in 1000 of 1000 documents, idf = log(1000/1000) = 0\n\n\n\nNote that word that appears in no documents would result in a division by zero. Therefore it is common to add 1 to the denominator of idf\n\nWe should be aware of some of the limitations of BoW:\n\nVocabulary: The vocabulary requires careful thought\n\nChoice of stop words (global and subject specific) can matter\nChoice about size (number of tokens) can matter\n\nSparsity:\n\nSparse representations (features that contain mostly zeros) are hard to model for computational reasons (space and time)\nSparse representations present a challenge to extract signal in a large representational space\n\nMeaning: Discarding word order ignores the context, and in turn meaning of words in the document (semantics).\n\nContext and meaning can offer a lot to the model\n“this is interesting” vs “is this interesting”\n“old bike” vs “used bike”",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#bringing-it-all-together",
    "href": "012_nlp.html#bringing-it-all-together",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "12.8 Bringing it all together",
    "text": "12.8 Bringing it all together\n\n12.8.1 General\nWe will now explore a series of model configurations to predict the sentiment (positive vs negative) of IMDB.com reviews\n\nTo keep things simple, all model configurations will use only one statistical algorithm - glmnet\n\nWithin algorithm, configurations will differ by penatly and `dials::mixture’ - see grid below\n\nWe will consider BoW features derived by TF-IDF only\nWe will remove global stop words from some configurations\nWe will stem words from some configurations\nThese BoW configurations will also differ by token type\n\nWord/unigram\nA combination of uni- and bigrams\nPrior to casting the tf-idf document-term matrix, we will filter tokens to various sizes - see grid below\n\nWe will also train a word embedding model to demonstrate how to implement that feature engineering technique in tidymodels\n\n\nWe are applying these feature engineering steps blindly. YOU should not.\n\nYou will want to explore the impact of your feature engineering either\n\nDuring EDA with unnest_tokens()\nAfter making your recipe by using it to make a feature matrix\nLikely some of both\n\n\nLets start fresh with our training data\n\ndata_trn &lt;- read_csv(here::here(path_data, \"imdb_trn.csv\"),\n                     show_col_types = FALSE) |&gt; \n  rowid_to_column(var = \"doc_num\") |&gt; \n  mutate(sentiment = factor(sentiment, levels = c(\"neg\", \"pos\"))) |&gt; \n  glimpse()\n\nRows: 25,000\nColumns: 3\n$ doc_num   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ sentiment &lt;fct&gt; neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, …\n$ text      &lt;chr&gt; \"Story of a man who has unnatural feelings for a pig. Starts…\n\n\n\nAnd do our (very minimal) cleaning\n\ndata_trn &lt;- data_trn |&gt; \n  mutate(text = str_replace_all(text, \"&lt;br /&gt;&lt;br /&gt;\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n\n\nWe will select among model configurations using a validation split resampled accuracy to ease computational costs\n\nset.seed(123456)\nsplits &lt;- data_trn |&gt; \n  validation_split(strata = sentiment)\n\n\nWe will use a simple union of stop words for some configurations:\n\nIt could help to edit the global list (or use a smaller list like snowball only)\nIt could help to have subject specific stop words too AND they might need to be different for words vs. bigrams\n\n\nall_stops &lt;- union(pull(get_stopwords(source = \"smart\"), word), pull(get_stopwords(source = \"snowball\"), word))\n\n\nAll of our model configurations will be tuned on penalty, mixture and max_tokens\n\ngrid_tokens &lt;- grid_regular(penalty(range = c(-7, 3)), \n                            mixture(), \n                            max_tokens(range = c(5000, 10000)), \n                            levels = c(20, 6, 2))\n\n\n\n12.8.2 Words (unigrams)\nWe will start by fitting a BoW model configuration for word tokens\n\nRecipe for Word Tokens. NOTE:\n\ntoken = \"words\" (default)\nmax_tokens = tune() - We are now set to tune recipes!!! options = list(stopwords = all_stops) - Passing in options to tokenizers::tokenize_words()\n\n\nrec_word &lt;-  recipe(sentiment ~ text, data = data_trn) |&gt;\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\") |&gt;    \n  step_tokenfilter(text, max_tokens = tune::tune()) |&gt;\n  step_tfidf(text)  |&gt; \n  step_normalize(all_predictors())\n\n\nTuning hyperparameters for Word Tokens\n\nfits_word &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), \n                 mixture = tune::tune()) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      tune_grid(preprocessor = rec_word, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_word\")\n\n\nConfirm that the range of hyperparameters we considered was sufficient\n\nautoplot(fits_word)\n\n\n\n\n\n\n\n\n\nDisplay performance of best configuration for Word Tokens.\n\n\nWow, pretty good!\n\nshow_best(fits_word)\n\n# A tibble: 5 × 9\n  penalty mixture max_tokens .metric  .estimator  mean     n std_err\n    &lt;dbl&gt;   &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 0.207       0         5000 accuracy binary     0.896     1      NA\n2 0.0183      0.2      10000 accuracy binary     0.896     1      NA\n3 0.00546     0.4      10000 accuracy binary     0.895     1      NA\n4 0.00546     0.6      10000 accuracy binary     0.895     1      NA\n5 0.207       0        10000 accuracy binary     0.893     1      NA\n  .config               \n  &lt;chr&gt;                 \n1 Preprocessor1_Model013\n2 Preprocessor2_Model031\n3 Preprocessor2_Model050\n4 Preprocessor2_Model070\n5 Preprocessor2_Model013\n\n\n\n\n\n12.8.3 Words (unigrams) Excluding Stop Words\nLets try a configuration that removes stop words\n\nRecipe for Word Tokens excluding Stop Words. NOTE:\n\noptions = list(stopwords = all_stops) - Passing in options to tokenizers::tokenize_words()\n\n\nrec_word_nsw &lt;-  recipe(sentiment ~ text, data = data_trn) |&gt;\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\",   # included for clarity\n                options = list(stopwords = all_stops)) |&gt;\n  step_tokenfilter(text, max_tokens = tune::tune()) |&gt;\n  step_tfidf(text)  |&gt; \n  step_normalize(all_predictors())\n\n\nTuning hyperparameters for Word Tokens excluding Stop Words\n\nfits_word_nsw &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), \n                 mixture = tune::tune()) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      tune_grid(preprocessor = rec_word_nsw, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_word_nws\")\n\n\nConfirm that the range of hyperparameters we considered was sufficient\n\nautoplot(fits_word_nsw)\n\n\n\n\n\n\n\n\n\nDisplay performance of best configuration for Word Tokens excluding Stop Words.\n\nLooks like our mindless use of a large list of global stop words didn’t help.\n\nWe might consider the more focused snowball list\nWe might consider subject specific stop words\nFor this demonstration, we will just move forward without stop words\n\n\nshow_best(fits_word_nsw)\n\n# A tibble: 5 × 9\n  penalty mixture max_tokens .metric  .estimator  mean     n std_err\n    &lt;dbl&gt;   &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 0.00546     0.6      10000 accuracy binary     0.880     1      NA\n2 0.0183      0.2      10000 accuracy binary     0.879     1      NA\n3 0.00546     0.4       5000 accuracy binary     0.879     1      NA\n4 0.0183      0.2       5000 accuracy binary     0.878     1      NA\n5 0.00162     1         5000 accuracy binary     0.877     1      NA\n  .config               \n  &lt;chr&gt;                 \n1 Preprocessor2_Model070\n2 Preprocessor2_Model031\n3 Preprocessor1_Model050\n4 Preprocessor1_Model031\n5 Preprocessor1_Model109\n\n\n\n\n\n12.8.4 Stemmed Words (unigrams)\nNow we will try using stemmed words\n\nRecipe for Stemmed Word Tokens. NOTE: step_stem()\n\nrec_stemmed_word &lt;-  recipe(sentiment ~ text, data = data_trn) |&gt;\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\") |&gt;    # included for clarity\n  step_stem(text) |&gt; \n  step_tokenfilter(text, max_tokens = tune()) |&gt;\n  step_tfidf(text)  |&gt; \n  step_normalize(all_predictors())\n\n\nTuning hyperparameters for Stemmed Word Tokens\n\nfits_stemmed_word &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), mixture = tune()) |&gt; \n    set_engine(\"glmnet\") |&gt; \n    tune_grid(preprocessor = rec_stemmed_word, \n              resamples = splits, \n              grid = grid_tokens, \n              metrics = metric_set(accuracy))\n},\n  rerun = rerun_setting, \n  dir = \"cache/012/\",\n  file = \"fits_stemmed_word\")\n\n\nConfirm that the range of hyperparameters we considered was sufficient\n\nautoplot(fits_stemmed_word)\n\n\n\n\n\n\n\n\n\nDisplay performance of best configuration for Stemmed Word Tokens\n\nNot much change\n\nshow_best(fits_stemmed_word)\n\n# A tibble: 5 × 9\n  penalty mixture max_tokens .metric  .estimator  mean     n std_err\n    &lt;dbl&gt;   &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 0.207       0        10000 accuracy binary     0.896     1      NA\n2 0.695       0        10000 accuracy binary     0.896     1      NA\n3 0.0183      0.2      10000 accuracy binary     0.896     1      NA\n4 0.00546     0.4      10000 accuracy binary     0.895     1      NA\n5 0.00546     0.6      10000 accuracy binary     0.894     1      NA\n  .config               \n  &lt;chr&gt;                 \n1 Preprocessor2_Model013\n2 Preprocessor2_Model014\n3 Preprocessor2_Model031\n4 Preprocessor2_Model050\n5 Preprocessor2_Model070\n\n\n\n\n\n12.8.5 ngrams (unigrams and bigrams)\nNow we try both unigrams and bigrams\n\nRecipe for unigrams and bigrams. NOTES:\n\ntoken = \"ngrams\"\noptions = list(n = 2, n_min = 1) - includes uni (1) and bi(2) grams\nnot stemmed (stemming doesn’t work by default for bigrams)\n\n\nrec_ngrams &lt;-  recipe(sentiment ~ text, data = data_trn) |&gt;\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"ngrams\",\n                options = list(n = 2, n_min = 1)) |&gt;\n  step_tokenfilter(text, max_tokens =  tune::tune()) |&gt;\n  step_tfidf(text) |&gt; \n  step_normalize(all_predictors())\n\n\nTuning hyperparameters for ngrams\n\nfits_ngrams &lt;- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), mixture = tune::tune()) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      tune_grid(preprocessor = rec_ngrams, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(yardstick::accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_ngrams\")\n\n\nConfirm that the range of hyperparameters we considered was sufficient\n\nautoplot(fits_ngrams)\n\n\n\n\n\n\n\n\n\nDisplay performance of best configuration\n\nOur best model yet!\n\nshow_best(fits_ngrams)\n\n# A tibble: 5 × 9\n  penalty mixture max_tokens .metric  .estimator  mean     n std_err\n    &lt;dbl&gt;   &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 0.207       0        10000 accuracy binary     0.901     1      NA\n2 0.695       0        10000 accuracy binary     0.900     1      NA\n3 0.0183      0.2      10000 accuracy binary     0.900     1      NA\n4 0.00546     0.4      10000 accuracy binary     0.898     1      NA\n5 0.00546     0.6      10000 accuracy binary     0.897     1      NA\n  .config               \n  &lt;chr&gt;                 \n1 Preprocessor2_Model013\n2 Preprocessor2_Model014\n3 Preprocessor2_Model031\n4 Preprocessor2_Model050\n5 Preprocessor2_Model070\n\n\n\n\n\n12.8.6 Word Embeddings\nBoW is an introductory approach for feature engineering.\n\n\nAs you have read, word embeddings are a common alternative that addresses some of the limitations of BoW. Word embeddings are also well-support in the tidyrecipes package.\n\nLet’s switch gears away from document term matrices and BoW to word embeddings\n\nYou can find pre-trained word embeddings on the web\n\nGloVe\nWord2Vec\nFasttek\n\nBelow, we download and open pre-trained GloVe embeddings\n\nI chose a smaller set of embeddings to ease computational cost\n\nWikipedia 2014 + Gigaword 5\n6B tokens, 400K vocab, uncased, 50d\n\n\n\ntemp &lt;- tempfile()\noptions(timeout = max(300, getOption(\"timeout\")))  # need more time to download big file\ndownload.file(\"https://nlp.stanford.edu/data/glove.6B.zip\", temp)\nunzip(temp, files = \"glove.6B.50d.txt\")\nglove_embeddings &lt;- read_delim(here::here(\"glove.6B.50d.txt\"),\n                               delim = \" \",\n                               col_names = FALSE) \n\nRows: 8 Columns: 51\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr  (1): X1\ndbl (50): X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nRecipe for GloVe embedding. NOTES:\n\ntoken = \"words\"\nNo need to filter tokens\nNew step is step_word_embeddings(text, embeddings = glove_embeddings)\n\n\n# rec_glove &lt;-  recipe(sentiment ~ text, data = data_trn) |&gt;\n#   step_tokenize(text, \n#                 engine = \"tokenizers\", token = \"words\",   # included for clarity\n#                 options = list(stopwords = all_stops)) |&gt;\n#   step_word_embeddings(text, embeddings = glove_embeddings)\n\n\nHyperparameter grid for GloVe embedding (no need for max_tokens)\n\n# grid_glove &lt;- grid_regular(penalty(range = c(-7, 3)), \n#                             dials::mixture(), \n#                             levels = c(20, 6))\n\n\nTuning hyperparameters for GloVe embedding\n\n# fits_glove &lt;-\n#   logistic_reg(penalty = tune(), \n#              mixture = tune()) |&gt; \n#   set_engine(\"glmnet\") |&gt; \n#   tune_grid(preprocessor = rec_glove, \n#             resamples = splits, \n#             grid = grid_glove, \n#             metrics = metric_set(accuracy))\n\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n# autoplot(fits_glove)\n\n\nDisplay performance of best configuration for GloVe embedding\n\n# show_best(fits_glove)\n\n\n\n\n12.8.7 Best Configuration\n\nFit best model configuration to training set\n\n\nrec_final &lt;-\n  recipe(sentiment ~ text, data = data_trn) |&gt;\n    step_tokenize(text, \n                  engine = \"tokenizers\", token = \"ngrams\",\n                  options = list(n = 2, n_min = 1)) |&gt;\n    step_tokenfilter(text, max_tokens = select_best(fits_ngrams)$max_tokens) |&gt;\n    step_tfidf(text) |&gt; \n    step_normalize(all_predictors()) \n\n\nrec_final_prep &lt;- rec_final |&gt;\n  prep(data_trn)\n\nWarning in asMethod(object): sparse-&gt;dense coercion: allocating vector of size\n1.9 GiB\n\nfeat_trn &lt;- rec_final_prep |&gt; \n  bake(NULL) \n\n\n\nfit_final &lt;-\n  logistic_reg(penalty = select_best(fits_ngrams)$penalty, \n             mixture = select_best(fits_ngrams)$mixture) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  fit(sentiment ~ ., data = feat_trn)\n\n\n\n\nOpen and clean test to make features\n\n\ndata_test &lt;- read_csv(here::here(path_data, \"imdb_test.csv\"),\n                      show_col_types = FALSE) |&gt; \n  rowid_to_column(var = \"doc_num\") |&gt; \n  mutate(sentiment = factor(sentiment, levels = c(\"neg\", \"pos\"))) |&gt; \n  glimpse()\n\nRows: 25,000\nColumns: 3\n$ doc_num   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ sentiment &lt;fct&gt; neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, …\n$ text      &lt;chr&gt; \"Once again Mr. Costner has dragged out a movie for far long…\n\ndata_test &lt;- data_test |&gt; \n  mutate(text = str_replace_all(text, \"&lt;br /&gt;&lt;br /&gt;\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n\nfeat_test &lt;- rec_final_prep |&gt; \n  bake(data_test)\n\nWarning in asMethod(object): sparse-&gt;dense coercion: allocating vector of size\n1.9 GiB\n\n\n\n\nPredict into test\n\n\naccuracy_vec(feat_test$sentiment, predict(fit_final, feat_test)$.pred_class)\n\n[1] 0.89268\n\n\n\nConfusion matrix\n\n\ncm &lt;- tibble(truth = feat_test$sentiment,\n             estimate = predict(fit_final, feat_test)$.pred_class) |&gt; \n  conf_mat(truth, estimate)\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\nAnd lets end by calculating Permutation feature importance scores in test set using DALEX\n\nlibrary(DALEX, exclude= \"explain\")\n\nWelcome to DALEX (version: 2.4.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\n\nlibrary(DALEXtra)\n\n\nWe are going to sample only a subset of the test set to keep the computational costs lower for this example.\n\nset.seed(12345)\nfeat_subtest &lt;- feat_test |&gt; \n  slice_sample(prop = .05) # 5% of data \n\n\nNow we can get a df for the features (without the outcome) and a separate vector for the outcome.\n\nx &lt;- feat_subtest |&gt; select(-sentiment)\n\n\n\nFor outcome, we need to convert to 0/1 (if classification), and then pull the vector out of the dataframe\n\ny &lt;- feat_subtest |&gt; \n  mutate(sentiment = if_else(sentiment == \"pos\", 1, 0)) |&gt; \n  pull(sentiment)\n\n\nWe also need a specific predictor function that will work with the DALEX package\n\npredict_wrapper &lt;- function(model, newdata) {\n  predict(model, newdata, type = \"prob\") |&gt; \n    pull(.pred_pos)\n}\n\n\nWe will also need an explainer object based on our model and data\n\nexplain_test &lt;- explain_tidymodels(fit_final, # our model object \n                                   data = x, # df with features without outcome\n                                   y = y, # outcome vector\n                                   # our custom predictor function\n                                   predict_function = predict_wrapper)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  model_fit  (  default  )\n  -&gt; data              :  1250  rows  10000  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1250  values \n  -&gt; predict function  :  predict_function \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package parsnip , ver. 1.1.0.9004 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.0003611459 , mean =  0.5143743 , max =  0.9999001  \n  -&gt; residual function :  residual_function \n  -&gt; residuals         :  numerical, min =  0 , mean =  0 , max =  0  \n  A new explainer has been created!  \n\n\n\nFinally, we need to define a custom function for our performance metric as well\n\naccuracy_wrapper &lt;- function(observed, predicted) {\n  observed &lt;- fct(if_else(observed == 1, \"pos\", \"neg\"),\n                  levels = c(\"pos\", \"neg\"))\n  predicted &lt;- fct(if_else(predicted &gt; .5, \"pos\", \"neg\"), \n                   levels  = c(\"pos\", \"neg\"))\n  accuracy_vec(observed, predicted)\n}\n\n\n\nWe are now ready to calculate feature importance metrics\n\nOnly doing 1 permutation for each feature to keep computational costs lower for this demonstration. In real, life do more!\n\nset.seed(123456)\nimp_permute &lt;- cache_rds(\n  expr = {model_parts(explain_test, \n                      type = \"raw\", \n                      loss_function = accuracy_wrapper,\n                      B = 1)\n  },\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"imp_permute\"\n)\n\n\nPlot top 30 in an informative display\n\nimp_permute |&gt; \n  filter(variable != \"_full_model_\",\n         variable != \"_baseline_\") |&gt; \n  mutate(variable = fct_reorder(variable, dropout_loss)) |&gt; \n  slice_head(n = 30) |&gt; \n  print()\n\n                   variable permutation dropout_loss     label\n1              tfidf_text_2           0        0.902 model_fit\n2        tfidf_text_a great           0        0.902 model_fit\n3         tfidf_text_at all           0        0.902 model_fit\n4            tfidf_text_and           0        0.903 model_fit\n5          tfidf_text_below           0        0.903 model_fit\n6         tfidf_text_decent           0        0.903 model_fit\n7     tfidf_text_especially           0        0.903 model_fit\n8           tfidf_text_true           0        0.903 model_fit\n9      tfidf_text_very good           0        0.903 model_fit\n10        tfidf_text_actors           0        0.904 model_fit\n11           tfidf_text_age           0        0.904 model_fit\n12          tfidf_text_aged           0        0.904 model_fit\n13        tfidf_text_always           0        0.904 model_fit\n14    tfidf_text_an amazing           0        0.904 model_fit\n15     tfidf_text_and while           0        0.904 model_fit\n16       tfidf_text_appears           0        0.904 model_fit\n17         tfidf_text_as he           0        0.904 model_fit\n18         tfidf_text_avoid           0        0.904 model_fit\n19        tfidf_text_beauty           0        0.904 model_fit\n20           tfidf_text_bit           0        0.904 model_fit\n21         tfidf_text_bored           0        0.904 model_fit\n22        tfidf_text_boring           0        0.904 model_fit\n23    tfidf_text_boring and           0        0.904 model_fit\n24       tfidf_text_did not           0        0.904 model_fit\n25 tfidf_text_disappointing           0        0.904 model_fit\n26          tfidf_text_doll           0        0.904 model_fit\n27          tfidf_text_door           0        0.904 model_fit\n28           tfidf_text_dvd           0        0.904 model_fit\n29        tfidf_text_end up           0        0.904 model_fit\n30          tfidf_text_good           0        0.904 model_fit\n\n\n\nimp_permute |&gt; \n  filter(variable != \"_full_model_\",\n         variable != \"_baseline_\") |&gt; \n  mutate(variable = fct_reorder(variable, dropout_loss, \n                                .desc = TRUE)) |&gt; \n  slice_tail(n = 30) |&gt; \n  print()\n\n                  variable permutation dropout_loss     label\n1      tfidf_text_anywhere           0        0.908 model_fit\n2         tfidf_text_as if           0        0.908 model_fit\n3        tfidf_text_as the           0        0.908 model_fit\n4          tfidf_text_best           0        0.908 model_fit\n5       tfidf_text_blatant           0        0.908 model_fit\n6  tfidf_text_disappointed           0        0.908 model_fit\n7       tfidf_text_episode           0        0.908 model_fit\n8    tfidf_text_first half           0        0.908 model_fit\n9         tfidf_text_giant           0        0.908 model_fit\n10       tfidf_text_got to           0        0.908 model_fit\n11     tfidf_text_help the           0        0.908 model_fit\n12     tfidf_text_is still           0        0.908 model_fit\n13      tfidf_text_it when           0        0.908 model_fit\n14         tfidf_text_jack           0        0.908 model_fit\n15         tfidf_text_main           0        0.908 model_fit\n16         tfidf_text_meat           0        0.908 model_fit\n17           tfidf_text_ok           0        0.908 model_fit\n18         tfidf_text_once           0        0.908 model_fit\n19       tfidf_text_one of           0        0.908 model_fit\n20        tfidf_text_other           0        0.908 model_fit\n21      tfidf_text_perfect           0        0.908 model_fit\n22         tfidf_text_ship           0        0.908 model_fit\n23        tfidf_text_shoot           0        0.908 model_fit\n24      tfidf_text_sisters           0        0.908 model_fit\n25 tfidf_text_something to           0        0.908 model_fit\n26    tfidf_text_the lives           0        0.908 model_fit\n27        tfidf_text_tries           0        0.908 model_fit\n28     tfidf_text_tries to           0        0.908 model_fit\n29    tfidf_text_very well           0        0.908 model_fit\n30       tfidf_text_effort           0        0.909 model_fit\n\n\n\n# full_model &lt;- imp_permute |&gt;  \n#    filter(variable == \"_full_model_\")\n  \n# imp_permute |&gt; \n#   filter(variable != \"_full_model_\",\n#          variable != \"_baseline_\") |&gt; \n#   mutate(variable = fct_reorder(variable, dropout_loss)) |&gt; \n#   arrange(desc(dropout_loss) |&gt;\n#   slice(n = 30) |&gt; \n#   ggplot(aes(dropout_loss, variable)) +\n#     geom_vline(data = full_model, aes(xintercept = dropout_loss),\n#                linewidth = 1.4, lty = 2, alpha = 0.7) +\n#    geom_boxplot(fill = \"#91CBD765\", alpha = 0.4) +\n#    theme(legend.position = \"none\") +\n#    labs(x = \"accuracy\", y = NULL)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#announcements",
    "href": "012_nlp.html#announcements",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "13.1 Announcements",
    "text": "13.1 Announcements\n\nLast application assignment done!\nRemaining units\n\nBring it together for applications\nEthics and Algorithmic Fairness\nReading and discussion only for both units. Discussion requires you!\n\nTwo weeks left!\n\nnext tuesday (lab on NLP)\nnext thursday (discussion on applications)\nfollowing tuesday (discussion on ethics/fairness)\nfollowing thursday (concepts final review; 50 minutes only)\n\nEarly start to final application assignment (assigned next thursday, April 25th) and due Wednesday, May 8th at 8pm\nConcepts exam at lab time during finals period (Tuesday, May 7th, 11-12:15 in this room)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#single-nominal-variable",
    "href": "012_nlp.html#single-nominal-variable",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "13.2 Single nominal variable",
    "text": "13.2 Single nominal variable\n\nOur algorithms need features coded with numbers\n\nHow did we generally do this?\nWhat about algorithms like random forest?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#single-nominal-variable-example",
    "href": "012_nlp.html#single-nominal-variable-example",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "13.3 Single nominal variable Example",
    "text": "13.3 Single nominal variable Example\n\nCurrent emotional state\n\nangry, afraid, sad, excited, happy, calm\n\nHow represented with one-hot?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#what-about-document-of-words-e.g.-sentence-rather-than-single-word",
    "href": "012_nlp.html#what-about-document-of-words-e.g.-sentence-rather-than-single-word",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "13.4 What about document of words (e.g., sentence rather than single word)",
    "text": "13.4 What about document of words (e.g., sentence rather than single word)\n\nI watched a scary movie last night and couldn’t get to sleep because I was so afraid\nI went out with my friends to a bar and slept poorly because I drank too much\n\n\n\n\n\nHow represented with bag of words?\nbinary, count, tf-idf\n\n\n\n\n\nWhat are the problems with these approaches\n\nrelationships between features or similarity between full vector across observations\ncontext/order\ndimensionality\nsparsity",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#n-grams-vs.-simple-1-gram-bow",
    "href": "012_nlp.html#n-grams-vs.-simple-1-gram-bow",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "13.5 N-grams vs. simple (1-gram) BOW",
    "text": "13.5 N-grams vs. simple (1-gram) BOW\n\nHow different?\n\n\n\n\n\nsome context/order\nbut still no relationship/meaning or similarity\neven higher dimesions",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#linguistic-inquiry-and-word-count-liwc",
    "href": "012_nlp.html#linguistic-inquiry-and-word-count-liwc",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "13.6 Linguistic Inquiry and Word Count (LIWC)",
    "text": "13.6 Linguistic Inquiry and Word Count (LIWC)\n\nTausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and computerized text analysis methods. Journal of Language and Social Psychology, 29(1), 24–54. https://doi.org/\n\n\n\n\n\nMeaningful features? (based on domain expertise)\nLower dimensional\nVery limited breadth\n\n\n\n- Can add domain specific dictionaries",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#word-embeddings-1",
    "href": "012_nlp.html#word-embeddings-1",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "13.7 Word Embeddings",
    "text": "13.7 Word Embeddings\n\nEncodes word meaning\nWords that are similar get similar vectors\nMeaning derived from context in various text corpora\nLower dimensional\nLess sparse",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#examples",
    "href": "012_nlp.html#examples",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "13.8 Examples",
    "text": "13.8 Examples\n\nAffect Words\n\n\n\n\n\n\n\nTwo more general\nword2vec (google)\n\nCBOW\nskipgram",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#fasttext-facebook-ai-team",
    "href": "012_nlp.html#fasttext-facebook-ai-team",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "13.9 fasttext (facebook ai team)",
    "text": "13.9 fasttext (facebook ai team)\n\nn-grams (e.g., 3-6 character representations)\nword vector is sum of its ngrams\ncan handle low frequecynor even novel wordsp",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "012_nlp.html#other-methods",
    "href": "012_nlp.html#other-methods",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "13.10 Other Methods",
    "text": "13.10 Other Methods\n\nOther approaches\n\nglove\nelmo\nBERT :w\n\n\n\n\n\n\nHvitfeldt, Emil, and Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. https://smltar.com/.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. 1rst ed. Beijing; Boston: O’Reilly Media.\n\n\nWickham, Hadley, Çetinkaya-Rundel Mine, and Garrett Grolemund. 2023. R for Data Science: Visualize, Model, Transform, and Import Data. 2nd ed. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Natural Language Processing: Text Processing and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "013_applications.html",
    "href": "013_applications.html",
    "title": "13  Applications for Machine Learning: Synthesis and Concept Generalization",
    "section": "",
    "text": "13.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applications for Machine Learning: Synthesis and Concept Generalization</span>"
    ]
  },
  {
    "objectID": "013_applications.html#overview-of-unit",
    "href": "013_applications.html#overview-of-unit",
    "title": "13  Applications for Machine Learning: Synthesis and Concept Generalization",
    "section": "",
    "text": "13.1.1 Learning Objectives**\nThis week we will read a short “book” by Andrew Ng, a very well known computer scientist in the field of AI who also offers one of the best known MOOCs on machine learning at Coursera\n\nThis book is very application oriented and practical. The goals for reading it are:\n\nTo encourage generalization of your learning - some terminology and perspectives will be different here from the authors you have been reading. The concepts are all the same. We hope that seeing these ideas in a new context will help you recognize and generalize them.\nThe book is applied. It will help you consider carefully how to make best use of your data for model selection, evaluation, error analysis and other related tasks to figure out what to do when your models don’t perform well.\nIt is a “course in a book”, such that we hope this will encourage you to see the big picture of how all the pieces from our course fit together.\n\n\n\nFinally, it is a quick read so don’t worry that it is a full book. Also, you have no lectures or application assigment this week. It is all about consolidation now!!!! You are in the home stretch.\n\n\n\n13.1.2 Readings\n\nNg (2018) pdf\n\nPost questions to the readings channel in Slack\n\n\n\n13.1.3 Lecture Videos\nNo lectures this week. Only lab and discussion section.\n\n\n\n13.1.4 Application Assignment and Quiz\nNo assignment this week!\n\nThe unit quiz is due by 8 pm on Wednesday, April 24th",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applications for Machine Learning: Synthesis and Concept Generalization</span>"
    ]
  },
  {
    "objectID": "013_applications.html#discussion",
    "href": "013_applications.html#discussion",
    "title": "13  Applications for Machine Learning: Synthesis and Concept Generalization",
    "section": "13.2 Discussion",
    "text": "13.2 Discussion\n\n13.2.1 Sets and iteration\n\ntrain/val(dev)/test vs kfold/bootstrap with test vs nested cv\nchoosing sample sizes for train, val, test\nRandom vs non-random splits into train/val/test. what needs to be true? what is less important?\noverfit val? Use test more than once? Do vs. don’t and risks\nReview 12 Takeaways\n\n\n\n13.2.2 Error analysis\n\nwhy\nstart with baseline/basic system first- why?\neyeball and black box dev sets\nsize of eyeball sample?\nrisk of using all dev as eyeball sample\nerror analysus for regression?\nreview 19 takeaways\n\n\n\n13.2.3 bias/variance\n\nidentify by differences in train/val error\n\n1/11\n\n15/16\n15/30\n1/2\n\n\nrole of optimal error rate\navoidable bias and variance using train/val/ and optimal error\nhow to fix avoidable bias (25)\nhow to fix variance (27)\n\n\n\n13.2.4 learning curves\n\nHow and why\nhow to calculate with small samples? issues with large samples?\nreview figs (from book, saved in figs)\n\n\n\n13.2.5 pipeline vs end to end\n\nwhen have we seen each?\ncosts and benefits vs. possible?\n\n\n\n\n\nNg, Andrew. 2018. Machine Learning Yearning: Technical Strategy for AI Engineers in the Age of Deep Learning. DeepLearning.AI.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applications for Machine Learning: Synthesis and Concept Generalization</span>"
    ]
  },
  {
    "objectID": "014_ethics.html",
    "href": "014_ethics.html",
    "title": "14  Ethical Issues in Machine Learning Research and Applications",
    "section": "",
    "text": "14.1 Overview of Unit",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ethical Issues in Machine Learning Research and Applications</span>"
    ]
  },
  {
    "objectID": "014_ethics.html#overview-of-unit",
    "href": "014_ethics.html#overview-of-unit",
    "title": "14  Ethical Issues in Machine Learning Research and Applications",
    "section": "",
    "text": "14.1.1 Learning Objectives\nAll semester, we have been learning how to develop and evaluate machine learning models. In this final unit, we will now consider the impact that these models have had to date on society. Of course, there is the potential for many benefits from these models but they have also produced substantial harm. It is important that we recognize what contributes to their harm so that we can strive to avoid these problems in our own work.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ethical Issues in Machine Learning Research and Applications</span>"
    ]
  },
  {
    "objectID": "014_ethics.html#readings",
    "href": "014_ethics.html#readings",
    "title": "14  Ethical Issues in Machine Learning Research and Applications",
    "section": "14.2 Readings",
    "text": "14.2 Readings\n\nThe readings this week will come from O’Neil (2016); We will read the introduction, chapters 1, 3, 5, and the conclusion and afterword sections. A pdf of the book will be shared directly with you.\nWe will also read this article on emerging methods and tools for assessing model fairness.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ethical Issues in Machine Learning Research and Applications</span>"
    ]
  },
  {
    "objectID": "014_ethics.html#lecture-videos-application-assignment-and-quiz",
    "href": "014_ethics.html#lecture-videos-application-assignment-and-quiz",
    "title": "14  Ethical Issues in Machine Learning Research and Applications",
    "section": "14.3 Lecture Videos, Application Assignment, and Quiz",
    "text": "14.3 Lecture Videos, Application Assignment, and Quiz\nThere are no lecture videos, or application assignment this week. The unit quiz will be free-response questions on the reading to help you prepare your thoughts to share during our discussion.\n\nThe [unit quiz] (https://canvas.wisc.edu/courses/395546/quizzes/514052) is due by 8 pm on Wednesday, May 1st. We will meet on Tuesday to discuss these assigned papers and Thursday for a review session for the concepts final exam.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ethical Issues in Machine Learning Research and Applications</span>"
    ]
  },
  {
    "objectID": "014_ethics.html#discussion",
    "href": "014_ethics.html#discussion",
    "title": "14  Ethical Issues in Machine Learning Research and Applications",
    "section": "14.4 Discussion",
    "text": "14.4 Discussion\n\n14.4.1 Announcements\n\nQuiz\nFinal concepts\nFinal applications\nReview\nOffice hours and meetings\n\n\n\n\n14.4.2 Discuss Model Scale\n\nWhat is it?\nExamples\nCosts and Benefits\nRole of homegeneity\n\n\n\n\n14.4.3 Discuss Model Opacity\n\nWhat is it?\nExamples\nInputs, Outputs, Algorithm\nConnection to high dimensionality\nConnection to model performance (why is college rank system ostentiably opaque)\nIntersection with models in health care\n\n\n\n\n14.4.4 Discuss Proxy outcomes\n\nWhat do we mean when the outcome is a proxy\nIs this common in models, in social science?\nTwo examples from book (recidivism, college rankings)\nExamples?\nWhat is the problem?\n\n\n\n\n14.4.5 Discuss issue of self-perpetuating (feedback loops) vs self-correcting\n\nWhat is the issue/problem\nDescribe the problem in the context of book examples (college ranks, recidivism, credit scores)\nDescribe a novel example\nHow does this issue interact with opacity, errors, and ability to appeal?\n\n\n\n\n14.4.6 Are our models objective?\n\n“Values are reflected in what we target. My point is that police make choices about where they direct their attention. Today they focus almost exclusively on the poor. That’s their heritage, and their mission, as they understand it. And now data scientists are stitching this status quo of the social order into models, like PredPol, that hold ever-greater sway over our lives.”\n\n\n“Would society be so willing to sacrifice the concept of probable cause if everyone had to endure the harassment and indignities of stop and frisk? Chicago police have their own stop-and-frisk program. In the name of fairness, what if they sent a bunch of patrollers into the city’s exclusive Gold Coast? Maybe they’d arrest joggers for jaywalking from the park across W. North Boulevard or crack down on poodle pooping along Lakeshore Drive. This heightened police presence would probably pick up more drunk drivers and perhaps uncover a few cases of insurance fraud, spousal abuse, or racketeering. Occasionally, just to give everyone a taste of the unvarnished experience, the cops might throw wealthy citizens on the trunks of their cruisers, wrench their arms, and snap on the handcuffs, perhaps while swearing and calling them hateful names. In time, this focus on the Gold Coast would create data. It would describe an increase in crime there, which would draw even more police into the fray. This would no doubt lead to growing anger and confrontations. I picture a double parker talking back to police, refusing to get out of his Mercedes, and finding himself facing charges for resisting arrest. Yet another Gold Coast crime.”\n\n\nExamples of models of “what* we target with our models – nuisance crimes vs. white collar\nExamples of problems with where models applied - Stop and frisk as WMD.\n\nExamples of how we define our constructs (Is tuition/cost part of being a good school?)\nBias can enter through proxies. What about inputs, observed associations, and training data?\n\n\n\n\n14.4.7 What should and should not be included as inputs to a model?\n\nAre there inputs that shouldnt be allowed?\nImplications of including race/ethnicity as feature in our models?\n\n\n\n\n14.4.8 Damage\n\nHow well does it perform?\n\nHow well for subgroups?\nAccuracy not enough if costs of different errors are different\nWhat is it used for?\nHow strongly is it used?\nWhat if there was no model?\n\n\n\n\n14.4.9 Critique RISK and RISK2 ethics\n\nWho developed the model?\nWho provided the data?\nWhat are values or assumptions? Lapse?\nHow will it be used - by who and for what?\nHow should it be evaluated for implementation?\nIs it fair?\nHow could we evaluate it in an ongoing fashion?\nUnanticipated consequences of its use?\n\n\n\n\n\nO’Neil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Reprint Edition. Broadway Books.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ethical Issues in Machine Learning Research and Applications</span>"
    ]
  },
  {
    "objectID": "015_final.html",
    "href": "015_final.html",
    "title": "15  Final Exam and Project",
    "section": "",
    "text": "15.1 Applications (take-home) Exam\nThe final exam/project consists of two separate questions/analyses. Please see this pdf for a complete description of the overall requirements and these two separate questions.\nYou should complete the exam as you have previously completed the application assignments:\nIn contrast to the application assignments, the TAs and I will not be able to answer substantive questions about the exam. However, if you need us to clarify what we are requesting you to do for any specific question or believe you have found an error in the exam, please post your question to the exam channel in Slack and we will respond ASAP\nFinally, it is worth noting that we will approach this final exam/project in a manner consistent with the rest of the course - We care most about learning not grades, and so although the final will be of course graded, our goal is for everyone to succeed, and what we’re looking for is for people to do their best and to demonstrate what you have learned this semester. Good luck.\nData:",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Final Exam and Project</span>"
    ]
  },
  {
    "objectID": "015_final.html#applications-take-home-exam",
    "href": "015_final.html#applications-take-home-exam",
    "title": "15  Final Exam and Project",
    "section": "",
    "text": "Download the two datasets that you will use for these two questions (note that for the second question you may choose to substitute your own data instead).\nProvide complete analyses and annotations for the two questions in a series of rmd file (see the specific questions for more details)\nWhen you are done, knit the rmd files to html and upload these knit files through the Canvas\nThe exam is due on Wednesday, May 8th, at 8pm\n\n\n\n\n\ntips.csv\nairline_passenger_satisfaction.csv",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Final Exam and Project</span>"
    ]
  },
  {
    "objectID": "015_final.html#conceptual-exam",
    "href": "015_final.html#conceptual-exam",
    "title": "15  Final Exam and Project",
    "section": "15.2 Conceptual Exam",
    "text": "15.2 Conceptual Exam\nThe conceptual final exam will be held in our normal classroom during finals week on Tuesday May 7th at 11 am.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Final Exam and Project</span>"
    ]
  },
  {
    "objectID": "app_terminology.html",
    "href": "app_terminology.html",
    "title": "16  Key Terminology and Concepts",
    "section": "",
    "text": "Bias\n\nBias is a property of estimates “estimates” of a true population value. Sample statistics can be biased estimates of the population values for those statistics. Parameter estimates (e.g., from a linear model) can be biased estimates of the true population values for those parameters. And our models can be biased estimates of the true DGP for y. A model or other estimate is biased if it systematically over- or under-predicts the true population values that are being estimated overall or for some combination of inputs (e.g. features). Biased models are often described as “underfit” to the true DGP.\n\n\nsee also: underfit, overfit, variance; bias-variance trade-off\n\nBias-Variance trade-off\n\nThe error associated with our estimates of the DGP (i.e., prediction errors from our models) come from two sources: irreducible error and reducible error. Reducible error can be parsed into error due to bias and variance. As we seek to reduce the reduce the error from our models, we will often make changes to the model or the modeling process to reduce either the bias or variance of the fitted model. However, many of the changes we make to reduce bias often increase variance and many changes we make reduce variance often increase bias. Our goal is to make changes that produce bigger decreases in one of these sources of error than the often associated increase in the other.\n\n\nsee also: bias, variance\n\nCross-validation\n\ninsert definition\n\n\nsee also resampling, bootstrapping\n\nFeature\n\nFeatures are used as inputs to our model to make predictions for y. Feature engineering is the process of transforming our original predictors that we measure into the form that they will be input to our model. In some instances, our features may be exact copies of our predictors. But often, we transform the original features (e.g., power-transformations, dummy coding, missing data imputation) to allow them to better map onto the DGP for Y and reduce the error of our models.\n\n\nsynonyms: X, regressor, input\n\n\nsee also: predictor, independent variable\n\nHeld-out (data)set\n\ninsert definition\n\n\nsee also: Validation set, Test set\n\nHyperparameter\n\ninsert definition\n\n\nexamples include: k from KNN, lambda and alpha from elastic net (glmnet)\n\nModel configuration\n\ninsert definition\n\nPerformance metric\n\nA performance metric is a measure of model performance that allows us to judge the degree of error (or lack thereof) associated with our model’s predictions. Different performance metrics are used for regression vs. classification problems. For regression, we will often use root mean square error, mean (or median) absolute error, or \\(R^2\\). For classification, accuracy is the most well known but we will learn many other performance metrics including balanced accuracy, sensitivity, specificity, positive predictive value, negative predictive value, auROC, and f-scores.\n\nPredictor\n\nThe initial (i.e., raw, original) variables that we collect/measure or manipulate in our studies and use to predict or explain our outcome (or dependent measure) are called predictors. If these predictors were manipulated in an experiment, we often call them independent variables. We use a process called feature engineering to transform the predictor variables from how they were initially measured in their final form as features (Xs, inputs) in our models.\n\n\nrelated concepts/terms: independent variable, feature, X, inputs, regressor, outcome, label\n\nResampling\n\nResampling is a process of creating multiple subsets of our full dataset. We often use resampling to get better performance estimates for our models than would be possible if we fit our models and estimated their performance in the sample (full) sample. There are two broad classes of resampling methods we will learn in this course: cross validation and bootstrapping. Cross validation itself is a class of resampling methods with many different variants (leave one out CV, K-fold CV, repeated K-fold CV, grouped K-fold CV, nested CV) that have different strengths and weaknesses.\n\n\nsee also: Cross-validation, Bootstrapping\n\nTest set\n\ninsert definition\n\n\nsee also: Validation set, Test set\n\nTraining set\n\nA training set is a subset of our full data (obtained using a resampling method). We use training sets to fit various model configurations to develop our machine learning models. Model performance estimates based on training data are poor estimates of performance because our models are often overfit (sometimes by a large degree) to the training data. Therefore model performance estimates based on training data will generally overestimate how well our models will work with new data.\n\n\nsee also: validation set, test set, resampling, held-in set, held-out set\n\nValidation set\n\nA validation set is a subset of our full data (obtained using a resampling method). When we have several (or more, or many more) model configurations that we are considering, we use validation set(s) to calculate performance metrics to estimate the relative performance of these model configurations in new data. It is important to use validation sets for this purpose because evaluating our model configurations in training sets could lead us to select the configurations that are most overfit to the training data rather than the model configurations that are expect to perform best with new data that were not used to fit them.\n\n\nsee also: training set, test set, resampling, held-in set, held-out set\n\nVariance\n\ninsert definition\n\n\nsee also: overfit, bias-variance trade off, bias, underfit",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Key Terminology and Concepts</span>"
    ]
  },
  {
    "objectID": "app_dummy_coding.html",
    "href": "app_dummy_coding.html",
    "title": "17  Novel Levels in Held-Out Set(s)",
    "section": "",
    "text": "When you have nominal/ordinal predictors that have levels that are infrequent, you will occasionally find that an infrequent level appears in your held out set (i.e., validation or test) but not in your training set. This can cause problems when you try to make predictions for these new values. Specifically, the feature values for this level will be set to NA and therefore, you will get predictions of NA for these observations.\nIn this appendix, we demonstrate this problem and our preferred solution given our workflow of classing all nominal/ordinal predictors as factors in our dataframes.\n\nlibrary(tidyverse) \nlibrary(tidymodels) \ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\n\nMake simple data sets with an outcome (y) and one nominal predictor (x). Note that x will have a novel value (foo) in the test set that wasnt present in the training set.\n\nn &lt;- 6\ndata_trn &lt;- tibble(y = rnorm(n), \n                   x = rep (c(\"a\", \"b\", \"c\"), n/3)) |&gt;\n  mutate(x = factor(x, levels = c(\"a\", \"b\", \"c\"))) |&gt; \n  print()\n\n# A tibble: 6 × 2\n       y x    \n   &lt;dbl&gt; &lt;fct&gt;\n1  1.03  a    \n2 -0.478 b    \n3  0.772 c    \n4  0.948 a    \n5  0.214 b    \n6  0.333 c    \n\ndata_test &lt;- tibble(y = c(rnorm(n), rnorm(1)),\n                    x = c(rep (c(\"a\", \"b\", \"c\"), n/3), \"foo\")) |&gt; \n  mutate(x = factor(x, levels = c(\"a\", \"b\", \"c\", \"foo\"))) |&gt; \n  print()\n\n# A tibble: 7 × 2\n        y x    \n    &lt;dbl&gt; &lt;fct&gt;\n1 -0.0664 a    \n2 -0.520  b    \n3 -0.994  c    \n4 -0.980  a    \n5 -0.191  b    \n6  0.125  c    \n7  0.783  foo  \n\n\nMake a recipe\n\nrec &lt;- recipe(y ~ x, data = data_trn) %&gt;% \n  step_dummy(x)\n\nPrep the recipe with training data\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nFeatures for training set. No problems\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n6\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ny\n0\n1\n0.47\n0.57\n-0.48\n0.24\n0.55\n0.90\n1.03\n-0.51\n-1.45\n\n\nx_b\n0\n1\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\nx_c\n0\n1\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\n\n\n\nFeatures for test set.\n\nNow we see the problem indicated by the warning about new level in test.\nWe see that one observation is missing for x in test. If we looked closer, we would see this is the observation for foo\n\n\nfeat_test &lt;- rec_prep |&gt; \n  bake(data_test)\n\nWarning: ! There are new levels in a factor: `foo`.\n\nfeat_test |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_test\n\n\nNumber of rows\n7\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ny\n0\n1.00\n-0.26\n0.63\n-0.99\n-0.75\n-0.19\n0.03\n0.78\n0.25\n-1.42\n\n\nx_b\n1\n0.86\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\nx_c\n1\n0.86\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\n\n\n\nWe solve this problem but just making sure this level was listed when we created the factor in training (e.g., use this mutate earlier when classing x in data_trn: mutate(x = factor(x, levels = c(\"a\", \"b\", \"c\", \"foo\"))).\nOr we can add the level after the fact, when we discover the problem (as below).\n\ndata_trn1 &lt;- data_trn |&gt; \n  mutate(x = factor(x, levels = c(\"a\", \"b\", \"c\", \"foo\")))\n\nNow prep recipe with this updated training set that includes foo level\n\nrec_prep1 &lt;- rec |&gt; \n  prep(data_trn1)\n\nFeatures for training as before\n\nWe now have a feature for this new level\nIt is set to 0 for all observations (because there are no observations with a value of foo in training set)\n\n\nfeat_trn1 &lt;- rec_prep1 |&gt; \n  bake(data_trn1)\n\nfeat_trn1 |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn1\n\n\nNumber of rows\n6\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ny\n0\n1\n0.47\n0.57\n-0.48\n0.24\n0.55\n0.90\n1.03\n-0.51\n-1.45\n\n\nx_b\n0\n1\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\nx_c\n0\n1\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\nx_foo\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nNaN\nNaN\n\n\n\n\n\nNow there is no problem when we find this value for an observation in the test set.\n\nfeat_test1 &lt;- rec_prep1 |&gt; \n  bake(data_test)\n\nfeat_test1 |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_test1\n\n\nNumber of rows\n7\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ny\n0\n1\n-0.26\n0.63\n-0.99\n-0.75\n-0.19\n0.03\n0.78\n0.25\n-1.42\n\n\nx_b\n0\n1\n0.29\n0.49\n0.00\n0.00\n0.00\n0.50\n1.00\n0.75\n-1.60\n\n\nx_c\n0\n1\n0.29\n0.49\n0.00\n0.00\n0.00\n0.50\n1.00\n0.75\n-1.60\n\n\nx_foo\n0\n1\n0.14\n0.38\n0.00\n0.00\n0.00\n0.00\n1.00\n1.62\n0.80\n\n\n\n\n\nAll is good. BUT, there are still some complexities when we fit this model in train and predict into test. In training, the x_foo feature is a constant (all 0) so this will present some issues for some statistical algorithms. Lets see what happens when we fit a linear model and use it to predict into test.\n\nfit1 &lt;-\n  linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(y ~ ., data = feat_trn1)\n\nIf we look at the parameter estimates, we see that the algorithm was unable to estimate a parameter for x_foo because it was a constant in train\n\nfit1 %&gt;% tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    0.989     0.238      4.16  0.0253\n2 x_b           -1.12      0.336     -3.33  0.0446\n3 x_c           -0.436     0.336     -1.30  0.285 \n4 x_foo         NA        NA         NA    NA     \n\n\nThis will generate a warning (“prediction from a rank-deficient fit has doubtful cases”) when you use this model to make predictions for values it didnt see in the training set.\n\nThe consequence is that the model will predict a y value associated with the reference level (coded 0 for all other dummy features) for all foo observations. This is probably the best we can do for these new (previously unseen) values for x.\nalso note that the column name for predictions, which is usually called .pred, is now called .pred_res. You will need to accomodate this in your code as well. Just rename it.\n\n\npredict(fit1, feat_test1) |&gt;  \n  bind_cols(feat_test1)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\", : prediction from rank-deficient fit; consider predict(.,\nrankdeficient=\"NA\")\n\n\n# A tibble: 7 × 5\n   .pred       y   x_b   x_c x_foo\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.989 -0.0664     0     0     0\n2 -0.132 -0.520      1     0     0\n3  0.553 -0.994      0     1     0\n4  0.989 -0.980      0     0     0\n5 -0.132 -0.191      1     0     0\n6  0.553  0.125      0     1     0\n7  0.989  0.783      0     0     1\n\n\nThis is our preferred solution when new/previously unseen values exist in held out data. A comparable solution is offered as a recipe step. See step_novel()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Novel Levels in Held-Out Set(s)</span>"
    ]
  },
  {
    "objectID": "app_keras.html",
    "href": "app_keras.html",
    "title": "18  Installing Keras for Neural Networks",
    "section": "",
    "text": "18.1 Windows\nFollow the steps below to install keras in R as well as the backend software that lets you use keras in R. Take a little extra time to get things working in R before the start of the unit so you can start on the homework once it is released.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Installing Keras for Neural Networks</span>"
    ]
  },
  {
    "objectID": "app_keras.html#windows",
    "href": "app_keras.html#windows",
    "title": "18  Installing Keras for Neural Networks",
    "section": "",
    "text": "18.1.1 Install keras package in R\n\nOpen RStudio.\nType install.packages(\"keras\") in the console\nOnce installed, type library(keras) in the console\nIn order to run keras you then need to install the backend. To do this, type install_keras() in the console. You will be prompted to install a software called Miniconda, Select Yes/Y. [Details: This will install a distribution of the python language and a number of libraries, including tensorflow and keras. These are all needed to use keras for fitting neural networks in tidymodels. This installation process may take a few minutes, and will result in about 6GB of disk space used.]\nRestart your R session (Session &gt; Restart R from the menu)\n\n\n\n18.1.2 Test for install\n\nType library(keras) in the console again to load the package.\nType backend() in the console. It should return something like Module(tensorflow_core.keras.backend) or Module(keras.api._v2.keras.backend). If it doesn’t, or if it provides a warning or error that suggests that tensorflow was not installed, reach out to the TAs and John.\n\n\n\n18.1.3 Common problems\nThe most common problem with getting keras/tensorflow to run in R (assuming you’ve followed the steps above) has to do with keras using the correct distribution of python on your computer (which typically results from having more than one distribution of python on your machine). You’ll be alerted to this by an error message that tells you that keras/tensorflow isn’t installed on your computer (even though you installed it above). This can be solved with the function keras::use_condaenv(), where the obligatory argument of this function is a path to the miniconda environment you created in step 4. Alternatively, you can use keras::use_python() where instead you provide the path to the specific version of python within the miniconda installed in step 4. The challenge is finding the path to put as the first argument to either of these functions. If this comes up and you need help, reach out to the TAs and John.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Installing Keras for Neural Networks</span>"
    ]
  },
  {
    "objectID": "app_keras.html#mac-os",
    "href": "app_keras.html#mac-os",
    "title": "18  Installing Keras for Neural Networks",
    "section": "18.2 Mac OS",
    "text": "18.2 Mac OS\nThe steps below will allow Mac users to install keras in R and the backend software to support fitting neural networks in R. Ideally, get your installation going well in advance of the homework being released so that you can hit the ground running on this unit. Also note that Linux users can follow this tutorial and everything should work out fine because of the similarity between Linux and MacOS (UNIX and UNIX-like systems).\n\n18.2.1 First thing: Check your processor\nThe first thing to do is determine the type of processor you have on your computer. Go to the Apple icon in the upper left of your screen and select “About this Mac”. In the window that pops up one of the lines will show your “Processor”. It will either tell you that you have some type of Intel processor, or (importantly) it will tell you that you have an “M1” or “M2” processor. If you have an M1 or M2 (or other M) processor, check that you have the proper version of R installed on your machine for the M-class processors by typing R.version in the console in RStudio. For Macs with the M-class processor (M1 or M2) you need to have a distribution of R identified with the arm64 (64-bit Advanced RISC Machines) in the version name. If your version of R isn’t identified in this way, you need to re-install R with the appropriate arm64 distribution from CRAN before proceeding to install the necessary modeling libraries for using neural networks in R with the python backend. If you don’t, it will not work to install these libraries and you won’t be able to do the work in this unit. We’ve found that it is better to check for this before you try to install keras and related libraries in R because the wrong distribution of R will cause you problems once you move on to install keras. If you have an M1 or M2 processor and you have the proper version of R installed for that machinery, you can move on to the steps below outlining how to install keras in R. For related issues that arise after installing the proper version of R for M-class processors, see the “Common problems” section below. Sometimes additional issues arise beyond the R version problem.\nAlternatively, if you have an Intel processor, you shouldn’t encounter problems and you can move on to step 1 below for Install keras in R\n\n\n18.2.2 Install keras in R\n\nOpen RStudio\nTo install keras in R, type install.packages(\"keras\") in the console\nThen load the library by typing library(keras) in the console\n\n\n\n18.2.3 Install backend python and libraries\n\nIn order to run keras you then need to install the backend. Run install_keras(). This should prompt you to install Miniconda. This is a distribution of the python language and a number of libraries, including tensorflow and keras, all of which we need to run ANNs. If you are comfortable installing this software, select Yes/Y. This installation process may take a few minutes, and will result in about 6GB of disk space used.\nRestart R session (Session &gt; Restart R from the menu)\n\n\n\n18.2.4 Test the backend\n\nLoad the keras library by typing (library(keras)) in the console\nTo see if the backend installed, type backend() in the console. [Details: This function should provide some information about tensorflow as your backend library to keras. If it provides you something like Module(tensorflow_core.keras.backend) or Module(keras.api._v2.keras.backend), then you should be good. If it doesn’t, or if it provides a warning or error that suggests that tensorflow was not installed, reach out to the TAs and John for help.]\n\n\n\n18.2.5 Common problems\nIf keras in R won’t install, then you either need to try again with a fresh install (starting from the top of this tutorial), or you need to point R to a proper installation of keras/tensorflow on your machine. Most commonly, this would be done with another conda distribution of python (other than the one that keras installs for you) that you install yourself outside of R. It could be that you already have another conda distribution of python installed if you’ve used python in other work/ courses. If you get to this point, you will likely need the support of an instructor, so reach out and they can help. Nonetheless, the important details to consider are discussed below.\nIf the R installation of keras doesn’t work and you need to install keras in a different conda environment than the one R installs for you, then (once you install keras in that other environment) you need to specify the correct distribution of python (i.e., the one associated with the correct conda environment) on your machine for use in keras in R. The “correct” one is the one that has a successfully installed version of tensorflow and keras. Most likely, if this is a problem, it is due to the fact that your Mac has an M1 or M2 processor. This is an idiosyncratic case because these processors sometimes require a version of tensorflow/keras that isn’t the version automatically installed when you use keras::install_keras() in R. You’ll know if this is the case if you try to complete the installation steps above using keras::install_keras() in R and problems arise (i.e., R tells you that keras and/or tensorflow weren’t properly installed).\nIn order to point R to another version of conda/python on your machine (assuming you’ve installed one), use the function keras::use_python(). Here, the obligatory argument of this function is a path to the miniconda python distribution that contains the successful installation of tensorflow and keras. Calling this function will need to be done at the top of any scripts where you intend to use keras in R. This would look something like use_python(\"/Users/johncurtin/miniconda3/bin/python\"). The challenge in using this function is finding this path to put as the first argument. If this comes up and you need help, reach out to the instructors.\nFor more on this and related methods of sourcing specific versions of python from within R see this page from the reticulate package. Note that reticulate is the package in R that supports the keras implementation, and is automatically installed when you install keras in R. Many reticulate functions are inherited by keras such that you can call the same function (with the same name) in keras or in reticulate from within R and have the same outcome (e.g., keras::use_python() is equivalent to reticulate::use_python()).\nThe other common issue concerns the M-class processor and your version of R, described above - so follow that suggestion if it applies to you. If you’ve gotten this far, hopefully you’ve read through that issue already.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Installing Keras for Neural Networks</span>"
    ]
  },
  {
    "objectID": "app_keras.html#trying-it-out",
    "href": "app_keras.html#trying-it-out",
    "title": "18  Installing Keras for Neural Networks",
    "section": "18.3 Trying it out",
    "text": "18.3 Trying it out\nFeel free to test out your new library with the tutorial for MNIST here, or follow along with Lecture 6 in the Unit 11 lectures. Note that if you use the vignette linked above, it is not in tidymodels, so the code looks different than what you’ll see in the homework, but would still be worthwhile (especially because it shows you output that looks like what you would see in python).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Installing Keras for Neural Networks</span>"
    ]
  },
  {
    "objectID": "app_keras.html#remember",
    "href": "app_keras.html#remember",
    "title": "18  Installing Keras for Neural Networks",
    "section": "18.4 Remember",
    "text": "18.4 Remember\nIf you run into trouble above, reach out on Slack. We will hold a special session with the group to focus on installation if need be. Likewise, you can reach out to the tutorial author mcooperborkenhagen@fsu.edu for support.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Installing Keras for Neural Networks</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html",
    "href": "app_model_performance_simulations.html",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "",
    "text": "19.1 General Setup\nLoad libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ntheme_set(theme_classic())",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html#dgp",
    "href": "app_model_performance_simulations.html#dgp",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "19.2 DGP",
    "text": "19.2 DGP\nFunction to generate n_obs of simulated observations\n\nDGP is linear on all x + normal error with sd = irr_err\ny is simple sum such that all coefficients = 1\nfeatures are correlated based on sigma\n\n\nsimulate_DGP &lt;- function (n_obs, n_features, irr_err, mu, sigma){\n\n  x &lt;- MASS::mvrnorm(n_obs, mu = mu, Sigma = sigma) |&gt; \n    magrittr::set_colnames(str_c(\"x\", 1:n_features)) |&gt; \n    as_tibble()\n  \n  x |&gt; \n     mutate(y = rowSums(t(t(x)*b)) + rnorm(n_obs, \n                                        mean = 0, \n                                        sd = irr_err))\n}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html#simulation-settings",
    "href": "app_model_performance_simulations.html#simulation-settings",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "19.3 Simulation settings",
    "text": "19.3 Simulation settings\n\nn_sims &lt;- 1000  # number of simulations\nn_obs &lt;- 300 # number of observations\n\nn_features &lt;- 20\nmu &lt;- rep(0, n_features)\nsigma &lt;- matrix(.3, nrow = n_features, ncol = n_features)\ndiag(sigma) &lt;- 1\nirr_err &lt;- 10\nb &lt;- rep(0.5, n_features) # no b0 so set to 0\n  \nset.seed(123456)\n# first call so that we can set up recipe\ndf &lt;- simulate_DGP(n_obs, n_features, irr_err, mu, sigma) \nrec &lt;- recipe(y ~ ., data = df)\n\nrmse_combined = NULL  # to store RMSE for all methods",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html#get-simulation-dfs",
    "href": "app_model_performance_simulations.html#get-simulation-dfs",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "19.4 Get simulation dfs",
    "text": "19.4 Get simulation dfs\n\ndfs &lt;- rep(n_obs, n_sims) |&gt;\n  map(\\(n_obs) simulate_DGP(n_obs, n_features, irr_err, mu, sigma))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html#what-is-the-true-model-performance",
    "href": "app_model_performance_simulations.html#what-is-the-true-model-performance",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "19.5 What is the TRUE model performance",
    "text": "19.5 What is the TRUE model performance\nThe irreducible error is set to 10 but our model will have some reducible error too so the true performance of our model will be worse than 10\n\nThe DGP is linear\nbut we only have 300 observations\nand we have 20 features\n\nLets fit many models of n = 300 (the size of our final model) and assess performance in really big samples of new data (ah the luxury of simulated data!)\n\n# models based on full n for each simulation run\nmodels &lt;- dfs |&gt; \n  map(\\(df) linear_reg() |&gt; fit(y ~ ., data = df))\n\n# big samples of held out data for high precision assessment of models\n# 1 for each model\nouts &lt;- rep(10000, n_sims) |&gt; \n  map(\\(n_obs) simulate_DGP(n_obs, n_features, irr_err, mu, sigma))\n\n# list of predictions for out from each model\npreds &lt;- map2(models, outs, \\(model, out) predict(model, out))\n\n# get mean rmse in big held out data set for 1000 full n models\n# should be very precise\nrmse_true &lt;- map2_dbl(outs, preds, \\(out, pred)  rmse_vec(out$y,\n                                                      pred$.pred)) |&gt; \n  mean()\n\nmessage(\"True RMSE = \", rmse_true)\n\nTrue RMSE = 10.3728762850761\n\n\nParallel processing for resampling methods with fit_resamples()\n\ncl &lt;- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))\ndoParallel::registerDoParallel(cl)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html#validation-set",
    "href": "app_model_performance_simulations.html#validation-set",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "19.6 Validation set",
    "text": "19.6 Validation set\n\n19.6.1 80/20 split\nHere we simulate repeated use of the validation set approach to assess our model performance\n\nrmse_combined &lt;- dfs |&gt; \n  map(\\(df) validation_split(df, prop = .80)) |&gt;   # validation set split\n  map(\\(split) linear_reg() |&gt; fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |&gt; \n  map(\\(fits) collect_metrics(fits, summarise = TRUE)) |&gt; \n  list_rbind() |&gt; \n  mutate(method = \"val_set_80\") |&gt;       # label results in df\n  bind_rows(rmse_combined)\n\nWarning: `validation_split()` was deprecated in rsample 1.2.0.\nℹ Please use `initial_validation_split()` instead.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html#split-1",
    "href": "app_model_performance_simulations.html#split-1",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "19.7 50/50 split",
    "text": "19.7 50/50 split\nHere we simulate repeated use of the validation set approach to assess our model performance\n\nrmse_combined &lt;- dfs |&gt; \n  map(\\(df) validation_split(df, prop = .50)) |&gt;   # validation set split\n  map(\\(split) linear_reg() |&gt; fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |&gt; \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |&gt; \n  list_rbind() |&gt; \n  mutate(method = \"val_set_50\") |&gt;       # label results in df\n  bind_rows(rmse_combined)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html#k-fold",
    "href": "app_model_performance_simulations.html#k-fold",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "19.8 K-fold",
    "text": "19.8 K-fold\n\n19.8.1 Simple 5-fold\n\nrmse_combined &lt;- dfs |&gt; \n  map(\\(df) vfold_cv(df, v = 5)) |&gt;   # 5-fold\n  map(\\(split) linear_reg() |&gt; fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |&gt; \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |&gt; \n  list_rbind() |&gt; \n  mutate(method = \"5-fold\") |&gt;       # label results in df\n  bind_rows(rmse_combined)\n\n\n\n19.8.2 Simple 10-fold\n\nrmse_combined &lt;- dfs |&gt; \n  map(\\(df) vfold_cv(df, v = 10)) |&gt;   # 10-fold\n  map(\\(split) linear_reg() |&gt; fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |&gt; \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |&gt; \n  list_rbind() |&gt; \n  mutate(method = \"10-fold\") |&gt;       # label results in df\n  bind_rows(rmse_combined)\n\n\n\n19.8.3 3x 10-Fold\n\nrmse_combined &lt;- dfs |&gt; \n  map(\\(df) vfold_cv(df, v = 10, repeats = 3)) |&gt;   # 3x10-fold\n  map(\\(split) linear_reg() |&gt; fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |&gt; \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |&gt; \n  list_rbind() |&gt; \n  mutate(method = \"3x10-fold\") |&gt; # label results in df\n  bind_rows(rmse_combined)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html#bootstrap-resampling",
    "href": "app_model_performance_simulations.html#bootstrap-resampling",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "19.9 Bootstrap Resampling",
    "text": "19.9 Bootstrap Resampling\n\n19.9.1 10 resamples\n\nrmse_combined &lt;- dfs |&gt; \n  map(\\(df) bootstraps(df, times = 10)) |&gt;   # 10 boots\n  map(\\(split) linear_reg() |&gt; fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |&gt; \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |&gt; \n  list_rbind() |&gt; \n  mutate(method = \"boot_10\") |&gt;       # label results in df\n  bind_rows(rmse_combined)\n\n\n\n19.9.2 100 resamples\n\nrmse_combined &lt;- dfs |&gt; \n  map(\\(df) bootstraps(df, times = 100)) |&gt;   # 100 boots\n  map(\\(split) linear_reg() |&gt; fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |&gt; \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |&gt; \n  list_rbind() |&gt; \n  mutate(method = \"boot_100\") |&gt;       # label results in df\n  bind_rows(rmse_combined)\n\n\n\n19.9.3 1000 resamples\n\nrmse_combined &lt;- dfs |&gt; \n  map(\\(df) bootstraps(df, times = 1000)) |&gt;   # 1000 boots\n  map(\\(split) linear_reg() |&gt; fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |&gt; \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |&gt; \n  list_rbind() |&gt; \n  mutate(method = \"boot_1000\") |&gt;       # label results in df\n  bind_rows(rmse_combined)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html#summarize",
    "href": "app_model_performance_simulations.html#summarize",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "19.10 Summarize",
    "text": "19.10 Summarize\n\nrmse_combined |&gt; \n  group_by(method) |&gt; \n  summarize(rmse_mean = mean(mean),\n            rmse_sd = sd(mean),\n            n = n())\n\n# A tibble: 8 × 4\n  method     rmse_mean rmse_sd     n\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 10-fold         10.4   0.442  1000\n2 3x10-fold       10.4   0.436  1000\n3 5-fold          10.5   0.455  1000\n4 boot_10         10.7   0.491  1000\n5 boot_100        10.7   0.451  1000\n6 boot_1000       10.7   0.449  1000\n7 val_set_50      10.8   0.672  1000\n8 val_set_80      10.5   0.985  1000",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_model_performance_simulations.html#plot-sampling-distributions",
    "href": "app_model_performance_simulations.html#plot-sampling-distributions",
    "title": "19  Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques",
    "section": "19.11 Plot sampling distributions",
    "text": "19.11 Plot sampling distributions\n\nrmse_combined |&gt; \n  ggplot(aes(x = mean, color = method)) + \n  geom_density() +\n  geom_vline(aes(xintercept = mean(rmse_true)),\n            color = \"blue\", linetype = \"dashed\", linewidth = 1)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques</span>"
    ]
  },
  {
    "objectID": "app_test.html",
    "href": "app_test.html",
    "title": "22  Test",
    "section": "",
    "text": "22.1 Standards",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Test</span>"
    ]
  },
  {
    "objectID": "app_test.html#standards",
    "href": "app_test.html#standards",
    "title": "22  Test",
    "section": "",
    "text": "22.1.1 For displaying figures:\n\n\n\n\n\n\n\n22.1.2 For displaying code and even variable names\ndemo &lt;- function(x) variable_2\n\n\n22.1.3 For sizing graphs and code-generated plots\n\nYou can specify out-height or out-width as either pixels, inches, or percentages\nYou can specify both using different units\nIf you specify only one, the other should default to “auto” which keeps the figured scaled as in the original\n\n\nplot(pressure)\n\n\n\n\n\n\n\n\n\n\n22.1.4 For code annotation\n\nlibrary(tidyverse)\n\n\n1\n\nmake a df\n\n2\n\nglimpse a df\n\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n1d &lt;- tibble (x = 1:5) |&gt;\n2  glimpse()\n\nRows: 5\nColumns: 1\n$ x &lt;int&gt; 1, 2, 3, 4, 5\n\n\n\n\n22.1.5 For displaying color (like red questions)\nthis sentence is in red\nThis is an alternative method but more complex: I love R.\nred yellow green blue\nyellow background for better contrast\n\n\n22.1.6 Callouts for questions\nHTML:\n\n\n\n\n\n\nThis is the question you want displayed?\n\n\n\n\n\nHere’s the answer! This can be as long as you want.\n\n\n\nRevealJS does not yet have callout collapse implemented. A workaround is to use code folding:\n\n\n\n\n\n\nIf the DGP for y is a cubic function of x, what do we know about the expected bias for our three candidate model configurations in this example?\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model will underfit the true DGP and therefore it will be biased b/c \nit can only represent y as a linear function of x.  \n\nThe two polynomial models will be generally unbiased b/c they have x represented \nwith 20th order polynomials.  \n\nLASSO will be slightly biased due to regularization but more on that in a later unit\n\n\n\n\n\n\n\n22.1.7 Conditional Content\nUse divs to specify content to only appear in certain formats:\nWill only appear in HTML.\n\n\n22.1.8 Links to qmd files as plaintext\nNormally, a link to a qmd file functions as a link to the rendered html of that file.\nTo link to a qmd file that is to be downloaded or viewed as raw text, you must link to the raw.github user content:\n\ncleaning EDA: qmd\nmodeling EDA: qmd",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Test</span>"
    ]
  }
]