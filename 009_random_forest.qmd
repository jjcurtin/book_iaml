# Advanced Models: Decision Trees, Bagging Trees, and Random Forest


## Overview of Unit

**Learning Objectives**

* Decision trees
* Bagged trees
* How to bag models and the benefits
* Random Forest
* How Random Forest extends bagged trees
* Feature interpretation with:
  * Decision tree plots
  * Variable importance


**Readings**

* @ISL [Chapter 8, Tree Based Methods; pp 327 - 352](pdfs/isl_2.pdf)

In addition, much of the content from this unit has been drawn from four chapters in a book called [Hands On Machine Learning In R](https://bradleyboehmke.github.io/HOML/).  It is a great book and I used it heavily (and at times verbatim) b/c it is quite clear in its coverage of these algorithms.  If you want more depth, you might read chapters 9-12 from this book as a supplement to this unit in our course.

**Lecture Videos**

* [Lecture 1: Decision Trees]()
* [Lecture 2: Decision Trees in Ames]()
* [Lecture 3: Bagged Trees]()
* [Lecture 4: Bagged Trees in Ames]()
* [Lecture 5: Random Forest]()
* [Lecture 6: Random Forest in Ames]()

* [Discussion]()


**Coding Assignment**

* [data](homework/unit_10/fifa.csv)
* [rmd shell](homework/unit_10/hw_unit_10_random_forest.Rmd)
* solution: [rmd](); [html]()


 Submit the application assignment [here](https://canvas.wisc.edu/courses/395546/assignments/2187684) and complete the [unit quiz](https://canvas.wisc.edu/courses/395546/quizzes/514054) by 8 pm on Wednesday, April 3rd 


