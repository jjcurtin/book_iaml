# NLP Discussion

## Announcements

- Last application assignment done!
- Remaining units
  - Bring it together for applications
  - Ethics and Algorithmic Fairness
  - Reading and discussion only for both units.  Discussion requires you!
  
- Two weeks left!
  - next tuesday (lab on NLP)
  - next thursday (discussion on applications)
  - following tuesday (discussion on ethics/fairness)
  - following thursday (concepts final review; 50 minutes only)

- Early start to final application assignment (assigned next thursday, April 25th) and due Wednesday, May 8th at 8pm
- Concepts exam at lab time during finals period (Tuesday, May 7th, 11-12:15 in this room)

-----

## Single nominal variable

- Our algorithms need features coded with numbers
  - How did we generally do this?
  - What about algorithms like random forest?
  
-----

##  Single nominal variable Example

- Current emotional state
  - angry, afraid, sad, excited, happy, calm

- How represented with one-hot?

-----

## What about document of words (e.g., sentence rather than single word)

- I watched a scary movie last night and couldn't get to sleep because I was so afraid
- I went out with my friends to a bar and slept poorly because I drank too much

\
\

- How represented with bag of words?
- binary, count, tf-idf

\
\

- What are the problems with these approaches
  - relationships between features or similarity between full vector across observations 
  - context/order
  - dimensionality
  - sparsity

-----

## N-grams vs. simple (1-gram) BOW

- How different?

\
\

- some context/order
- but still no relationship/meaning or similarity
- even higher dimesions

-----

## Linguistic Inquiry and Word Count (LIWC)

- Tausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and computerized text analysis methods. Journal of Language and Social Psychology, 29(1), 24â€“54. https://doi.org/

\
\

- Meaningful features? (based on domain expertise)
- Lower dimensional
- Very limited breadth

\
\
- Can add domain specific dictionaries

-----

## Word Embeddings

- Encodes word meaning
- Words that are similar get similar vectors
- Meaning derived from context in various text corpora
- Lower dimensional
- Less sparse

-----

## Examples


- Affect Words

![](figs/nlp_1.PNG){height=5in}

-----

![](figs/nlp_2.PNG){height=5in}

-----
 
![](figs/nlp_3.PNG){height=5in}



- Two more general

- word2vec (google)
  - CBOW
  - skipgram

![](figs/nlp_4.PNG){height=5in}

-----

![](figs/nlp_5.PNG){height=5in}

-----

![](figs/nlp_6.PNG){height=5in}


-----

## fasttext (facebook ai team)

- n-grams (e.g., 3-6 character representations)
- word vector is sum of its ngrams
- can handle low frequecynor even novel wordsp

## Other Methods

- Other approaches
  - glove
  - elmo
  - BERT
