---
editor_options: 
  chunk_output_type: console
---

# IAML Unit 6: Discussion 

## Announcements

- Midterm - application and conceptual parts
- Can we have a midterm review session?

-----

## feature matrix vs. raw predictors

When to use which and why could I use raw predictors in simulated data example from lectures

-----


## Cost functions

What is Cost function?

Linear model 

- $\frac{1}{n}\sum_{i = 1}^{n} (Y_i - \hat{Y_i})^{2}$

Ridge (L2)

- $\frac{1}{n}([\sum_{i = 1}^{n} (Y_i - \hat{Y_i})^{2}] + [\:\lambda\sum_{j = 1}^{p} \beta_j^{2}\:])$

- $\frac{1}{n}([\sum_{i = 1}^{n} (Y_i - \hat{Y_i})^{2}] + [\:\lambda\sum_{j = 1}^{p} |\beta_j|\:])$

Elastic Net

- $\frac{1}{n}([\sum_{i = 1}^{n} (Y_i - \hat{Y_i})^{2}] + [\:\lambda (\alpha\sum_{j = 1}^{p} |\beta_j| + (1-\alpha)\sum_{j = 1}^{p} \beta_j^{2})\:])$

-----

## LASSO, Ridge, and elastic net questions

- The pros and cons of Lasso and Ridge vs. Elastic Net

- I'm still a little confused as to why you would ever use Ridge or LASSO separately when you can just selectively use one or the other through elastic net. Wouldn't it make sense to just always use elastic net and then change the penalty accordingly for when you wanted to use a Ridge or LASSO regression approach?
  - Pure LASSO
  - model variance
  - two self-report measure scales vs. items

- Some explanation why Lasso is more robust to outliers and ridge is more robust to measurement error would be appreciated.

- I'm not very clear why LASSO could limit some parameters to be zero but ridge regression cannot. Can we go through this a bit?

- How do you know what numbers to start at for tuning lamba (like in the code below)? I think John mentioned he has a function to find these, but I'm wondering if there are any rules of thumb.

- How do we know how "strong" of a penalty we need to be applying to our cost function? Does the reduction in variance increase as we increase the strength of the penalty?

- Is a decreased number of features always good or bad, or does it depend on the model/recipe

- Can you talk more about how to interpret the scale of the parameter estimates? In the lecture you said the following and I'm not quite sure what that means:

- I might be totally wrong but I wonder if we have to care about the multi-collinearity or high dimension on classification as well. Or this is only limited to regression and so we are solving with regularising only regression model? 

-----

## Stepwise questions

- Could you go over Forward, Backward, and Best Subset subsetting? I think I understand the algorithm they use, but I do not understand the penalty function they use for picking the "best" model. In the book, it looks like it uses R-squared to pick the best model, but wouldn't the full model always have the greatest R-squared?

- Is there a reason why we do not discuss variable selection using subset methods? 

- Is there specific cases when you would pick backwards or forwards selection, or is it up to the researcher?

- Training vs. val/val error in stepwise approaches.   

- Use of AIC, BIC, Cp, and adjusted R2 vs. cross-validation

-----

## Explanatory goals

- Can LASSO be used for variable selection when engaging in cross sectional data analysis to identify which variables in a large set of Xs are important for a particular outcome? 

- In practice, what elements should be considered before selecting IVs and covariates?