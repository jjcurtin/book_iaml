---
title: 'Unit 4 (Classification): Fit RDA Models'
author: "Your name here"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

## Assignment overview

**NOTE:** If you have not yet worked through hw_unit_4_eda_cleaning.Rmd, you are in the wrong place. Start there!

To begin, download the following from the course web book (Unit 4):

* Three RMarkdown (.Rmd) files: hw_unit_4_eda_cleaning.Rmd, hw_unit_4_fit_knn, & hw_unit_4_fit_rda.Rmd)

* titanic_raw.csv (data to be used for this script to create training & validation)

* titanic_test_cln.csv (already-cleaned held-out test data with outcome labels removed)

* fun_modeling.R (for custom course functions)

* titanic_data_dictionary.png

The data for this week's assignment (across all scripts) will be the Titanic dataset, which contains survival outcomes and passenger characteristics for the Titanic's passengers. You can find information about the dataset in the data dictionary (available for download from the course website). This is a popular dataset that has been used for machine learning competitions and is noted for being a fun and easy introduction into data science. There is tons of information on how others have worked with this dataset online, and you are encouraged to incorporate knowledge that others have gained if you want! However, only code from the class web book will be needed for full credit on this assignment.

In this script, you will fit a series of RDA models in your training data and use your validation data to select the best model. This script starts with reading in the titanic_train.csv & titanic_val.csv files that you created in the cleaning EDA script. Although this assignment does not have an explicit script for modeling EDA, we expect that you will have done some modeling EDA on your own to be able to do good feature engineering and fit good models. You should now have multiple examples of good modeling EDA (the web book and two modeling EDA homework keys). You do not need to turn in any code for modeling EDA that you complete.

At the end of this assignment, you will select your best performing model across all KNN and RDA models that you fit. Like last week, you will use the titanic_test_cln.csv file **only once** at the end of the assignment to generate your best model's predictions of survived in the held-out test set. We will assess everyone's predictions on the held-out set to determine the best fitting model in the class. **The winner gets a high five from John & a free lunch!**

Your assignment is due Wednesday at 5:00 PM. Let's get started!

## Setup

Load packages 
```{r load_packages}
library(tidymodels) # for modeling
library(tidyverse) # for general data wrangling
library(kableExtra) # for displaying formatted tables w/ kbl()
library(skimr) # for skim()
```

### Create file path object

**NOTE:** change this path to be where YOUR unit 4 homework files are saved.
```{r set_path}
path_hw_unit_4 <- ""
```

### Source functions script

**NOTE:** change the path (before "/fun_modeling.R") to where YOU have saved this file.  Do download the file again b/c John has continued to improve the functions
```{r source_custom_functions}
source("C:/Users/skittleson/iaml/fun_modeling.R")
```

### Read in data

Read in data files for cleaned training, validation, and test data (generated in hw_unit_4_eda_cleaning.Rmd). **Please load your data using the provided names**
```{r read_data}
data_trn <- read_csv()

data_val <- read_csv()

data_test <- read_csv()
```

### Create tracking tibble

Create an empty tracking tibble to track the validation error across the various model configurations you will fit.
```{r create_tracker}

```

## RDA model building instructions

You may consider as many RDA model configurations as you wish, but there are a few rules:

* All models must use survived as the outcome variable

* You must build a minimum of 3 models (this is not a trick - if you build 3 models, that is sufficient to receive full credit on this component of the homework assignment!)

* At least one model must include a variable with missing data (you will need to handle that missingness before using the variable)

* At least one model must include a transformation of a numeric variable

* At least one model must include interaction(s) among features

* All models you fit should use the hyperparameter values associated with a QDA because you don't know how to tune hyperparameters yet (though you will by next week!). You can get more details on this in section 4.7.2 of the web book, but the code needed for this appears in the model-fitting chunks below (fit_model_1).

Your models may include as many other predictors as you would like (e.g., your model satisfying the 2 numeric predictors requirement may include more than 2 numeric variables, categorical variables, etc.). Be as creative as you'd like!

## RDA model configuration 1

Indicate which required model components you are including for your first model by deleting the requirements that do **not** apply in the text below:
*Variable with missingness*
*Transformed numeric*
*Interaction(s)*

### Set up recipe

Informed by EDA from your modeling EDA, create a recipe for your first model
```{r recipe_1}

```

### Training feature matrix

Use the make_features() function to generate the feature matrix of the training data
```{r features_train_1}

```

### Fit your model

Fit a RDA model predicting survived from your training feature matrix
```{r fit_model_1}
# use these hyperparamater values:
# discrim_regularized(frac_common_cov = 0, frac_identity = 0)
```

### Validation feature matrix

Use the make_features() function to generate the feature matrix of the validation data that you will use to assess your model
```{r features_val_1}

```

### Assess your model

Use the accuracy_vec() function to calculate the validation error (accuracy) of your model. Add this value to your validation error tracking tibble
```{r assess_model_1}

```

## RDA model configuration 2

Indicate which required model components you are including for your second model by deleting the requirements that do **not** apply in the text below:
*Variable with missingness*
*Transformed numeric*
*Interaction(s)*

### Set up recipe

Informed by EDA from your modeling EDA, create a recipe for your first model
```{r recipe_2}

```

### Training feature matrix

Use the make_features() function to generate the feature matrix of the training data
```{r features_train_2}

```

### Fit your model

Fit a RDA model predicting survived from your training feature matrix
```{r fit_model_2}
# use these hyperparamater values:
# discrim_regularized(frac_common_cov = 0, frac_identity = 0)
```

### Validation feature matrix

Use the make_features() function to generate the feature matrix of the validation data that you will use to assess your model
```{r features_val_2}

```

### Assess your model

Use the accuracy_vec() function to calculate the validation error (accuracy) of your model. Add this value to your validation error tracking tibble
```{r assess_model_2}

```

## RDA model configuration 3

Indicate which required model components you are including for your third model by deleting the requirements that do **not** apply in the text below:
*Variable with missingness*
*Transformed numeric*
*Interaction(s)*

### Set up recipe

Informed by EDA from your modeling EDA, create a recipe for your first model
```{r recipe_3}

```

### Training feature matrix

Use the make_features() function to generate the feature matrix of the training data
```{r features_train_3}

```

### Fit your model

Fit a RDA model predicting survived from your training feature matrix
```{r fit_model_3}
# use these hyperparamater values:
# discrim_regularized(frac_common_cov = 0, frac_identity = 0)
```

### Validation feature matrix

Use the make_features() function to generate the feature matrix of the validation data that you will use to assess your model
```{r features_val_3}

```

### Assess your model

Use the accuracy_vec() function to calculate the validation error (accuracy) of your model. Add this value to your validation error tracking tibble
```{r assess_model_3}

```

## Additional model configurations (Optional)

Create as many code chunks as you would like below to test additional RDA model configurations. Record your models' performance in the validation data (accuracy) in your validation error tracking tibble.

```{r additional_knns}

```

## Generate predictions

This section is for generating predictions for your best model in the held-out test set. You should only generate predictions for **ONE MODEL** out of all KNN and RDA models you fit across both model fitting files (hw_unit_4_fit_rda.Rmd and hw_unit_4_fit_knn.Rmd). We will use these predictions to generate your ONE estimate of model performance in the held-out data.

If your **best** model is a RDA (from this script), follow the steps below. If your best model is a KNN (from your other script), skip to **Save & Knit** at the end of the document.

### Confirm best model is RDA

Change "FALSE" to "TRUE" if your best model is a RDA. Add your last name between the quotation marks.
```{r}
rda_best <- FALSE
last_name <- ""
```

### Combine training and validation sets as full held-in data

Now that a single, best model has been selected, you want to fit that model in all your held-in data (training PLUS validation). The following code will combine your data appropriately as long as you have used the data_trn & data_val object names (no need to change anything in this chunk).
```{r}
if (rda_best) {
 data_held_in <- bind_rows(data_trn, data_val) 
}
```

### Make feature matrices from all held-in data

Make feature matrix for held-in data from held-in data. Substitute the name of your best recipe object where it says "INSERT."
```{r}
if (rda_best) {
  feat_held_in <- INSERT %>% 
    make_features(data_held_in, data_held_in)
}
```

Make feature matrix for held-out (test) data from held-in data. Substitute the name of your best recipe object where it says "INSERT."
```{r}
if (rda_best) {
 feat_test <- INSERT %>% 
  make_features(data_held_in, data_test) 
}
```

### Fit best model

Update the code below to reflect your best model. You need to adjust the formula based on the best model you've selected.
```{r}
if (rda_best) {
  best_model <- discrim_regularized(frac_common_cov = 0, 
                                    frac_identity = 0) %>% 
  set_engine("klaR") %>% 
  fit(survived ~ INSERT, data = feat_held_in)
}
```

### Generate test predictions

Run this code chunk to save out your best model's predictions in the held-out test set. Look over your predictions to confirm your model generated valid predictions for each test observation. If you left rda_best as "FALSE" no file will be saved out. (no need to change anything in this chunk)
```{r}
if (rda_best) {
  feat_test %>% 
    mutate(survived = predict(best_model, feat_test)$.pred_class) %>% 
    select(passenger_id, survived) %>% 
    glimpse() %>% 
    write_csv(file.path(path_hw_unit_4,
                        str_c("test_preds_", last_name, ".csv")))
}
```

## Save & knit

Save this .Rmd file with your last name at the end (e.g., hw_unit_4_fit_knn_santana). Make sure you changed "Your name here" at the top of the file to be your own name. Knit the file to .html, and upload the knit file to Canvas. Upload your saved test_preds_[lastname].csv (if you created it in this script) to Canvas. 

We will assess everyone's predictions in the test, and the winner gets free lunch AND a high five from John!

Don't forget to complete the KNN portion of this assignment and submit it to Canvas (hw_unit_4_fit_knn.Rmd) if you haven't already.

**Way to go!!**

