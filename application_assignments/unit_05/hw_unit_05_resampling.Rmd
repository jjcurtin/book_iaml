---
title: 'Unit 5 (Resampling)'
author: "Your name here"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

## Assignment overview

To begin, download the following from the course web book (Unit 5):

* One RMarkdown (.Rmd) file: hw_unit_5_resampling.Rmd

* smoking_ema.csv (data for this assignment)

* data_dictionary.csv (a .csv file with data dictionary information for the smoking_ema dataset)

The data for this week's assignment contains self-report ecological momentary assessment (EMA) data from a cigarette smoking cessation attempt study. These are real (de-identified) data from our laboratory! We've tried to structure the assignment so that you don't really need to understand the data, but looking quickly at the data dictionary may be helpful. Briefly, the outcome variable is "cigs" (whether a cigarette has been smoked since previous report). The "cigs" variable comes from the report *following* the report from which all other data come. In other words, all predictor variables come from time 1, and the outcome (cigs) variable comes from time 2. Predictor variables include things like current craving, several measures of affect, the intensity of any stressful events, and the random treatment assignment (active "nrt" treatment, or "placebo"). 

The data are already cleaned, and you don't need to do any modeling EDA. Of course you **normally** would do modeling EDA before fitting any models, but we're trying to keep the assignment from getting too long!

In this assignment, you will practice selecting among model configurations and tuning hyperparameters (K in a KNN) using resampling methods. As you'll see when you load the data file, there are a lot of observations! This fact combined with the more complex resampling procedures you'll be using in this assignment means that **fitting models may take a little longer**. We've kept the active coding component of the assignment to a reasonable length, but the run time may be a little longer, so please try to plan for this!

Your assignment is due Wednesday at 5:00 PM. Let's get started!

## Setup

### Load packages 

```{r load_packages}
library(tidymodels) # for modeling
library(tidyverse) # for general data wrangling
library(kableExtra) # for displaying formatted tables w/ kbl()
library(skimr) # for skim()
library(cowplot) # for plot_grid() and theme_half_open()
```

### Source functions script

```{r source_custom_functions}
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true")
```

### Set plotting theme

```{r set_theme}
theme_set(theme_half_open())
```

### Create file path object

Use `here` and a relative path for your data.  Make sure your iaml project is open.
**Note:** Set path_hw_unit_5 to where YOUR hw unit 5 files are saved
```{r path}
library(here)
path_hw_unit_5 <- "homework/unit_5"
```

### Read in data

Read in the smoking_ema.csv data file
```{r read_data}

```

## Part 1: Selecting among model configurations with k-fold cross-validation

In this portion of the assignment, you will be using k-fold cross-validation to select among two different model configurations. These model configurations will differ only on feature set; they will both use GLM logistic regression as the statistical algorithm. 

### Split data

Split your data into 10 folds (k = 10; repeats = 1) stratifying on the outcome (cigs)
```{r split_kfold}
set.seed(01131997)
splits_kfold <- NA
```

### Model Configuration 1

**Recipe:** 

* Build a recipe with cigs as the outcome variable regressed on craving.

* Set up the outcome (cigs) as a factor with the positive class ("yes") as the second level

```{r rec_kfold_1}
rec_kfold_1 <- NA
```

**Fit model with k-fold cross-validation**  

Use logistic regression as your statistical algorithm, rec_kfold_1 as your recipe, and accuracy as your metric.
```{r fit_kfold_1}
fits_kfold_1 <- NA
```

**Examine performance estimates**

Use the collect_metrics() function to make a table of the cross-validated results.
```{r table_kfold_1}
metrics_kfold_1 <- NA
```

Plot a histogram of the performance estimates
```{r hist_kfold_1}

```

Print the average performance over folds with the summarize = TRUE argument.
```{r est_kfold_1}

```

### Model Configuration 2

**Recipe:** 

* Build a recipe with cigs as the outcome variable regressed on all variables

* Set up the outcome (cigs) as a factor with the positive class ("yes") as the second level

* Set up other variables as factors where needed

* Apply dummy coding

* Create an interaction between stressful_event and lag

* Remove the subid variable with step_rm()

```{r rec_kfold_2}
rec_kfold_2 <- NA
```

**Fit model with k-fold cross-validation**  

Use logistic regression as your statistical algorithm, rec_kfold_2 as your recipe, and accuracy as your metric.
```{r fit_kfold_2}
fits_kfold_2 <- NA
```

**Examine performance estimates**

Use the collect_metrics() function to make a table of the cross-validated results.
```{r table_kfold_2}
metrics_kfold_2 <- NA
```

Plot a histogram of the performance estimates
```{r hist_kfold_2}

```

Print the average performance over folds with the summarize = TRUE argument.
```{r est_kfold_2}

```

### Selecting the best model configuration

Look back at your two average performance estimates (one from each model configuration; code chunks est_kfold_1 and est_kfold_2). Which model configuration would you select? Why? Type your response between the asterisks below.

*Type your response here*

## Part 2: Tuning hyperparameters with bootstrap resampling

Now we'll use bootstrapping to tune a hyperparameter (k) in KNN models. This means that you'll consider multiple values of k in a more principled way. You'll do this for both model configurations (same as above), and you'll use your resampled performance estimates to select the best model configuration with the best hyperparameter value.

### Split data

Split your data using the bootstrap. Stratify on cigs, and use 100 bootstraps.
```{r split_boot}
set.seed(10031960)
splits_boot <- NA
```

## Set up hyperparameter grid

Create a tibble with values of the hyperparameter (k) to consider. Include at least 25 values of k; you decide which values!
```{r hyper_boot_1}
hyper_grid <- NA
```

### Model Configuration 1

**Recipe:** 

* Build a recipe with cigs as the outcome variable regressed on craving.

* Set up the outcome (cigs) as a factor with the positive class ("yes") as the second level

* Take appropriate normalizing steps for craving to be able to fit a KNN

```{r rec_boot_1}
rec_boot_1 <- NA
```

**Tune model with bootstrapping for cross-validation**  

Use KNN classification as your statistical algorithm, rec_boot_1 as your recipe, hyper_grid in your "grid =" argument, and accuracy as your metric.
```{r tune_boot_1}
tune_boot_1 <- NA
```

**Examine performance estimates**

Print the average performance over configurations with the collect_metrics() function and the summarize = TRUE argument. Remember that you will now see one estimate per hyperparameter value, averaged across bootstraps.
```{r all_boot_1}

```

Plot average performance by the values of the hyperparameter. You can use plot_hyperparameters() from fun_modeling.R or your own code.
```{r plot_boot_1}

```

Have you considered a wide enough range of k values? How do you know? Type your response between the asterisks. If you didn't use a wide enough range, try again with a wider range

*Type your response here*

Print the performance of your best model configuration with the show_best() function.
```{r est_boot_1}

```

### Model Configuration 2

**Recipe:** 

* Build a recipe with cigs as the outcome variable regressed on all variables

* Set up the outcome (cigs) as a factor with the positive class ("yes") as the second level

* Set up other variables as factors where needed

* Apply dummy coding

* Create an interaction between stressful_event and lag

* Remove the subid variable with step_rm()

* Take appropriate normalizing steps for any predictors you've kept as numeric to be able to fit a KNN

```{r rec_boot_2}
rec_boot_2 <- NA
```

**Tune model with bootstrapping for cross-validation**  

Use KNN classification as your statistical algorithm, rec_boot_2 as your recipe, hyper_grid in your "grid =" argument, and accuracy as your metric.
```{r tune_boot_2}
tune_boot_2 <- NA
```

**Examine performance estimates**

Print the average performance over configurations with the collect_metrics() function and the summarize = TRUE argument. Remember that you will now see one estimate per hyperparameter value, averaged across bootstraps.
```{r all_boot_2}

```

Plot average performance by the values of the hyperparameter. You can use plot_hyperparameters() from fun_modeling.R or your own code.
```{r plot_boot_2}

```

Have you considered a wide enough range of k values? How do you know? Type your response between the asterisks. If you didn't use a wide enough range, try again with a wider range

*Type your response here*

Print the performance of your best model configuration with the show_best() function.
```{r est_boot_2}

```

## Part 3: Selecting among model configurations with grouped k-fold cross-validation

Thus far, you've ignored the subid variable. However, what that variable reveals is that the many observations within the smoking_ema.csv dataset are grouped within 125 unique participants. In the final part of this assignment, you will use grouped k-fold cross-validation to match the data structure. 

### Split data

Split your data into 10 folds (k = 10; repeats = 1) using the group_vfold_cv() function. Use the grouping argument (group = "subid"). For this example, do **not** stratify on cigs. 

```{r split_group_kfold}
set.seed(12031957)
splits_group_kfold <- NA
```

### Model Configuration 1

**Fit model with grouped k-fold cross-validation**  

Use logistic regression as your statistical algorithm, rec_kfold_1 as your recipe (doesn't need to change from above!), and accuracy as your metric.
```{r fit_group_kfold_1}
fits_group_kfold_1 <- NA
```

**Examine performance estimates**

Use the collect_metrics() function to make a table of the cross-validated results.
```{r table_group_kfold_1}
metrics_group_kfold_1 <- NA
```

Plot a histogram of the performance estimates
```{r hist_group_kfold_1}

```

Print the average performance over folds with the summarize = TRUE argument.
```{r est_group_kfold_1}

```

### Model Configuration 2

**Fit model with grouped k-fold cross-validation**  

Use logistic regression as your statistical algorithm, rec_kfold_2 as your recipe (doesn't need to change from above!), and accuracy as your metric.
```{r fit_group_kfold_2}
fits_group_kfold_2 <- NA
```

**Examine performance estimates**

Use the collect_metrics() function to make a table of the cross-validated results.
```{r table_group_kfold_2}
metrics_group_kfold_2 <- NA
```

Plot a histogram of the performance estimates
```{r hist_group_kfold_2}

```

Print the average performance over folds with the summarize = TRUE argument.
```{r est_group_kfold_2}

```

### Selecting the best model configuration

Look back at your two average performance estimates for model configurations 1 and 2 from grouped k-fold cross-validation (est_group_kfold_1 and est_group_kfold_2). Which model configuration would you select? Why? Type your response between the asterisks.

*Type your response here*

## Save & knit

Save this .Rmd file with your last name at the end (e.g., hw_unit_5_resampling_santana). Make sure you changed "Your name here" at the top of the file to be your own name. Knit the file to .html, and upload the knit file to Canvas. 

**Way to go!!**