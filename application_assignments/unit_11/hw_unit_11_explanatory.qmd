---
title: "Homework Unit 11: Model Comparisons and Other Explanatory Goals"
author: "Your name here"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---


## Introduction

To begin, download the following from the course web book (Unit 11):

* `hw_unit_11_explanatory.qmd` (notebook for this assignment)

* `student_perf_cln.csv` (data for this assignment)

The data for this week's assignment have information about student performance in math.  

The data will be cleaned using code included for you below, and you don't need to show any modeling EDA to keep the assignment from getting too long. *However, you should still always be checking your data as you progress through the assignment!* We have described the specific steps you'll take to implement the recipes.

In this assignment, you will practice comparing models to determine the importance of a pair of focal variables. In particular, you will be determining whether parental education level (`mother_educ` and `father_educ`) matter for predicting student performance. You will then use the Bayesian approach to evaluate the effect of that focal variable. 

To reduce run time we are not going to do any hyperparameter tuning. We will be using 30 splits (3 repeats of 10 fold cv). Remember that setting up parallel processing and using `cache_rds()` will dramatically reduce your run times.   

Let's get started!

-----


##  Setup

Set up your notebook in this section. You will want to set your path and initiate parallel processing here!

```{r}

```



## Read in your data

Use this code chunk to read in the `student_perf_cln.csv` data file and class your focal variables. Check data as needed to be sure everything looks good!
```{r}
data_all <- read_csv(here::here(path_data, "student_perf_cln.csv"),
                     col_types = cols()) 

focal_levels <- c("none", "primary", "middle", "secondary", "higher")
data_all <- data_all |> 
  mutate(across(ends_with("_educ"), ~ factor(.x, levels = 0:4, labels = focal_levels)),
         across(where(is.character), as_factor)) 
```

## Set up splits

### Divide data into train and test

Hold out 25% of the data as a test set for evaluation (`data_test`). Assign the rest of the data to `data_trn`.
```{r}
set.seed(12345)

```

### Make cross-validation splits

Split `data_trn` into repeated k-fold cross-validation splits using 3 repeats and 10 folds. You do not need to set a new seed. Save your splits as an object named `splits`
```{r}


```

## Build recipes

Follow the instructions below to build two recipes from your training data: one for a "full" model (contains all predictors), and one for a compact model (contains all predictors except the two focal variables: `mother_educ` and `father_educ`).

### Recipe 1: Full model

Follow these steps in your recipe. **Use your own judgment about the correct order of steps!** This is an UNORDERED list of the things you should include. 

* Regress the outcome on all variables

* Create dummy coded features for categorical predictors

* Handle missing data for both categorical and numeric predictors where needed

* Remove near-zero variance features

* Remove the id variable

```{r}
rec_full <- 
```

### Recipe 2: Compact model

Because your recipe for the compact model will only differ from your recipe for the full model by one step (removing your focal variables), you can start with `rec_full`. Then add a step that will remove the focal variables `father_educ` and `mother_educ`.

```{r}
rec_compact <- 
```

## Fit models

### Fit the full model

Use `rec_full` and `splits` to fit GLMs across folds. Use RMSE as your metric. Save your model fits as `fits_full`. 
```{r}

```

Print the mean RMSE across the 30 held-out folds.
```{r}

```

### Fit the compact model

Use `rec_compact` and `splits` to fit GLMs across folds. Use RMSE as your metric. Save your model fits as `fits_compact`. 
```{r}

```

Print the mean RMSE across the 30 held-out folds.
```{r}

```


## Model comparison with the Bayesian approach
You will now compare your models using the Bayesian parameter estimation

### Gather performance estimates

First, make a dataframe containing the 30 performance estimates from held out folds for your full and compact model
```{r}


```


### Posterior probabilities

Next, derive the posterior probabilites for the RMSE of each of these two models
```{r}


```

### Graph positerior probabilities

Display your posterior probabilities using both a density plot and a histogram. Choose plots that would be the most useful to display your results to a collaborator.
```{r}


```

### Determine if the full model is more accurate

Calculate the probability that the full model is more accurate than the compact model. 

```{r}


```

Type out a summary of what you find that includes:

* Your interpretation of what the mean increase in accuracy and 95% HDI values represent

* Your conclusion if the full model is meaningfully better than your compact model and why   

*Type your summary here*



## Feature importance

You will now look at feature importance in your full model using Shapely Values.

### Prep data
Get your data ready for use with the `iml` package. Do the following:

* Create a feature matrix of your full model 

* Pull out your raw features and outcome to be utilized in calculating feature importance

* Define a predictor function so we can make predictions from our model using the `iml` package
```{r}



```

### Calculate Shapely Values for a single participant

First, we will look at shapely values for a single participant. For this example, lets look at the last participant in our data set. Do the following:

* Print the raw feature values for the last participant. (Be sure to do this in a way that will print your output in your knit document)

* Generate Shapely Values for this participant and output the raw results

* Use ggplot to plot the Shapely Values for this participant

```{r}


```

Provide a brief interpretation of what the top shapely value means.    

*Type your response here*


### Calculate mean absolute Shapely Values across all participants

Now we will look at feature importance across all participants. Include the following steps (and make sure each step prints to your knit document):

* Calculate Shapely Values for each observation and glimpse() the resulting tibble

* Plot the mean absolute Shapely Values across all participants

```{r}


```

Write a brief description of your top take-aways from this plot.    

*Type your response here.*


## Interactions

Lastly, you will explore interactions among all features in your full model. 

*Generate a plot (using ggplot) of the total variance across all the (two-way) interactions a feature can have with other features 

* Generate a plot displaying we all specific two-way interactions individually for our target GRE score feature

```{r}



```


Provide a brief summary of what you notice about the plots.

*Type your response here*



