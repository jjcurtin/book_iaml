---
title: "Homework Unit 11: Model Comparisons and Other Explanatory Goals"
author: "Your name here"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---


## Introduction

To begin, download the following from the course web book (Unit 11):

* `hw_unit_11_explanatory.qmd` (notebook for this assignment)

* `student_perf_cln.csv` (data for this assignment)

The data for this week's assignment have information about student performance in math.  

The data will be cleaned using code included for you below, and you don't need to show any modeling EDA to keep the assignment from getting too long. *However, you should still always be checking your data as you progress through the assignment!*

In this assignment, you will practice comparing models to determine the importance of a pair of focal variables. In particular, you will be determining whether parental education level (`mother_educ` and `father_educ`) matter for predicting student performance. You will then use the Bayesian approach to evaluate the effect of that focal variable. 

To reduce run time we are not going to do any hyperparameter tuning. We will be using 30 splits (3 repeats of 10 fold cv). Remember that setting up parallel processing and using `cache_rds()` will dramatically reduce your run times.   

Let's get started!

-----


##  Setup

Set up your notebook in this section. You will want to set your path and initiate parallel processing here!

```{r}

```



## Read in your data

Use this code chunk to read in the `student_perf_cln.csv` data file and class your focal variables. Check data as needed to be sure everything looks good!
```{r}
data_all <- read_csv(here::here(path_data, "student_perf_cln.csv"),
                     col_types = cols()) 

focal_levels <- c("none", "primary", "middle", "secondary", "higher")
data_all <- data_all |> 
  mutate(across(ends_with("_educ"), ~ factor(.x, levels = 0:4, labels = focal_levels)),
         across(where(is.character), as_factor)) 
```

## Set up splits

### Divide data into train and test

Hold out 25% of the data as a test set for evaluation (`data_test`). Assign the rest of the data to `data_trn`.
```{r}
set.seed(12345)

```

### Make cross-validation splits

Split `data_trn` into repeated k-fold cross-validation splits using 3 repeats and 10 folds. You do not need to set a new seed. Save your splits as an object named `splits`
```{r}


```

## Build recipes

Follow the instructions below to build two recipes from your training data: one for a "full" model (contains all predictors), and one for a compact model (contains all predictors except the two focal variables: `mother_educ` and `father_educ`).

### Recipe 1: Full model

Follow these steps in your recipe. **Use your own judgment about the correct order of steps!** This is an UNORDERED list of the things you should include. Feel free to add any additional steps!

* Regress the outcome on all variables

* Create dummy coded features for categorical predictors

* Remove near-zero variance features

```{r}
rec_full <- 
```

### Recipe 2: Compact model

Because your recipe for the compact model will only differ from your recipe for the full model by one step (removing your focal variables), you can start with `rec_full`. Then add a step that will remove the *dummy coded* focal variables `father_educ` and `mother_educ`. Remember you can make a feature matrix to see what these dummy coded variables end up being named!

```{r}
rec_compact <- 
```

## Fit models

### Set up a hyperparemeter tuning grid
```{r}
tune_grid <- 
```


### Fit the full model

Use `rec_full`, `splits`, and `tune_grid` to fit GLM's across folds.  Use RMSE as your metric. Save your model fits as `fits_full`. 
```{r}

```

Make sure you considered a good range of hyperparameter values
```{r}

```

Select best model configuration
```{r}

```

Print the mean RMSE of the best model configuration across the 30 held-out folds.
```{r}
collect_metrics(fits_full, summarize = TRUE) |> 
  filter(.config == select_best(fits_full)$.config)
```

### Fit the compact model

Use `rec_compact`, `splits`, and `tune_grid` to fit GLM's across folds. Use RMSE as your metric. Save your model fits as `fits_compact`. 
```{r}

```

Select best model configuration
```{r}

```

Print the mean RMSE of the best model configuration across the 30 held-out folds.
```{r}
collect_metrics(fits_full, summarize = TRUE) |> 
  filter(.config == select_best(fits_compact)$.config)
```

## Model comparison with the Bayesian approach
You will now compare your models using the Bayesian parameter estimation

### Gather performance estimates

First, make a dataframe containing the 30 performance estimates from held out folds for your full and compact model. Hint you can use the code abover but change `summarize = TRUE` to `summarize = FALSE`. Filter down so you only have the following variables (`id`, `id2`, `.estimate`). Rename `.estimate` to `full` or `compact` before joining your estimates.
```{r}


```


### Posterior probabilities

Next, derive the posterior probabilities for the RMSE of each of these two models
```{r}


```

### Graph positerior probabilities

Display your posterior probabilities using both a density plot and a histogram. Choose plots that would be the most useful to display your results to a collaborator.
```{r}


```

### Determine if the full model has better performance

Calculate the probability that the full model is performing with lower error than the compact model. 

```{r}


```

Type out a summary of what you find that includes:

* Your interpretation of what the mean increase in RMSE and 95% HDI values represent

* Your conclusion if the full model is meaningfully better than your compact model and why   

*Type your summary here*

### Evaluate best model performance

Finally, get a final performance estimate for your best model using `data_test` 

```{r}

```


## Feature importance

You will now look at feature importance in your full model using Shapely Values.

### Prep data
Get your data ready for use with the `DALEX` package. Do the following:    

Create a feature matrix of your full model 

```{r}

```


Pull out your raw features and outcome to be utilized in calculating feature importance

```{r}

```


Use the following code to define your predictor function (`predict_wrapper`), explainer object (`explain_full`), and RMSE wrapper function (`rmse_wrapper`).
```{r}
predict_wrapper <- function(model, newdata) {
  predict(model, newdata) |>  
    dplyr::select(grade = .pred)
}

explain_full <- explain_tidymodels(fit_all_data, 
                                   data = x, 
                                   y = y, 
                                   predict_function = predict_wrapper,
                                   predict_function_target_column = "grade")

rmse_wrapper <- function(observed, predicted) {
  rmse_vec(observed, predicted)
}
```

### Calculate Shapely Values for a single participant

First, we will look at shapely values for a single participant. For this example, lets look at the last participant in our data set. Do the following:

* Print the raw feature values for the last participant. 

* Generate Shapely Values for this participant (you might consider caching this!)

* Use `ggplot()` to plot the Shapely Values for this participant

```{r}


```



### Calculate mean absolute Shapely Values across all participants

Now we will look at feature importance across all participants. Include the following steps:

* Calculate Shapely Values for each observation and `glimpse()` the resulting tibble

* Plot the mean absolute Shapely Values across all participants

```{r}


```

Write a brief description of your top takeaways from this plot.    

*Type your response here.*





