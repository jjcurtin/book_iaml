---
title: 'Unit 10 (Random Forest And Ensemble methods)'
author: "Your name here"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
    toc_float: true
    code_folding: show
editor_options: 
  chunk_output_type: console
---
## Assignment overview

To begin, download the following from the course web book (Unit 10):

* One RMarkdown (.Rmd) file: hw_unit_10_random_forest.Rmd (this file)

* fifa.csv : training & test data for this assignment


The data for this week's assignment provide rankings data of FIFA soccer players. While the entire database is available [online][https://www.fifaindex.com/], I have subsetted the data down to a random sample of 2000 players for the sake of computational costs.

The outcome variable in this dataset the *overall* variable, which is the overall quality rating for the player out of 100. This file also includes 15 predictor variables that describe differing attributes of each player (e.g. height, age, nationality, agility) as well as an ID variable listing the name of the player.

The goal of this assignment is to build decision tree and random forest regression models to predict overall player quality of FIFA soccer players.


## Setup

Load any packages, function scripts, and file paths you will need for this assignment.

```{r}


```

## 1. Prep data

Prepare the data to be used to fit both a decision tree and random forest model to predict *overall* from all predictor variables.

* Make sure variable names are clean

* Make sure your code is reproducible

* Split your data into overall training and test sets

* Set up 2 recipes (one for a decision tree, one for random forest) to predict *overall* from all predictor variables. Include the minimum necessary steps you would need in your recipe to fit each model (i.e., do the fewest amount of feature engineering steps possible; you do not need to do any additional/advanced feature engineering)

```{r}




```

## 2. Decision Tree

Fit a decision tree to predict **overall** from all predictor variables in your training data.

* Use the *rpart* engine

* Use *rmse* as your metric

* Tune on *cost_complexity* and *min_n* using 100 bootstraps. Set *tree_depth* to 10 (Practically, you would never do this. This is just to lower run time of your models)

* Provide visualizations (graphs) to support that you considered a wide enough range of  hyperparameter values

* Print your best hyperparameter values and the held out/out of bag training RMSE

* Examine variable importance in your best performing model in train

```{r}


```

## 3. Random Forest

Now let's see what bagging and decorrelating can do for this model. Fit a random forest model to predict **overall** from all predictor variables in your training data.

* Use the *ranger* engine

* Use *rmse* as your metric

* Tune on *mtry* and *min_n* using 100 bootstraps. Set *trees* to 100 (Practically, you would never do this. This is just to lower run time of your models)

* Provide visualizations (graphs) to support that you considered a wide enough range of  hyperparameter values

* Print your best hyperparameter values and the held out/out of bag training RMSE

* Examine variable importance in your best performing model in train. Use permutation feature importance as the method

```{r}


```

## 4. Evaluate best model in test

* Select your best model based on bootstrapped performance in the training data

* Evaluate the performance of this best model in your held out test set

* Print the RMSE of your best model in test

* Provide a visualization (graph) of your model's performance in test

```{r}



```
