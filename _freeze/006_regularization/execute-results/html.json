{
  "hash": "02d9530a2de72035a271c4f451e9795f",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Regularization and Penalized Models\n\n## Overview of Unit\n\n###  Learning Objectives\n\n- Subsetting approaches: Forward, Backward, Best Subset (covered in reading only)\n- Cost and Loss functions\n  - What are they and how are they used\n  - What are the specific formulas for linear model, logistic regression, and variants of glmnet (ridge, LASSO, full elasticnet)\n- What is regularization\n  - What are its benefits?\n  - What are its costs?\n- How does lambda affect bias-variance trade-off in glmnet\n- What does alpha do?\n- Feature engineering approaches for dimensionality reduction: PCA (covered in reading only)\n- Other algorithms that do feature selection/dimensionality reduction: PCR and PLS (covered in reading only)\n- Contrasts of PCA, PCR, PLS, and glmnet/LASSO for dimensionality reduction (covered in reading only)\n\n-----\n\n### Readings\n\n- @ISL [Chapter 6, pp 225 - 267](pdfs/isl_2.pdf)\n\nPost questions to the readings channel in Slack\n\n### Lecture Videos\n\n- [Lecture 1: An Introduction to Penalized/Regularized Algorithms](https://mediaspace.wisc.edu/media/iaml+unit+6-1/1_dqafjta6) ~ 15 mins\n- [Lecture 2: [Intuitions about Penalized Cost Functions and Regularization](https://mediaspace.wisc.edu/media/iaml+unit+6-2/1_uxp8w6am) ~ 11 mins\n- [Lecture 3: Ridge Regression](https://mediaspace.wisc.edu/media/iaml+unit+6-3/1_hcghfpo3) ~ 9 mins\n- [Lecture 4: LASSO](https://mediaspace.wisc.edu/media/iaml+unit+6-4/1_803c42m3) ~ 8 mins\n- [Lecture 5: The Elastic net](https://mediaspace.wisc.edu/media/iaml+unit+6-5/1_r4xy5dws) ~ 4 mins\n- [Lecture 6: Emprical Example - Many good predictors](https://mediaspace.wisc.edu/media/iaml+unit+6-6/1_2608sidm) ~ 23 mins\n- [Lecture 7: Emprical Example - Good and zero predictors](https://mediaspace.wisc.edu/media/iaml+unit+6-7/1_xmgkkr8t) ~ 9 mins\n- [Lecture 8: Emprical Example - LASSO for covariate selection](https://mediaspace.wisc.edu/media/iaml+unit+6-8/1_hlknwl7m) ~ 8 mins\n\nPost questions to the video-lectures channel in Slack\n\n-----\n\n### Application Assignment and Quiz\n\n- [data](application_assignments/unit_06/ames_full_cln.csv)\n- [data dictionary](application_assignments/unit_06/ames_data_dictionary.pdf)\n- [qmd shell](https://raw.githubusercontent.com/jjcurtin/book_iaml/main/application_assignments/unit_06/hw_unit_06_regularization.qmd)\n- [solution](https://dionysus.psych.wisc.edu/iaml/key_unit_06_regularization.html) \n\n\n\nSubmit the application assignment [here](https://canvas.wisc.edu/courses/395546/assignments/2187691) and complete the [unit quiz](https://canvas.wisc.edu/courses/395546/quizzes/514047) by 8 pm on Wednesday, February 28th \n\n-----\n\n## Introduction to Penalized/Regularized Statistical Algorithms\n\n### Overview\n\nComplex (e.g., flexible) models increase the chance of overfitting to the training set.  This leads to:\n\n- Poor prediction\n- Burdensome prediction models for implementation (need to measure lots of predictors)\n- Low power to test hypothesis about predictor effects\n\\\n\\\n\nComplex models are difficult to interpret\n\n-----\n\nRegularization is technique that:\n\n- Reduces overfitting\n- Allows for p >> n (!!!)\n- May yield more interpretable models (LASSO, Elastic Net)\n- May reduce implementation burden (LASSO, Elastic Net)\n\n-----\n\nRegularization does this by applying a penalty to the parametric model coefficients (parameter estimates)\n\n- This constrains/shrinks these coefficients to yield a simpler/less overfit model\n- Some types of penalties shrink the coefficients to zero (feature selection)\n\\\n\\\n\nWe will consider three approaches to regularization\n\n- L2 (Ridge)\n- L1 (LASSO)\n- Elastic net\n\\\n\\\n\nThese approaches are available for both regression and classification problems and for a variety of parametric statistical algorithms\n\n-----\n\n### Cost functions\n\nTo understand regularization, we need to first explicitly consider **loss/cost functions** for the parametric statistical models we have been using.\n\n- A **loss function** quantifies the error between a single predicted and observed outcome within some statistical model.  \n\n- A **cost function** is simply the aggregate of the loss across all observations in the training sample.\n\\\n\\\n\nOptimization procedures (least squares, maximum likelihood, gradient descent) seek to determine a set of parameter estimates that minimize some specific cost function for the training sample.\n\n-----\n\nThe cost function for the linear model is the mean squared error (squared loss): \n\n- $\\frac{1}{n}\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}$\n\n- No constraints or penalties are placed on the parameter estimates ($\\beta_k$)\n\n- They can take on any values with the only goal to minimize the MSE in the training sample\n\n-----\n\nThe cost function for logistic regression is log loss:\n\n- $\\frac{1}{n}\\sum_{i = 1}^{n} -Y_ilog(\\hat{Y_i}) - (1-Y_i)log(1-\\hat{Y_i})$\n\n- where $Y_i$ is coded 0,1 and $\\hat{Y_i}$ is the predicted probability that Y = 1\n\n- Again, no constraints or penalties are placed on the parameter estimates ($\\beta_k$)\n\n- They can take on any values with the only goal to minimize the sum of the log loss in the training sample\n\\\n\\\n\n![](figs/unit5_log_loss.png){height=5in}\n-----\n\n## Intuitions about Penalized Cost Functions and Regularization \n\nThis is an example from a series of wonderfully clear lectures in a [machine learning course](https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN) by Andrew Ng in Coursera.\n\n- [Regularization: The Problem Of Overfitting](https://www.youtube.com/watch?v=u73PU6Qwl1I)\n- [Regularization: Cost Functions](https://www.youtube.com/watch?v=KvtGD37Rm5I)\n\n-----\n\nLets imagine a training set:\n\n- House sale price predicted by house size\n- True DGP is quadratic.  Diminishing increase in sale price as size increases\n- N = 5 in training set\n\n\\\n\\\n\n![](figs/unit5_ng1.png){height=5in}\n\n-----\n\nIf we fit a linear model with size as the only feature...\n\n- $\\hat{sale\\_price_i} = \\beta_0 + \\beta_1 * size$\n\n- In this training set, we might get the model below (in blue)\n\n- This is a biased model (predicts too high for low and high house sizes; predicts too low for moderate size houses)\n\n- If we took this model to new data from the same quadratic DGP, it would clearly not predict very well\n\n![](figs/unit5_ng2.png){height=5in}\n\n-----\n\nLets consider the other extreme\n\n- If we fit a 4th order polynomial model using size...\n- $\\hat{sale\\_price_i} = \\beta_0 + \\beta_1 * size + \\beta_2 * size^2 + \\beta_3 * size^3 + \\beta_4 * size^4$\n- In this training set, we would get the model below (in blue)\n- This is model is overfit to this training set.  It would not predict well in new data from the same quadratic DGP\n- Also, the model would have high variance (if we estimated the parameters in another N = 5 training set, they would be very different)\n\n![](figs/unit5_ng3.png){height=5in}\n\n-----\n\nThis problem with overfitting and variance isn't limited to polynomial regression.\n\n- We would have the same problem (perfect fit in training with poor fit in new val data) if we predicted housing prices with many features when the training N = 5.  e.g.,\n\n- $\\hat{sale\\_price_i} = \\beta_0 + \\beta_1 * size + \\beta_2 * year\\_built + \\beta_3 * num\\_garages + \\beta_4 * quality$\n\n-----\n\nObviously, the correct model to fit is a second order polynomial model with size\n\n- $\\hat{sale\\_price_i} = \\beta_0 + \\beta_1 * size + \\beta_2 * size^2$\n- But we couldn't know this with real data because we wouldn't know the underlying DGP\n- When we don't know the underlying DGP, we need to be able to consider potentially complex models with many features in some way that diminishes the potential problem with overfitting/model variance\n\n![](figs/unit5_ng4.png){height=5in}\n\n-----\n\nWhat if we still fit a fourth order polynomial but changed the cost function to penalize the absolute value of $\\beta_3$ and $\\beta_4$ parameter estimates?\n\\\n\\\n**Typical cost based on MSE/squared loss:**\n\n- $\\frac{1}{n}\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}$\n\\\n\\\n\n**Our new cost function:**\n\n- $[\\frac{1}{n}\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] +  [1000 * \\beta_3 + 1000 * \\beta_4]$\n\n-----\n\n$[\\frac{1}{n}\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [1000 * \\beta_3 + 1000 * \\beta_4]$\n\n- The only way to make the value of this new cost function small is to make $\\beta_3$ and $\\beta_4$ small\n\n- If we made the penalty applied to $\\beta_3$ and $\\beta_4$ large (e.g., 1000 as above), we will end up with the  parameter estimates for these two features at approximately 0.\n\n- With a sufficient penalty applied, their parameter estimates will only change from zero to the degree that these changes accounted for a large enough drop in MSE to offset this penalty in the overall aggregate cost function.\n\n-----\n\n$[\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + 1000 * \\beta_3 + 1000 * \\beta_4$\n\n- With this penalty in place, our final model might shift from the blue model to the pink model below.  The pink model is mostly quadratic but with a few extra \"wiggles\" if $\\beta_3$ and $\\beta_4$ are not exactly 0.\n\n![](figs/unit5_ng5.png){height=5in}\n\n-----\n\nOf course, we don't typically know in advance which parameter estimates to penalize. \n\n- Instead, we apply some penalty to all the parameter estimates (except $\\beta_0$)\n- This shrinks the parameter estimates for all the features to some degree\n- However, features that do reduce MSE meaningfully will be \"worth\" including with non-zero parameter estimates\n- You can also control the shrinkage by controlling the size of the penalty\n\n-----\n\nIn general, regularization produces models that: \n\n- Are simpler (e.g. smoother, smaller coefficients/parameter estimates)\n- Are less prone to overfitting\n- Allow for models with p >> n\n- Are sometimes more interpretable (LASSO, Elastic Net)\n\\\n\\\n\nThese benefits are provided by the introduction of some bias into the parameter estimates\n\\\n\\\nThis allows for a bias-variance trade-off where some bias is introduced for a big reduction in variance of model fit\n\n-----\n\nWe will now consider three regularization approaches that introduce different types of penalties to shrink the parameter estimates\n\n- L2 (Ridge)\n- L1 (LASSO)\n- Elastic net\n\\\n\\\n\nThese approaches are available for both regression and classification problems and for a variety of parametric statistical algorithms\n\\\n\\\nA fourth common regularized classification model (also sometimes used for regression) is the support vector machine (not covered in class but commonly used as well and easy to understand with this foundation)\n\\\n\\\nEach of these approaches uses a different specific penalty, which has implications for how the model performs in different settings\n\n-----\n\n## Ridge Regression\n\nThe cost function for Ridge Regression is:\n\n- $\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:])$\n\\\n\\\n\nIt has two components:\n\n- Inside the left brackets is the SSE from linear regression\n- Inside the right brackets is the **Ridge penalty**.  \n\\\n\\\n\nThis penalty:\n\n- Includes the sum of the squared parameter estimates (excluding $\\beta_0$).  Squaring removes the sign of these parameter estimates.\n- This sum is multiplied by $\\lambda$, a hyperparameter in Ridge regression.  Lambda allows us to tune the size of the penalty.\n- This is an application of the L2 norm (matrix algebra) to the vector of parameter estimates\n\n-----\n\n$\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:])$\n\\\n\\\n\n::: {.callout-important collapse=\"false\"}\n### Question: What will happen to a Ridge regression model's parameter estimates and its performance (i.e., its bias & variance) as lambda increases/decreases?\n\n::: {.cell}\n\n```{.html .cell-code  code-fold=\"true\" code-summary=\"Show Answer\"}\nAs lambda increases, the model becomes less flexible b/c its parameter estimates \nbecome constrained/shrunk.  This will increase bias but decrease variance for model \nperformance.\n```\n:::\n\n:::\n\n-----\n\n$\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:])$\n\\\n\\\n\n::: {.callout-important collapse=\"false\"}\n### Question: What is the special case of Ridge regression when lambda = 0?\n\n::: {.cell}\n\n```{.html .cell-code  code-fold=\"true\" code-summary=\"Show Answer\"}\nThe OLS regression is a special case where lambda = 0 (i.e., no penalty is applied).  \nThis is the most flexible. It is unbiased but with higher variance than for \nnon-zero values of lambda\n```\n:::\n\n:::\n\n-----\n\nLets compare Ridge regression to OLS (ordinary least squares with squared loss cost function) linear regression\n\n- Ridge parameter estimates are biased but have lower variance (smaller SE) than OLS\n\n- Ridge may predict better in new data\n  - This depends on the value of $\\lambda$ selected and its impact on bias-variance trade-off in Ridge regression vs. OLS\n  - There does exist a value of $\\lambda$ for which Ridge predicts better than OLS in new data\n\n- Ridge regression (but not OLS) allows for p > (or even >>) than n\n\n- Ridge regression (but not OLS) accommodates highly correlated (or even perfectly multi-collinear) features \n\n- OLS (but not Ridge regression) is scale invariant\n  - You should scale (mean and standard deviation correct) features for use with Ridge regression\n\n-----\n\n$\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:])$\n\\\n\\\n\n::: {.callout-important collapse=\"false\"}\n### Question: Why does the scale of the features matter for Ridge regression?\n\n::: {.cell}\n\n```{.html .cell-code  code-fold=\"true\" code-summary=\"Show Answer\"}\nFeatures with bigger SDs will have smaller parameter estimates.  Therefore they \nwill be less affected by the penalty.\n```\n:::\n\n:::\n\\\n\\\nUnless the features are on the same scale to start, you should standardize them for all applications (regression and classification) of Ridge (and also LASSO and elastic net).  You can handle this during feature engineering in the recipe.\n\n-----\n\n## LASSO Regression\n\nLASSO is an acronym for Least Absolute Shrinkage and Selection Operator\n\\\n\\\nThe cost function for LASSO Regression is:\n\n- $\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} |\\beta_j|\\:])$\n\\\n\\\n\nIt has two components:\n\n- Inside the left brackets is the SSE from linear regression\n- Inside the right brackets is the **LASSO penalty**.  \n\\\n\\\n\nThis penalty:\n\n  - Includes the sum of the absolute value of the parameter estimates (excluding $\\beta_0$).  The absolute value removes the sign of these parameter estimates.\n  - This sum is multiplied by $\\lambda$, a hyperparameter in LASSO regression.  Lambda allows us to tune the size of the penalty.\n  - This is an application of the L1 norm to the vector of parameter estimates\n\n-----\n\n### LASSO vs. Ridge Comparison\n\nWith respect to the parameter estimates: \n\n- LASSO yields sparse solution (some parameter estimates set to **exactly zero**)\n\n- Ridge tends to retain all features (parameter estimates don't get set to exactly zero)\n\n- LASSO selects one feature among correlated group and sets others to zero\n\n- Ridge shrinks all parameter estimates for correlated features \n\\\n\\\n\nRidge tends to outperform LASSO wrt prediction in new data.  There are cases where LASSO can predict better (most features have zero effect and only a few are non-zero) but even in those cases, Ridge is competitive.  \n\n-----\n\n### Advantages of LASSO\n\n- Does feature selection (sets parameter estimates to exactly 0)\n  - Yields a sparse solution\n  - Sparse model is more interpretable?\n  - Sparse model is easier to implement? (fewer features included so don’t need to measure as many predictors)\n\n- More robust to outliers (similar to LAD vs. OLS)\n\n- Tends to do better when there are a small number of robust features and the others are close to zero or zero\n\n-----\n\n### Advantages of Ridge\n\n- Computationally superior (closed form solution vs. iterative;  Only one solution to minimize the cost function)\n- More robust to measurement error in features (remember no measurement error is an assumption for unbiased estimates in OLS regression)\n- Tends to do better when there are many features with large (and comparable) effects (i.e., most features are related to the outcome)\n\n-----\n\n## Elastic Net Regression\n\nThe Elastic Net blends the L1 and L2 penalties to obtain the benefits of each of those approaches.\n\\\n\\\nWe will use the implementation of the Elastic Net in [glmnet](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf) in R.  \n\\\n\\\nYou can also read additional [introductory documentation](https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet.pdf) for this package\n\n-----\n\nIn the Gaussian regression context, the Elastic Net cost function is:\n\n- $\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda (\\alpha\\sum_{j = 1}^{p} |\\beta_j| + (1-\\alpha)\\sum_{j = 1}^{p} \\beta_j^{2})\\:])$\n\\\n\\\n\nThis model has two hyper-parameters\n\n- $\\lambda$ controls the degree of regularization as before\n- $\\alpha$ is a \"mixing\" parameter that blends the degree of L1 and L2 contributions to the aggregate penalty. (Proportion of LASSO penalty)\n  - $\\alpha$ = 1 results in the LASSO model\n  - $\\alpha$ = 0 results in the Ridge model\n  - Intermediate values for $\\alpha$ blend these penalties together proportionally to include more or less LASSO penalty\n\n-----\n\nAs before (e.g., KNN), best values of $\\lambda$ (and $\\alpha$) can be selected using resampling using `tune_grid()`\n\\\n\\\nThe grid needs to have crossed values of both `penalty` ($lambda$) and `mixture` ($alpha$) for glmnet\n\n  - Can use `expand_grid()`\n  - Only penalty is needed in grid if fitting a Ridge or LASSO model.\n\n-----\n\n\n## Empirical Example 1: Many \"good\" but correlated predictors\n\n\n\n\n\nFor the first example, we will simulate data with:\n\n- Many correlated predictors\n- All related to outcome\n- Get a small training sample\n- Get a big test sample (for more precise estimate of model performance)\n\n-----\n\nFirst we set the predictors for our simulation\n\n::: {.cell}\n\n```{.r .cell-code}\nn_cases_trn <- 100\nn_cases_test <- 1000\nn_x <- 20\ncovs_x <- 50\nvars_x <- 100\nb_x <- rep(1, n_x) # one unit change in y for 1 unit change in x\ny_error <- 100\n```\n:::\n\n\n-----\n\nThen we draw samples from population\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nmu <- rep(0, n_x)  # means for all variables = 0\nsigma <- matrix(covs_x, nrow = n_x, ncol = n_x)\ndiag(sigma) <- vars_x  \nsigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]  100   50   50   50   50   50   50   50   50    50    50    50    50\n [2,]   50  100   50   50   50   50   50   50   50    50    50    50    50\n [3,]   50   50  100   50   50   50   50   50   50    50    50    50    50\n [4,]   50   50   50  100   50   50   50   50   50    50    50    50    50\n [5,]   50   50   50   50  100   50   50   50   50    50    50    50    50\n [6,]   50   50   50   50   50  100   50   50   50    50    50    50    50\n [7,]   50   50   50   50   50   50  100   50   50    50    50    50    50\n [8,]   50   50   50   50   50   50   50  100   50    50    50    50    50\n [9,]   50   50   50   50   50   50   50   50  100    50    50    50    50\n[10,]   50   50   50   50   50   50   50   50   50   100    50    50    50\n[11,]   50   50   50   50   50   50   50   50   50    50   100    50    50\n[12,]   50   50   50   50   50   50   50   50   50    50    50   100    50\n[13,]   50   50   50   50   50   50   50   50   50    50    50    50   100\n[14,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[15,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[16,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[17,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[18,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[19,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n[20,]   50   50   50   50   50   50   50   50   50    50    50    50    50\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20]\n [1,]    50    50    50    50    50    50    50\n [2,]    50    50    50    50    50    50    50\n [3,]    50    50    50    50    50    50    50\n [4,]    50    50    50    50    50    50    50\n [5,]    50    50    50    50    50    50    50\n [6,]    50    50    50    50    50    50    50\n [7,]    50    50    50    50    50    50    50\n [8,]    50    50    50    50    50    50    50\n [9,]    50    50    50    50    50    50    50\n[10,]    50    50    50    50    50    50    50\n[11,]    50    50    50    50    50    50    50\n[12,]    50    50    50    50    50    50    50\n[13,]    50    50    50    50    50    50    50\n[14,]   100    50    50    50    50    50    50\n[15,]    50   100    50    50    50    50    50\n[16,]    50    50   100    50    50    50    50\n[17,]    50    50    50   100    50    50    50\n[18,]    50    50    50    50   100    50    50\n[19,]    50    50    50    50    50   100    50\n[20,]    50    50    50    50    50    50   100\n```\n\n\n:::\n\n```{.r .cell-code}\nx <- MASS::mvrnorm(n = n_cases_trn, mu, sigma) |> \n  magrittr::set_colnames(str_c(\"x_\", 1:n_x)) |>\n  as_tibble()\ndata_trn_1 <- x |> \n  mutate(y = rowSums(t(t(x)*b_x)) + rnorm(n_cases_trn, 0, y_error)) |>  \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100\nColumns: 21\n$ x_1  <dbl> 0.9961431, 14.6304657, 4.7113834, 5.4731915, 2.2835857, -16.15454…\n$ x_2  <dbl> 8.6957591, 0.8347032, -0.5353713, -12.5511722, -1.0787145, -16.42…\n$ x_3  <dbl> 10.6129264, 20.4883323, -8.6503901, -1.5413989, -17.7641062, -14.…\n$ x_4  <dbl> 11.80601585, 11.29743568, 4.46220219, 0.79397084, -3.78878523, -2…\n$ x_5  <dbl> 18.835971, 6.095917, -3.272026, -1.839770, -1.610545, -6.040994, …\n$ x_6  <dbl> 2.223292893, 11.500104110, -0.457979676, -8.496314755, 7.87014122…\n$ x_7  <dbl> 10.5129546, 6.4060743, 12.0210560, -13.7151243, 11.1422588, -14.2…\n$ x_8  <dbl> 8.8124570, 9.6462680, 2.4037228, -0.2015028, 6.3737654, -3.064850…\n$ x_9  <dbl> -3.67513802, 3.26579725, -7.25050948, -12.31704216, -1.69613573, …\n$ x_10 <dbl> -2.38262537, -6.46882639, 1.83840881, 2.57893960, 6.40948837, -27…\n$ x_11 <dbl> 14.9730316, 15.6370590, -1.1323317, 6.5593694, 5.1039078, -7.5830…\n$ x_12 <dbl> -4.47755314, 0.04826209, 2.59168486, -4.26699855, 16.49056393, -1…\n$ x_13 <dbl> 8.5928287, -4.0292190, -14.4559758, 6.0387060, 2.3659570, -13.319…\n$ x_14 <dbl> -10.54664618, 3.85439473, -7.14415536, 5.64697664, 2.86171243, -1…\n$ x_15 <dbl> -5.9948683, 0.6097567, 4.8671687, 0.7012719, 14.8754506, 3.478677…\n$ x_16 <dbl> 7.9805572, 12.1442098, -2.2095452, -9.1514837, 10.0124739, -2.676…\n$ x_17 <dbl> -2.4674864, -13.6498178, 1.0131614, 3.7409088, 5.9397242, -10.243…\n$ x_18 <dbl> 9.9571661, 5.7690926, 5.3204675, -13.9289436, 14.6130884, -19.710…\n$ x_19 <dbl> -3.37220750, 1.23419418, -6.67125147, -4.95630437, 8.94440082, -5…\n$ x_20 <dbl> 3.7686080, 3.4971897, -3.2892748, -14.2852637, -1.5467975, -28.72…\n$ y    <dbl> 24.067076, 210.433707, -73.482134, 44.144652, 228.535603, -259.78…\n```\n\n\n:::\n\n```{.r .cell-code}\nx <- MASS::mvrnorm(n = n_cases_test, mu, sigma) |> \n  magrittr::set_colnames(str_c(\"x_\", 1:n_x)) |>\n  as_tibble() \ndata_test_1 <- x |> \n  mutate(y = rowSums(t(t(x)*b_x)) + rnorm(n_cases_test, 0, y_error))\n```\n:::\n\n\n-----\n\nSet up a tibble to track model performance in train and test sets\n\n- We are using test to repeatedly to get rigorous held-out performance separate from model selection process.  \n- Just for our understanding \n- We would not choose a model configuration based on test set error\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerror_ex1 <- tibble(model = character(), \n                    rmse_trn = numeric(), \n                    rmse_test = numeric()) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 0\nColumns: 3\n$ model     <chr> \n$ rmse_trn  <dbl> \n$ rmse_test <dbl> \n```\n\n\n:::\n:::\n\n\n-----\n\n### Fit a standard (OLS) linear regression\n\nFit the linear model\n\n- No feature engineering needed. Can use raw predictors as features\n- No resampling needed b/c there are no hyperparameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lm_1 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(y ~ ., data = data_trn_1)\n\nfit_lm_1 |> \n  tidy() |> \n  print(n = 21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 5\n   term        estimate std.error statistic p.value\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)   -5.22      12.0    -0.436   0.664 \n 2 x_1            1.22       1.40    0.869   0.388 \n 3 x_2           -1.42       1.62   -0.874   0.385 \n 4 x_3           -0.679      1.59   -0.427   0.671 \n 5 x_4            0.392      1.49    0.263   0.793 \n 6 x_5            4.20       1.81    2.33    0.0226\n 7 x_6            0.804      1.58    0.510   0.612 \n 8 x_7            0.953      1.50    0.634   0.528 \n 9 x_8            2.30       1.53    1.50    0.137 \n10 x_9            2.37       1.53    1.55    0.126 \n11 x_10           0.758      1.67    0.454   0.651 \n12 x_11           0.530      1.87    0.283   0.778 \n13 x_12           1.44       1.78    0.813   0.419 \n14 x_13           2.40       1.65    1.45    0.150 \n15 x_14           3.26       1.89    1.72    0.0889\n16 x_15           0.110      1.53    0.0719  0.943 \n17 x_16          -1.28       1.68   -0.761   0.449 \n18 x_17          -0.144      1.55   -0.0926  0.926 \n19 x_18           2.74       1.66    1.65    0.102 \n20 x_19           1.36       1.61    0.843   0.402 \n21 x_20          -1.49       1.67   -0.893   0.375 \n```\n\n\n:::\n:::\n\n\n-----\n\nIrreducible error was set by `y_error` (100)\n\n- Overfit to train\n- Much worse in test\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_vec(truth = data_trn_1$y, \n         estimate = predict(fit_lm_1, data_trn_1)$.pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 91.86214\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_vec(truth = data_test_1$y, \n         estimate = predict(fit_lm_1, data_test_1)$.pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 112.3208\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n-----\n\n### Fit LASSO\n\nLASSO, Ridge, and glmnet all need features on same scale to apply penalty consistently\n\n- Use `step_normalize()`.  This sets mean = 0, sd = 1 (NOTE: Bad name as it does NOT change shape of distribution!)\n- Can use same recipe for LASSO, Ridge, and glmnet\n- Can use same train and test feature matrices as well\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nrec_1 <- recipe(y ~ ., data = data_trn_1) |> \n  step_normalize(all_predictors())\n\nrec_prep_1 <- rec_1 |> \n  prep(data_trn_1)\n\nfeat_trn_1 <- rec_prep_1 |> \n  bake(data_trn_1)\n\nfeat_test_1 <- rec_prep_1 |> \n  bake(data_test_1)\n```\n:::\n\n\n-----\n\nSet up splits for resampling for tuning hyperparameters\n\n- Use bootstrap for more precise estimation (even if more biased).  Good for selection\n- Can use same bootstrap splits for LASSO, Ridge, and glmnet\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20140102)\nsplits_boot_1 <- data_trn_1 |> \n   bootstraps(times = 100, strata = \"y\")  \n```\n:::\n\n\n-----\n\nNow onto the LASSO....\n\nWe need to tune $\\lambda$ (tidymodels calls this `penalty`)\n\n- $\\alpha$ = 1 (tidymodels calls this `mixture`)\n- Set up grid with exponential values for `penalty`\n- `glmnet` uses warm starts so can fit lots of values for $\\lambda$ quickly\n- Could also use `cv.glmnet()` directly in `glmnet` package to find good values.  See `get_lamdas()` in fun_modeling.R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_lasso <- expand_grid(penalty = exp(seq(-4, 4, length.out = 500)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_lasso_1 <- xfun::cache_rds(\n  expr = {\n  linear_reg(penalty = tune(), \n               mixture = 1) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_1, \n              resamples = splits_boot_1, \n              grid = grid_lasso, \n              metrics = metric_set(rmse))\n\n   },\n   rerun = rerun_setting,\n   dir = \"cache/006/\",\n   file = \"fits_lasso_1\")\n```\n:::\n\n\n-----\n\nEvaluate model performance in validation sets (OOB)\n\nMake sure that you have hit a clear minimum (bottom of U or at least an asymptote)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_hyperparameters(fits_lasso_1, hp1 = \"penalty\", metric = \"rmse\")\n```\n\n::: {.cell-output-display}\n![](006_regularization_files/figure-html/u6-examples-1-8-1.png){width=672}\n:::\n:::\n\n\n-----\n\nFit best configuration (i.e., best lambda) to full train set\n\n- Use `select_best()`\n- Don't forget to indicate which column ($penalty$)\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lasso_1 <-\n  linear_reg(penalty = select_best(fits_lasso_1)$penalty, \n             mixture = 1) |>\n  set_engine(\"glmnet\") |> \n  fit(y ~ ., data = feat_trn_1)\n```\n:::\n\n\n-----\n\nWe can now use `tidy()` to look at the LASSO parameter estimates\n\n- `tidy()`  uses `Matrix` package, which has conflicts with `tidyr`.  Load the package without those conflicting functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Matrix, exclude = c(\"expand\", \"pack\", \"unpack\"))\n```\n:::\n\n\n-----\n\nNow call `tidy()`\n\n- Notice that LASSO sets some $\\beta$ to 0 even though none are 0 in DGP\n- LASSO is not great at reproducing the DGP!\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lasso_1 |> \n  tidy() |> \n  print(n = 21)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 3\n   term        estimate penalty\n   <chr>          <dbl>   <dbl>\n 1 (Intercept)    29.9     10.8\n 2 x_1             7.43    10.8\n 3 x_2             0       10.8\n 4 x_3             0       10.8\n 5 x_4             0       10.8\n 6 x_5            34.2     10.8\n 7 x_6             8.03    10.8\n 8 x_7             1.75    10.8\n 9 x_8            17.5     10.8\n10 x_9            21.0     10.8\n11 x_10            7.97    10.8\n12 x_11            3.43    10.8\n13 x_12            7.08    10.8\n14 x_13           20.8     10.8\n15 x_14           25.8     10.8\n16 x_15            4.94    10.8\n17 x_16            0       10.8\n18 x_17            0       10.8\n19 x_18           23.5     10.8\n20 x_19            9.70    10.8\n21 x_20            0       10.8\n```\n\n\n:::\n:::\n\n\n-----\n\nIrreducible error was set by `y_error` (100)\n\n- Somewhat overfit to train\n- Somewhat better in test \n\n::: {.cell}\n\n```{.r .cell-code}\n(error_ex1 <- error_ex1 |> \n  bind_rows(tibble(model = \"LASSO model\",                       \n                   rmse_trn = rmse_vec(truth = feat_trn_1$y, \n                                       estimate = predict(fit_lasso_1,\n                                                          feat_trn_1)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_1$y, \n                                        estimate = predict(fit_lasso_1,\n                                                           feat_test_1)$.pred))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  model        rmse_trn rmse_test\n  <chr>           <dbl>     <dbl>\n1 linear model     91.9      112.\n2 LASSO model      94.5      107.\n```\n\n\n:::\n:::\n\n\n-----\n\n### Fit Ridge\n\nFit Ridge algorithm\n\n- Tune $\\lambda$ (`penalty`)\n- May need to experiment to get right range of values for lambda\n- $\\alpha$ = 0 (`mixture`)\n- Evaluate model configurations in OOB validation sets\n<<<<<<< HEAD\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_ridge <- expand_grid(penalty = exp(seq(-1, 7, length.out = 500)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_ridge <- expand_grid(penalty = exp(seq(-1, 7, length.out = 500)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_ridge_1 <- xfun::cache_rds(\n  expr = {\n    linear_reg(penalty = tune(), \n               mixture = 0) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_1, \n              resamples = splits_boot_1, \n              grid = grid_ridge, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/006/\",\n  file = \"fits_ridge_1\")\n```\n:::\n\n\n-----\n\nReview hyperparameter plot\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_hyperparameters(fits_ridge_1, hp1 = \"penalty\", metric = \"rmse\")\n```\n\n::: {.cell-output-display}\n![](006_regularization_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n-----\n\nFit best model configuration (i.e., best lambda) in full train set\n\n- Notice that no $\\beta$ are exactly 0\n- [Why are parameter estimates not near 1 for LASSO and Ridge?]{.red}\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ridge_1 <-\n  linear_reg(penalty = select_best(fits_ridge_1)$penalty, \n             mixture = 0) |>\n  set_engine(\"glmnet\") |> \n  fit(y ~ ., data = feat_trn_1)\n\nfit_ridge_1 |> \n  tidy() |> \n  print(n = 21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 3\n   term        estimate penalty\n   <chr>          <dbl>   <dbl>\n 1 (Intercept)    29.9     276.\n 2 x_1            10.1     276.\n 3 x_2             5.49    276.\n 4 x_3             5.81    276.\n 5 x_4             6.37    276.\n 6 x_5            14.4     276.\n 7 x_6             9.80    276.\n 8 x_7             5.79    276.\n 9 x_8            12.2     276.\n10 x_9            13.3     276.\n11 x_10           10.5     276.\n12 x_11           10.3     276.\n13 x_12            8.44    276.\n14 x_13           12.8     276.\n15 x_14           12.3     276.\n16 x_15           10.2     276.\n17 x_16            3.89    276.\n18 x_17            7.67    276.\n19 x_18           12.4     276.\n20 x_19           11.2     276.\n21 x_20            5.87    276.\n```\n\n\n:::\n:::\n\n\n-----\n\nIrreducible error was set by `y_error` (100)\n\n- Much less overfit to train\n- Still not bad in test \n\n::: {.cell}\n\n```{.r .cell-code}\n(error_ex1 <- error_ex1 |> \n  bind_rows(tibble(model = \"Ridge model\",   \n                   rmse_trn = rmse_vec(truth = feat_trn_1$y, \n                                       estimate = predict(fit_ridge_1,\n                                                          feat_trn_1)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_1$y, \n                                        estimate = predict(fit_ridge_1,\n                                                           feat_test_1)$.pred))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  model        rmse_trn rmse_test\n  <chr>           <dbl>     <dbl>\n1 linear model     91.9      112.\n2 LASSO model      94.5      107.\n3 Ridge model      98.5      103.\n```\n\n\n:::\n:::\n\n\n-----\n\n### Fit glmnet\n\nNow we need to tune both\n\n- $\\lambda$ (`penalty`)\n- $\\alpha$ (`mixture`)\n- Typical to only evaluate a small number of $alpha$\n- Warm starts across $\\lambda$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_glmnet <- expand_grid(penalty = exp(seq(-1, 7, length.out = 500)),\n                           mixture = seq(0, 1, length.out = 6))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_glmnet_1 <- xfun::cache_rds(\n  expr = {\n    linear_reg(penalty = tune(), \n               mixture = tune()) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_1, \n              resamples = splits_boot_1, \n              grid = grid_glmnet, \n              metrics = metric_set(rmse))\n  \n  },\n  rerun = rerun_setting,\n  dir = \"cache/006/\",\n  file = \"fits_glmnet_1\")\n```\n:::\n\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_hyperparameters(fits_glmnet_1, hp1 = \"penalty\", hp2 = \"mixture\", metric = \"rmse\")\n```\n\n::: {.cell-output-display}\n![](006_regularization_files/figure-html/u6-examples-1-15a-1.png){width=672}\n:::\n:::\n\n\n-----\n\nFit best configuration in full train set\n\n- Can use `select_best()` for both hyperparameters, separately\n- Ridge was best (but cool that glmnet could determine that empirically!)\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(fits_glmnet_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  penalty mixture .config                \n    <dbl>   <dbl> <chr>                  \n1    276.       0 Preprocessor1_Model0414\n```\n\n\n:::\n\n```{.r .cell-code}\nfit_glmnet_1 <-\n  linear_reg(penalty = select_best(fits_glmnet_1)$penalty, \n             mixture = select_best(fits_glmnet_1)$mixture) |>\n  set_engine(\"glmnet\") |> \n  fit(y ~ ., data = feat_trn_1)\n```\n:::\n\n\n-----\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_glmnet_1 |> \n  tidy() |> \n  print(n = 21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 3\n   term        estimate penalty\n   <chr>          <dbl>   <dbl>\n 1 (Intercept)    29.9     276.\n 2 x_1            10.1     276.\n 3 x_2             5.49    276.\n 4 x_3             5.81    276.\n 5 x_4             6.37    276.\n 6 x_5            14.4     276.\n 7 x_6             9.80    276.\n 8 x_7             5.79    276.\n 9 x_8            12.2     276.\n10 x_9            13.3     276.\n11 x_10           10.5     276.\n12 x_11           10.3     276.\n13 x_12            8.44    276.\n14 x_13           12.8     276.\n15 x_14           12.3     276.\n16 x_15           10.2     276.\n17 x_16            3.89    276.\n18 x_17            7.67    276.\n19 x_18           12.4     276.\n20 x_19           11.2     276.\n21 x_20            5.87    276.\n```\n\n\n:::\n:::\n\n\n-----\n\nA final comparison of training and test error for the four statistical algorithms\n\n::: {.cell}\n\n```{.r .cell-code}\n(error_ex1 <- error_ex1 |> \n  bind_rows(tibble(model = \"glmnet model\",   \n                   rmse_trn = rmse_vec(truth = feat_trn_1$y, \n                                       estimate = predict(fit_glmnet_1,\n                                                          feat_trn_1)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_1$y, \n                                        estimate = predict(fit_glmnet_1,\n                                                           feat_test_1)$.pred))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  model        rmse_trn rmse_test\n  <chr>           <dbl>     <dbl>\n1 linear model     91.9      112.\n2 LASSO model      94.5      107.\n3 Ridge model      98.5      103.\n4 glmnet model     98.5      103.\n```\n\n\n:::\n:::\n\n\n\n-----\n\n## Empirical Example 2: Good and Zero Predictors\n\nFor the second example, we will simulate data with:\n\n- Two sets of correlated predictors\n- First set related to outcome\n- Second set unrelated to outcome\n- Get a small training sample\n- Get a big test sample (for more precise estimates of performance of our model configurations)\n\n-----\n\nSet up simulation parameters \n\n::: {.cell}\n\n```{.r .cell-code}\nn_cases_trn <- 100\nn_cases_test <- 1000\nn_x <- 20\ncovs_x <- 50 \nvars_x <- 100\nb_x <- rep(c(1,0), each = n_x / 2)\ny_error <- 100\n```\n:::\n\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- rep(0, n_x)  \n\nsigma <- matrix(0, nrow = n_x, ncol = n_x)\nfor (i in 1:(n_x/2)){\n  for(j in 1:(n_x/2)){\n    sigma[i, j] <- covs_x\n  }\n} \nfor (i in (n_x/2 + 1):n_x){\n  for(j in (n_x/2 + 1):n_x){\n    sigma[i, j] <- covs_x\n  }\n} \n\ndiag(sigma) <- vars_x  \n```\n:::\n\n\n-----\n\nSimulate predictors and Y\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2468)\n\nx <- MASS::mvrnorm(n = n_cases_trn, mu, sigma) |> \n  magrittr::set_colnames(str_c(\"x_\", 1:n_x)) |>\n  as_tibble()\ndata_trn_2 <- x |> \n  mutate(y = rowSums(t(t(x)*b_x)) + rnorm(n_cases_trn, 0, y_error)) |>  \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100\nColumns: 21\n$ x_1  <dbl> -1.4627996, -1.8160400, 2.9068102, -6.5569418, -8.0480270, 9.2138…\n$ x_2  <dbl> 17.7272165, 0.6310498, 9.7749301, -9.7193778, -4.3778964, 6.72976…\n$ x_3  <dbl> -0.5051625, -17.7689516, 6.3105699, -10.5572604, -3.9617496, 3.90…\n$ x_4  <dbl> 3.281062, 8.955012, 3.352868, -11.268461, -4.286181, 13.815268, -…\n$ x_5  <dbl> 8.35180401, 6.81665987, 0.06645588, -9.66213146, -8.85459170, 16.…\n$ x_6  <dbl> -5.9821809, 18.5962999, -0.8264266, 1.4451062, -9.3772084, 14.342…\n$ x_7  <dbl> -12.40624948, 4.28231426, 1.53437679, 5.47908047, -4.77878760, 15…\n$ x_8  <dbl> 3.479352, -7.562426, 16.716015, -4.841876, 1.927100, 27.236838, 1…\n$ x_9  <dbl> 1.102297, 1.638666, 10.573017, 5.936702, 2.084463, 11.768179, 5.7…\n$ x_10 <dbl> -6.6174049, 9.8993606, 5.6307484, 5.7061525, -12.3675238, 0.64541…\n$ x_11 <dbl> -0.797748528, -18.721814405, -3.992559581, -21.480685628, -4.0253…\n$ x_12 <dbl> 6.7928298, -7.3322948, -5.8933992, -13.0153958, 2.4618048, -12.34…\n$ x_13 <dbl> -5.68712456, -1.01785876, 7.86559179, -18.79422479, -3.31514615, …\n$ x_14 <dbl> -9.99575229, -16.35133901, -18.02068048, -26.14924286, 1.46020815…\n$ x_15 <dbl> -4.2672643, -4.4350928, -13.6207539, -15.1147495, -5.5577594, -13…\n$ x_16 <dbl> 8.5520238, -2.5639516, 5.5768023, -19.1732900, -4.1939131, 0.7558…\n$ x_17 <dbl> 4.93829278, -14.17190979, -10.23808120, -8.90013599, -9.33297489,…\n$ x_18 <dbl> -1.987745, -9.259699, -13.225873, -28.090077, -13.168306, -1.3565…\n$ x_19 <dbl> 1.5085562, -13.9721276, -5.6622726, -10.9398210, -22.5233973, -8.…\n$ x_20 <dbl> 1.2541564, -19.0101170, -1.6780834, -16.9207862, -7.2618287, 1.13…\n$ y    <dbl> -94.94590, 69.75956, 88.46711, 15.72162, 145.49463, 101.30513, 10…\n```\n\n\n:::\n\n```{.r .cell-code}\nx <- MASS::mvrnorm(n = n_cases_test, mu, sigma) |> \n  magrittr::set_colnames(str_c(\"x_\", 1:n_x)) |>\n  as_tibble()\ndata_test_2 <- x |> \n  mutate(y = rowSums(t(t(x)*b_x)) + rnorm(n_cases_test, 0, y_error)) |>  \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,000\nColumns: 21\n$ x_1  <dbl> -11.082333, 10.623468, -7.962704, -4.360526, 4.170808, -8.056227,…\n$ x_2  <dbl> 4.4892149, 6.5181854, 7.4099931, 5.1254868, 12.7502461, -9.275426…\n$ x_3  <dbl> -3.94113759, 2.65646872, -0.10913588, 13.39114437, 12.99616116, 3…\n$ x_4  <dbl> -17.3732360, 2.6196690, -6.8545899, 3.6877167, 9.5982646, 7.09623…\n$ x_5  <dbl> 2.1853683, 25.4130435, 0.2749666, 3.9995128, 16.5471387, 9.649271…\n$ x_6  <dbl> -13.8387545, -7.7514735, -1.0824170, -10.3723020, 13.3474819, -4.…\n$ x_7  <dbl> -8.1371882, 14.0102608, 2.6593081, 5.2537524, 0.2673897, -5.48200…\n$ x_8  <dbl> -22.8820028, 0.8089559, -2.8108121, 10.4672192, 10.9376332, 1.937…\n$ x_9  <dbl> -20.8906202, 13.3578359, 19.4734781, 0.7248788, 7.3258207, 14.565…\n$ x_10 <dbl> -18.2918851, 3.0034874, 5.3284673, -1.6321880, 5.1493291, 4.60337…\n$ x_11 <dbl> 18.672349, 11.091231, 14.868589, 9.865558, 4.726164, 14.227183, 1…\n$ x_12 <dbl> 5.5770933, 0.5463577, -9.0256983, 3.8787265, 9.8113720, -3.925492…\n$ x_13 <dbl> -6.996611892, 19.551699340, 15.284866989, -2.964783803, 3.1004588…\n$ x_14 <dbl> 21.031040, 13.320671, 18.770094, 10.911134, -0.382450, 18.983760,…\n$ x_15 <dbl> -1.06370853, 14.64546199, -2.46018710, 3.09542088, -0.52366758, 8…\n$ x_16 <dbl> -0.8450554, 17.3738247, 4.3254573, 12.7759481, 13.7785211, 13.078…\n$ x_17 <dbl> 4.6867098, -1.5470071, 2.1471326, -9.5172721, 11.7413577, 6.54228…\n$ x_18 <dbl> 6.3226155, 9.0624381, 0.5223122, 6.1315901, 5.0103132, 7.2802472,…\n$ x_19 <dbl> 7.6803428, -2.6084423, 4.2716484, 22.0685295, 0.4731131, 8.255899…\n$ x_20 <dbl> -5.4343168, -4.9070165, 13.5187641, 14.4286786, 8.4636535, 8.9575…\n$ y    <dbl> -83.529025, 83.571326, 86.073513, -95.654257, 79.326001, 114.5172…\n```\n\n\n:::\n:::\n\n\n-----\n\nSet up a tibble to track model performance in train and test\n\n::: {.cell}\n\n```{.r .cell-code}\nerror_ex2 <- tibble(model = character(), rmse_trn = numeric(), rmse_test = numeric()) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 0\nColumns: 3\n$ model     <chr> \n$ rmse_trn  <dbl> \n$ rmse_test <dbl> \n```\n\n\n:::\n:::\n\n\n-----\n\n### Fit a standard (OLS) linear regression\n\nFit and evaluate the linear model\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lm_2 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(y ~ ., data = data_trn_2)\n\nfit_lm_2 |> \n  tidy() |> \n  print(n = 21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 5\n   term        estimate std.error statistic p.value\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept) -14.0        10.8    -1.30    0.198 \n 2 x_1          -0.372       1.45   -0.256   0.798 \n 3 x_2           3.54        1.39    2.55    0.0126\n 4 x_3           1.54        1.53    1.01    0.317 \n 5 x_4           2.90        1.42    2.05    0.0441\n 6 x_5           2.98        1.50    1.98    0.0509\n 7 x_6          -1.28        1.34   -0.956   0.342 \n 8 x_7           0.821       1.48    0.553   0.582 \n 9 x_8           1.38        1.26    1.10    0.276 \n10 x_9           0.280       1.25    0.225   0.823 \n11 x_10          0.0247      1.33    0.0186  0.985 \n12 x_11         -2.09        1.56   -1.34    0.185 \n13 x_12          1.71        1.47    1.16    0.248 \n14 x_13          1.33        1.42    0.936   0.352 \n15 x_14         -0.927       1.24   -0.747   0.457 \n16 x_15          2.99        1.53    1.96    0.0536\n17 x_16         -0.602       1.40   -0.431   0.668 \n18 x_17          1.78        1.28    1.39    0.168 \n19 x_18         -1.73        1.73   -1.00    0.321 \n20 x_19         -1.28        1.37   -0.938   0.351 \n21 x_20         -2.83        1.24   -2.28    0.0253\n```\n\n\n:::\n:::\n\n\n-----\n\nIrreducible error was set by `y_error` (100)\n\n- Very overfit to train\n- Very poor performance in test \n\n::: {.cell}\n\n```{.r .cell-code}\n(error_ex2 <- error_ex2 |> \n  bind_rows(tibble(model = \"linear model\",                       \n                   rmse_trn = rmse_vec(truth = data_trn_2$y, \n                                       estimate = predict(fit_lm_2,\n                                                          data_trn_2)$.pred),\n                   rmse_test = rmse_vec(truth = data_test_2$y, \n                                        estimate = predict(fit_lm_2,\n                                                           data_test_2)$.pred))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  model        rmse_trn rmse_test\n  <chr>           <dbl>     <dbl>\n1 linear model     83.4      117.\n```\n\n\n:::\n:::\n\n\n-----\n\n### Fit LASSO\n\nFor all glmnet algorithms, set up:\n\n- Recipe\n- Feature matrices\n- Bootstraps for model configuration selection (tuning)\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_2 <- recipe(y ~ ., data = data_trn_2) |> \n  step_normalize(all_predictors())\n\nrec_prep_2 <- rec_2 |> \n  prep(data_trn_2)\n\nfeat_trn_2 <- rec_prep_2 |> \n  bake(data_trn_2)\n\nfeat_test_2 <- rec_prep_2 |> \n  bake(data_test_2)\n\nset.seed(20140102)\nsplits_boot_2 <- data_trn_2 |> \n   bootstraps(times = 100, strata = \"y\") \n```\n:::\n\n\n-----\n\nTune $\\lambda$ for LASSO\n\n- We can use sample penalty grids from earlier example because sample size and number of features hasnt changed so likely still good\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_lasso_2 <- xfun::cache_rds(\n  expr = {\n    linear_reg(penalty = tune(), \n               mixture = 1) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_2, \n              resamples = splits_boot_2, \n              grid = grid_lasso, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/006/\",\n  file = \"fits_lasso_2\")\n```\n:::\n\n\n-----\n\nPlot hyperparameters\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_hyperparameters(fits_lasso_2, hp1 = \"penalty\", metric = \"rmse\")\n```\n\n::: {.cell-output-display}\n![](006_regularization_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n-----\n\nFit best LASSO to full training set\n\n- Notice the many $\\beta$ = 0\n- It did set some of the \"good\" features to 0 as well\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lasso_2 <-\n  linear_reg(penalty = select_best(fits_lasso_2)$penalty, \n             mixture = 1) |>\n  set_engine(\"glmnet\") |> \n  fit(y ~ ., data = feat_trn_2)\n\nfit_lasso_2 |> \n  tidy() |> \n  print(n = 21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 3\n   term        estimate penalty\n   <chr>          <dbl>   <dbl>\n 1 (Intercept)    5.52     8.78\n 2 x_1            0.465    8.78\n 3 x_2           22.8      8.78\n 4 x_3           10.5      8.78\n 5 x_4           24.1      8.78\n 6 x_5           20.2      8.78\n 7 x_6            0        8.78\n 8 x_7            1.19     8.78\n 9 x_8           11.5      8.78\n10 x_9            1.62     8.78\n11 x_10           0        8.78\n12 x_11          -3.91     8.78\n13 x_12           0        8.78\n14 x_13           0        8.78\n15 x_14           0        8.78\n16 x_15           1.03     8.78\n17 x_16           0        8.78\n18 x_17           0        8.78\n19 x_18           0        8.78\n20 x_19          -0.397    8.78\n21 x_20          -8.80     8.78\n```\n\n\n:::\n:::\n\n\n-----\n\nIrreducible error was set by `y_error` (100)\n\n- Somewhat overfit to train\n- Good in val\n\n::: {.cell}\n\n```{.r .cell-code}\n(error_ex2 <- error_ex2 |> \n  bind_rows(tibble(model = \"LASSO model\",                       \n                   rmse_trn = rmse_vec(truth = feat_trn_2$y, \n                                       estimate = predict(fit_lasso_2,\n                                                          feat_trn_2)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_2$y, \n                                        estimate = predict(fit_lasso_2,\n                                                           feat_test_2)$.pred))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  model        rmse_trn rmse_test\n  <chr>           <dbl>     <dbl>\n1 linear model     83.4      117.\n2 LASSO model      91.3      104.\n```\n\n\n:::\n:::\n\n\n-----\n\n### Fit Ridge\n\nTune $\\lambda$ for Ridge\n\n- can use ridge penalty grid from example 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_ridge_2 <- xfun::cache_rds(\n  expr = {\n    linear_reg(penalty = tune(), \n               mixture = 0) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_2, \n              resamples = splits_boot_2, \n              grid = grid_ridge, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/006/\",\n  file = \"fits_ridge_2\")\n```\n:::\n\n\n-----\n\nPlot hyperparameters\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_hyperparameters(fits_ridge_2, hp1 = \"penalty\", metric = \"rmse\")\n```\n\n::: {.cell-output-display}\n![](006_regularization_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_hyperparameters(fits_ridge_2, hp1 = \"penalty\", metric = \"rmse\")\n```\n\n::: {.cell-output-display}\n![](006_regularization_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n-----\n\nFit best Ridge to full training set\n\n- Notice no $\\beta$ = 0\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ridge_2 <-\n  linear_reg(penalty = select_best(fits_ridge_2)$penalty, \n             mixture = 0) |>\n  set_engine(\"glmnet\") |> \n  fit(y ~ ., data = feat_trn_2)\n\nfit_ridge_2 |> \n  tidy() |> \n  print(n = 21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 3\n   term        estimate penalty\n   <chr>          <dbl>   <dbl>\n 1 (Intercept)     5.52    124.\n 2 x_1             7.14    124.\n 3 x_2            15.1     124.\n 4 x_3             9.34    124.\n 5 x_4            14.9     124.\n 6 x_5            13.0     124.\n 7 x_6             3.93    124.\n 8 x_7             6.28    124.\n 9 x_8             9.89    124.\n10 x_9             5.80    124.\n11 x_10            5.13    124.\n12 x_11           -6.89    124.\n13 x_12            4.47    124.\n14 x_13            4.03    124.\n15 x_14           -2.58    124.\n16 x_15            6.17    124.\n17 x_16           -1.45    124.\n18 x_17            4.41    124.\n19 x_18           -5.29    124.\n20 x_19           -6.03    124.\n21 x_20           -8.91    124.\n```\n\n\n:::\n:::\n\n\n-----\n\nIrreducible error was set by `y_error` (100)\n\n- Somewhat overfit to train\n- Still slightly better than LASSO in test \n\n::: {.cell}\n\n```{.r .cell-code}\n(error_ex2 <- error_ex2 |> \n  bind_rows(tibble(model = \"Ridge model\",                       \n                   rmse_trn = rmse_vec(truth = feat_trn_2$y, \n                                       estimate = predict(fit_ridge_2,\n                                                          feat_trn_2)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_2$y, \n                                        estimate = predict(fit_ridge_2,\n                                                           feat_test_2)$.pred))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  model        rmse_trn rmse_test\n  <chr>           <dbl>     <dbl>\n1 linear model     83.4      117.\n2 LASSO model      91.3      104.\n3 Ridge model      90.3      102.\n```\n\n\n:::\n:::\n\n\n-----\n\n### Fit Complete glmnet\n\nTune $\\lambda$ and $\\alpha$ for glmnet\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_glmnet_2 <- xfun::cache_rds(\n  expr = {\n    linear_reg(penalty = tune(), \n               mixture = tune()) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_2, \n              resamples = splits_boot_2, \n              grid = grid_glmnet, \n              metrics = metric_set(rmse))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/006/\",\n  file = \"fits_glmnet_2\")\n```\n:::\n\n\n\n-----\n\nPlot hyperparameters\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_hyperparameters(fits_glmnet_2, hp1 = \"penalty\", hp2 = \"mixture\", metric = \"rmse\")\n```\n\n::: {.cell-output-display}\n![](006_regularization_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\n-----\n\nFit Best glmnet in full train set\n\n- Still Ridge (but won't always be)\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(fits_glmnet_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  penalty mixture .config                \n    <dbl>   <dbl> <chr>                  \n1    124.       0 Preprocessor1_Model0364\n```\n\n\n:::\n\n```{.r .cell-code}\nfit_glmnet_2 <-\n  linear_reg(penalty = select_best(fits_glmnet_2)$penalty, \n             mixture = select_best(fits_glmnet_2)$mixture) |>\n  set_engine(\"glmnet\") |> \n  fit(y ~ ., data = feat_trn_2)\n\nfit_glmnet_2 |> \n  tidy() |> \n  print(n = 21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 3\n   term        estimate penalty\n   <chr>          <dbl>   <dbl>\n 1 (Intercept)     5.52    124.\n 2 x_1             7.14    124.\n 3 x_2            15.1     124.\n 4 x_3             9.34    124.\n 5 x_4            14.9     124.\n 6 x_5            13.0     124.\n 7 x_6             3.93    124.\n 8 x_7             6.28    124.\n 9 x_8             9.89    124.\n10 x_9             5.80    124.\n11 x_10            5.13    124.\n12 x_11           -6.89    124.\n13 x_12            4.47    124.\n14 x_13            4.03    124.\n15 x_14           -2.58    124.\n16 x_15            6.17    124.\n17 x_16           -1.45    124.\n18 x_17            4.41    124.\n19 x_18           -5.29    124.\n20 x_19           -6.03    124.\n21 x_20           -8.91    124.\n```\n\n\n:::\n:::\n\n\n-----\n\nIrreducible error was set by `y_error` (100)\n\n- Somewhat overfit to train\n- Still not bad in validate\n\n::: {.cell}\n\n```{.r .cell-code}\n(error_ex2 <- error_ex2 |> \n  bind_rows(tibble(model = \"glmnet model\",   \n                   rmse_trn = rmse_vec(truth = feat_trn_2$y, \n                                       estimate = predict(fit_glmnet_2,\n                                                          feat_trn_2)$.pred),\n                   rmse_test = rmse_vec(truth = feat_test_2$y, \n                                        estimate = predict(fit_glmnet_2,\n                                                           feat_test_2)$.pred))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  model        rmse_trn rmse_test\n  <chr>           <dbl>     <dbl>\n1 linear model     83.4      117.\n2 LASSO model      91.3      104.\n3 Ridge model      90.3      102.\n4 glmnet model     90.3      102.\n```\n\n\n:::\n:::\n\n\n-----\n\n## LASSO for Feature (e.g., Covariate) Selection?\n\nLets consider a typical explanatory setting in Psychology\n\n- A focal dichotomous IV (your experimental manipulation)\n- A number of covariates (some good, some bad)\n- A quantitative outcome (y)\n- Covariates are uncorrelated with IV b/c IV is manipulated\n\nLet's pretend the previous 20 `x`s were your covariates\n\n-----\n\nWhat are your options to test `iv` prior to this course?\n\n- You want to use covariates to increase power \n\n- BUT you don't know which covariates to use\n\n  - You might use all of them\n\n  - Or you might use none of them (a clear lost opportunity)\n\n  - Or you might hack it by using those increase your focal IV effect (very bad!)\n  \n-----\n\n\nNOW, We might use the feature selection characteristics for LASSO to select which covariates are included\n\\\n\\\nThere are two possibilities that occur to me\n\n-----\n\n1.  Use LASSO to build best DGP for a covariates only model\n\\\n\\\n\n- Could be more conservative (fewer covariates) by using within 1 SE of best performance but less flexible (i.e., will set more parameter estimates to 0)\n- Follow up with a linear model (using $lm$), regressing y on $iv$ and covariates from LASSO that are non-zero\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lasso_2 |> \n  tidy() |> \n  print(n = 21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 3\n   term        estimate penalty\n   <chr>          <dbl>   <dbl>\n 1 (Intercept)    5.52     8.78\n 2 x_1            0.465    8.78\n 3 x_2           22.8      8.78\n 4 x_3           10.5      8.78\n 5 x_4           24.1      8.78\n 6 x_5           20.2      8.78\n 7 x_6            0        8.78\n 8 x_7            1.19     8.78\n 9 x_8           11.5      8.78\n10 x_9            1.62     8.78\n11 x_10           0        8.78\n12 x_11          -3.91     8.78\n13 x_12           0        8.78\n14 x_13           0        8.78\n15 x_14           0        8.78\n16 x_15           1.03     8.78\n17 x_16           0        8.78\n18 x_17           0        8.78\n19 x_18           0        8.78\n20 x_19          -0.397    8.78\n21 x_20          -8.80     8.78\n```\n\n\n:::\n:::\n\n\n- You clearly improved your best guess on covariates to include \n- You will regress y on `iv` and the 11 covariates with non-zero effects\n\n-----\n\n2. Use LASSO to build best DGP including `iv` and covariates but don't penalize `iv`\n\\\n\\\n\n- Look at `penalty.factor = rep(1, nvars)` argument in `glmnet()`\n- Can fit LASSO with unbiased? estimate of `iv`\n- Need to bootstrap for SE for `iv` (next unit)\n- Only appropriate if IV is manipulated\n\\\n\\\n\nShould really conduct simulation study of both of these options (vs. all and no covariates).  \n\n- I want to\n- Want to do the study with me?\n\n-----\n\n## Ridge, LASSO, and Elastic net models for other Y distributions\n\nThese penalties can be added to the cost functions of other generalized linear models to yield regularized/penalized versions of those models as well.  For example\n\\\n\\\nL1 penalized (LASSO) logistic regression (w/ labels coded 0,1):\n\n- $\\frac{1}{n}([\\:\\sum_{i = 1}^{n} -Y_ilog(\\hat{Y_i}) - (1-Y_i)log(1-\\hat{Y_i})\\:]\\:+\\:[\\:\\lambda\\sum_{j = 1}^{p} |\\beta_j|\\:]$\n\\\n\\\n\nFor L2 penalized (Ridge) logistic regression (w/ labels coded 0,1)\n\n- $\\frac{1}{n}([\\:\\sum_{i = 1}^{n} -Y_ilog(\\hat{Y_i}) - (1-Y_i)log(1-\\hat{Y_i})\\:]\\:+\\:[\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:]$\n\\\n\\\n\n`glmnet` implements: \n\n- `family = c(\"gaussian\", \"binomial\", \"poisson\", \"multinomial\", \"cox\", \"mgaussian\")`\n- Full range of $\\alpha$ to mix two types of penalties\n\n-----\n\n## Discussion\n\n### Announcements\n\n- Midterm - application and conceptual parts\n- Can we have a midterm review session?\n\n-----\n\n### feature matrix vs. raw predictors\n\nWhen to use which and why could I use raw predictors in simulated data example from lectures\n\n-----\n\n\n### Cost functions\n\nWhat is Cost function?\n\\\n\\\nLinear model \n\n- $\\frac{1}{n}\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}$\n\\\n\\\n\nRidge (L2)\n\n- $\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} \\beta_j^{2}\\:])$\n\\\n\\\n\nLASSO (L1)\n\n- $\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda\\sum_{j = 1}^{p} |\\beta_j|\\:])$\n\\\n\\\n\nElastic Net\n\n- $\\frac{1}{n}([\\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^{2}] + [\\:\\lambda (\\alpha\\sum_{j = 1}^{p} |\\beta_j| + (1-\\alpha)\\sum_{j = 1}^{p} \\beta_j^{2})\\:])$\n\n-----\n\n### LASSO, Ridge, and elastic net questions\n\n- The pros and cons of Lasso and Ridge vs. Elastic Net\n\n- I'm still a little confused as to why you would ever use Ridge or LASSO separately when you can just selectively use one or the other through elastic net. Wouldn't it make sense to just always use elastic net and then change the penalty accordingly for when you wanted to use a Ridge or LASSO regression approach?\n  - Pure LASSO\n  - model variance\n  - two self-report measure scales vs. items\n\n- Some explanation why Lasso is more robust to outliers and ridge is more robust to measurement error would be appreciated.\n\n- I'm not very clear why LASSO could limit some parameters to be zero but ridge regression cannot. Can we go through this a bit?\n\n- How do you know what numbers to start at for tuning lamba (like in the code below)? I think John mentioned he has a function to find these, but I'm wondering if there are any rules of thumb.\n\n- How do we know how \"strong\" of a penalty we need to be applying to our cost function? Does the reduction in variance increase as we increase the strength of the penalty?\n\n- Is a decreased number of features always good or bad, or does it depend on the model/recipe\n\n- Can you talk more about how to interpret the scale of the parameter estimates? In the lecture you said the following and I'm not quite sure what that means:\n\n- I might be totally wrong but I wonder if we have to care about the multi-collinearity or high dimension on classification as well. Or this is only limited to regression and so we are solving with regularising only regression model? \n\n\n-----\n\n### Stepwise questions\n\n- Could you go over Forward, Backward, and Best Subset subsetting? I think I understand the algorithm they use, but I do not understand the penalty function they use for picking the \"best\" model. In the book, it looks like it uses R-squared to pick the best model, but wouldn't the full model always have the greatest R-squared?\n\n- Is there a reason why we do not discuss variable selection using subset methods? \n\n- Is there specific cases when you would pick backwards or forwards selection, or is it up to the researcher?\n\n- Training vs. val/val error in stepwise approaches.   \n\n- Use of AIC, BIC, Cp, and adjusted R2 vs. cross-validation\n\n-----\n\n### Explanatory goals\n\n- Can LASSO be used for variable selection when engaging in cross sectional data analysis to identify which variables in a large set of Xs are important for a particular outcome? \n\n- In practice, what elements should be considered before selecting IVs and covariates?\n\n-----\n\n### PCA\n\n- https://setosa.io/ev/principal-component-analysis/\n\n",
    "supporting": [
      "006_regularization_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}