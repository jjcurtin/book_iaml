{
  "hash": "6d9b7d1d1ce9f6a07a79eeba8e83ba3c",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Natural Language Processing: Text Processing and Feature Engineering\n\n## Overview of Unit\n\n### Learning Objectives\n\n- Objective 1\n- Objective 2\n\n-----\n\n### Readings\n\n- @SMLTAR [Chapter 2: Tokenization](https://smltar.com/tokenization.html)\n- @SMLTAR [Chapter 5: Word Embeddings](https://smltar.com/embeddings.html)\n\nNOTES: Please read the above chapters more with an eye toward concepts and issues \nrather than code.  I will demonstrate a minimum set of functions to accomplish\nthe NLP modeling tasks for this unit.\n\nAlso know that the entire @SMLTAR [book] is really mandatory reading.  I would also strongly recommend this entire @TMR [book](https://www.tidytextmining.com/).  Both  will be important references at a minimum.\n\nPost questions to the readings channel in Slack\n\n-----\n\n### Lecture Videos\n\n- [Lecture 1: General Text (Pre-) Processing - the stringr package](https://mediaspace.wisc.edu/media/unit_12_lecture_1/1_mq7a05cw) ~ 9 mins\n- [Lecture 2: General Text (Pre-) Processing - regular expressions](https://mediaspace.wisc.edu/media/unit_12_lecture_2/1_jijno1uc) ~ 13 mins\n- [Lecture 3: The IMDB Reviews Dataset](https://mediaspace.wisc.edu/media/unit_12_lecture_3/1_33gjwtbv) ~ 6 mins\n- [Lecture 4: Tokenization- Part 1](https://mediaspace.wisc.edu/media/unit_12_lecture_4/1_ean14ejo) ~ 27 mins\n- [Lecture 5: Tokenization- Part 2](https://mediaspace.wisc.edu/media/unit_12_lecture_5/1_z3ywipyc) ~ 13 mins\n- [Lecture 6: Stopwords](https://mediaspace.wisc.edu/media/unit_12_lecture_6/1_flyszeb5) ~ 12 mins\n- [Lecture 7: Stemming](https://mediaspace.wisc.edu/media/unit_12_lecture_7/1_r8rfzgxy) ~12 mins\n- [Lecture 8: Bag of Words](https://mediaspace.wisc.edu/media/unit_12_lecture_8/1_g9hesmnb) ~19 mins\n- [Lecture 9: NLP in Action - Part 1](https://mediaspace.wisc.edu/media/unit_12_lecture_9/1_qvb4fkaj) ~ 17 mins\n- [Lecture 10: NLP in Action - Part 2](https://mediaspace.wisc.edu/media/unit_12_lecture_10/1_x7jcxtaf) ~ 20 mins\n\nPost questions to the video-lectures channel in Slack\n\n-----\n\n### Application Assignment\n\n- [data](application_assignments/unit_12/alcohol_tweets.csv)\n- [qmd shell](https://raw.githubusercontent.com/jjcurtin/book_iaml/main/application_assignments/unit_12/hw_unit_12_nlp.qmd)\n- [GloVE embeddings for twitter](https://dionysus.psych.wisc.edu/iaml/glove_twitter.csv)\n- [solution]()\n\n\n Submit the application assignment [here](https://canvas.wisc.edu/courses/395546/assignments/2187686) and complete the [unit quiz](https://canvas.wisc.edu/courses/395546/quizzes/514056) by 8 pm on Wednesday, April 17th\n \n\n-----\n\n## General Text (Pre-) Processing\n\nIn order to work with text, we need to be able to manipulate text.  We have two \nsets of tools to master:\n\n- The `stringr` [package](https://cran.r-project.org/web/packages/stringr/index.html)\n- Regular expressions (regex)\n\n### The stringr package\n\nThere are many functions in the `stringr` package that are very useful for searching\nand manipulating text.   \n\n- `stringr` is included in tidyverse\n- I recommend keeping the [stringr cheatsheet](pdfs/cheatsheet_strings.pdf) open whenever you are working with text until you learn these functions well.\n\n-----\n\nAll functions in `stringr` start with `str_` and take a vector of strings as the first argument.\n\\\n\\\nHere is a simple vector of strings to use as an example\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nx <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\n```\n:::\n\n\n- Length of each string\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_length(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3 5 5 5 4 9\n```\n\n\n:::\n:::\n\n\n- Collapse all strings in vector into one long string that is comma separated\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_c(x, collapse = \", \")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"why, video, cross, extra, deal, authority\"\n```\n\n\n:::\n:::\n\n\n- Get substring based on position (start and end position)\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_sub(x, 1, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"wh\" \"vi\" \"cr\" \"ex\" \"de\" \"au\"\n```\n\n\n:::\n:::\n\n\n-----\n\nMost `stringr` functions work with regular expressions, a concise language for describing patterns of text. \n\\\n\\\nFor example, the regular expression \"[aeiou]\" matches any single character that is a vowel.  \n\n- Here we use `str_subset()` to return the strings that contain vowels (doesnt include \"why\")\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_subset(x, \"[aeiou]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"video\"     \"cross\"     \"extra\"     \"deal\"      \"authority\"\n```\n\n\n:::\n:::\n\n\n- Here we count the vowels in each string\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_count(x, \"[aeiou]\") \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0 3 1 2 2 4\n```\n\n\n:::\n:::\n\n\n-----\n\nThere are eight main verbs that work with patterns:\n\n1 `str_detect(x, pattern)` tells you if there is any match to the pattern in each string\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_detect(x, \"[aeiou]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n```\n\n\n:::\n:::\n\n\n-----\n\n2 `str_count(x, pattern)` counts the number of patterns\n\n::: {.cell}\n\n```{.r .cell-code}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_count(x, \"[aeiou]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0 3 1 2 2 4\n```\n\n\n:::\n:::\n\n\n-----\n\n3 `str_subset(x, pattern)` extracts the matching components\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_subset(x, \"[aeiou]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"video\"     \"cross\"     \"extra\"     \"deal\"      \"authority\"\n```\n\n\n:::\n:::\n\n\n-----\n\n4 `str_locate(x, pattern)` gives the position of the match\n\n::: {.cell}\n\n```{.r .cell-code}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_locate(x, \"[aeiou]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     start end\n[1,]    NA  NA\n[2,]     2   2\n[3,]     3   3\n[4,]     1   1\n[5,]     2   2\n[6,]     1   1\n```\n\n\n:::\n:::\n\n\n-----\n\n5 `str_extract(x, pattern)` extracts the text of the match\n\n::: {.cell}\n\n```{.r .cell-code}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_extract(x, \"[aeiou]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NA  \"i\" \"o\" \"e\" \"e\" \"a\"\n```\n\n\n:::\n:::\n\n\n-----\n\n6 `str_match(x, pattern)` extracts parts of the match defined by parentheses. In this case, the characters on either side of the vowel \n\n::: {.cell}\n\n```{.r .cell-code}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_match(x, \"(.)[aeiou](.)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]  [,2] [,3]\n[1,] NA    NA   NA  \n[2,] \"vid\" \"v\"  \"d\" \n[3,] \"ros\" \"r\"  \"s\" \n[4,] NA    NA   NA  \n[5,] \"dea\" \"d\"  \"a\" \n[6,] \"aut\" \"a\"  \"t\" \n```\n\n\n:::\n:::\n\n\n-----\n\n7 `str_replace(x, pattern, replacement)` replaces the matches with new text\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_replace(x, \"[aeiou]\", \"?\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"why\"       \"v?deo\"     \"cr?ss\"     \"?xtra\"     \"d?al\"      \"?uthority\"\n```\n\n\n:::\n:::\n\n\n-----\n\n8 `str_split(x, pattern)` splits up a string into multiple pieces.\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_split(c(\"a,b\", \"c,d,e\"), \",\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] \"a\" \"b\"\n\n[[2]]\n[1] \"c\" \"d\" \"e\"\n```\n\n\n:::\n:::\n\n\n-----\n\n###  Regular Expressions\n\nRegular expressions are a way to specify or search for patterns of strings using a sequence of characters. By combining a selection of simple patterns, we can capture quite complicated strings.\n\\\n\\\nThe `stringr` package uses regular expressions extensively\n\\\n\\\nThe regular expressions are passed as the pattern = argument. Regular expressions can be used to detect, locate, or extract parts of a string.\n\n-----\n\nJulia Silge has put together a wonderful [tutorial/primer on the use of regular expressions](https://smltar.com/regexp.html#literal-characters). After reading it, I finally had a solid grasp on them.  Rather than grab sections, I will direct you to it (and review it live in our filmed lectures).  She does it much better than I could!\n\\\n\\\nYou might consider installing the `RegExplain` [package](https://www.garrickadenbuie.com/project/regexplain/) using devtools if you want more support working with regular expressions.  They are powerful but they are complicated to learn initially\n\\\n\\\nThere is also a very helpful [cheatsheet](pdfs/cheatsheert_regex.pdf) for regular expressions\n\\\n\\\nAnd finally, there is a great @RDS chapter on [strings](https://r4ds.had.co.nz/strings.html) more generally, which covers both `stringr` and regex.\n\n-----\n\n## The IMDB Dataset\n\n\n\n\n\nNow that we have a basic understanding of how to manipulation raw text, we can get set up for NLP and introduce a guiding example for this unit\n\nWe can start with our normal cast of characters RE packages, source, and settings (not displayed here)\n\nHowever, we will also install a few new ones that are specific to working with text.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(textrecipes)  #step_- functions for NLP\nlibrary(SnowballC)  \nlibrary(stopwords)\n```\n:::\n\n\n-----\n\nThe IMDB Reviews dataset is a classic NLP dataset that is used for sentiment analysis\n\\\n\\\nIt contains: \n\n- 25,000 movie reviews in train and test\n- Balanced on positive and negative sentiment (labeled outcome)\n- For more info, see the [website](http://ai.stanford.edu/~amaas/data/sentiment/)\n\n-----\n\nLet's start by loading the dataset and adding an identifier for each review (i.e., **document**, `doc_num`)\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn <- read_csv(here::here(path_data, \"imdb_trn.csv\"), \n                     show_col_types = FALSE) |>\n  rowid_to_column(var = \"doc_num\") |> \n  mutate(sentiment = fct(sentiment, levels = c(\"neg\", \"pos\"))) \n\ndata_trn  |>\n  skim_some()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |         |\n|:------------------------|:--------|\n|Name                     |data_trn |\n|Number of rows           |25000    |\n|Number of columns        |3        |\n|_______________________  |         |\n|Column type frequency:   |         |\n|character                |1        |\n|factor                   |1        |\n|numeric                  |1        |\n|________________________ |         |\n|Group variables          |None     |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min|   max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|-----:|-----:|--------:|----------:|\n|text          |         0|             1|  52| 13704|     0|    24904|          0|\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts             |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------------|\n|sentiment     |         0|             1|FALSE   |        2|neg: 12500, pos: 12500 |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate| p0|  p100|\n|:-------------|---------:|-------------:|--:|-----:|\n|doc_num       |         0|             1|  1| 25000|\n\n\n:::\n:::\n\n\n-----\n\nLet's look at our outcome\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> tab(sentiment)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  sentiment     n  prop\n  <fct>     <int> <dbl>\n1 neg       12500   0.5\n2 pos       12500   0.5\n```\n\n\n:::\n:::\n\n\n-----\n\nTo get a better sense of the dataset, We can view first five negative reviews from the training set\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  filter(sentiment == \"neg\") |> \n  slice(1:5) |> \n  pull(text) |> \n  print_kbl() \n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; \"><table class=\"table table-striped table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> x </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings &amp; such belonging to rich businessman Philip Stevens (James Stewart) who is flying them &amp; a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) &amp; her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) &amp; his two accomplice's Banker (Monte Markham) &amp; Wilson (Michael Pataki) who knock the passengers &amp; crew out with sleeping gas, they plan to steal the valuable cargo &amp; land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean &amp; loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in &amp; having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...&lt;br /&gt;&lt;br /&gt;Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson &amp; while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking &amp; then the crashing (didn't he see the oil rig?) &amp; sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) &amp; submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened &amp; it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces &amp; a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks &amp; there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships &amp; helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes &amp; barely even says anything preferring to just look worried in the background.&lt;br /&gt;&lt;br /&gt;The home video &amp; theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes &amp; the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions &amp; interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow &amp; not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.&lt;br /&gt;&lt;br /&gt;The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old &amp; frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do &amp; there are plenty of other familiar faces to look out for too.&lt;br /&gt;&lt;br /&gt;Airport '77 is the most disaster orientated of the three Airport films so far &amp; I liked the ideas behind it even if they were a bit silly, the production &amp; bland direction doesn't help though &amp; a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.&lt;br /&gt;&lt;br /&gt;But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.&lt;br /&gt;&lt;br /&gt;I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i'm no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> When I was little my parents took me along to the theater to see Interiors. It was one of many movies I watched with my parents, but this was the only one we walked out of. Since then I had never seen Interiors until just recently, and I could have lived out the rest of my life without it. What a pretentious, ponderous, and painfully boring piece of 70's wine and cheese tripe. Woody Allen is one of my favorite directors but Interiors is by far the worst piece of crap of his career. In the unmistakable style of Ingmar Berman, Allen gives us a dark, angular, muted, insight in to the lives of a family wrought by the psychological damage caused by divorce, estrangement, career, love, non-love, halitosis, whatever. The film, intentionally, has no comic relief, no music, and is drenched in shadowy pathos. This film style can be best defined as expressionist in nature, using an improvisational method of dialogue to illicit a \"more pronounced depth of meaning and truth\". But Woody Allen is no Ingmar Bergman. The film is painfully slow and dull. But beyond that, I simply had no connection with or sympathy for any of the characters. Instead I felt only contempt for this parade of shuffling, whining, nicotine stained, martyrs in a perpetual quest for identity. Amid a backdrop of cosmopolitan affluence and baked Brie intelligentsia the story looms like a fart in the room. Everyone speaks in affected platitudes and elevated language between cigarettes. Everyone is \"lost\" and \"struggling\", desperate to find direction or understanding or whatever and it just goes on and on to the point where you just want to slap all of them. It's never about resolution, it's only about interminable introspective babble. It is nothing more than a psychological drama taken to an extreme beyond the audience's ability to connect. Woody Allen chose to make characters so immersed in themselves we feel left out. And for that reason I found this movie painfully self indulgent and spiritually draining. I see what he was going for but his insistence on promoting his message through Prozac prose and distorted film techniques jettisons it past the point of relevance. I highly recommend this one if you're feeling a little too happy and need something to remind you of death. Otherwise, let's just pretend this film never happened. </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n-----\n\nand the first five positive reviews from the training set\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  filter(sentiment == \"pos\") |> \n  slice(1:5) |> \n  pull(text) |> \n  print_kbl()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; \"><table class=\"table table-striped table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> x </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't! </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.&lt;br /&gt;&lt;br /&gt;But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.&lt;br /&gt;&lt;br /&gt;Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.&lt;br /&gt;&lt;br /&gt;While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.&lt;br /&gt;&lt;br /&gt;Or maybe this film will inspire you to help others. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not give a realistic view of homelessness (unlike, say, how Citizen Kane gave a realistic view of lounge singers, or Titanic gave a realistic view of Italians YOU IDIOTS). Many of the jokes fall flat. But still, this film is very lovable in a way many comedies are not, and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive. Its not The Fisher King, but its not crap, either. My only complaint is that Brooks should have cast someone else in the lead (I love Mel as a Director and Writer, not so much as a lead). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and actually had a plot that was followable. Leslie Ann Warren made the movie, she is such a fantastic, under-rated actress. There were some moments that could have been fleshed out a bit more, and some scenes that could probably have been cut to make the room to do so, but all in all, this is worth the price to rent and see it. The acting was good overall, Brooks himself did a good job without his characteristic speaking to directly to the audience. Again, Warren was the best actor in the movie, but \"Fume\" and \"Sailor\" both played their parts well. </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n-----\n\nYou need to spend a LOT of time reviewing the text before you begin to process it.\n\\\n\\\nI have NOT done this yet!\n\\\n\\\nMy models will be sub-optimal!\n\n-----\n\n## Tokens\n\nMachine learning algorithms cannot work with raw text (documents) directly\n\\\n\\\nWe must feature engineer these documents to allow them to serve as input to statistical algorithms\n\\\n\\\nThe first step for most NLP feature engineering methods is to represent text (documents) as \ntokens (words, ngrams)\n\\\n\\\nGiven that **tokenization** is often one of our first steps for extracting features from text, it is important to consider carefully what happens during this step and its implications for your subsequent modeling\n\n-----\n\nIn tokenization, we take input documents (text strings) and a token type (a meaningful \nunit of text, such as a word) and split the document into pieces (tokens) that correspond to the type\n\\\n\\\nWe can tokenize text into a variety of token types: \n\n- characters\n- words (most common - our focus;  unigrams)\n- sentences\n- lines\n- paragraphs\n- n-grams (bigrams, trigrams)\n\n-----\n\nAn n-gram consists of a sequence of n items from a given sequence of text. Most often, it is a group of n words (bigrams, trigrams)  \n\\\n\\\nn-grams retain word order which would otherwise be lost if we were just using words as the token type\n\\\n\\\n\"I am not happy\"\n\\\n\\\nTokenized by word, yields:\n\n- I\n- am\n- not\n- happy\n\n\\\n\\\nTokenized by 2-gram words:\n\n- I am\n- am not\n- not happy\n\n-----\n\nWe will be using tokenizer functions from the `tokenizers` package.  Three in particular are:\n\n- `tokenize_words(x, lowercase = TRUE, stopwords = NULL, strip_punct = TRUE, strip_numeric = FALSE, simplify = FALSE)`\n- `tokenize_ngrams(x, lowercase = TRUE, n = 3L, n_min = n, stopwords = character(), ngram_delim = \" \", simplify = FALSE)`\n- `tokenize_regex(x, pattern = \"\\\\s+\", simplify = FALSE)`\n\n-----\n\nHowever, we will be accessing these functions through wrappers: \n\n- `tidytext::unnest_tokens(tbl, output, input, token = \"words\", format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\"), to_lower = TRUE, drop = TRUE, collapse = NULL)` for tidyverse data exploration of tokens within tibbles\n- `textrecipes::step_tokenize()` for tokenization in our recipes\n\n-----\n\nWord-level tokenization by `tokenize_words()` is done by finding word boundaries as follows:\n\n- Break at the start and end of text, unless the text is empty\n- Do not break within CRLF (new line characters)\n- Otherwise, break before and after new lines (including CR and LF)\n- Do not break between most letters\n- Do not break letters across certain punctuation\n- Do not break within sequences of digits, or digits adjacent to letters (“3a,” or “A3”)\n- Do not break within sequences, such as “3.2” or “3,456.789.”\n- Do not break between Katakana\n- Do not break from extenders\n- Do not break within emoji zwj sequences\n- Do not break within emoji flag sequences\n- Ignore Format and Extend characters, except after sot, CR, LF, and new line\n- Keep horizontal whitespace together\n- Otherwise, break everywhere (including around ideographs, e.g., @, %, >)\n\n-----\n\nLet's start with using `tokenize_words()` to get a sense of how it works by default\n\n- Splits on spaces\n- Converts to lowercase by default (does it matter? MAYBE!!!)\n- Retains apostrophes but drops other punctuation (`,`, `.`, `!`) and some symbols (e.g., `-` `\\\\`,  `@`) by default.  Does not drop `_` (Do you need punctuation?  !!!!)\n- Retains numbers (by default) and decimals but drops `+` appended to `4+`\n- Has trouble with URLs and email address (do you need this?)\n- Often, these issues may NOT matter\n\n::: {.cell}\n\n```{.r .cell-code}\n\"Here is a sample document to tokenize.  How EXCITING (I _love_ it).  Sarah has spent 4 or 4.1 or 4P or 4+ or >4 years developing her pre-processing and NLP skills.  You can learn more about tokenization here: https://smltar.com/tokenization.html or by emailing me at jjcurtin@wisc.edu\" |> \n  tokenizers::tokenize_words()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n [1] \"here\"              \"is\"                \"a\"                \n [4] \"sample\"            \"document\"          \"to\"               \n [7] \"tokenize\"          \"how\"               \"exciting\"         \n[10] \"i\"                 \"_love_\"            \"it\"               \n[13] \"sarah\"             \"has\"               \"spent\"            \n[16] \"4\"                 \"or\"                \"4.1\"              \n[19] \"or\"                \"4p\"                \"or\"               \n[22] \"4\"                 \"or\"                \"4\"                \n[25] \"years\"             \"developing\"        \"her\"              \n[28] \"pre\"               \"processing\"        \"and\"              \n[31] \"nlp\"               \"skills\"            \"you\"              \n[34] \"can\"               \"learn\"             \"more\"             \n[37] \"about\"             \"tokenization\"      \"here\"             \n[40] \"https\"             \"smltar.com\"        \"tokenization.html\"\n[43] \"or\"                \"by\"                \"emailing\"         \n[46] \"me\"                \"at\"                \"jjcurtin\"         \n[49] \"wisc.edu\"         \n```\n\n\n:::\n:::\n\n\n-----\n\nSome of these behaviors can be altered from their defaults\n\n- `lowercase = TRUE`\n- `strip_punc = TRUE`\n- `strip_numeric = FALSE`\n\n-----\n\nSome of these issues can be corrected by pre-processing the text\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_replace(\"jjcurtin@wisc.edu\", \"@\", \"_at_\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"jjcurtin_at_wisc.edu\"\n```\n\n\n:::\n:::\n\n\n-----\n\nIf you need finer control, you can use `tokenize_regex()` and then do further processing with `stringr` functions and regex\n\\\n\\\nNow it may be easier to build up from here (e.g., ):\n\n- `str_to_lower(word)`\n- `str_replace(word, \".$\", \"\")`\n\n::: {.cell}\n\n```{.r .cell-code}\n\"Here is a sample document to tokenize.  How EXCITING (I _love_ it).  Sarah has spent 4 or 4.1 or 4P or 4+ years developing her pre-processing and NLP skills.  You can learn more about tokenization here: https://smltar.com/tokenization.html or by emailing me at jjcurtin@wisc.edu\" |> \n  tokenizers::tokenize_regex(pattern = \"\\\\s+\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n [1] \"Here\"                                \n [2] \"is\"                                  \n [3] \"a\"                                   \n [4] \"sample\"                              \n [5] \"document\"                            \n [6] \"to\"                                  \n [7] \"tokenize.\"                           \n [8] \"How\"                                 \n [9] \"EXCITING\"                            \n[10] \"(I\"                                  \n[11] \"_love_\"                              \n[12] \"it).\"                                \n[13] \"Sarah\"                               \n[14] \"has\"                                 \n[15] \"spent\"                               \n[16] \"4\"                                   \n[17] \"or\"                                  \n[18] \"4.1\"                                 \n[19] \"or\"                                  \n[20] \"4P\"                                  \n[21] \"or\"                                  \n[22] \"4+\"                                  \n[23] \"years\"                               \n[24] \"developing\"                          \n[25] \"her\"                                 \n[26] \"pre-processing\"                      \n[27] \"and\"                                 \n[28] \"NLP\"                                 \n[29] \"skills.\"                             \n[30] \"You\"                                 \n[31] \"can\"                                 \n[32] \"learn\"                               \n[33] \"more\"                                \n[34] \"about\"                               \n[35] \"tokenization\"                        \n[36] \"here:\"                               \n[37] \"https://smltar.com/tokenization.html\"\n[38] \"or\"                                  \n[39] \"by\"                                  \n[40] \"emailing\"                            \n[41] \"me\"                                  \n[42] \"at\"                                  \n[43] \"jjcurtin@wisc.edu\"                   \n```\n\n\n:::\n:::\n\n\n-----\n\nYou can explore the tokens that will be formed using `unnest_tokens()` and basic tidyverse data wrangling using a tidied format of your documents as part of your EDA\n\n- We unnest to 1 token (word) per row (tidy format)\n- We keep track of `doc_num` (added earlier)\n\n\\\n\\\nHere, we tokenize the IMDB training set.  \n\n- Using defaults\n- Can change other default for `tokenize_*()` by passing into function via `...`\n- Can set `drop = TRUE` (default) to discard the original document column (text)\n- Its pretty fast!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens <- data_trn |> \n  unnest_tokens(word, text, token = \"words\", to_lower = TRUE, drop = FALSE) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 5,935,548\nColumns: 4\n$ doc_num   <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ sentiment <fct> neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, …\n$ text      <chr> \"Story of a man who has unnatural feelings for a pig. Starts…\n$ word      <chr> \"story\", \"of\", \"a\", \"man\", \"who\", \"has\", \"unnatural\", \"feeli…\n```\n\n\n:::\n:::\n\n\\\n\\\n**Coding sidebar:** You can take a much deeper dive into tidyverse text processing in [chapter 1](https://www.tidytextmining.com/tidytext.html) of @TMR.   \n\n-----\n\nLet's get oriented by reviewing the tokens from the first document\n\n- Raw form\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn$text[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\"\n```\n\n\n:::\n:::\n\n\n- Tokenized by word and tidied\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens |> \n  filter(doc_num == 1) |> \n  select(word) |> \n  print(n = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 112 × 1\n    word          \n    <chr>         \n  1 story         \n  2 of            \n  3 a             \n  4 man           \n  5 who           \n  6 has           \n  7 unnatural     \n  8 feelings      \n  9 for           \n 10 a             \n 11 pig           \n 12 starts        \n 13 out           \n 14 with          \n 15 a             \n 16 opening       \n 17 scene         \n 18 that          \n 19 is            \n 20 a             \n 21 terrific      \n 22 example       \n 23 of            \n 24 absurd        \n 25 comedy        \n 26 a             \n 27 formal        \n 28 orchestra     \n 29 audience      \n 30 is            \n 31 turned        \n 32 into          \n 33 an            \n 34 insane        \n 35 violent       \n 36 mob           \n 37 by            \n 38 the           \n 39 crazy         \n 40 chantings     \n 41 of            \n 42 it's          \n 43 singers       \n 44 unfortunately \n 45 it            \n 46 stays         \n 47 absurd        \n 48 the           \n 49 whole         \n 50 time          \n 51 with          \n 52 no            \n 53 general       \n 54 narrative     \n 55 eventually    \n 56 making        \n 57 it            \n 58 just          \n 59 too           \n 60 off           \n 61 putting       \n 62 even          \n 63 those         \n 64 from          \n 65 the           \n 66 era           \n 67 should        \n 68 be            \n 69 turned        \n 70 off           \n 71 the           \n 72 cryptic       \n 73 dialogue      \n 74 would         \n 75 make          \n 76 shakespeare   \n 77 seem          \n 78 easy          \n 79 to            \n 80 a             \n 81 third         \n 82 grader        \n 83 on            \n 84 a             \n 85 technical     \n 86 level         \n 87 it's          \n 88 better        \n 89 than          \n 90 you           \n 91 might         \n 92 think         \n 93 with          \n 94 some          \n 95 good          \n 96 cinematography\n 97 by            \n 98 future        \n 99 great         \n100 vilmos        \n101 zsigmond      \n102 future        \n103 stars         \n104 sally         \n105 kirkland      \n106 and           \n107 frederic      \n108 forrest       \n109 can           \n110 be            \n111 seen          \n112 briefly       \n```\n\n\n:::\n:::\n\n\n-----\n\nConsidering all the tokens across all documents\n\n- There are almost 6 million words\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(tokens$word)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5935548\n```\n\n\n:::\n:::\n\n\n- The total unique *vocabulary* is around 85 thousand words\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(unique(tokens$word))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 85574\n```\n\n\n:::\n:::\n\n\n-----\n\nWord frequency is VERY skewed\n\n- These are the counts for the most frequent 750 words\n- there are 84,000 additional infrequent words in the right tail not shown here!\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens |>\n  count(word, sort = TRUE) |> \n  slice(1:750) |> \n  mutate(word = reorder(word, -n)) |>\n  ggplot(aes(word, n)) +\n    geom_col() +\n    xlab(\"Words\") +\n    ylab(\"Raw Count\") +\n    theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())\n```\n\n::: {.cell-output-display}\n![](012_nlp_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n-----\n\nNow let's review the 100 most common words\n\n- We SHOULD review MUCH deeper than this\n- Some of our feature engineering approaches (e.g., BoW) will use the first 5K - 20K tokens\n- Some of our feature engineering approaches (e.g., word embeddings) may use ALL tokens\n- Some of these are likely not very informative (the, a, of, to, is).  We will return to those words in a bit when we consider **stopwords**\n- Notice `br`.   Why is it so common?\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens |>\n  count(word, sort = TRUE) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 85,574 × 2\n    word         n\n    <chr>    <int>\n  1 the     336179\n  2 and     164061\n  3 a       162738\n  4 of      145848\n  5 to      135695\n  6 is      107320\n  7 br      101871\n  8 in       93920\n  9 it       78874\n 10 i        76508\n 11 this     75814\n 12 that     69794\n 13 was      48189\n 14 as       46903\n 15 for      44321\n 16 with     44115\n 17 movie    43509\n 18 but      42531\n 19 film     39058\n 20 on       34185\n 21 not      30608\n 22 you      29886\n 23 are      29431\n 24 his      29352\n 25 have     27725\n 26 be       26947\n 27 he       26894\n 28 one      26502\n 29 all      23927\n 30 at       23500\n 31 by       22538\n 32 an       21550\n 33 they     21096\n 34 who      20604\n 35 so       20573\n 36 from     20488\n 37 like     20268\n 38 her      18399\n 39 or       17997\n 40 just     17764\n 41 about    17368\n 42 out      17099\n 43 it's     17094\n 44 has      16789\n 45 if       16746\n 46 some     15734\n 47 there    15671\n 48 what     15374\n 49 good     15110\n 50 more     14242\n 51 when     14161\n 52 very     14059\n 53 up       13283\n 54 no       12698\n 55 time     12691\n 56 even     12638\n 57 she      12624\n 58 my       12485\n 59 would    12236\n 60 which    12047\n 61 story    11918\n 62 only     11910\n 63 really   11734\n 64 see      11465\n 65 their    11376\n 66 had      11289\n 67 can      11144\n 68 were     10782\n 69 me       10745\n 70 well     10637\n 71 than      9920\n 72 we        9858\n 73 much      9750\n 74 bad       9292\n 75 been      9287\n 76 get       9279\n 77 will      9195\n 78 do        9159\n 79 also      9130\n 80 into      9109\n 81 people    9107\n 82 other     9083\n 83 first     9054\n 84 because   9045\n 85 great     9033\n 86 how       8870\n 87 him       8865\n 88 most      8775\n 89 don't     8445\n 90 made      8351\n 91 its       8156\n 92 then      8097\n 93 make      8018\n 94 way       8005\n 95 them      7954\n 96 too       7820\n 97 could     7745\n 98 any       7653\n 99 movies    7648\n100 after     7617\n# ℹ 85,474 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\nHere is the first document that has the `br` token in it. \n\\\n\\\nIt is html code for a line break.\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens |> \n  filter(word == \"br\") |> \n  slice(1) |> \n  pull(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...<br /><br />Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.<br /><br />The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.<br /><br />The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.<br /><br />Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\"\n```\n\n\n:::\n:::\n\n\n-----\n\nLet's clean it in the raw documents and re-tokenize\n\nYou should always check your replacements CAREFULLY before doing them for unexpected matches and side-effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn <- data_trn |> \n  mutate(text = str_replace_all(text, \"<br /><br />\", \" \"))\n\ntokens <- data_trn |> \n  unnest_tokens(word, text, drop = FALSE)\n```\n:::\n\n\\\n\\\nWe should continue to review MUCH deeper into the common tokens to detect other\ntokenization errors.  I will not demonstrate that here.\n\n-----\n\nWe should also review the least common tokens\n\n- Worth searching to make sure they haven't resulted from tokenization errors\n- Can use this to tune our pre-processing and tokenization\n- BUT, not as important b/c these will be dropped by some of our feature engineering\napproaches (BoW) and may not present too much problem to others (embeddings)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens |>\n  count(word) |> \n  arrange(n) |> \n  print(n = 200)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 85,574 × 2\n    word                            n\n    <chr>                       <int>\n  1 0.10                            1\n  2 0.48                            1\n  3 0.7                             1\n  4 0.79                            1\n  5 0.89                            1\n  6 00.01                           1\n  7 000                             1\n  8 0000000000001                   1\n  9 00015                           1\n 10 003830                          1\n 11 006                             1\n 12 0079                            1\n 13 0093638                         1\n 14 01pm                            1\n 15 020410                          1\n 16 029                             1\n 17 041                             1\n 18 050                             1\n 19 06th                            1\n 20 087                             1\n 21 089                             1\n 22 08th                            1\n 23 0f                              1\n 24 0ne                             1\n 25 0r                              1\n 26 0s                              1\n 27 1'40                            1\n 28 1,000,000,000,000               1\n 29 1,000.00                        1\n 30 1,000s                          1\n 31 1,2,3                           1\n 32 1,2,3,4,5                       1\n 33 1,2,3,5                         1\n 34 1,300                           1\n 35 1,400                           1\n 36 1,430                           1\n 37 1,500,000                       1\n 38 1,600                           1\n 39 1,65m                           1\n 40 1,700                           1\n 41 1,999,999                       1\n 42 1.000                           1\n 43 1.19                            1\n 44 1.30                            1\n 45 1.30am                          1\n 46 1.3516                          1\n 47 1.37                            1\n 48 1.47                            1\n 49 1.49                            1\n 50 1.4x                            1\n 51 1.60                            1\n 52 1.66                            1\n 53 1.78                            1\n 54 1.9                             1\n 55 1.95                            1\n 56 10,000,000                      1\n 57 10.000                          1\n 58 10.75                           1\n 59 10.95                           1\n 60 100.00                          1\n 61 1000000                         1\n 62 1000lb                          1\n 63 100b                            1\n 64 100k                            1\n 65 100m                            1\n 66 100mph                          1\n 67 100yards                        1\n 68 102nd                           1\n 69 1040                            1\n 70 1040a                           1\n 71 1040s                           1\n 72 1050                            1\n 73 105lbs                          1\n 74 106min                          1\n 75 10am                            1\n 76 10lines                         1\n 77 10mil                           1\n 78 10min                           1\n 79 10minutes                       1\n 80 10p.m                           1\n 81 10star                          1\n 82 10x's                           1\n 83 10yr                            1\n 84 11,2001                         1\n 85 11.00                           1\n 86 11001001                        1\n 87 1100ad                          1\n 88 1146                            1\n 89 11f                             1\n 90 11m                             1\n 91 12.000.000                      1\n 92 120,000.00                      1\n 93 1200f                           1\n 94 1201                            1\n 95 1202                            1\n 96 123,000,000                     1\n 97 12383499143743701               1\n 98 125,000                         1\n 99 125m                            1\n100 127                             1\n101 12hr                            1\n102 12mm                            1\n103 12s                             1\n104 13,15,16                        1\n105 13.00                           1\n106 1300                            1\n107 1318                            1\n108 135m                            1\n109 137                             1\n110 139                             1\n111 13k                             1\n112 14.00                           1\n113 14.99                           1\n114 140hp                           1\n115 1415                            1\n116 142                             1\n117 1454                            1\n118 1473                            1\n119 1492                            1\n120 14ieme                          1\n121 14yr                            1\n122 15,000,000                      1\n123 15.00                           1\n124 150,000                         1\n125 1500.00                         1\n126 150_worst_cases_of_nepotism     1\n127 150k                            1\n128 150m                            1\n129 151                             1\n130 152                             1\n131 153                             1\n132 1547                            1\n133 155                             1\n134 156                             1\n135 1561                            1\n136 1594                            1\n137 15mins                          1\n138 15minutes                       1\n139 16,000                          1\n140 16.9                            1\n141 16.97                           1\n142 1600s                           1\n143 160lbs                          1\n144 1610                            1\n145 163,000                         1\n146 164                             1\n147 165                             1\n148 166                             1\n149 1660s                           1\n150 16ieme                          1\n151 16k                             1\n152 16x9                            1\n153 16éme                           1\n154 17,000                          1\n155 17,2003                         1\n156 17.75                           1\n157 1700s                           1\n158 1701                            1\n159 171                             1\n160 175                             1\n161 177                             1\n162 1775                            1\n163 1790s                           1\n164 1794                            1\n165 17million                       1\n166 18,000,000                      1\n167 1800mph                         1\n168 1801                            1\n169 1805                            1\n170 1809                            1\n171 180d                            1\n172 1812                            1\n173 18137                           1\n174 1814                            1\n175 1832                            1\n176 1838                            1\n177 1844                            1\n178 1850ies                         1\n179 1852                            1\n180 1860s                           1\n181 1870                            1\n182 1871                            1\n183 1874                            1\n184 1875                            1\n185 188                             1\n186 1887                            1\n187 1889                            1\n188 188o                            1\n189 1893                            1\n190 18year                          1\n191 19,000,000                      1\n192 190                             1\n193 1904                            1\n194 1908                            1\n195 192                             1\n196 1920ies                         1\n197 1923                            1\n198 1930ies                         1\n199 193o's                          1\n200 194                             1\n# ℹ 85,374 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\nWhat is the deal with `_*_`?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens |> \n  filter(word == \"_anything_\") |> \n  slice(1) |> \n  pull(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"I am shocked. Shocked and dismayed that the 428 of you IMDB users who voted before me have not given this film a rating of higher than 7. 7?!?? - that's a C!. If I could give FOBH a 20, I'd gladly do it. This film ranks high atop the pantheon of modern comedy, alongside Half Baked and Mallrats, as one of the most hilarious films of all time. If you know _anything_ about rap music - YOU MUST SEE THIS!! If you know nothing about rap music - learn something!, and then see this! Comparisons to 'Spinal Tap' fail to appreciate the inspired genius of this unique film. If you liked Bob Roberts, you'll love this. Watch it and vote it a 10!\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens |> \n  filter(word == \"_love_\") |> \n  slice(1) |> \n  pull(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Before I start, I _love_ Eddie Izzard. I think he's one of the funniest stand-ups around today. Possibly that means I'm going into this with too high expectations, but I just didn't find Eddie funny in this outing. I think the main problem is Eddie is trying too hard to be Eddie. Everyone knows him as a completely irrelevant comic, and we all love him for it. But in Circle, he appears to be going more for irrelevant than funny, and completely lost me in places. Many of the topics he covers he has covered before - I even think I recognised a few recycled jokes in there. If you buy the DVD you'll find a behind-the-scenes look at Eddie's tour (interesting in places, but not very funny), and a French language version of one of his shows. Die-hards will enjoy seeing Eddie in a different language, but subtitled comedy isn't very funny. If you're a fan of Eddie you've either got this already or you're going to buy it whatever I say. If you're just passing through, buy Glorious or Dressed to Kill - you won't be disappointed. With Circle, you probably will.\"\n```\n\n\n:::\n:::\n\n\n-----\n\nLet's find all the tokens that start or end with `_`\n\\\n\\\nA few of these are even repeatedly used\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens |> \n  filter(str_detect(word, \"^_\") | str_detect(word, \"_$\")) |> \n  count(word, sort = TRUE) |>\n  print(n = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 130 × 2\n    word                                                                   n\n    <chr>                                                              <int>\n  1 _the                                                                   6\n  2 _a                                                                     5\n  3 thing_                                                                 4\n  4 ____                                                                   3\n  5 _atlantis_                                                             3\n  6 _is_                                                                   3\n  7 ______                                                                 2\n  8 _____________________________________                                  2\n  9 _bounce_                                                               2\n 10 _night                                                                 2\n 11 _not_                                                                  2\n 12 _plan                                                                  2\n 13 _real_                                                                 2\n 14 _waterdance_                                                           2\n 15 story_                                                                 2\n 16 9_                                                                     1\n 17 _____                                                                  1\n 18 _________                                                              1\n 19 ____________________________________                                   1\n 20 __________________________________________________________________     1\n 21 _absolute                                                              1\n 22 _am_                                                                   1\n 23 _and_                                                                  1\n 24 _angel_                                                                1\n 25 _annie_                                                                1\n 26 _any_                                                                  1\n 27 _anything_                                                             1\n 28 _apocalyptically                                                       1\n 29 _as                                                                    1\n 30 _atlantis                                                              1\n 31 _attack                                                                1\n 32 _before_                                                               1\n 33 _blair                                                                 1\n 34 _both_                                                                 1\n 35 _by                                                                    1\n 36 _can't_                                                                1\n 37 _cannon_                                                               1\n 38 _certainly_                                                            1\n 39 _could                                                                 1\n 40 _cruel                                                                 1\n 41 _dirty                                                                 1\n 42 _discuss_                                                              1\n 43 _discussing_                                                           1\n 44 _do_                                                                   1\n 45 _dr                                                                    1\n 46 _dying                                                                 1\n 47 _earned_                                                               1\n 48 _everything_                                                           1\n 49 _ex_executives                                                         1\n 50 _extremeley_                                                           1\n 51 _extremely_                                                            1\n 52 _film_                                                                 1\n 53 _get_                                                                  1\n 54 _have_                                                                 1\n 55 _i_                                                                    1\n 56 _innerly                                                               1\n 57 _inside_                                                               1\n 58 _inspire_                                                              1\n 59 _les                                                                   1\n 60 _love_                                                                 1\n 61 _magic_                                                                1\n 62 _much_                                                                 1\n 63 _mystery                                                               1\n 64 _napolean                                                              1\n 65 _new                                                                   1\n 66 _obviously_                                                            1\n 67 _other_                                                                1\n 68 _penetrate_                                                            1\n 69 _possible_                                                             1\n 70 _really_is_                                                            1\n 71 _shall                                                                 1\n 72 _shock                                                                 1\n 73 _so_                                                                   1\n 74 _so_much_                                                              1\n 75 _somewhere_                                                            1\n 76 _spiritited                                                            1\n 77 _starstruck_                                                           1\n 78 _strictly                                                              1\n 79 _sung_                                                                 1\n 80 _the_                                                                  1\n 81 _the_lost_empire_                                                      1\n 82 _there's_                                                              1\n 83 _they_                                                                 1\n 84 _think_                                                                1\n 85 _told_                                                                 1\n 86 _toy                                                                   1\n 87 _tried                                                                 1\n 88 _twice                                                                 1\n 89 _undertow_                                                             1\n 90 _very_                                                                 1\n 91 _voice_                                                                1\n 92 _want_                                                                 1\n 93 _we've                                                                 1\n 94 _well_                                                                 1\n 95 _whale_                                                                1\n 96 _wrong_                                                                1\n 97 _x                                                                     1\n 98 acteurs_                                                               1\n 99 apple_                                                                 1\n100 away_                                                                  1\n101 ballroom_                                                              1\n102 been_                                                                  1\n103 beginners_                                                             1\n104 brail_                                                                 1\n105 casablanca_                                                            1\n106 composer_                                                              1\n107 dancing_                                                               1\n108 dougray_scott_                                                         1\n109 dozen_                                                                 1\n110 dynamite_                                                              1\n111 eaters_                                                                1\n112 eyre_                                                                  1\n113 f___                                                                   1\n114 film_                                                                  1\n115 hard_                                                                  1\n116 men_                                                                   1\n117 night_                                                                 1\n118 opera_                                                                 1\n119 rehearsals_                                                            1\n120 shrews_                                                                1\n121 space_                                                                 1\n122 starts__                                                               1\n123 that_                                                                  1\n124 treatment_                                                             1\n125 watchmen_                                                              1\n126 what_the_bleep_                                                        1\n127 witch_                                                                 1\n128 words_                                                                 1\n129 you_                                                                   1\n130 zhivago_                                                               1\n```\n\n\n:::\n:::\n\n\n-----\n\nNow we can clean the raw documents again.  This works but there is probably a better regex using `^_` and `_$`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn <- data_trn |> \n  mutate(text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n```\n:::\n\n\n-----\n\nLet's take another look and uncommon tokens\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  unnest_tokens(word, text, drop = FALSE) |> \n  count(word) |> \n  arrange(n) |> \n  print(n = 200)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 85,535 × 2\n    word                            n\n    <chr>                       <int>\n  1 0.10                            1\n  2 0.48                            1\n  3 0.7                             1\n  4 0.79                            1\n  5 0.89                            1\n  6 00.01                           1\n  7 000                             1\n  8 0000000000001                   1\n  9 00015                           1\n 10 003830                          1\n 11 006                             1\n 12 0079                            1\n 13 0093638                         1\n 14 01pm                            1\n 15 020410                          1\n 16 029                             1\n 17 041                             1\n 18 050                             1\n 19 06th                            1\n 20 087                             1\n 21 089                             1\n 22 08th                            1\n 23 0f                              1\n 24 0ne                             1\n 25 0r                              1\n 26 0s                              1\n 27 1'40                            1\n 28 1,000,000,000,000               1\n 29 1,000.00                        1\n 30 1,000s                          1\n 31 1,2,3                           1\n 32 1,2,3,4,5                       1\n 33 1,2,3,5                         1\n 34 1,300                           1\n 35 1,400                           1\n 36 1,430                           1\n 37 1,500,000                       1\n 38 1,600                           1\n 39 1,65m                           1\n 40 1,700                           1\n 41 1,999,999                       1\n 42 1.000                           1\n 43 1.19                            1\n 44 1.30                            1\n 45 1.30am                          1\n 46 1.3516                          1\n 47 1.37                            1\n 48 1.47                            1\n 49 1.49                            1\n 50 1.4x                            1\n 51 1.60                            1\n 52 1.66                            1\n 53 1.78                            1\n 54 1.9                             1\n 55 1.95                            1\n 56 10,000,000                      1\n 57 10.000                          1\n 58 10.75                           1\n 59 10.95                           1\n 60 100.00                          1\n 61 1000000                         1\n 62 1000lb                          1\n 63 100b                            1\n 64 100k                            1\n 65 100m                            1\n 66 100mph                          1\n 67 100yards                        1\n 68 102nd                           1\n 69 1040                            1\n 70 1040a                           1\n 71 1040s                           1\n 72 1050                            1\n 73 105lbs                          1\n 74 106min                          1\n 75 10am                            1\n 76 10lines                         1\n 77 10mil                           1\n 78 10min                           1\n 79 10minutes                       1\n 80 10p.m                           1\n 81 10star                          1\n 82 10x's                           1\n 83 10yr                            1\n 84 11,2001                         1\n 85 11.00                           1\n 86 11001001                        1\n 87 1100ad                          1\n 88 1146                            1\n 89 11f                             1\n 90 11m                             1\n 91 12.000.000                      1\n 92 120,000.00                      1\n 93 1200f                           1\n 94 1201                            1\n 95 1202                            1\n 96 123,000,000                     1\n 97 12383499143743701               1\n 98 125,000                         1\n 99 125m                            1\n100 127                             1\n101 12hr                            1\n102 12mm                            1\n103 12s                             1\n104 13,15,16                        1\n105 13.00                           1\n106 1300                            1\n107 1318                            1\n108 135m                            1\n109 137                             1\n110 139                             1\n111 13k                             1\n112 14.00                           1\n113 14.99                           1\n114 140hp                           1\n115 1415                            1\n116 142                             1\n117 1454                            1\n118 1473                            1\n119 1492                            1\n120 14ieme                          1\n121 14yr                            1\n122 15,000,000                      1\n123 15.00                           1\n124 150,000                         1\n125 1500.00                         1\n126 150_worst_cases_of_nepotism     1\n127 150k                            1\n128 150m                            1\n129 151                             1\n130 152                             1\n131 153                             1\n132 1547                            1\n133 155                             1\n134 156                             1\n135 1561                            1\n136 1594                            1\n137 15mins                          1\n138 15minutes                       1\n139 16,000                          1\n140 16.9                            1\n141 16.97                           1\n142 1600s                           1\n143 160lbs                          1\n144 1610                            1\n145 163,000                         1\n146 164                             1\n147 165                             1\n148 166                             1\n149 1660s                           1\n150 16ieme                          1\n151 16k                             1\n152 16x9                            1\n153 16éme                           1\n154 17,000                          1\n155 17,2003                         1\n156 17.75                           1\n157 1700s                           1\n158 1701                            1\n159 171                             1\n160 175                             1\n161 177                             1\n162 1775                            1\n163 1790s                           1\n164 1794                            1\n165 17million                       1\n166 18,000,000                      1\n167 1800mph                         1\n168 1801                            1\n169 1805                            1\n170 1809                            1\n171 180d                            1\n172 1812                            1\n173 18137                           1\n174 1814                            1\n175 1832                            1\n176 1838                            1\n177 1844                            1\n178 1850ies                         1\n179 1852                            1\n180 1860s                           1\n181 1870                            1\n182 1871                            1\n183 1874                            1\n184 1875                            1\n185 188                             1\n186 1887                            1\n187 1889                            1\n188 188o                            1\n189 1893                            1\n190 18year                          1\n191 19,000,000                      1\n192 190                             1\n193 1904                            1\n194 1908                            1\n195 192                             1\n196 1920ies                         1\n197 1923                            1\n198 1930ies                         1\n199 193o's                          1\n200 194                             1\n# ℹ 85,335 more rows\n```\n\n\n:::\n:::\n\n\nLots of numbers.  **Probably?** not that important for our classification problem.  \n\\\n\\\nLet's strip them for demonstration purposes at least using `strip_numeric = TRUE`\n\\\n\\\nThis is likey good for unigrams but wouldn't be good/possible for bigrams (break sequence)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE) |>   \n  count(word) |> \n  arrange(n) |> \n  print(n = 200)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 84,392 × 2\n    word                            n\n    <chr>                       <int>\n  1 01pm                            1\n  2 06th                            1\n  3 08th                            1\n  4 0f                              1\n  5 0ne                             1\n  6 0r                              1\n  7 0s                              1\n  8 1,000s                          1\n  9 1,65m                           1\n 10 1.30am                          1\n 11 1.4x                            1\n 12 1000lb                          1\n 13 100b                            1\n 14 100k                            1\n 15 100m                            1\n 16 100mph                          1\n 17 100yards                        1\n 18 102nd                           1\n 19 1040a                           1\n 20 1040s                           1\n 21 105lbs                          1\n 22 106min                          1\n 23 10am                            1\n 24 10lines                         1\n 25 10mil                           1\n 26 10min                           1\n 27 10minutes                       1\n 28 10p.m                           1\n 29 10star                          1\n 30 10x's                           1\n 31 10yr                            1\n 32 1100ad                          1\n 33 11f                             1\n 34 11m                             1\n 35 1200f                           1\n 36 125m                            1\n 37 12hr                            1\n 38 12mm                            1\n 39 12s                             1\n 40 135m                            1\n 41 13k                             1\n 42 140hp                           1\n 43 14ieme                          1\n 44 14yr                            1\n 45 150_worst_cases_of_nepotism     1\n 46 150k                            1\n 47 150m                            1\n 48 15mins                          1\n 49 15minutes                       1\n 50 1600s                           1\n 51 160lbs                          1\n 52 1660s                           1\n 53 16ieme                          1\n 54 16k                             1\n 55 16éme                           1\n 56 1700s                           1\n 57 1790s                           1\n 58 17million                       1\n 59 1800mph                         1\n 60 180d                            1\n 61 1850ies                         1\n 62 1860s                           1\n 63 188o                            1\n 64 18year                          1\n 65 1920ies                         1\n 66 1930ies                         1\n 67 193o's                          1\n 68 1949er                          1\n 69 1961s                           1\n 70 1970ies                         1\n 71 1970s.i                         1\n 72 197o                            1\n 73 1980ies                         1\n 74 1982s                           1\n 75 1983s                           1\n 76 19k                             1\n 77 19thc                           1\n 78 1and                            1\n 79 1h40m                           1\n 80 1million                        1\n 81 1min                            1\n 82 1mln                            1\n 83 1o                              1\n 84 1ton                            1\n 85 1tv.ru                          1\n 86 1ç                              1\n 87 2.00am                          1\n 88 2.5hrs                          1\n 89 2000ad                          1\n 90 2004s                           1\n 91 200ft                           1\n 92 200th                           1\n 93 20c                             1\n 94 20ft                            1\n 95 20k                             1\n 96 20m                             1\n 97 20mins                          1\n 98 20minutes                       1\n 99 20mn                            1\n100 20p                             1\n101 20perr                          1\n102 20s.what                        1\n103 20ties                          1\n104 20widow                         1\n105 20x                             1\n106 20year                          1\n107 20yrs                           1\n108 225mins                         1\n109 22d                             1\n110 230lbs                          1\n111 230mph                          1\n112 23d                             1\n113 24m30s                          1\n114 24years                         1\n115 25million                       1\n116 25mins                          1\n117 25s                             1\n118 25yrs                           1\n119 261k                            1\n120 2fast                           1\n121 2furious                        1\n122 2h                              1\n123 2hour                           1\n124 2hr                             1\n125 2in                             1\n126 2inch                           1\n127 2more                           1\n128 300ad                           1\n129 300c                            1\n130 300lbs                          1\n131 300mln                          1\n132 30am                            1\n133 30ish                           1\n134 30k                             1\n135 30lbs                           1\n136 30s.like                        1\n137 30something                     1\n138 30ties                          1\n139 32lb                            1\n140 32nd                            1\n141 330am                           1\n142 330mins                         1\n143 336th                           1\n144 33m                             1\n145 35c                             1\n146 35mins                          1\n147 35pm                            1\n148 39th                            1\n149 3bs                             1\n150 3dvd                            1\n151 3lbs                            1\n152 3m                              1\n153 3mins                           1\n154 3pm                             1\n155 3po's                           1\n156 3th                             1\n157 3who                            1\n158 4.5hrs                          1\n159 401k                            1\n160 40am                            1\n161 40min                           1\n162 40mph                           1\n163 442nd                           1\n164 44c                             1\n165 44yrs                           1\n166 45am                            1\n167 45min                           1\n168 45s                             1\n169 480m                            1\n170 480p                            1\n171 4cylinder                       1\n172 4d                              1\n173 4eva                            1\n174 4f                              1\n175 4h                              1\n176 4hrs                            1\n177 4o                              1\n178 4pm                             1\n179 4w                              1\n180 4ward                           1\n181 4x                              1\n182 5.50usd                         1\n183 500db                           1\n184 500lbs                          1\n185 50c                             1\n186 50ft                            1\n187 50ies                           1\n188 50ish                           1\n189 50k                             1\n190 50mins                          1\n191 51b                             1\n192 51st                            1\n193 52s                             1\n194 53m                             1\n195 540i                            1\n196 54th                            1\n197 57d                             1\n198 58th                            1\n199 5kph                            1\n200 5min                            1\n# ℹ 84,192 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\nThe tokenizer didn't get rid of numbers connected to text\n\n- How do we want to handle these?\n- Could add a space to make them two words? \n- We will leave them as is\n\n-----\n\nOther issues?\n\n- Mis-spellings\n- Repeated letters for emphasize or effect?\n- Did caps matter (default tokenization was to convert to lowercase)?\n- Domain knowledge is useful here though multiple model configurations can be considered\n- strings of words that aren't meaningful (\"Welcome to Facebook\")\n\n-----\n\nIn the above workflow, we:\n\n1. tokenize\n2. review tokens for issues\n3. clean raw text documents\n4. re-tokenize\n\n\\\n\\\nIn some instances, it may be easier to clean the token and then put them back together\n\n1. tokenize\n2. review tokens for issues\n3. clean tokens\n4. recreate document\n5. tokenize\n\n-----\n\nIf this latter workflow feels easier (i.e., easier to regex into a token than a document), \nwe will need code to put the tokens back together into a document\n\\\n\\\nHere is an example using the first three documents (`slice(1:3)1) and no cleaning\n\n\n1. Tokenize\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens <- \n  data_trn |>\n  slice(1:3) |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE)\n```\n:::\n\n\n2. Now we can do further cleaning with tokens\n\n`INSERT CLEANING CHUNKS HERE`\n\n3. Then put back together.  Could also collapse into `text_cln` to retain original text column\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn_cln <-\n  tokens |>  \n  group_by(doc_num, sentiment) |> \n  summarize(text = str_c(word, collapse = \" \"),\n            .groups = \"drop\") \n```\n:::\n\n\n-----\n\nLets see what we have\n\n- Now all lower case\n- No numbers, punctuation, etc.\n- and any other cleaning we **could** have done with the tokens along the way....\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn_cln |> \n  pull(text) |>\n  print_kbl() \n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; \"><table class=\"table table-striped table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> x </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> story of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is turned into an insane violent mob by the crazy chantings of it's singers unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting even those from the era should be turned off the cryptic dialogue would make shakespeare seem easy to a third grader on a technical level it's better than you might think with some good cinematography by future great vilmos zsigmond future stars sally kirkland and frederic forrest can be seen briefly </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> airport starts as a brand new luxury plane is loaded up with valuable paintings such belonging to rich businessman philip stevens james stewart who is flying them a bunch of vip's to his estate in preparation of it being opened to the public as a museum also on board is stevens daughter julie kathleen quinlan her son the luxury jetliner takes off as planned but mid air the plane is hi jacked by the co pilot chambers robert foxworth his two accomplice's banker monte markham wilson michael pataki who knock the passengers crew out with sleeping gas they plan to steal the valuable cargo land on a disused plane strip on an isolated island but while making his descent chambers almost hits an oil rig in the ocean loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the bermuda triangle with air in short supply water leaking in having flown over miles off course the problems mount for the survivor's as they await help with time fast running out also known under the slightly different tile airport this second sequel to the smash hit disaster thriller airport was directed by jerry jameson while once again like it's predecessors i can't say airport is any sort of forgotten classic it is entertaining although not necessarily for the right reasons out of the three airport films i have seen so far i actually liked this one the best just it has my favourite plot of the three with a nice mid air hi jacking then the crashing didn't he see the oil rig sinking of the maybe the makers were trying to cross the original airport with another popular disaster flick of the period the poseidon adventure submerged is where it stays until the end with a stark dilemma facing those trapped inside either suffocate when the air runs out or drown as the floods or if any of the doors are opened it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's dull dialogue lethargic set pieces a real lack of danger or suspense or tension means this is a missed opportunity while the rather sluggish plot keeps one entertained for odd minutes not that much happens after the plane sinks there's not as much urgency as i thought there should have been even when the navy become involved things don't pick up that much with a few shots of huge ships helicopters flying about but there's just something lacking here george kennedy as the jinxed airline worker joe patroni is back but only gets a couple of scenes barely even says anything preferring to just look worried in the background the home video theatrical version of airport run minutes while the us tv versions add an extra hour of footage including a new opening credits sequence many more scenes with george kennedy as patroni flashbacks to flesh out character's longer rescue scenes the discovery or another couple of dead bodies including the navigator while i would like to see this extra footage i am not sure i could sit through a near three hour cut of airport as expected the film has dated badly with horrible fashions interior design choices i will say no more other than the toy plane model effects aren't great either along with the other two airport sequels this takes pride of place in the razzie award's hall of shame although i can think of lots of worse films than this so i reckon that's a little harsh the action scenes are a little dull unfortunately the pace is slow not much excitement or tension is generated which is a shame as i reckon this could have been a pretty good film if made properly the production values are alright if nothing spectacular the acting isn't great two time oscar winner jack lemmon has said since it was a mistake to star in this one time oscar winner james stewart looks old frail also one time oscar winner lee grant looks drunk while sir christopher lee is given little to do there are plenty of other familiar faces to look out for too airport is the most disaster orientated of the three airport films so far i liked the ideas behind it even if they were a bit silly the production bland direction doesn't help though a film about a sunken plane just shouldn't be this boring or lethargic followed by the concorde airport </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> this film lacked something i couldn't put my finger on at first charisma on the part of the leading actress this inevitably translated to lack of chemistry when she shared the screen with her leading man even the romantic scenes came across as being merely the actors at play it could very well have been the director who miscalculated what he needed from the actors i just don't know but could it have been the screenplay just exactly who was the chef in love with he seemed more enamored of his culinary skills and restaurant and ultimately of himself and his youthful exploits than of anybody or anything else he never convinced me he was in love with the princess i was disappointed in this movie but don't forget it was nominated for an oscar so judge for yourself </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n-----\n\n## Stop words\n\nNot all words are equally informative or useful to our model depending on the \nnature of our problem\n\\\n\\\nVery common words often may carry little or no meaningful information\n\\\n\\\nThese words are called **stop words**\n\\\n\\\nIt is common advice and practice to remove stop words for various NLP tasks\n\\\n\\\nNotice some of the top most frequent words among our tokens from IMDB reviews\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE) |> \n  count(word, sort = TRUE) |> \n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 84,392 × 2\n    word         n\n    <chr>    <int>\n  1 the     336185\n  2 and     164061\n  3 a       162743\n  4 of      145848\n  5 to      135695\n  6 is      107320\n  7 in       93920\n  8 it       78874\n  9 i        76508\n 10 this     75814\n 11 that     69794\n 12 was      48189\n 13 as       46904\n 14 for      44321\n 15 with     44115\n 16 movie    43509\n 17 but      42531\n 18 film     39058\n 19 on       34185\n 20 not      30608\n 21 you      29886\n 22 are      29431\n 23 his      29352\n 24 have     27725\n 25 be       26947\n 26 he       26894\n 27 one      26502\n 28 all      23927\n 29 at       23500\n 30 by       22539\n 31 an       21550\n 32 they     21096\n 33 who      20604\n 34 so       20573\n 35 from     20488\n 36 like     20268\n 37 her      18399\n 38 or       17997\n 39 just     17764\n 40 about    17368\n 41 out      17099\n 42 it's     17094\n 43 has      16789\n 44 if       16746\n 45 some     15734\n 46 there    15671\n 47 what     15374\n 48 good     15110\n 49 more     14242\n 50 when     14161\n 51 very     14059\n 52 up       13283\n 53 no       12698\n 54 time     12691\n 55 even     12638\n 56 she      12624\n 57 my       12485\n 58 would    12236\n 59 which    12047\n 60 story    11919\n 61 only     11910\n 62 really   11734\n 63 see      11465\n 64 their    11376\n 65 had      11289\n 66 can      11144\n 67 were     10782\n 68 me       10745\n 69 well     10637\n 70 than      9920\n 71 we        9858\n 72 much      9750\n 73 bad       9292\n 74 been      9287\n 75 get       9279\n 76 will      9195\n 77 do        9159\n 78 also      9130\n 79 into      9109\n 80 people    9107\n 81 other     9083\n 82 first     9054\n 83 because   9045\n 84 great     9033\n 85 how       8870\n 86 him       8865\n 87 most      8775\n 88 don't     8445\n 89 made      8351\n 90 its       8156\n 91 then      8097\n 92 make      8018\n 93 way       8005\n 94 them      7954\n 95 too       7820\n 96 could     7746\n 97 any       7653\n 98 movies    7648\n 99 after     7617\n100 think     7293\n# ℹ 84,292 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\nStop words can have different roles in a **corpus** (a set of documents)\n\\\n\\\nFor our purposes, we generally care about two different types of stop words:\n\n- Global\n- Subject-specific\n\n\n\\\n\\\nGlobal stop words almost always have very little value for our modeling goals\n\\\n\\\nThese are frequent words like \"the\", “of” and “and” in English.\n\\\n\\\nIt is typically pretty safe to remove these and you can find them in pre-made lists of stop words (see below)\n\n-----\n\nSubject-specific stop words are words that are common and uninformative given the subject or context within which your text/documents were collected and your modeling goals.  \n\\\n\\\nFor example, given our goal to classify movie reviews as positive or negative, subject-specific stop words might include:\n\n- movie\n- film\n- movies\n\n-----\n\nWe likely we see others if we expand our review of commons words a bit more (which we should!)\n\n- character\n- actor\n- actress\n- director\n- cast\n- scene\n\nThese are not general stop words but they will be common in this dataset and the **may* be uninformative RE our classification goal\n\n-----\n\nSubject-specific stop words may improve performance if you have the domain expertise to create a good list\n\\\n\\\nHOWEVER, you should think carefully about your goals and method.  For example, if you are using bigrams rather than single word (unigram) tokens, you might retain words like actor or director because them may be informative in bigrams\n\n- bad actor\n- great director\n\n\\\n\\\nThough it might be sufficient to just retain **bad** and **great** \n\n-----\n\nThe `stopwords` package contains many lists of stopwords.  \n\n- We can access these lists using through that package\n- Those lists are also available with `get_stopwords()` in the `tidytext` package (my preference)\n- `get_stopwords()` returns a tibble with two columns (see below)\n\n-----\n\nTwo commonly used stop word lists are:\n\n- snowball (175 words)\n\n::: {.cell}\n\n```{.r .cell-code}\nstop_snowball <- \n  get_stopwords(source = \"snowball\") |> \n  print(n = 50)  # review the first 50 words\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 175 × 2\n   word       lexicon \n   <chr>      <chr>   \n 1 i          snowball\n 2 me         snowball\n 3 my         snowball\n 4 myself     snowball\n 5 we         snowball\n 6 our        snowball\n 7 ours       snowball\n 8 ourselves  snowball\n 9 you        snowball\n10 your       snowball\n11 yours      snowball\n12 yourself   snowball\n13 yourselves snowball\n14 he         snowball\n15 him        snowball\n16 his        snowball\n17 himself    snowball\n18 she        snowball\n19 her        snowball\n20 hers       snowball\n21 herself    snowball\n22 it         snowball\n23 its        snowball\n24 itself     snowball\n25 they       snowball\n26 them       snowball\n27 their      snowball\n28 theirs     snowball\n29 themselves snowball\n30 what       snowball\n31 which      snowball\n32 who        snowball\n33 whom       snowball\n34 this       snowball\n35 that       snowball\n36 these      snowball\n37 those      snowball\n38 am         snowball\n39 is         snowball\n40 are        snowball\n41 was        snowball\n42 were       snowball\n43 be         snowball\n44 been       snowball\n45 being      snowball\n46 have       snowball\n47 has        snowball\n48 had        snowball\n49 having     snowball\n50 do         snowball\n# ℹ 125 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\n- smart (571 words)\n\n::: {.cell}\n\n```{.r .cell-code}\nstop_smart <-\n  get_stopwords(source = \"smart\") |> \n  print(n = 50)  # review the first 50 words\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 571 × 2\n   word        lexicon\n   <chr>       <chr>  \n 1 a           smart  \n 2 a's         smart  \n 3 able        smart  \n 4 about       smart  \n 5 above       smart  \n 6 according   smart  \n 7 accordingly smart  \n 8 across      smart  \n 9 actually    smart  \n10 after       smart  \n11 afterwards  smart  \n12 again       smart  \n13 against     smart  \n14 ain't       smart  \n15 all         smart  \n16 allow       smart  \n17 allows      smart  \n18 almost      smart  \n19 alone       smart  \n20 along       smart  \n21 already     smart  \n22 also        smart  \n23 although    smart  \n24 always      smart  \n25 am          smart  \n26 among       smart  \n27 amongst     smart  \n28 an          smart  \n29 and         smart  \n30 another     smart  \n31 any         smart  \n32 anybody     smart  \n33 anyhow      smart  \n34 anyone      smart  \n35 anything    smart  \n36 anyway      smart  \n37 anyways     smart  \n38 anywhere    smart  \n39 apart       smart  \n40 appear      smart  \n41 appreciate  smart  \n42 appropriate smart  \n43 are         smart  \n44 aren't      smart  \n45 around      smart  \n46 as          smart  \n47 aside       smart  \n48 ask         smart  \n49 asking      smart  \n50 associated  smart  \n# ℹ 521 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\nsmart is mostly a super-set of snowball except for these words which are only in snowball\n\\\n\\\nStop word lists aren't perfect.  Why does smart contain `he's` but not `she's`?\n\n::: {.cell}\n\n```{.r .cell-code}\nsetdiff(pull(stop_snowball, word),\n        pull(stop_smart, word))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"she's\"   \"he'd\"    \"she'd\"   \"he'll\"   \"she'll\"  \"shan't\"  \"mustn't\"\n [8] \"when's\"  \"why's\"   \"how's\"  \n```\n\n\n:::\n:::\n\n\n-----\n\nIt is common and appropriate to start with a pre-made word list or set of lists and combine, add, and/or remove words based on your specific needs\n\n- You can add global words that you feel are missed\n- You can add subject specific words\n- You can remove global words that might be relevant to your problem\n\n-----\n\nIn the service of simplicity, we will use the union of the two previous pre-made \nglobal lists without any additional subject specific lists\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_stops <- union(pull(stop_snowball, word), pull(stop_smart, word))\n```\n:::\n\n\n-----\n\nWe can remove stop words as part of tokenization using `stopwords = all_stops`\n\\\n\\\nLet's see our two 100 tokens now\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE,\n                stopwords = all_stops) |> \n  count(word) |> \n  arrange(desc(n)) |> \n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 83,822 × 2\n    word            n\n    <chr>       <int>\n  1 movie       43509\n  2 film        39058\n  3 good        15110\n  4 time        12691\n  5 story       11919\n  6 bad          9292\n  7 people       9107\n  8 great        9033\n  9 made         8351\n 10 make         8018\n 11 movies       7648\n 12 characters   7142\n 13 watch        6959\n 14 films        6881\n 15 character    6701\n 16 plot         6563\n 17 life         6560\n 18 acting       6482\n 19 love         6421\n 20 show         6171\n 21 end          5640\n 22 man          5630\n 23 scene        5356\n 24 scenes       5206\n 25 back         4965\n 26 real         4734\n 27 watching     4597\n 28 years        4508\n 29 thing        4498\n 30 actors       4476\n 31 work         4368\n 32 funny        4278\n 33 makes        4204\n 34 director     4184\n 35 find         4129\n 36 part         4020\n 37 lot          3965\n 38 cast         3816\n 39 world        3698\n 40 things       3685\n 41 pretty       3663\n 42 young        3634\n 43 horror       3578\n 44 fact         3521\n 45 big          3471\n 46 long         3441\n 47 thought      3434\n 48 series       3410\n 49 give         3374\n 50 original     3358\n 51 action       3351\n 52 comedy       3230\n 53 times        3223\n 54 point        3218\n 55 role         3175\n 56 interesting  3125\n 57 family       3109\n 58 bit          3052\n 59 music        3045\n 60 script       3007\n 61 guy          2962\n 62 making       2960\n 63 feel         2947\n 64 minutes      2944\n 65 performance  2887\n 66 kind         2780\n 67 girl         2739\n 68 tv           2732\n 69 worst        2730\n 70 day          2711\n 71 fun          2690\n 72 hard         2666\n 73 woman        2651\n 74 played       2586\n 75 found        2571\n 76 screen       2474\n 77 set          2452\n 78 place        2403\n 79 book         2394\n 80 put          2379\n 81 ending       2351\n 82 money        2351\n 83 true         2329\n 84 sense        2320\n 85 reason       2316\n 86 actor        2312\n 87 shows        2304\n 88 dvd          2282\n 89 worth        2274\n 90 job          2270\n 91 year         2268\n 92 main         2264\n 93 watched      2235\n 94 play         2222\n 95 american     2217\n 96 plays        2214\n 97 effects      2196\n 98 takes        2192\n 99 beautiful    2176\n100 house        2171\n# ℹ 83,722 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\nWhat if we were doing bigrams instead?\n\n- token = \"ngrams\"\n- n = 2,\n- n_min = 2\n- NOTE: can't strip numeric (would break the sequence of words)\n- NOTE: There are more bigrams than unigrams (more features for BoW!)\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                token = \"ngrams\",\n                stopwords = all_stops, \n                n = 2,\n                n_min = 2) |> \n  count(word) |> \n  arrange(desc(n)) |> \n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,616,477 × 2\n    word                      n\n    <chr>                 <int>\n  1 special effects        1110\n  2 low budget              881\n  3 waste time              793\n  4 good movie              785\n  5 watch movie             695\n  6 movie made              693\n  7 sci fi                  647\n  8 years ago               631\n  9 real life               617\n 10 film made               588\n 11 movie good              588\n 12 movie movie             561\n 13 pretty good             557\n 14 bad movie               552\n 15 high school             545\n 16 watching movie          534\n 17 movie bad               512\n 18 main character          509\n 19 good film               487\n 20 great movie             473\n 21 horror movie            468\n 22 horror film             454\n 23 long time               448\n 24 make movie              445\n 25 film making             417\n 26 film good               412\n 27 worth watching          406\n 28 10 10                   404\n 29 movie great             388\n 30 bad acting              387\n 31 worst movie             386\n 32 black white             385\n 33 main characters         381\n 34 end movie               380\n 35 film film               368\n 36 takes place             360\n 37 great film              358\n 38 camera work             356\n 39 make sense              348\n 40 good job                347\n 41 story line              347\n 42 watch film              344\n 43 movie watch             343\n 44 character development   341\n 45 supporting cast         338\n 46 1 2                     334\n 47 love story              334\n 48 read book               332\n 49 bad guys                327\n 50 end film                320\n 51 8 10                    318\n 52 horror movies           318\n 53 make film               317\n 54 made movie              315\n 55 good thing              307\n 56 7 10                    305\n 57 world war               289\n 58 bad film                288\n 59 horror films            285\n 60 watched movie           285\n 61 thing movie             283\n 62 1 10                    280\n 63 part movie              278\n 64 watching film           277\n 65 bad guy                 274\n 66 4 10                    273\n 67 made film               272\n 68 rest cast               268\n 69 tv series               268\n 70 writer director         267\n 71 time movie              266\n 72 half hour               265\n 73 production values       265\n 74 film great              262\n 75 highly recommend        261\n 76 makes sense             256\n 77 martial arts            256\n 78 love movie              255\n 79 science fiction         255\n 80 acting bad              254\n 81 tv movie                253\n 82 recommend movie         251\n 83 3 10                    246\n 84 entire movie            245\n 85 film makers             245\n 86 9 10                    242\n 87 movie time              242\n 88 fun watch               238\n 89 kung fu                 238\n 90 film bad                235\n 91 good acting             235\n 92 true story              233\n 93 movie make              232\n 94 movies made             232\n 95 point view              232\n 96 film festival           231\n 97 great job               227\n 98 young woman             227\n 99 good story              225\n100 star wars               224\n# ℹ 1,616,377 more rows\n```\n\n\n:::\n:::\n\n\\\n\\\nLooks like we are starting to get some signal\n\n-----\n\n## Stemming\n  \nDocuments often contain different versions of one base word\n\\\n\\\nWe refer to the common base as the stem\n\\\n\\\nOften, we may want to treat the different versions of the stem as the same token. \nThis can reduce the total number of tokens that we need to use for features later\nwhich can lead to a better performing model\n\\\n\\\nFor example, do we need to distinguish between movie vs. movies or actor vs. actors or should we collapse those pairs into a single token?\n\n-----\n\nThere are many different algorithms that can stem words for us (i.e., collapse multiple\nversions into the same base).  However, we will focus on only one here as an introduction to the concept and approach for stemming\n\\\n\\\nThis is the Porter method and a current implementation of it is available in the \nusing `wordStem()` in the `SnowballC` package\n\n-----\n\nThe goal of stemming is to reduce the dimensionality (size) of our vocabulary\n\\\n\\\nWhenever we can combine words that \"belong\" together with respect to our goal,\nwe may improve the performance of our model\n\\\n\\\nHowever, stemming is hard and it will also invariably combine words that shouldn't\nbe combined\n\\\n\\\nStemming is useful when it suceeds more than it fails or when it succees more with \nimportant words/tokens\n\n-----\n\nHere are examples of when it helps to reduce our vocabulary given our task\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwordStem(c(\"movie\", \"movies\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"movi\" \"movi\"\n```\n\n\n:::\n\n```{.r .cell-code}\nwordStem(c(\"actor\", \"actors\", \"actress\", \"actresses\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"actor\"   \"actor\"   \"actress\" \"actress\"\n```\n\n\n:::\n\n```{.r .cell-code}\nwordStem(c(\"wait\", \"waits\", \"waiting\", \"waited\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"wait\" \"wait\" \"wait\" \"wait\"\n```\n\n\n:::\n\n```{.r .cell-code}\nwordStem(c(\"play\", \"plays\", \"played\", \"playing\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"plai\" \"plai\" \"plai\" \"plai\"\n```\n\n\n:::\n:::\n\n\n-----\n\nSometimes it works partially, likely with still some benefit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwordStem(c(\"go\", \"gone\", \"going\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"go\"   \"gone\" \"go\"  \n```\n\n\n:::\n\n```{.r .cell-code}\nwordStem(c(\"send\", \"sending\", \"sent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"send\" \"send\" \"sent\"\n```\n\n\n:::\n\n```{.r .cell-code}\nwordStem(c(\"fishing\", \"fished\", \"fisher\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"fish\"   \"fish\"   \"fisher\"\n```\n\n\n:::\n:::\n\n\n-----\n\nBut it clearly makes salient errors too\n\n::: {.cell}\n\n```{.r .cell-code}\nwordStem(c(\"university\", \"universal\", \"universe\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"univers\" \"univers\" \"univers\"\n```\n\n\n:::\n\n```{.r .cell-code}\nwordStem(c(\"is\", \"are\", \"was\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"i\"  \"ar\" \"wa\"\n```\n\n\n:::\n\n```{.r .cell-code}\nwordStem(c(\"he\", \"his\", \"him\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"he\"  \"hi\"  \"him\"\n```\n\n\n:::\n\n```{.r .cell-code}\nwordStem(c(\"like\", \"liking\", \"likely\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"like\" \"like\" \"like\"\n```\n\n\n:::\n\n```{.r .cell-code}\nwordStem(c(\"mean\", \"meaning\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"mean\" \"mean\"\n```\n\n\n:::\n:::\n\n\n-----\n\nOf course, the errors are more important if they are with words that contain predictive signal\n\\\n\\\nTherefore, we should look at how it works with our text\n\\\n\\\nTo stem our tokens if we only care about unigrams:\n\n- We first tokenize as before (including removal of stop words)\n- Then we stem, for now using `wordStem()`\n- We are mutating the stemmed words into a new column, `stem`, so we can compare its effect\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens <- \n  data_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE,\n                stopwords = all_stops) |> \n  mutate(stem = wordStem(word)) |> \n  select(word, stem)\n```\n:::\n\n\n-----\n\nLet's compare vocabulary size\n\\  \n\\\nStemming produced a sizable reduction in vocabulary size\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(unique(tokens$word))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 83822\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(unique(tokens$stem))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 60813\n```\n\n\n:::\n:::\n\n\n-----\n\nLet's compare frequencies of the top 100 words vs. stems\n\n- Here are our normal (not stemmed words).  Notice vocabulary size\n\n::: {.cell}\n\n```{.r .cell-code}\nword_tokens <-\n  tokens |> \n  tab(word) |> \n  arrange(desc(n)) |> \n  slice(1:100) |> \n  select(word, n_word = n)\n\nstem_tokens <-\n  tokens |> \n  tab(stem) |> \n  arrange(desc(n)) |> \n  slice(1:100) |> \n  select(stem, n_stem = n)\n\nword_tokens |> \n  bind_cols(stem_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 4\n   word   n_word stem    n_stem\n   <chr>   <int> <chr>    <int>\n 1 movie   43509 movi     51159\n 2 film    39058 film     47096\n 3 good    15110 time     16146\n 4 time    12691 good     15327\n 5 story   11919 make     15203\n 6 bad      9292 watch    13920\n 7 people   9107 charact  13844\n 8 great    9033 stori    13104\n 9 made     8351 scene    10563\n10 make     8018 show      9750\n# ℹ 90 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\nStemming is often routinely used as part of an NLP pipeline.  \n\n- Often without thought\n- We should consider carefully if it will help or hurt\n- We should likely formally evaluate it (via model configurations), sometimes without much comment about when it is helpful or not. We encourage you to think of stemming as a pre-processing step in text modeling, one that must be thought through and chosen (or not) with good judgment.\n\n-----\n\nIn this example, we focused on unigrams.  \n\\\n\\\nIf we had wanted bigrams, we would have needed a different order of steps\n\n- Extract words\n- Stem\n- Put back together\n- Extract bigrams\n- Remove stop words\n\n\\\n\\\nThink carefully about what you are doing and what your goals are!\n\\\n\\\nYou can read more about stemming and related (more complicated but possibly more precise)\nprocedure called lemmazation in a [chapter](https://smltar.com/stemming.html#how-to-stem-text-in-r) from @SMLTAR\n\n-----\n\n## Bag of Words\n\nNow that we understand how to tokenize our documents, we can begin to consider how to feature engineer using these tokens\n\\\n\\\nThe **Bag-of-words (BoW)** method  \n\n- Provides one way of representing tokens within text data when modeling text with machine learning algorithms.\n- Is simple to understand and implement \n- Works well for problems such as document classification\n\n-----\n\nBoW is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n- A **vocabulary** of known \"words\" (I put words in quote because our tokens will sometimes be something other than a word)\n- A measure of the occurrence or frequency of these known words.\n\n\\\n\\\nIt is called a “bag” of words because information about the order or structure of words in the document is discarded. BoW is only concerned with occurrence or frequency of known words in the document, not where in the document they occur.\n\\\n\\\nBoW assumes that documents that contain the same content are similar and that we can learn something about the document by its content alone.\n\n-----\n\nBoW approaches vary on two primary characteristics:\n\n- What the token type is\n  - Word is most common\n  - Also common to use bigrams, combinations of unigrams (words) and bigrams\n  - Other options exist (e.g.,trigram)\n- How occurrence frequent of the word/token is measured\n  - Binary (presence or absence)\n  - Raw count\n  - Term frequency\n  - Term frequency - inverse document frequency (tf-idf)\n  - and other less common options\n  \n-----\n\nLets start with a very simple example\n\n- Two documents\n- Tokenization to words\n- Lowercase, strip punctuation, did not remove any stopwords, no stemming or lemmatization\n- Binary measurement (1 = yes, 0 = no)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; \"><table class=\"table table-striped table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> Document </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> i </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> loved </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> that </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> movie </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> am </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> so </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> happy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> was </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> not </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> good </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> I loved that movie! I am so so so happy. </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> That movie was not good. </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n-----\n\nThis matrix is referred to as a **Document-Term Matrix (DTM)**\n\n- Rows are documents\n- Columns are terms (tokens)\n- Combination of all terms/tokens is called the **vocabulary**\n- The terms columns (or a subset) will be used as features in our statistical algorithm to predict some outcome (not displayed)\n\n-----\n\nYou will also see the use of **raw counts** for measurement of the cell value for each term\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; \"><table class=\"table table-striped table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> Document </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> i </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> loved </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> that </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> movie </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> am </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> so </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> happy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> was </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> not </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> good </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> I loved that movie! I am so so so happy. </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> That movie was not good. </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\nBoth binary and raw count measures are biased (increased) for longer documents\n\n-----\n\nThe bias based on document length motivates the use of **term frequency**\n\n- Term frequency is NOT a simple frequency count (that is raw count)\n- Instead it is the raw count divided by the document length\n- This removes the bias based on document length\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## echo: false\n\ntibble::tribble(\n  ~Document, ~i, ~loved, ~that, ~movie, ~am, ~so, ~happy, ~was, ~not, ~good,\n  \"I loved that movie! I am so so so happy.\", .2, .1, .1, .1, .1, .3, .1, 0, 0, 0,\n  \"That movie was not good.\", 0, 0, .2, .2, 0, 0, 0, .2, .2, .2) |>\n  print_kbl()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; \"><table class=\"table table-striped table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> Document </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> i </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> loved </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> that </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> movie </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> am </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> so </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> happy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> was </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> not </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> good </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> I loved that movie! I am so so so happy. </td>\n   <td style=\"text-align:right;\"> 0.2 </td>\n   <td style=\"text-align:right;\"> 0.1 </td>\n   <td style=\"text-align:right;\"> 0.1 </td>\n   <td style=\"text-align:right;\"> 0.1 </td>\n   <td style=\"text-align:right;\"> 0.1 </td>\n   <td style=\"text-align:right;\"> 0.3 </td>\n   <td style=\"text-align:right;\"> 0.1 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> That movie was not good. </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0.2 </td>\n   <td style=\"text-align:right;\"> 0.2 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0.2 </td>\n   <td style=\"text-align:right;\"> 0.2 </td>\n   <td style=\"text-align:right;\"> 0.2 </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n-----\n\nTerm frequency can be dominated by frequently occurring words that may not be as important to understand the document as are rarer but more domain specific words\n\\\n\\\nThis was the motivation for removing stopwords but stopword removal may not be sufficient\n\\\n\\\n**Term Frequency - Inverse Document Frequency (tf-idf)** was developed to address this issue\n\\\n\\\nTF-IDF scales the term frequency by the inverse document frequency\n\n- Term frequency tells us that word is frequently used in the current document\n- IDF indexes how rare the word is across documents (higer IDF == more rare)\n\n\\\n\\\nThis emphasizes words used in specific documents that are not commonly used otherwise\n\n-----\n\n$IDF = log(\\frac{total\\:number\\:of\\:documents}{documents\\:containing\\:the\\:word})$\n\\\n\\\nThis results in larger values for words that arent used in many documents.  e.g., \n\n- for a word that appears in only 1 of 1000 documents, idf = log(1000/1) = 3\n- for a word that appears in 1000 of 1000 documents, idf = log(1000/1000) = 0\n\n\\\n\\\nNote that word that appears in no documents would result in a division by zero.  Therefore it is common to add 1 to the denominator of idf\n\n-----\n\nWe should be aware of some of the limitations of BoW:\n\n- Vocabulary: The vocabulary requires careful thought  \n  - Choice of stop words (global and subject specific) can matter\n  - Choice about size (number of tokens) can matter\n\n- Sparsity: \n  - Sparse representations (features that contain mostly zeros) are hard to model for computational reasons (space and time)\n  - Sparse representations present a challenge to extract signal in a large representational space\n  \n- Meaning: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). \n  - Context and meaning can offer a lot to the model\n  - “this is interesting” vs “is this interesting”\n  - “old bike” vs “used bike”\n\n\n\n\n\n-----\n\n## Bringing it all together\n\n### General\n\nWe will now explore a series of model configurations to predict the sentiment (positive vs negative) of IMDB.com reviews\n\n- To keep things simple, all model configurations will use only one statistical algorithm - glmnet\n  - Within algorithm, configurations will differ by `penatly` and `dials::mixture' - see grid below\n- We will consider BoW features derived by TF-IDF only\n- We will remove global stop words from some configurations\n- We will stem words from some configurations\n- These BoW configurations will also differ by token type\n    - Word/unigram\n    - A combination of uni- and bigrams\n    - Prior to casting the tf-idf document-term matrix, we will filter tokens to various sizes - see grid below\n- We will also train a word embedding model to demonstrate how to implement that feature\nengineering technique in tidymodels\n\n-----\n\nWe are applying these feature engineering steps blindly.  YOU should not.\n\\\n\\\nYou will want to explore the impact of your feature engineering either\n\n  - During EDA with unnest_tokens()\n  - After making your recipe by using it to make a feature matrix\n  - Likely some of both\n  \n-----\n\nLets start fresh with our training data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn <- read_csv(here::here(path_data, \"imdb_trn.csv\"),\n                     show_col_types = FALSE) |> \n  rowid_to_column(var = \"doc_num\") |> \n  mutate(sentiment = factor(sentiment, levels = c(\"neg\", \"pos\"))) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 25,000\nColumns: 3\n$ doc_num   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ sentiment <fct> neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, …\n$ text      <chr> \"Story of a man who has unnatural feelings for a pig. Starts…\n```\n\n\n:::\n:::\n\n\n-----\n\nAnd do our (very minimal) cleaning\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn <- data_trn |> \n  mutate(text = str_replace_all(text, \"<br /><br />\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n```\n:::\n\n\n-----\n\nWe will select among model configurations using a validation split resampled accuracy to ease computational costs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456)\nsplits <- data_trn |> \n  validation_split(strata = sentiment)\n```\n:::\n\n\n-----\n\nWe will use a simple union of stop words for some configurations:\n\n- It **could** help to edit the global list (or use a smaller list like snowball only)\n- It **could** help to have subject specific stop words too AND they might need to be different for words vs. bigrams\n\n::: {.cell}\n\n```{.r .cell-code}\nall_stops <- union(pull(get_stopwords(source = \"smart\"), word), pull(get_stopwords(source = \"snowball\"), word))\n```\n:::\n\n\n-----\n\nAll of our model configurations will be tuned on `penalty`, `mixture` and `max_tokens`\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_tokens <- grid_regular(penalty(range = c(-7, 3)), \n                            mixture(), \n                            max_tokens(range = c(5000, 10000)), \n                            levels = c(20, 6, 2))\n```\n:::\n\n\n### Words (unigrams)\n\nWe will start by fitting a BoW model configuration for word tokens\n\\\n\\\nRecipe for Word Tokens.  NOTE:\n\n- `token = \"words\"`  (default)\n- `max_tokens = tune()`  - We are now set to tune recipes!!!\n  `options = list(stopwords = all_stops)` - Passing in options to `tokenizers::tokenize_words()`\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_word <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\") |>    \n  step_tokenfilter(text, max_tokens = tune::tune()) |>\n  step_tfidf(text)  |> \n  step_normalize(all_predictors())\n```\n:::\n\n\n-----\n\nTuning hyperparameters for Word Tokens\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_word <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), \n                 mixture = tune::tune()) |> \n      set_engine(\"glmnet\") |> \n      tune_grid(preprocessor = rec_word, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_word\")\n```\n:::\n\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(fits_word)\n```\n\n::: {.cell-output-display}\n![](012_nlp_files/figure-html/unnamed-chunk-67-1.png){width=672}\n:::\n:::\n\n\n-----\n\nDisplay performance of best configuration for Word Tokens.  \n\\\n\\\nWow, pretty good!\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(fits_word)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n  penalty mixture max_tokens .metric  .estimator  mean     n std_err\n    <dbl>   <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl>\n1 0.207       0         5000 accuracy binary     0.896     1      NA\n2 0.0183      0.2      10000 accuracy binary     0.896     1      NA\n3 0.00546     0.4      10000 accuracy binary     0.895     1      NA\n4 0.00546     0.6      10000 accuracy binary     0.895     1      NA\n5 0.207       0        10000 accuracy binary     0.893     1      NA\n  .config               \n  <chr>                 \n1 Preprocessor1_Model013\n2 Preprocessor2_Model031\n3 Preprocessor2_Model050\n4 Preprocessor2_Model070\n5 Preprocessor2_Model013\n```\n\n\n:::\n:::\n\n\n-----\n\n### Words (unigrams) Excluding Stop Words\n\nLets try a configuration that removes stop words\n\\\n\\\nRecipe for Word Tokens excluding Stop Words.  NOTE:\n\n- `options = list(stopwords = all_stops)` - Passing in options to `tokenizers::tokenize_words()`\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_word_nsw <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\",   # included for clarity\n                options = list(stopwords = all_stops)) |>\n  step_tokenfilter(text, max_tokens = tune::tune()) |>\n  step_tfidf(text)  |> \n  step_normalize(all_predictors())\n```\n:::\n\n\n-----\n\nTuning hyperparameters for Word Tokens excluding Stop Words\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_word_nsw <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), \n                 mixture = tune::tune()) |> \n      set_engine(\"glmnet\") |> \n      tune_grid(preprocessor = rec_word_nsw, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_word_nws\")\n```\n:::\n\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(fits_word_nsw)\n```\n\n::: {.cell-output-display}\n![](012_nlp_files/figure-html/unnamed-chunk-71-1.png){width=672}\n:::\n:::\n\n\n-----\n\nDisplay performance of best configuration for Word Tokens excluding Stop Words.  \n\n- Looks like our mindless use of a large list of global stop words didn't help.   \n- We might consider the more focused snowball list\n- We might consider subject specific stop words\n- For this demonstration, we will just move forward without stop words\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(fits_word_nsw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n  penalty mixture max_tokens .metric  .estimator  mean     n std_err\n    <dbl>   <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl>\n1 0.00546     0.6      10000 accuracy binary     0.880     1      NA\n2 0.0183      0.2      10000 accuracy binary     0.879     1      NA\n3 0.00546     0.4       5000 accuracy binary     0.879     1      NA\n4 0.0183      0.2       5000 accuracy binary     0.878     1      NA\n5 0.00162     1         5000 accuracy binary     0.877     1      NA\n  .config               \n  <chr>                 \n1 Preprocessor2_Model070\n2 Preprocessor2_Model031\n3 Preprocessor1_Model050\n4 Preprocessor1_Model031\n5 Preprocessor1_Model109\n```\n\n\n:::\n:::\n\n\n\n-----\n\n### Stemmed Words (unigrams)\n\nNow we will try using stemmed words\n\\\n\\\nRecipe for Stemmed Word Tokens.  NOTE: `step_stem()`\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_stemmed_word <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\") |>    # included for clarity\n  step_stem(text) |> \n  step_tokenfilter(text, max_tokens = tune()) |>\n  step_tfidf(text)  |> \n  step_normalize(all_predictors())\n```\n:::\n\n\n-----\n\nTuning hyperparameters for Stemmed Word Tokens\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_stemmed_word <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), mixture = tune()) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_stemmed_word, \n              resamples = splits, \n              grid = grid_tokens, \n              metrics = metric_set(accuracy))\n},\n  rerun = rerun_setting, \n  dir = \"cache/012/\",\n  file = \"fits_stemmed_word\")\n```\n:::\n\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(fits_stemmed_word)\n```\n\n::: {.cell-output-display}\n![](012_nlp_files/figure-html/unnamed-chunk-75-1.png){width=672}\n:::\n:::\n\n\n-----\n\nDisplay performance of best configuration for Stemmed Word Tokens\n\\\n\\\nNot much change\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(fits_stemmed_word)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n  penalty mixture max_tokens .metric  .estimator  mean     n std_err\n    <dbl>   <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl>\n1 0.207       0        10000 accuracy binary     0.896     1      NA\n2 0.695       0        10000 accuracy binary     0.896     1      NA\n3 0.0183      0.2      10000 accuracy binary     0.896     1      NA\n4 0.00546     0.4      10000 accuracy binary     0.895     1      NA\n5 0.00546     0.6      10000 accuracy binary     0.894     1      NA\n  .config               \n  <chr>                 \n1 Preprocessor2_Model013\n2 Preprocessor2_Model014\n3 Preprocessor2_Model031\n4 Preprocessor2_Model050\n5 Preprocessor2_Model070\n```\n\n\n:::\n:::\n\n\n-----\n\n### ngrams (unigrams and bigrams)\n\nNow we try both unigrams and bigrams\n\\\n\\\nRecipe for unigrams and bigrams. NOTES:\n\n  - `token = \"ngrams\"`\n  - `options = list(n = 2, n_min = 1)`  - includes uni (1) and bi(2) grams\n  - not stemmed (stemming doesn't work by default for bigrams)\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_ngrams <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"ngrams\",\n                options = list(n = 2, n_min = 1)) |>\n  step_tokenfilter(text, max_tokens =  tune::tune()) |>\n  step_tfidf(text) |> \n  step_normalize(all_predictors())\n```\n:::\n\n\n-----\n\nTuning hyperparameters for ngrams\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_ngrams <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), mixture = tune::tune()) |> \n      set_engine(\"glmnet\") |> \n      tune_grid(preprocessor = rec_ngrams, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(yardstick::accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_ngrams\")\n```\n:::\n\n\n------\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(fits_ngrams)\n```\n\n::: {.cell-output-display}\n![](012_nlp_files/figure-html/unnamed-chunk-79-1.png){width=672}\n:::\n:::\n\n\n-----\n\nDisplay performance of best configuration\n\\\n\\\nOur best model yet!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(fits_ngrams)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n  penalty mixture max_tokens .metric  .estimator  mean     n std_err\n    <dbl>   <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl>\n1 0.207       0        10000 accuracy binary     0.901     1      NA\n2 0.695       0        10000 accuracy binary     0.900     1      NA\n3 0.0183      0.2      10000 accuracy binary     0.900     1      NA\n4 0.00546     0.4      10000 accuracy binary     0.898     1      NA\n5 0.00546     0.6      10000 accuracy binary     0.897     1      NA\n  .config               \n  <chr>                 \n1 Preprocessor2_Model013\n2 Preprocessor2_Model014\n3 Preprocessor2_Model031\n4 Preprocessor2_Model050\n5 Preprocessor2_Model070\n```\n\n\n:::\n:::\n\n\n\n-----\n\n### Word Embeddings\n\nBoW is an introductory approach for feature engineering.  \n\\\n\\\nAs you have read, word embeddings are a common alternative that addresses some of the limitations of BoW.  Word embeddings are also well-support in the `tidyrecipes` package.\n\\\n\\\nLet's switch gears away from document term matrices and BoW to word embeddings\n\n-----\n\nYou can find pre-trained word embeddings on the web\n\n- [GloVe](https://nlp.stanford.edu/projects/glove/)\n- [Word2Vec](https://code.google.com/archive/p/word2vec/)\n- [Fasttek](https://fasttext.cc/docs/en/english-vectors.html)\n\nBelow, we download and open pre-trained GloVe embeddings\n\n- I chose a smaller set of embeddings to ease computational cost\n  - Wikipedia 2014 + Gigaword 5\n  - 6B tokens, 400K vocab, uncased, 50d\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp <- tempfile()\noptions(timeout = max(300, getOption(\"timeout\")))  # need more time to download big file\ndownload.file(\"https://nlp.stanford.edu/data/glove.6B.zip\", temp)\nunzip(temp, files = \"glove.6B.50d.txt\")\nglove_embeddings <- read_delim(here::here(\"glove.6B.50d.txt\"),\n                               delim = \" \",\n                               col_names = FALSE) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 8 Columns: 51\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr  (1): X1\ndbl (50): X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\nRecipe for GloVe embedding. NOTES:\n\n- `token = \"words\"`\n- No need to filter tokens\n- New step is `step_word_embeddings(text, embeddings = glove_embeddings)`\n\n::: {.cell}\n\n```{.r .cell-code}\n# rec_glove <-  recipe(sentiment ~ text, data = data_trn) |>\n#   step_tokenize(text, \n#                 engine = \"tokenizers\", token = \"words\",   # included for clarity\n#                 options = list(stopwords = all_stops)) |>\n#   step_word_embeddings(text, embeddings = glove_embeddings)\n```\n:::\n\n\n-----\n\nHyperparameter grid for GloVe embedding (no need for `max_tokens`)\n\n::: {.cell}\n\n```{.r .cell-code}\n# grid_glove <- grid_regular(penalty(range = c(-7, 3)), \n#                             dials::mixture(), \n#                             levels = c(20, 6))\n```\n:::\n\n\n-----\n\nTuning hyperparameters for GloVe embedding\n\n::: {.cell}\n\n```{.r .cell-code}\n# fits_glove <-\n#   logistic_reg(penalty = tune(), \n#              mixture = tune()) |> \n#   set_engine(\"glmnet\") |> \n#   tune_grid(preprocessor = rec_glove, \n#             resamples = splits, \n#             grid = grid_glove, \n#             metrics = metric_set(accuracy))\n```\n:::\n\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n::: {.cell}\n\n```{.r .cell-code}\n# autoplot(fits_glove)\n```\n:::\n\n\n-----\n\nDisplay performance of best configuration for GloVe embedding\n\n::: {.cell}\n\n```{.r .cell-code}\n# show_best(fits_glove)\n```\n:::\n\n\n-----\n\n### Best Configuration\n\n- Fit best model configuration to training set\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_final <-\n  recipe(sentiment ~ text, data = data_trn) |>\n    step_tokenize(text, \n                  engine = \"tokenizers\", token = \"ngrams\",\n                  options = list(n = 2, n_min = 1)) |>\n    step_tokenfilter(text, max_tokens = select_best(fits_ngrams)$max_tokens) |>\n    step_tfidf(text) |> \n    step_normalize(all_predictors()) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_final_prep <- rec_final |>\n  prep(data_trn)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in asMethod(object): sparse->dense coercion: allocating vector of size\n1.9 GiB\n```\n\n\n:::\n\n```{.r .cell-code}\nfeat_trn <- rec_final_prep |> \n  bake(NULL) \n```\n:::\n\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_final <-\n  logistic_reg(penalty = select_best(fits_ngrams)$penalty, \n             mixture = select_best(fits_ngrams)$mixture) |> \n  set_engine(\"glmnet\") |> \n  fit(sentiment ~ ., data = feat_trn)\n```\n:::\n\n\n-----\n\n-----\n\n- Open and clean test to make features\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_test <- read_csv(here::here(path_data, \"imdb_test.csv\"),\n                      show_col_types = FALSE) |> \n  rowid_to_column(var = \"doc_num\") |> \n  mutate(sentiment = factor(sentiment, levels = c(\"neg\", \"pos\"))) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 25,000\nColumns: 3\n$ doc_num   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ sentiment <fct> neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, …\n$ text      <chr> \"Once again Mr. Costner has dragged out a movie for far long…\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_test <- data_test |> \n  mutate(text = str_replace_all(text, \"<br /><br />\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n\nfeat_test <- rec_final_prep |> \n  bake(data_test)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in asMethod(object): sparse->dense coercion: allocating vector of size\n1.9 GiB\n```\n\n\n:::\n:::\n\n\n-----\n\n- Predict into test\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_test$sentiment, predict(fit_final, feat_test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.89268\n```\n\n\n:::\n:::\n\n\n- Confusion matrix\n\n::: {.cell}\n\n```{.r .cell-code}\ncm <- tibble(truth = feat_test$sentiment,\n             estimate = predict(fit_final, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\nautoplot(cm, type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](012_nlp_files/figure-html/unnamed-chunk-93-1.png){width=672}\n:::\n:::\n\n\n-----\n\nAnd lets end by calculating Permutation feature importance scores in test set using DALEX\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DALEX, exclude= \"explain\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWelcome to DALEX (version: 2.4.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(DALEXtra)\n```\n:::\n\n\n-----\n\nWe are going to sample only a subset of the test set to keep the computational costs lower for this example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nfeat_subtest <- feat_test |> \n  slice_sample(prop = .05) # 5% of data \n```\n:::\n\n\n-----\n\nNow we can get a df for the features (without the outcome) and a separate vector for the outcome.  \n\n::: {.cell}\n\n```{.r .cell-code}\nx <- feat_subtest |> select(-sentiment)\n```\n:::\n\n\\\n\\\nFor outcome, we need to convert to 0/1 (if classification), and then pull the vector out of the dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- feat_subtest |> \n  mutate(sentiment = if_else(sentiment == \"pos\", 1, 0)) |> \n  pull(sentiment)\n```\n:::\n\n\n-----\n\nWe also need a specific predictor function that will work with the DALEX package\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_wrapper <- function(model, newdata) {\n  predict(model, newdata, type = \"prob\") |> \n    pull(.pred_pos)\n}\n```\n:::\n\n\n-----\n\nWe will also need an `explainer` object based on our model and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexplain_test <- explain_tidymodels(fit_final, # our model object \n                                   data = x, # df with features without outcome\n                                   y = y, # outcome vector\n                                   # our custom predictor function\n                                   predict_function = predict_wrapper)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPreparation of a new explainer is initiated\n  -> model label       :  model_fit  (  default  )\n  -> data              :  1250  rows  10000  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  1250  values \n  -> predict function  :  predict_function \n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package parsnip , ver. 1.1.0.9004 , task classification (  default  ) \n  -> predicted values  :  numerical, min =  0.0003611459 , mean =  0.5143743 , max =  0.9999001  \n  -> residual function :  residual_function \n  -> residuals         :  numerical, min =  0 , mean =  0 , max =  0  \n  A new explainer has been created!  \n```\n\n\n:::\n:::\n\n\n-----\n\nFinally, we need to define a custom function for our performance metric as well\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_wrapper <- function(observed, predicted) {\n  observed <- fct(if_else(observed == 1, \"pos\", \"neg\"),\n                  levels = c(\"pos\", \"neg\"))\n  predicted <- fct(if_else(predicted > .5, \"pos\", \"neg\"), \n                   levels  = c(\"pos\", \"neg\"))\n  accuracy_vec(observed, predicted)\n}\n```\n:::\n\n\\\n\\\nWe are now ready to calculate feature importance metrics\n\n-----\n\nOnly doing 1 permutation for each feature to keep computational costs lower for this demonstration.  In real, life do more!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456)\nimp_permute <- cache_rds(\n  expr = {model_parts(explain_test, \n                      type = \"raw\", \n                      loss_function = accuracy_wrapper,\n                      B = 1)\n  },\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"imp_permute\"\n)\n```\n:::\n\n\n-----\n\nPlot top 30 in an informative display\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_permute |> \n  filter(variable != \"_full_model_\",\n         variable != \"_baseline_\") |> \n  mutate(variable = fct_reorder(variable, dropout_loss)) |> \n  slice_head(n = 30) |> \n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   variable permutation dropout_loss     label\n1              tfidf_text_2           0        0.902 model_fit\n2        tfidf_text_a great           0        0.902 model_fit\n3         tfidf_text_at all           0        0.902 model_fit\n4            tfidf_text_and           0        0.903 model_fit\n5          tfidf_text_below           0        0.903 model_fit\n6         tfidf_text_decent           0        0.903 model_fit\n7     tfidf_text_especially           0        0.903 model_fit\n8           tfidf_text_true           0        0.903 model_fit\n9      tfidf_text_very good           0        0.903 model_fit\n10        tfidf_text_actors           0        0.904 model_fit\n11           tfidf_text_age           0        0.904 model_fit\n12          tfidf_text_aged           0        0.904 model_fit\n13        tfidf_text_always           0        0.904 model_fit\n14    tfidf_text_an amazing           0        0.904 model_fit\n15     tfidf_text_and while           0        0.904 model_fit\n16       tfidf_text_appears           0        0.904 model_fit\n17         tfidf_text_as he           0        0.904 model_fit\n18         tfidf_text_avoid           0        0.904 model_fit\n19        tfidf_text_beauty           0        0.904 model_fit\n20           tfidf_text_bit           0        0.904 model_fit\n21         tfidf_text_bored           0        0.904 model_fit\n22        tfidf_text_boring           0        0.904 model_fit\n23    tfidf_text_boring and           0        0.904 model_fit\n24       tfidf_text_did not           0        0.904 model_fit\n25 tfidf_text_disappointing           0        0.904 model_fit\n26          tfidf_text_doll           0        0.904 model_fit\n27          tfidf_text_door           0        0.904 model_fit\n28           tfidf_text_dvd           0        0.904 model_fit\n29        tfidf_text_end up           0        0.904 model_fit\n30          tfidf_text_good           0        0.904 model_fit\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_permute |> \n  filter(variable != \"_full_model_\",\n         variable != \"_baseline_\") |> \n  mutate(variable = fct_reorder(variable, dropout_loss, \n                                .desc = TRUE)) |> \n  slice_tail(n = 30) |> \n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  variable permutation dropout_loss     label\n1      tfidf_text_anywhere           0        0.908 model_fit\n2         tfidf_text_as if           0        0.908 model_fit\n3        tfidf_text_as the           0        0.908 model_fit\n4          tfidf_text_best           0        0.908 model_fit\n5       tfidf_text_blatant           0        0.908 model_fit\n6  tfidf_text_disappointed           0        0.908 model_fit\n7       tfidf_text_episode           0        0.908 model_fit\n8    tfidf_text_first half           0        0.908 model_fit\n9         tfidf_text_giant           0        0.908 model_fit\n10       tfidf_text_got to           0        0.908 model_fit\n11     tfidf_text_help the           0        0.908 model_fit\n12     tfidf_text_is still           0        0.908 model_fit\n13      tfidf_text_it when           0        0.908 model_fit\n14         tfidf_text_jack           0        0.908 model_fit\n15         tfidf_text_main           0        0.908 model_fit\n16         tfidf_text_meat           0        0.908 model_fit\n17           tfidf_text_ok           0        0.908 model_fit\n18         tfidf_text_once           0        0.908 model_fit\n19       tfidf_text_one of           0        0.908 model_fit\n20        tfidf_text_other           0        0.908 model_fit\n21      tfidf_text_perfect           0        0.908 model_fit\n22         tfidf_text_ship           0        0.908 model_fit\n23        tfidf_text_shoot           0        0.908 model_fit\n24      tfidf_text_sisters           0        0.908 model_fit\n25 tfidf_text_something to           0        0.908 model_fit\n26    tfidf_text_the lives           0        0.908 model_fit\n27        tfidf_text_tries           0        0.908 model_fit\n28     tfidf_text_tries to           0        0.908 model_fit\n29    tfidf_text_very well           0        0.908 model_fit\n30       tfidf_text_effort           0        0.909 model_fit\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# full_model <- imp_permute |>  \n#    filter(variable == \"_full_model_\")\n  \n# imp_permute |> \n#   filter(variable != \"_full_model_\",\n#          variable != \"_baseline_\") |> \n#   mutate(variable = fct_reorder(variable, dropout_loss)) |> \n#   arrange(desc(dropout_loss) |>\n#   slice(n = 30) |> \n#   ggplot(aes(dropout_loss, variable)) +\n#     geom_vline(data = full_model, aes(xintercept = dropout_loss),\n#                linewidth = 1.4, lty = 2, alpha = 0.7) +\n#    geom_boxplot(fill = \"#91CBD765\", alpha = 0.4) +\n#    theme(legend.position = \"none\") +\n#    labs(x = \"accuracy\", y = NULL)\n```\n:::\n\n\n\n\n# NLP Discussion\n\n## Announcements\n\n- Last application assignment done!\n- Remaining units\n  - Bring it together for applications\n  - Ethics and Algorithmic Fairness\n  - Reading and discussion only for both units.  Discussion requires you!\n  \n- Two weeks left!\n  - next tuesday (lab on NLP)\n  - next thursday (discussion on applications)\n  - following tuesday (discussion on ethics/fairness)\n  - following thursday (concepts final review; 50 minutes only)\n\n- Early start to final application assignment (assigned next thursday, April 25th) and due Wednesday, May 8th at 8pm\n- Concepts exam at lab time during finals period (Tuesday, May 7th, 11-12:15 in this room)\n\n-----\n\n## Single nominal variable\n\n- Our algorithms need features coded with numbers\n  - How did we generally do this?\n  - What about algorithms like random forest?\n  \n-----\n\n##  Single nominal variable Example\n\n- Current emotional state\n  - angry, afraid, sad, excited, happy, calm\n\n- How represented with one-hot?\n\n-----\n\n## What about document of words (e.g., sentence rather than single word)\n\n- I watched a scary movie last night and couldn't get to sleep because I was so afraid\n- I went out with my friends to a bar and slept poorly because I drank too much\n\n\\\n\\\n\n- How represented with bag of words?\n- binary, count, tf-idf\n\n\\\n\\\n\n- What are the problems with these approaches\n  - relationships between features or similarity between full vector across observations \n  - context/order\n  - dimensionality\n  - sparsity\n\n-----\n\n## N-grams vs. simple (1-gram) BOW\n\n- How different?\n\n\\\n\\\n\n- some context/order\n- but still no relationship/meaning or similarity\n- even higher dimesions\n\n-----\n\n## Linguistic Inquiry and Word Count (LIWC)\n\n- Tausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and computerized text analysis methods. Journal of Language and Social Psychology, 29(1), 24–54. https://doi.org/\n\n\\\n\\\n\n- Meaningful features? (based on domain expertise)\n- Lower dimensional\n- Very limited breadth\n\n\\\n\\\n- Can add domain specific dictionaries\n\n-----\n\n## Word Embeddings\n\n- Encodes word meaning\n- Words that are similar get similar vectors\n- Meaning derived from context in various text corpora\n- Lower dimensional\n- Less sparse\n\n-----\n\n## Examples\n\n\n- Affect Words\n\n![](figs/nlp_1.PNG){height=5in}\n\n-----\n\n![](figs/nlp_2.PNG){height=5in}\n\n-----\n \n![](figs/nlp_3.PNG){height=5in}\n\n\n\n- Two more general\n\n- word2vec (google)\n  - CBOW\n  - skipgram\n\n![](figs/nlp_4.PNG){height=5in}\n\n-----\n\n![](figs/nlp_5.PNG){height=5in}\n\n-----\n\n![](figs/nlp_6.PNG){height=5in}\n\n\n-----\n\n## fasttext (facebook ai team)\n\n- n-grams (e.g., 3-6 character representations)\n- word vector is sum of its ngrams\n- can handle low frequecynor even novel wordsp\n\n## Other Methods\n\n- Other approaches\n  - glove\n  - elmo\n  - BERT\n:w\n",
    "supporting": [
      "012_nlp_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}