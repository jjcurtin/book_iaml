{
  "hash": "71ba5c28f708191f01bca6a75c77e82b",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n# Advanced Models: Neural Networks\n\n## Overview of Unit\n\n### Learning Objectives\n\n- What are neural networks\n- Types of neural networks\n- Neural network architecture\n  - layers and units\n  - weights and biases\n  - activation functions\n  - cost functions\n  - optimization\n    - epochs\n    - batches\n    - learning rate\n- How to fit 3 layer MLPs in tidymodels using Keras\n\n-----\n\n### Readings\n\n- [Neural Networks and Deep Learning, Chapter 1: Using neural networks to recognize handwritten digits](http://neuralnetworksanddeeplearning.com/chap1.html)\n\nPost questions to the readings channel in Slack\n\n### Lecture Videos\n\n- [Lecture 1: But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=1&t) ~ 19 mins\n- [Lecture 2: Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2&t) ~ 21 mins\n- [Lecture 3: What is backpropagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3&t) ~ 13 mins\n- [Optional Lecture 4: Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4&t) ~ 10 mins\n- [Lecture 5: Introduction and the MNIST dataset](https://mediaspace.wisc.edu/media/iaml+10-5/1_uqw3f0o9) ~ 14 mins\n- [Lecture 6: Fitting neural networks in tidymodels with Keras](https://mediaspace.wisc.edu/media/iaml+unit+10-6/1_sv7eicou) ~ 18 mins\n- [Lecture 7: Addressing overfitting - Intro and L2](https://mediaspace.wisc.edu/media/iaml+unit+10-7/1_ulfnye8s) ~ 5 mins\n- [Lecture 8: Addressing overfitting - Dropout](https://mediaspace.wisc.edu/media/iaml+unit+10-8/1_4k4cf1aj) ~ 4 mins\n- [Lecture 9: Addressing overfitting - Early stopping](https://mediaspace.wisc.edu/media/iaml+unit+10-9/1_5dlsj34t) ~ 8 mins\n- [Lecture 10: Selecting model configurations and final remarks](https://mediaspace.wisc.edu/media/iaml+unit+10-10/1_i448a6ta) ~ 8 mins\n\nPost questions to the video-lectures channel in Slack\n\n-----\n\n### Coding Assignment\n\n- wine quality datasets:  [training](application_assignments/unit_10/wine_quality_trn.csv); [test](application_assignments/unit_10/wine_quality_test.csv)\n- [qmd shell](https://raw.githubusercontent.com/jjcurtin/book_iaml/main/application_assignments/unit_10/hw_unit_10_neural_nets.qmd)\n- [solution]()\n\nPost questions to application-assignments Slack channel\n\nSubmit the application assignment [here](https://canvas.wisc.edu/courses/395546/assignments/2187685) and complete the [unit quiz](https://canvas.wisc.edu/courses/395546/quizzes/514045) by 8 pm on Wednesday, April 3rd\n\n-----\n\n## Introduction to Nerual Networks with Keras in R\n\nWe will be using the `keras` engine to fit our neural networks in R.\n\nThe `keras` [package](https://cran.r-project.org/web/packages/keras/index.html) provides an [R Interface](https://keras.rstudio.com/) to the [Keras API in Python](https://keras.io/).\n\n-----\n\nFrom the website:\n\n- Keras is a high-level neural networks API developed with a focus on **enabling fast experimentation**. Being able to go from idea to result with the least possible delay is key to doing good research. \n\n- Keras has the following key features:\n  - Allows the same code to run on CPU or on GPU, seamlessly.\n  - **User-friendly API** - which makes it easy to quickly prototype deep learning models.\n  - Built-in support for basic multi-layer perceptrons, convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\n  - Supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.\n\n-----\n\nKeras is actually a wrapper around an even more extensive open source platform, [TensorFlow](https://www.tensorflow.org/), which has also been ported to the [R environment](https://tensorflow.rstudio.com/)\n\n- TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\n\n- TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research\n\n-----\n\nIf you are serious about focusing primarily or exclusively on neural networks, you will probably work directly within Keras in R or Python. However, tidymodels gives us access to 3 layer (single hidden layer) MLP neural networks through the `keras` engine.  This allows us to fit simple (but still powerful) neural networks using all the tools (and code/syntax) that you already know.  Yay! \n\\\n\\\nIf you plan to use Keras directly in R, you might start with this [book](https://www.manning.com/books/deep-learning-with-r?utm_source=google&utm_medium=shopping&utm_campaign=shopping1&gclid=Cj0KCQjwo-aCBhC-ARIsAAkNQivtssXhY1RdFiYXSwu8eRWRXLUqveHkzyhdJU48W-jTU7O-qeKmONsaAmePEALw_wcB).  I've actually found it useful even in thinking about how to interface with Keras through tidymodels.\n\n-----\n\nGetting tidymodels configured to use the `keras` engine can take a  little bit of upfront effort.\n\\\n\\\nWe provide [an appendix](https://jjcurtin.github.io/book_iaml/app_keras.html) to guide you through this process\n\\\n\\\nIf you havent already set this up, please do so immediately so that you can reach out to us for support if you need it\n\\\n\\\nOnce you have completed this one-time installation, you can now use the `keras` engine through tidymodels like any other engine.  No need to do anything different from your normal tidymodeling workflow.\n\n-----\n\nYou should also know that Keras is configured to use GPUs rather than CPU (GPUs allow for highly parallel fitting of neural networks).  \n\n- However, it works fine with just a CPU as well.  \n- It will generate some errors to tell you that you aren't set up with a GPU (and then it will tell you to ignore those error messages). \n- This is an instance where you can ignore the messages!\n\n-----\n\n## Setting up our Environment \n\nNow lets start fresh\n\n- We load our normal environment including source files, parallel  processing and cache support if we plan to use it (code not displayed)\n- keras will work with R without loading it or other packages (beyond what we always load).  However, there will be some function conflicts.\n  - So we will load keras and exclude the conflict\n  - We also need to load magrittr and exclude two of its conflicting functions\n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras, exclude = \"get_weights\")\nlibrary(magrittr, exclude = c(\"set_names\", \"extract\"))\n```\n:::\n\n\n-----\n\n## The MNIST dataset\n\nThe [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training and testing in the field of machine learning.\n\\\n\\\nIt consists of two sets: \n\n  - There are 60,000 images from 250 people in train\n  - There are 10,000 images from a different 250 people in test (from different people than in train)\n\n-----\n\nEach observation in the datasets represent a single image and its label\n\n- Each image is a 28 X 28 grid of pixels = 784 predictors (x1 - x784)\n- Each label is the actual value (0-9; y).  We will treat it as categorical because we are trying to identify each number \"category\", predicting a label of \"4\" when the image is a \"5\" is just as bad as predicting \"9\"\n\n-----\n\nLet's start by reading train and test sets\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn <- read_csv(here::here(path_data, \"mnist_train.csv.gz\"),\n                     col_types = cols()) |> \n  mutate(y = factor(y, levels = 0:9, labels = 0:9))\ndata_trn |> dim()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 60000   785\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_test <- read_csv(here::here(path_data, \"mnist_test.csv\"),\n                      col_types = cols()) |> \n    mutate(y = factor(y, levels = 0:9, labels = 0:9))\ndata_test |> dim()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10000   785\n```\n\n\n:::\n:::\n\n\n-----\n\nHere is some very basic info on the outcome distribution \n\n- in train\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> tab(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 3\n   y         n   prop\n   <fct> <int>  <dbl>\n 1 0      5923 0.0987\n 2 1      6742 0.112 \n 3 2      5958 0.0993\n 4 3      6131 0.102 \n 5 4      5842 0.0974\n 6 5      5421 0.0904\n 7 6      5918 0.0986\n 8 7      6265 0.104 \n 9 8      5851 0.0975\n10 9      5949 0.0992\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> plot_bar(\"y\")\n```\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/unnamed-chunk-5-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n- in test\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_test|> tab(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 3\n   y         n   prop\n   <fct> <int>  <dbl>\n 1 0       980 0.098 \n 2 1      1135 0.114 \n 3 2      1032 0.103 \n 4 3      1010 0.101 \n 5 4       982 0.0982\n 6 5       892 0.0892\n 7 6       958 0.0958\n 8 7      1028 0.103 \n 9 8       974 0.0974\n10 9      1009 0.101 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_test |> plot_bar(\"y\")\n```\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/unnamed-chunk-7-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\nLet's look at some of the images.  We will need a function to display these images.  We will use `as.cimg()` from the `imager` package\n\n::: {.cell}\n\n```{.r .cell-code}\ndisplay_image <- function(data){\n  message(\"Displaying: \", data$y)\n  \n  data |> \n    select(-y) |> \n    unlist(use.names = FALSE) |> \n    imager::as.cimg(x = 28, y = 28) |> \n    plot(axes = FALSE)\n}\n```\n:::\n\n\n-----\n\nObservations 1, 3, 10, and 100 in training set\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  slice(1) |> \n  display_image()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDisplaying: 5\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  slice(3) |> \n  display_image()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDisplaying: 4\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  slice(10) |> \n  display_image()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDisplaying: 4\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  slice(100) |> \n  display_image()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDisplaying: 1\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n-----\n\nAnd here is the first observation in test set\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_test |> \n  slice(1) |> \n  display_image()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDisplaying: 7\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n-----\n\nLet's understand the individual predictors a bit more\n\n- Each predictor is a pixel in the 28 X 28 grid for the image\n- Pixel intensity is coded for intensity in the range from 0 (black)\nto 255 (white)\n- First 28 variables are the top row of 28 pixels\n- Next 28 variables are the second row of 28 pixels\n- There are 28 rows of 28 predictors total (784 predictors)\n\n-----\n\n- Lets understand this by changing values for  individual predictors\n- Here is the third image again\n\n[What will happen to the image if I change the value of predictor `x25` to 255]{.red}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  slice(3) |> \n  display_image()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDisplaying: 4\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n-----\n\n- Change the `x25` to 255\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  slice(3) |>\n  mutate(x25 = 255) |> \n  display_image()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDisplaying: 4\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\\\n\\\n[What will happen to the image if I change the value of predictor `x29` to 255]{.red}\n\n-----\n\n- Change the `x29` to 255\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  slice(3) |>\n  mutate(x29 = 255) |> \n  display_image()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDisplaying: 4\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/u11-mnist-12-1.png){width=672}\n:::\n:::\n\n\n[What will happen to the image if I change the value of predictor `x784` to 255]{.red}\n\n-----\n\n- Change the `x784` to 255\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  slice(3) |>\n  mutate(x784 = 255) |> \n  display_image()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDisplaying: 4\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](010_neural_networks_files/figure-html/u11-mnist-13-1.png){width=672}\n:::\n:::\n\n\n-----\n\n## Fitting Neural Networks\n\nLet's train some models to understand some basics about neural networks and the use of\nKeras within tidymodels\n\n- We will fit some configurations in the full training set and evaluate their performance in test\n\n- We are NOT using test to select among configurations (it wouldn't be a true test set then)\nbut only for instructional purposes.\n\n- We will start with an absolute minimal recipe and mostly defaults for the statistical\nalgorithm\n\n- We will build up to more complex (and better) configurations\n\n- We will end with a demonstration of the use of the single validation set approach to select among model configurations\n\n-----\n\nLet's start with a minimal recipe\n\n- 10 level categorical outcome as factor\n- Will be used to establish 10 output neurons\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_min <- \n  recipe(y ~ ., data = data_trn)\n```\n:::\n\n\n-----\n\nHere are feature matrices for train and test using this recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_min_prep <- rec_min |> \n  prep(data_trn)\n\nfeat_trn <- rec_min_prep |> \n  bake(NULL)\n\nfeat_test <-rec_min_prep |> \n  bake(data_test)\n```\n:::\n\n\n-----\n\nAnd let's use a mostly out of the box (defaults) 3 layer (1 hidden layer) using Keras engine\n\\\n\\\nDefaults:\n\n- hidden units = 5\n- penalty = 0\n- dropout = 0\n- activation = \"softmax\" for hidden units layer\n- epochs = 20\n- seeds = sample.int(10^5, size = 3)\n\n-----\n\nThe default activation for the hidden units when using Keras through tidymodels is `softmax` not sigmoid as per the basic models discussed in the book and lectures.\n\n- The activation for the output layer will always be `softmax` for classification problems when using Keras through tidymodels\n  - This is likely a good choice\n  - It provides scores that function like probabilities for each categorical response\n- The activation for the output layer will always be 'linear' for regression problems.  \n  - Also a generally good choice\n- The hidden units can have a variety of different activation functions\n  - `linear`, `softmax`, `relu`, and `elu` through tidymodels\n  - Additional activation functions (and many other \"dials\") are available in Keras directly\n\n-----\n\nWe will adjust `seeds` from the start\n\\\n\\\nThere are a number of points in the fitting process where random numbers needed by Keras\n\n- initializing weights for hidden and output layers\n- selecting units for `dropout`\n- selecting batches within epochs\n\n`tidymodels` lets us provide three seeds to make the first two bullet points more reproducible.  \n\\\n\\\nThere seems to still be some randomness across runs due to batch selection (and possibly other opaque steps)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234567)\nfit_seeds <- sample.int(10^5, size = 3)  # c(87591, 536, 27860)\n```\n:::\n\n\n-----\n\nWe will also set verbose = 0 for now\n\n- This turns off messages and plots about epoch level performance\n- At this point, verbose would only report performance in the training data, which isn't that informative\n- We will turn it on later when we learn how to get performance in a validation set\n- Nonetheless, you might still turn it on if you just want feedback on how long it will take for the fit to complete.\n\n-----\n\nLet's fit this first model configuration in training set\n\n- `verbose = 0`\n- `seeds = fit_seeds`\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_1 <-\n    mlp() |>\n    set_mode(\"classification\") |> \n    set_engine(\"keras\", \n               verbose = 0, \n               seeds = fit_seeds) |>\n    fit(y ~ ., data = feat_trn)\n```\n:::\n\n\nNOTE: The first model fit with Keras in each new session will generate those warnings/errors about GPU. You can ignore them.\n\n-----\n\nHere is this model's performance in test\n\\\n\\\nIt's not that great ([What would you expect by chance?]{.red})\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_test$y, predict(fit_1, feat_test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n313/313 - 0s - 442ms/epoch - 1ms/step\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.207\n```\n\n\n:::\n:::\n\n\n-----\n\nTheoretically, the scale of the inputs should not matter\n\\\n\\\nHOWEVER, gradient descent works better with inputs on the same scale\n\\\n\\\nWe will also want inputs with the same variance if we later apply L2 regularization to our models\n\n- There is a lot of discussion about how best to scale inputs\n- Best if the input means are near zero\n- Best if variances are comparable\n\n-----\n\nWe could:\n\n- Use `step_normalize()`  [Bad choice of function names by tidymodel folks; standardize vs. normalize]\n- Use `step_range()`\n- Book range corrected based on known true range (`/ 255`)\n\\\n\\\nWe will use `step_normalize()`\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_scaled_wrong <- \n  recipe(y ~ ., data = data_trn) |>\n  step_normalize(all_predictors())\n```\n:::\n\n\nThis is wrong! Luckily we glimpsed our feature matrix (not displayed here)\n\n::: {.callout-important collapse=\"false\"}\n### Question: What went wrong and what should we do?\n\n::: {.cell}\n\n```{.html .cell-code  code-fold=\"true\" code-summary=\"Show Answer\"}\nMany of the features have zero variance b/c they are black for ALL of the \nimages (e.g., top rows of pixels.  We can not scale a predictor with zero variance \nb/c when we divide by the SD = 0, we get NaN).  At a minimum, we should remove \nzero variance predictors in training from training and test\n```\n:::\n\n:::\n\n-----\n\nFor example\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn$x1 |> sd()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n-----\n\nLet's remove zero variance predictors before we scale\n\n- To be clear, zero variance features are NOT a problem for neural networks (though clearly they won't help either).\n- But they WILL definitely cause problems for some scaling transformations.\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_scaled <- \n  recipe(y ~ ., data = data_trn) |>\n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n```\n:::\n\n\n-----\n\nWe now have 717 (+ y) features rather than 28 * 28 = 784 features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_scaled_prep <- rec_scaled |> \n  prep(data_trn)\n\nfeat_trn <- rec_scaled_prep |> \n  bake(NULL)\n\ndim(feat_trn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 60000   718\n```\n\n\n:::\n:::\n\n\n-----\n\nLet's also make the feature matrix for test.  This will exclude features that were zero variance in **train** and scale them by their mean and sd in **train**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeat_test <- rec_scaled_prep |> \n  bake(data_test)\n\ndim(feat_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10000   718\n```\n\n\n:::\n:::\n\n\n-----\n\nLet's fit and evaluate this new feature set with no other changes to the model configuration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_2 <-\n  mlp() |>\n  set_mode(\"classification\") |> \n  set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |>\n  fit(y ~ ., data = feat_trn)\n```\n:::\n\n\n\n- That helped a LOT\n- Still could be better though (but it always impresses me! ;-)\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_test$y, predict(fit_2, feat_test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n313/313 - 0s - 401ms/epoch - 1ms/step\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.496\n```\n\n\n:::\n:::\n\n\n-----\n\nThere are many other recommendations about feature engineering to improve the inputs\n\nThese include:\n\n- Normalize (and here I mean true normalization; e.g., `step_BoxCox()`, `step_YeoJohnson()`)\n- De-correlate (e.g., `step_pca()` but retain all features?)\n\nYou can see some discussion of these issues [here](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/#:~:text=Scaling%20input%20and%20output%20variables,is%20presented%20to%20a%20network) and [here](https://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks) to get you started.  The [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) linked in the stack overflow response is also a useful starting point.\n\n-----\n\nSome **preliminary** modeling EDA on my part suggested these additional considerations didn't have major impact on the performance of our models with this dataset so we will stick with just scaling the features.\n\n-----\n\nIt is not surprising that a model configuration with only one hidden layer and 5 units isn't sufficient for this complex task\n\nLet's try 30 units (cheating based on the book chapter!! ;-)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_5units <- mlp(hidden_units = 30) |>\n    set_mode(\"classification\") |> \n    set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |>\n    fit(y ~ ., data = feat_trn)\n```\n:::\n\n\n-----\n\n- Bingo!  Much, much better!\n- We could see if even more units works better still but I won't follow that through here for sake of simplicity\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_test$y, predict(fit_5units, feat_test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n313/313 - 0s - 412ms/epoch - 1ms/step\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9397\n```\n\n\n:::\n:::\n\n\n-----\n\nThe Three Blue 1 Brown videos had a brief discussion of the [relu activation function](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/).\n\nLet's see how to use other activation functions and if this one helps.\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_relu <- mlp(hidden_units = 30, activation = \"relu\") |>\n  set_mode(\"classification\") |> \n  set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |>\n  fit(y ~ ., data = feat_trn)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_test$y, predict(fit_relu, feat_test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n313/313 - 0s - 408ms/epoch - 1ms/step\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9651\n```\n\n\n:::\n:::\n\n\n-----\n\n## Dealing with Overfitting\n\nAs you might imagine, given the number of weights to be fit in even a modest neural network\n(our 30 hidden unit network has 21,850 parameters to estimate), it is easy to become overfit\n\n- 21,540 for hidden layer (717 * 30 weight + 30 biases)\n- 310 for output layer (30 * 10 weights, + 10 biases)\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_relu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\nModel: \"sequential_3\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_6 (Dense)                    (None, 30)                      21540       \n dense_7 (Dense)                    (None, 10)                      310         \n================================================================================\nTotal params: 21850 (85.35 KB)\nTrainable params: 21850 (85.35 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n```\n\n\n:::\n:::\n\n\nThis will be an even bigger problem if you aren't using \"big\" data\n\n-----\n\nThere are a number of different methods available to reduce potential overfitting\n\n- Simplify the network architecture (fewer units, fewer layers)\n- L2 regularization \n- Dropout\n- Early stopping or monitoring validation error to prevent too many epochs\n\n-----\n\n### Regularization or Weight Decay\n\nL2 regularization is implemented in essentially the same fashion as you have seen \nit previously (e.g., glmnet)\n\nThe cost function is expanded to include a penalty based on the sum of the squared weights multiplied by $\\lambda$.  \n\nIn the tidymodels implementation of Keras:\n\n- $\\lambda$ is called `penalty` and is set and/or (ideally) tuned via the \n`penalty` argument in `mlp()`\n\n- Common values for the L2 `penalty` to tune a neural network are often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc.\n\n- `penalty = 0` (the default) means no L2 regularization\n\n- Keras implements other penalties (L1, and a mixture) but not currently through tidymodels\n\n- [Here](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/) is a starting point for more reading on regularization in neural networks\n\n-----\n\nLet's set `penalty = .0001`.   \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_penalty <- mlp(hidden_units = 30, activation = \"relu\", penalty = .0001) |>\n  set_mode(\"classification\") |> \n  set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |>\n  fit(y ~ ., data = feat_trn)\n```\n:::\n\n\n-----\n\n- Looks like there is not much benefit to regularization for this network.  \n- Would likely provide much greater benefit in smaller N contexts or with more complicated model architectures (more hidden units, more hidden unit layers).\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_test$y, predict(fit_penalty, feat_test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n313/313 - 0s - 405ms/epoch - 1ms/step\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.965\n```\n\n\n:::\n:::\n\n\n-----\n\n### Dropout\n\nDropout is a second technique to minimize overfitting.\n\\\n\\\nHere is a clear description of dropout from a blog post on the Machine Learning Mastery: \n\n- Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n\n- As a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features providing some specialization. Neighboring neurons come to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data. \n\n- You can imagine that if neurons are randomly dropped out of the network during training, that other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network.\n\n- The effect is that the network becomes less sensitive to the specific weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overfit the training data.\n\n-----\n\nFor further reading, you might start with the 2014 paper by [Srivastava, et al](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) that proposed the technique.\n\\\n\\\nIn tidymodels, you can set or tune the amount of dropout via the `dropout` argument in `mlp()`\n\n- Srivastava, et al suggest starting with values around .5.  \n- You might consider a range between .1 and .5\n- `droppout = 0` (the default) means no dropout\n- In tidymodels implementation of Keras, you can use a non-zero `penalty` or `dropout` but not both\n\n-----\n\nLet's try `dropout = .1`.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_dropout <- mlp(hidden_units = 30, activation = \"relu\", dropout = .1) |>\n  set_mode(\"classification\") |>  \n  set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |>\n  fit(y ~ ., data = feat_trn)\n```\n:::\n\n\n-----\n\n- Looks like there may be a little benefit but not substantial.  \n- Would likely provide much greater benefit in smaller N contexts or with more complicated model architectures (more hidden units, more hidden unit layers).\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_test$y, predict(fit_dropout, feat_test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n313/313 - 0s - 432ms/epoch - 1ms/step\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9665\n```\n\n\n:::\n:::\n\n\n-----\n\n### Number of Epochs and Early Stopping\n\nNow that we have a model that is working well, lets return to the issue of number of epochs\n\n- Too many epochs can lead to overfitting\n- Too many epochs also just slow things down (not a bit deal if using GPU or overnight but still.....)\n- Too few epochs can lead to under-fitting (which also produces poor performance)\n- The default of `epochs = 20` is a reasonable starting point for a network with one hidden layer but may not work for all situations\n\n-----\n\nMonitoring training error (loss, accuracy) is not ideal b/c it will tend to always decrease\n\n- This is what you would get if you set `verbose = 1`\n\n\\\n\\\nValidation error is what you need to monitor\n\n- Validation error will increase when the model becomes overfit to training\n- We can have Keras hold back some portion of the training data for validation\n  - `validation_split = 1/6`\n  - We pass it in as an optional argument in `set_engine()`\n  - We can use this to monitor validation error rather than training error by epoch. \n  - You can fit an exploratory model with `epochs = 50` to review the plot\n  - This can allow us to determine an appropriate value for `epochs`\n \n-----\n\nLet's see this in action in the best model configuration without regularization or dropout\n\nNOTE:\n\n- `epochs = 50`\n- `verbose = 1`\n- `metrics = c(\"accuracy\")`\n- `validation_split = 1/6`\n- You will see message updates and a plot that tracks training and validation loss and accuracy across epochs\n- This is not rendered into my slides but the plot and messages are pretty clear\n- You can use this information to choose appropriate values for `epoch`\n- `val_accuracy` had plateau and `val_loss` had started to creep up by 10 epochs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_epochs50 <- mlp(hidden_units = 30, activation = \"relu\", epochs = 50) |>\n  set_mode(\"classification\") |>  \n  set_engine(\"keras\", verbose = 1, seeds = fit_seeds, \n             metrics = c(\"accuracy\"), \n             validation_split = 1/6) |>\n  fit(y ~ ., data = feat_trn)\n```\n:::\n\n\n-----\n\nIn some instances, it may be that we want to do more than simply look at epoch performance plots during modeling EDA\n\\\n\\\nWe can instead set the number of epochs to be high but use an early stopping callback to end \nthe training early at an optimal time\n\n-----\n\nCallbacks allow us to interrupt training.   \n\n- There are many types of callbacks in Keras\n- We will only discuss `callback_early_stopping()`\n- We set up callbacks in a list\n- We pass them in as an optional argument in `set_engine()` using `callbacks = `\n- Notice the arguments for `callback_early_stopping()`\n- We also must provide validation error.  Here we set `validation_split = 1/6`\n- This is a method to **tune or select** best number of epochs.  \n  - I haven't yet figured out where the epochs at termination are saved so need to watch the feedback.  It was 35 epochs here\n  - As always, we **could** next refit to the full training set after we have determined the optimal number of epochs.  We won't do that here.\n\n-----\n\nThis fit stopped at 15 epochs\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncallback_list <- list(keras::callback_early_stopping(monitor = \"val_loss\", \n                                                     min_delta = 0, \n                                                     patience = 10))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_early <- mlp(hidden_units = 30, activation = \"relu\", epochs = 200) |>\n  set_mode(\"classification\") |> \n  set_engine(\"keras\", verbose = 1,\n             seeds = fit_seeds, \n             metrics = c(\"accuracy\" ), \n             validation_split = 1/6,\n             callbacks = callback_list) |>\n  fit(y ~ ., data = feat_trn)\n```\n:::\n\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_test$y, predict(fit_early, feat_test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n313/313 - 0s - 410ms/epoch - 1ms/step\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9649\n```\n\n\n:::\n:::\n\n\n-----\n\n**Coding sidebar:** You can see many of the optional arguments you can set for Keras in the help [here](https://keras.rstudio.com/reference/fit.html).   \n\\\n\\\nAnd you can see more info about `callback_early_stopping()` [here](https://keras.rstudio.com/reference/callback_early_stopping.html)\n\n-----\n\n## Using Resampling to Select Best Model Configuration\n\nDeveloping a good network artchitecture and considering feature enginnering options involves experimentation\n  \n- This is what Keras is designed to do\n- tidymodels allows this too\n- We need to evaluate configurations with a valid method to evaluate performance\n  - validation split metric\n  - k-fold metric\n  - bootstrap method\n  - Each can be paired with `fit_resamples()` or `tune_grid()`\n- We need to be systematic\n  - `tune_grid()` helps with this too\n  - recipes can be tuned as well (outside the scope of this course)\n\n-----\n\nHere is an example where we can select among many model configurations that differ across multiple network characteristics\n\n- Evaluate with validation split accuracy \n- Sample size is relatively big so we have 10,000 validation set observations.  Should offer a low variance performance estimate\n- K-fold and bootstrap would still be better but big computation costs (too big for this web book but could be done in real life!)\n\n-----\n\nIts really just our normal workflow at this point\n\n- Get splits (validation splits in this example)\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(102030)\nsplits_validation <-\n  data_trn |> \n  validation_split(prop = 5/6)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `validation_split()` was deprecated in rsample 1.2.0.\nℹ Please use `initial_validation_split()` instead.\n```\n\n\n:::\n:::\n\n\n-----\n\n- Set up grid of hyperparameter values\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_keras <- expand_grid(hidden_units = c(5, 10, 20, 30, 50, 100), \n                          penalty = c(.00001, .0001, .01, .1))\n```\n:::\n\n\n-----\n\n- Use `tune_grid()` to fit models in training and predict into validation set for each combination of hyperparameter values\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_nn <- cache_rds(\n  expr = {\n    mlp(hidden_units = tune(), penalty = tune(), activation = \"relu\") |>\n    set_mode(\"classification\") |> \n    # setting to verbose = 1 to track progress.  Training error not that useful\n    set_engine(\"keras\", verbose = 1, seeds = fit_seeds) |>  \n    tune_grid(preprocessor = rec_scaled, \n                  grid = grid_keras,\n                  resamples = splits_validation,\n                  metrics = metric_set(accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/010/\",\n  file = \"fits_nn\")\n```\n:::\n\n\n-----\n\n- Find model configuration with best performance in the held-out validation set\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(fits_nn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 8\n  hidden_units penalty .metric  .estimator  mean     n std_err\n         <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl>\n1          100 0.00001 accuracy multiclass 0.973     1      NA\n2          100 0.0001  accuracy multiclass 0.970     1      NA\n3           50 0.0001  accuracy multiclass 0.967     1      NA\n4           50 0.00001 accuracy multiclass 0.966     1      NA\n5           30 0.00001 accuracy multiclass 0.962     1      NA\n  .config              \n  <chr>                \n1 Preprocessor1_Model21\n2 Preprocessor1_Model22\n3 Preprocessor1_Model18\n4 Preprocessor1_Model17\n5 Preprocessor1_Model13\n```\n\n\n:::\n:::\n\n\n-----\n\n## Other Details\n\nWe can get a better sense of how tidymodels is interacting with Keras by looking at the function that is called\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlp(hidden_units = 30, activation = \"relu\", dropout = .1) |>\n  set_mode(\"classification\") |>  \n  set_engine(\"keras\", verbose = 0, seeds = fit_seeds) |> \n  translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle Layer Neural Network Model Specification (classification)\n\nMain Arguments:\n  hidden_units = 30\n  dropout = 0.1\n  activation = relu\n\nEngine-Specific Arguments:\n  verbose = 0\n  seeds = fit_seeds\n\nComputational engine: keras \n\nModel fit template:\nparsnip::keras_mlp(x = missing_arg(), y = missing_arg(), hidden_units = 30, \n    dropout = 0.1, activation = \"relu\", verbose = 0, seeds = fit_seeds)\n```\n\n\n:::\n:::\n\n\n-----\n\n`keras_mlp()` is a wrapper around the calls to Keras.  Lets see what it does\n\n::: {.cell}\n\n```{.r .cell-code}\nkeras_mlp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (x, y, hidden_units = 5, penalty = 0, dropout = 0, epochs = 20, \n    activation = \"softmax\", seeds = sample.int(10^5, size = 3), \n    ...) \n{\n    act_funs <- c(\"linear\", \"softmax\", \"relu\", \"elu\")\n    rlang::arg_match(activation, act_funs, )\n    if (penalty > 0 & dropout > 0) {\n        rlang::abort(\"Please use either dropoput or weight decay.\", \n            call. = FALSE)\n    }\n    if (!is.matrix(x)) {\n        x <- as.matrix(x)\n    }\n    if (is.character(y)) {\n        y <- as.factor(y)\n    }\n    factor_y <- is.factor(y)\n    if (factor_y) {\n        y <- class2ind(y)\n    }\n    else {\n        if (isTRUE(ncol(y) > 1)) {\n            y <- as.matrix(y)\n        }\n        else {\n            y <- matrix(y, ncol = 1)\n        }\n    }\n    model <- keras::keras_model_sequential()\n    if (penalty > 0) {\n        model %>% keras::layer_dense(units = hidden_units, activation = activation, \n            input_shape = ncol(x), kernel_regularizer = keras::regularizer_l2(penalty), \n            kernel_initializer = keras::initializer_glorot_uniform(seed = seeds[1]))\n    }\n    else {\n        model %>% keras::layer_dense(units = hidden_units, activation = activation, \n            input_shape = ncol(x), kernel_initializer = keras::initializer_glorot_uniform(seed = seeds[1]))\n    }\n    if (dropout > 0) {\n        model %>% keras::layer_dense(units = hidden_units, activation = activation, \n            input_shape = ncol(x), kernel_initializer = keras::initializer_glorot_uniform(seed = seeds[1])) %>% \n            keras::layer_dropout(rate = dropout, seed = seeds[2])\n    }\n    if (factor_y) {\n        model <- model %>% keras::layer_dense(units = ncol(y), \n            activation = \"softmax\", kernel_initializer = keras::initializer_glorot_uniform(seed = seeds[3]))\n    }\n    else {\n        model <- model %>% keras::layer_dense(units = ncol(y), \n            activation = \"linear\", kernel_initializer = keras::initializer_glorot_uniform(seed = seeds[3]))\n    }\n    arg_values <- parse_keras_args(...)\n    compile_call <- expr(keras::compile(object = model))\n    if (!any(names(arg_values$compile) == \"loss\")) {\n        if (factor_y) {\n            compile_call$loss <- \"binary_crossentropy\"\n        }\n        else {\n            compile_call$loss <- \"mse\"\n        }\n    }\n    if (!any(names(arg_values$compile) == \"optimizer\")) {\n        compile_call$optimizer <- \"adam\"\n    }\n    compile_call <- rlang::call_modify(compile_call, !!!arg_values$compile)\n    model <- eval_tidy(compile_call)\n    fit_call <- expr(keras::fit(object = model))\n    fit_call$x <- quote(x)\n    fit_call$y <- quote(y)\n    fit_call$epochs <- epochs\n    fit_call <- rlang::call_modify(fit_call, !!!arg_values$fit)\n    history <- eval_tidy(fit_call)\n    model$y_names <- colnames(y)\n    model\n}\n<bytecode: 0x5bc53877a4b8>\n<environment: namespace:parsnip>\n```\n\n\n:::\n:::\n\n\nWe can see:\n\n- How the activation functions are setup\n- The choice of cost function: mse or binary_crossentropy\n- The optimizer: Adam\n- How the three seeds are being used\n\n-----\n\nFinally, you might  have noticed that we never set a learning rate anywhere\n\nThe [Adam optimizer](pdfs/kingma_adam_optimizer.pdf) is used instead of classic stochastic gradient descent.  The authors of this optimizer state it is:\n\n- Straightforward to implement.\n- Computationally efficient.\n- Little memory requirements.\n- Invariant to diagonal rescale of the gradients.\n- Well suited for problems that are large in terms of data and/or parameters.\n- Appropriate for non-stationary objectives.\n- Appropriate for problems with very noisy/or sparse gradients.\n- Hyper-parameters have intuitive interpretation and typically require little tuning.\n\n-----\n\nYou can start additional reading about Adam [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems)\n\nFor now, if you want another optimizer or much more control over your network architecture, you may need to work directly in Keras.\n\n-----\n\n## Discussion Notes\n\n0. Keras and tidymodels\n\n  - power/flexibility vs. simplicity\n  - `nnet` package\n\n1. Discuss conceptual questions about neural networks\n\n  - The MLP (and deeper learning)\n  - Compare algorithms\n    - native feature engineering for very complex non-linear, non-linear interactions, combinations, etc\n    - better with more units and more layers\n  - Handling overfitting\n    - L2\n    - dropout\n    - number of epochs\n\n2.  Continued meta-discussion\n\n  - Why/When do we estimate model performance?\n\n    - What approaches have we discussed to estimate model performance\n  \n    - What types of problems can we encounter when we estimate performance and what factors\n    influence the degree to which these problems will manifest\n    \n    - What are the strengths and weaknesses of each of these model performance estimation approaches (e.g., how much do they suffer from the problems you described)\n  \n  - What inferential tests have we discussed [We will discuss this point only if time permits]\n  \n    - When would we use them?\n    - How do we do them?\n\n\n\n",
    "supporting": [
      "010_neural_networks_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}