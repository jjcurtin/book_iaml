{
  "hash": "f0ed40f0e8070abaead12eba4e9ce043",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Introduction to Classification Models\n\n## Unit Overview\n\n### Learning Objectives\n\n- Bayes classifier \n- Logistic regression\n  - probability, odds, and logit models\n  - definitions of odds and odds ratios\n- K nearest neighbors for classification\n- Linear discriminant analysis\n- Quadratic discriminant analysis\n- Regularized discriminant analysis\n- Decision boundaries in the two feature space\n- Relative costs and benefits of these different statistical algorithms\n\n-----\n\n### Readings\n\n- @ISL [Chapter 4, pp 129 - 164](pdfs/isl_2.pdf)\n\nPost  questions to the readings channel in Slack\n\n### Lecture Videos\n  \n- [Lecture 1: The Bayes Classifier](https://mediaspace.wisc.edu/media/iaml+unit+4-1/1_5xvs5z3f) ~ 9 mins\n- [Lecture 2: Conceptual Overview of Logistic Regression](https://mediaspace.wisc.edu/media/iaml+unit+4-2/1_ut2up1mo) ~ 19 mins\n- [Lecture 3: EDA with the Cars Dataset](https://mediaspace.wisc.edu/media/iaml+unit+4-3/1_vydb2j8t) ~ 12 mins\n- [Lecture 4: Logistic Regression with Cars Dataset](https://mediaspace.wisc.edu/media/iaml+unit+4-4/1_dzqnsnf0) ~ 32 mins\n- [Lecture 5: KNN with Cars Dataset](https://mediaspace.wisc.edu/media/iaml+unit+4-5/1_mj1cyhhj) ~ 19 mins\n- [Lecture 6: LDA, DQA, RDA with Cars Dataset](https://mediaspace.wisc.edu/media/iaml+unit+4-6/1_t7mnafsp) ~ 16 mins\n- [Lecture 7: Comparisons among Classifiers](https://mediaspace.wisc.edu/media/iaml+unit+4-7/1_tw44we5w) ~ 11 mins\n\nPost questions to the video-lectures channel in Slack\n\n### Application Assignment and Quiz\n  \n- data: [raw](application_assignments/unit_04/titanic_raw.csv); [test](application_assignments/unit_04/titanic_test_cln.csv)\n- [data dictionary](application_assignments/unit_04/titanic_data_dictionary.png)\n- [cleaning EDA qmd](https://raw.githubusercontent.com/jjcurtin/book_iaml/main/application_assignments/unit_04/hw_unit_04_eda_cleaning.qmd)\n- [rda qmd](https://raw.githubusercontent.com/jjcurtin/book_iaml/main/application_assignments/unit_04/hw_unit_04_fit_rda.qmd)\n- [knn qmd](https://raw.githubusercontent.com/jjcurtin/book_iaml/main/application_assignments/unit_04/hw_unit_04_fit_knn.qmd)\n- solution: [modeling EDA](https://dionysus.psych.wisc.edu/iaml/key_unit_04_eda_modeling.html) [rda](https://dionysus.psych.wisc.edu/iaml/key_unit_04_fit_rda.html); [knn](https://dionysus.psych.wisc.edu/iaml/key_unit_04_fit_knn.html)\n\nPost questions to application-assignments Slack Channel\n\nSubmit the application assignment [here](https://canvas.wisc.edu/courses/395546/assignments/2187689) and complete the [unit quiz](https://canvas.wisc.edu/courses/395546/quizzes/514051) by 8 pm on Wednesday, February 14th\n\n-----\n\nOur eventual goal in this unit is for you to build a machine learning classification model that can accurately predict who lived vs. died on the titanic.  \n\\\n\\\nPrior to that, we will work with an example where we classify a car as high or low fuel efficient (i.e., a dichtomous outcome based on miles per gallon) using features engineered from its characteristics\n\n----\n\n## Bayes Classifier\n\nFirst, lets introduce the Bayes classifier, which is the classifier that will have the lowest error rate of all classifiers using the same set of features. \n\\\n\\\nThe figure below displays simulated data for a classification problem for K = 2 classes as a function of `X1` and `X2`\n\n![](figs/unit4_two_class_decision_boundry.png){height=4in}\n\nThe Bayes classifier assigns each observation its most likely class given its conditional probabilities for the values for `X1` and `X2`\n\n- $Pr(Y = k | X = x_0) for\\:k = 1:K$\n- For K = 2, this means assigning to the class with Pr > .50\n- This decision boundary for the two class problem is displayed in the figure\n\n-----\n\nThe Bayes classifier provides the minimum error rate for test data\n\n- Error rate for any $x_0$ will be $1 - max (Pr( Y = k | X = x_0))$\n- Overall error rate will be the average of this across all possible X\n- This is the irreducible error for classification problems\n- This is a theoretical model b/c (except for simulated data), we don't know the conditional probabilities based on X\n- Many classification models try to estimate these conditionals\n\nLet's talk now about some of these classification models\n\n-----\n\n## Logistic Regression: A Conceptual Review\n\nLogistic regression (a special case of the **generalized** linear model) estimates the conditional probability for each class given X (a specific set of values for our features)\n\n- In the binary outcome case, we will often refer to the two outcomes as the positive class and the negative class\n- This makes most sense in some applied settings where we are most interested in predicting if one of the two classes is likely, e.g., \n  - Presence of heart disease\n  - Positive for some psychiatric diagnoses\n  - Lapse back to alcohol use in people with alcohol use disorder\n\n-----\n\nLogistic regression is used frequently for binary and multi-level outcomes because\n\n- The general linear model makes predictions that are not bounded by [0, 1] and do not represent true estimates of conditional probability for each class\n- Logistic regression approaches can be modified to accommodate multi-class outcomes (i.e., more than two levels) even when they are not ordered\n- Nonetheless, the general linear model is still used at times to predict binary outcomes (see [Linear Probability Model](https://en.wikipedia.org/wiki/Linear_probability_model)) so you should be aware of it.  We won't discuss it further here.\n\n![](figs/unit4_lm_vs_logistic.png){height=4in}\n\n-----\n\nLogistic regression provides predicted conditional probabilities for one class (positive class) for any specific set of values for our features (X)\n\n- $Pr(positive\\:class | X) = \\frac{e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}{1 + e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}$\n\n- These conditional probabilities are bounded by [0, 1]\n\n- To maximize accuracy (as per Bayes classifier), \n  - we predict the positive case if $Pr(positive class | X) > .5$ for any specific X\n  - otherwise we predict the negative class   \n  \n-----\n\nAs a simple parametric model, logistic regression is commonly used for explanatory purposes as well as prediction\n\\\n\\\nFor these reasons, it is worthwhile to fully understand how to work with the logistic function to quantify and describe the effects of your features/predictors in terms of \n\n- Probability \n- Odds\n- [Log-odds or logit]\n- Odds ratio\n\n-----\n\nThe logistic function yields the probability of the positive class given X\n\n- However, in some instances (e.g., horse racing, poker), it may make more sense to describe the odds of the positive case occurring rather than probability\n\\\n\\\n\nOdds are defined with respect to probabilities as follows:\n\n  - $odds = \\frac{Pr(positive\\:class|X)} {1 - Pr(positive\\:class|X)}$\n\n-----\n\nFor example, if the UW Badgers have a .5 probability of winning some upcoming game based on X, their odds of winning are 1 (to 1)\n\n- $\\frac{0.5} {1 - 0.5}$\n\\\n\\\n\nIf the UW Badgers have a .75 probability of winning some upcoming game based on X, their odds of winning are 3 (:1; '3 to 1')\n\n- $\\frac{0.75} {1 - 0.75}$\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: If the UW Badgers have a .25 probability of winning some upcoming game based on X, what are their odds of winning? \n\n::: {.cell}\n\n```{.html .cell-code  code-fold=\"true\" code-summary=\"Show Answer\"}\n.25 / (1 - .25) = 0.33 or 1:3\n```\n:::\n\n:::\n\n-----\n\nThe logistic function can be modified to provide odds directly:\n\\\n\\\n**Logistic function**:\n\n- $Pr(positive\\:class | X) = \\frac{e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}{1 + e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}$\n\\\n\\\n\n**Definition of odds**:\n\n- $odds = \\frac{Pr(positive\\:class|X)} {1 - Pr(positive\\:class|X)}$\n\\\n\\\n\nSubstitute logistic function for $Pr(positive\\:class|X)$ on top and bottom and simplify to get: \n\n- $odds(positive\\:class|X) = e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}$\n- Odds are bounded by [0, $\\infty$]\n\n-----\n\nThe logistic function can be modified further such that the outcome (log-odds/logit) is a linear function of your features\n\\\n\\\nIf we take the natural log (base e) of both sides of the odds equation, we get:\n\n- $log(odds(positive\\:class|X)) = \\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p$\n- Log-odds are unbounded $[-\\infty, \\infty]$\n\\\n\\\n\nUse of logit transformation:\n\n- provides the connection between the general linear model and generalized linear models (in this case with link = logit, family = binomial).  \n- Notice that the logit/log-odds is a linear combination of the features (just like in the general linear model)\n\n-----\n\n**Odds** and **probability** are descriptive but they are not linear functions of X\n\n  - Therefore, parameter estimates from these models aren't very useful to describe the effect of features\n  - This is because unit change in Y per unit change in any specific feature is not the same for all values of the feature\n\n-----\n\n**Log-odds** are a linear function of X\n\n- Therefore, you can say that log-odds of positive class increases by $b_1$ (our estimate of $\\beta_1$) for every one unit increase in $x_1$\n- However, log-odds are NOT very descriptive/intuitive so they are not that useful for explanatory purposes\n\n-----\n\nThe **odds ratio** addresses these problems\n\\\n\\\nOdds are defined at a specific set of values across the features in your model.  For example, with one feature:\n \n- $odds = \\frac{Pr(positive\\:class|x_1)} {1 - Pr(positive\\:class|x_1)}$\n\\\n\\\n\nThe odds ratio describes the change in odds for a change of c units in your feature.  With some manipulation: \n\n- $Odds\\:ratio  =  \\frac{odds(x+c)}{odds(x)}$\n- $Odds\\:ratio  =  \\frac{e^{\\beta_0 + \\beta_1*(x_1 + c)}}{e^{\\beta_0 + \\beta_1*x_1}}$\n- $Odds\\:ratio  = e^{c*\\beta_1}$\n\n-----\n\nAs an example, if we fit a logistic regression model to predict the probability of the Badgers winning a home football game given the attendance (measured in individual spectators at the game), we might find $b_1$ (our estimate of $\\beta_1$) = .000075.\n\\\n\\\nGiven this, the odds ratio associated with every increase in 10,000 spectators:\n\n- $= e^{c * \\b_1}$\n- $= e^{10000 * .000075}$\n- $= 2.1$\n- For every increase of 10,000 spectators, the odds of the Badgers winning doubles\n\n-----\n\n## The Cars Dataset\n\n\n\n\n\nNow, let's put this all of this together in the Cars dataset from Carnegie Mellon's [StatLib Dataset Archive](http://lib.stat.cmu.edu/datasets/)  \n\\\n\\\nOur goal is to build a classifier (machine learning model for a categorical outcome) that classifies cars as either high or low mpg.\n\n-----\n\n### Cleaning EDA\n\nLet's start with some cleaning EDA\n\n- Open and \"skim\" it RE variable names, classes & missing data\n- Variable names are already tidy\n- no missing data\n- mins (p0) and maxes(p100) for numeric look good\n- there are two character variables (`mpq` and `name`) that will need to be re-classed\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_all <- read_csv(here::here(path_data, \"auto_all.csv\"),\n                     col_types = cols()) \n\ndata_all |> skim_some()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |         |\n|:------------------------|:--------|\n|Name                     |data_all |\n|Number of rows           |392      |\n|Number of columns        |9        |\n|_______________________  |         |\n|Column type frequency:   |         |\n|character                |2        |\n|numeric                  |7        |\n|________________________ |         |\n|Group variables          |None     |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|mpg           |         0|             1|   3|   4|     0|        2|          0|\n|name          |         0|             1|   6|  36|     0|      301|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   p0|   p100|\n|:-------------|---------:|-------------:|----:|------:|\n|cylinders     |         0|             1|    3|    8.0|\n|displacement  |         0|             1|   68|  455.0|\n|horsepower    |         0|             1|   46|  230.0|\n|weight        |         0|             1| 1613| 5140.0|\n|acceleration  |         0|             1|    8|   24.8|\n|year          |         0|             1|   70|   82.0|\n|origin        |         0|             1|    1|    3.0|\n\n\n:::\n:::\n\n\n-----\n\n- `mpg` is ordinal, so lets set the levels to indicate the order.\n- After reviewing the [data dictionary](data/auto_data_dictionary.pdf), we see that `origin` is a nominal predictor that is coded numeric (where 1 = American, 2 = European, and 3 = Japanese).  Let's recode as character with meaningful labels and then class as a factor\n- and lets not forget to re-class `name` too\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_all <- data_all |> \n  mutate(mpg = factor(mpg, levels = c(\"low\", \"high\")),\n         name = factor(name), \n         origin = factor (origin),\n         origin = fct_recode(as.character(origin), #<1>\n                             \"american\" = \"1\",\n                            \"european\" = \"2\",\n                            \"japanese\" = \"3\")) \n```\n:::\n\n1. `fct_recode()` works on levels stored as characters, so convert 1, 2, 3, to character first.\n\n-----\n\nNow, we can explore responses for categorical variables\n\n  - Other than `name`, responses for all other variables are tidy\n  - `name` has many different responses\n  - We won't know how to handle this until we get to later units in the class on natural language processing!\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_all |> \n  select(where(is.factor)) |>\n  walk(\\(column) print(levels(column)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"low\"  \"high\"\n[1] \"american\" \"european\" \"japanese\"\n  [1] \"amc ambassador brougham\"             \n  [2] \"amc ambassador dpl\"                  \n  [3] \"amc ambassador sst\"                  \n  [4] \"amc concord\"                         \n  [5] \"amc concord d/l\"                     \n  [6] \"amc concord dl 6\"                    \n  [7] \"amc gremlin\"                         \n  [8] \"amc hornet\"                          \n  [9] \"amc hornet sportabout (sw)\"          \n [10] \"amc matador\"                         \n [11] \"amc matador (sw)\"                    \n [12] \"amc pacer\"                           \n [13] \"amc pacer d/l\"                       \n [14] \"amc rebel sst\"                       \n [15] \"amc spirit dl\"                       \n [16] \"audi 100 ls\"                         \n [17] \"audi 100ls\"                          \n [18] \"audi 4000\"                           \n [19] \"audi 5000\"                           \n [20] \"audi 5000s (diesel)\"                 \n [21] \"audi fox\"                            \n [22] \"bmw 2002\"                            \n [23] \"bmw 320i\"                            \n [24] \"buick century\"                       \n [25] \"buick century 350\"                   \n [26] \"buick century limited\"               \n [27] \"buick century luxus (sw)\"            \n [28] \"buick century special\"               \n [29] \"buick electra 225 custom\"            \n [30] \"buick estate wagon (sw)\"             \n [31] \"buick lesabre custom\"                \n [32] \"buick opel isuzu deluxe\"             \n [33] \"buick regal sport coupe (turbo)\"     \n [34] \"buick skyhawk\"                       \n [35] \"buick skylark\"                       \n [36] \"buick skylark 320\"                   \n [37] \"buick skylark limited\"               \n [38] \"cadillac eldorado\"                   \n [39] \"cadillac seville\"                    \n [40] \"capri ii\"                            \n [41] \"chevroelt chevelle malibu\"           \n [42] \"chevrolet bel air\"                   \n [43] \"chevrolet camaro\"                    \n [44] \"chevrolet caprice classic\"           \n [45] \"chevrolet cavalier\"                  \n [46] \"chevrolet cavalier 2-door\"           \n [47] \"chevrolet cavalier wagon\"            \n [48] \"chevrolet chevelle concours (sw)\"    \n [49] \"chevrolet chevelle malibu\"           \n [50] \"chevrolet chevelle malibu classic\"   \n [51] \"chevrolet chevette\"                  \n [52] \"chevrolet citation\"                  \n [53] \"chevrolet concours\"                  \n [54] \"chevrolet impala\"                    \n [55] \"chevrolet malibu\"                    \n [56] \"chevrolet malibu classic (sw)\"       \n [57] \"chevrolet monte carlo\"               \n [58] \"chevrolet monte carlo landau\"        \n [59] \"chevrolet monte carlo s\"             \n [60] \"chevrolet monza 2+2\"                 \n [61] \"chevrolet nova\"                      \n [62] \"chevrolet nova custom\"               \n [63] \"chevrolet vega\"                      \n [64] \"chevrolet vega (sw)\"                 \n [65] \"chevrolet vega 2300\"                 \n [66] \"chevrolet woody\"                     \n [67] \"chevy c10\"                           \n [68] \"chevy c20\"                           \n [69] \"chevy s-10\"                          \n [70] \"chrysler cordoba\"                    \n [71] \"chrysler lebaron medallion\"          \n [72] \"chrysler lebaron salon\"              \n [73] \"chrysler lebaron town @ country (sw)\"\n [74] \"chrysler new yorker brougham\"        \n [75] \"chrysler newport royal\"              \n [76] \"datsun 1200\"                         \n [77] \"datsun 200-sx\"                       \n [78] \"datsun 200sx\"                        \n [79] \"datsun 210\"                          \n [80] \"datsun 210 mpg\"                      \n [81] \"datsun 280-zx\"                       \n [82] \"datsun 310\"                          \n [83] \"datsun 310 gx\"                       \n [84] \"datsun 510\"                          \n [85] \"datsun 510 (sw)\"                     \n [86] \"datsun 510 hatchback\"                \n [87] \"datsun 610\"                          \n [88] \"datsun 710\"                          \n [89] \"datsun 810\"                          \n [90] \"datsun 810 maxima\"                   \n [91] \"datsun b-210\"                        \n [92] \"datsun b210\"                         \n [93] \"datsun b210 gx\"                      \n [94] \"datsun f-10 hatchback\"               \n [95] \"datsun pl510\"                        \n [96] \"dodge aries se\"                      \n [97] \"dodge aries wagon (sw)\"              \n [98] \"dodge aspen\"                         \n [99] \"dodge aspen 6\"                       \n[100] \"dodge aspen se\"                      \n[101] \"dodge challenger se\"                 \n[102] \"dodge charger 2.2\"                   \n[103] \"dodge colt\"                          \n[104] \"dodge colt (sw)\"                     \n[105] \"dodge colt hardtop\"                  \n[106] \"dodge colt hatchback custom\"         \n[107] \"dodge colt m/m\"                      \n[108] \"dodge coronet brougham\"              \n[109] \"dodge coronet custom\"                \n[110] \"dodge coronet custom (sw)\"           \n[111] \"dodge d100\"                          \n[112] \"dodge d200\"                          \n[113] \"dodge dart custom\"                   \n[114] \"dodge diplomat\"                      \n[115] \"dodge magnum xe\"                     \n[116] \"dodge monaco (sw)\"                   \n[117] \"dodge monaco brougham\"               \n[118] \"dodge omni\"                          \n[119] \"dodge rampage\"                       \n[120] \"dodge st. regis\"                     \n[121] \"fiat 124 sport coupe\"                \n[122] \"fiat 124 tc\"                         \n[123] \"fiat 124b\"                           \n[124] \"fiat 128\"                            \n[125] \"fiat 131\"                            \n[126] \"fiat strada custom\"                  \n[127] \"fiat x1.9\"                           \n[128] \"ford country\"                        \n[129] \"ford country squire (sw)\"            \n[130] \"ford escort 2h\"                      \n[131] \"ford escort 4w\"                      \n[132] \"ford f108\"                           \n[133] \"ford f250\"                           \n[134] \"ford fairmont\"                       \n[135] \"ford fairmont (auto)\"                \n[136] \"ford fairmont (man)\"                 \n[137] \"ford fairmont 4\"                     \n[138] \"ford fairmont futura\"                \n[139] \"ford fiesta\"                         \n[140] \"ford futura\"                         \n[141] \"ford galaxie 500\"                    \n[142] \"ford gran torino\"                    \n[143] \"ford gran torino (sw)\"               \n[144] \"ford granada\"                        \n[145] \"ford granada ghia\"                   \n[146] \"ford granada gl\"                     \n[147] \"ford granada l\"                      \n[148] \"ford ltd\"                            \n[149] \"ford ltd landau\"                     \n[150] \"ford maverick\"                       \n[151] \"ford mustang\"                        \n[152] \"ford mustang gl\"                     \n[153] \"ford mustang ii\"                     \n[154] \"ford mustang ii 2+2\"                 \n[155] \"ford pinto\"                          \n[156] \"ford pinto (sw)\"                     \n[157] \"ford pinto runabout\"                 \n[158] \"ford ranger\"                         \n[159] \"ford thunderbird\"                    \n[160] \"ford torino\"                         \n[161] \"ford torino 500\"                     \n[162] \"hi 1200d\"                            \n[163] \"honda accord\"                        \n[164] \"honda accord cvcc\"                   \n[165] \"honda accord lx\"                     \n[166] \"honda civic\"                         \n[167] \"honda civic (auto)\"                  \n[168] \"honda civic 1300\"                    \n[169] \"honda civic 1500 gl\"                 \n[170] \"honda civic cvcc\"                    \n[171] \"honda prelude\"                       \n[172] \"maxda glc deluxe\"                    \n[173] \"maxda rx3\"                           \n[174] \"mazda 626\"                           \n[175] \"mazda glc\"                           \n[176] \"mazda glc 4\"                         \n[177] \"mazda glc custom\"                    \n[178] \"mazda glc custom l\"                  \n[179] \"mazda glc deluxe\"                    \n[180] \"mazda rx-4\"                          \n[181] \"mazda rx-7 gs\"                       \n[182] \"mazda rx2 coupe\"                     \n[183] \"mercedes benz 300d\"                  \n[184] \"mercedes-benz 240d\"                  \n[185] \"mercedes-benz 280s\"                  \n[186] \"mercury capri 2000\"                  \n[187] \"mercury capri v6\"                    \n[188] \"mercury cougar brougham\"             \n[189] \"mercury grand marquis\"               \n[190] \"mercury lynx l\"                      \n[191] \"mercury marquis\"                     \n[192] \"mercury marquis brougham\"            \n[193] \"mercury monarch\"                     \n[194] \"mercury monarch ghia\"                \n[195] \"mercury zephyr\"                      \n[196] \"mercury zephyr 6\"                    \n[197] \"nissan stanza xe\"                    \n[198] \"oldsmobile cutlass ciera (diesel)\"   \n[199] \"oldsmobile cutlass ls\"               \n[200] \"oldsmobile cutlass salon brougham\"   \n[201] \"oldsmobile cutlass supreme\"          \n[202] \"oldsmobile delta 88 royale\"          \n[203] \"oldsmobile omega\"                    \n[204] \"oldsmobile omega brougham\"           \n[205] \"oldsmobile starfire sx\"              \n[206] \"oldsmobile vista cruiser\"            \n[207] \"opel 1900\"                           \n[208] \"opel manta\"                          \n[209] \"peugeot 304\"                         \n[210] \"peugeot 504\"                         \n[211] \"peugeot 504 (sw)\"                    \n[212] \"peugeot 505s turbo diesel\"           \n[213] \"peugeot 604sl\"                       \n[214] \"plymouth 'cuda 340\"                  \n[215] \"plymouth arrow gs\"                   \n[216] \"plymouth champ\"                      \n[217] \"plymouth cricket\"                    \n[218] \"plymouth custom suburb\"              \n[219] \"plymouth duster\"                     \n[220] \"plymouth fury\"                       \n[221] \"plymouth fury gran sedan\"            \n[222] \"plymouth fury iii\"                   \n[223] \"plymouth grand fury\"                 \n[224] \"plymouth horizon\"                    \n[225] \"plymouth horizon 4\"                  \n[226] \"plymouth horizon miser\"              \n[227] \"plymouth horizon tc3\"                \n[228] \"plymouth reliant\"                    \n[229] \"plymouth sapporo\"                    \n[230] \"plymouth satellite\"                  \n[231] \"plymouth satellite custom\"           \n[232] \"plymouth satellite custom (sw)\"      \n[233] \"plymouth satellite sebring\"          \n[234] \"plymouth valiant\"                    \n[235] \"plymouth valiant custom\"             \n[236] \"plymouth volare\"                     \n[237] \"plymouth volare custom\"              \n[238] \"plymouth volare premier v8\"          \n[239] \"pontiac astro\"                       \n[240] \"pontiac catalina\"                    \n[241] \"pontiac catalina brougham\"           \n[242] \"pontiac firebird\"                    \n[243] \"pontiac grand prix\"                  \n[244] \"pontiac grand prix lj\"               \n[245] \"pontiac j2000 se hatchback\"          \n[246] \"pontiac lemans v6\"                   \n[247] \"pontiac phoenix\"                     \n[248] \"pontiac phoenix lj\"                  \n[249] \"pontiac safari (sw)\"                 \n[250] \"pontiac sunbird coupe\"               \n[251] \"pontiac ventura sj\"                  \n[252] \"renault 12 (sw)\"                     \n[253] \"renault 12tl\"                        \n[254] \"renault 5 gtl\"                       \n[255] \"saab 99e\"                            \n[256] \"saab 99gle\"                          \n[257] \"saab 99le\"                           \n[258] \"subaru\"                              \n[259] \"subaru dl\"                           \n[260] \"toyota carina\"                       \n[261] \"toyota celica gt\"                    \n[262] \"toyota celica gt liftback\"           \n[263] \"toyota corolla\"                      \n[264] \"toyota corolla 1200\"                 \n[265] \"toyota corolla 1600 (sw)\"            \n[266] \"toyota corolla liftback\"             \n[267] \"toyota corolla tercel\"               \n[268] \"toyota corona\"                       \n[269] \"toyota corona hardtop\"               \n[270] \"toyota corona liftback\"              \n[271] \"toyota corona mark ii\"               \n[272] \"toyota cressida\"                     \n[273] \"toyota mark ii\"                      \n[274] \"toyota starlet\"                      \n[275] \"toyota tercel\"                       \n[276] \"toyouta corona mark ii (sw)\"         \n[277] \"triumph tr7 coupe\"                   \n[278] \"vokswagen rabbit\"                    \n[279] \"volkswagen 1131 deluxe sedan\"        \n[280] \"volkswagen 411 (sw)\"                 \n[281] \"volkswagen dasher\"                   \n[282] \"volkswagen jetta\"                    \n[283] \"volkswagen model 111\"                \n[284] \"volkswagen rabbit\"                   \n[285] \"volkswagen rabbit custom\"            \n[286] \"volkswagen rabbit custom diesel\"     \n[287] \"volkswagen rabbit l\"                 \n[288] \"volkswagen scirocco\"                 \n[289] \"volkswagen super beetle\"             \n[290] \"volkswagen type 3\"                   \n[291] \"volvo 144ea\"                         \n[292] \"volvo 145e (sw)\"                     \n[293] \"volvo 244dl\"                         \n[294] \"volvo 245\"                           \n[295] \"volvo 264gl\"                         \n[296] \"volvo diesel\"                        \n[297] \"vw dasher (diesel)\"                  \n[298] \"vw pickup\"                           \n[299] \"vw rabbit\"                           \n[300] \"vw rabbit c (diesel)\"                \n[301] \"vw rabbit custom\"                    \n```\n\n\n:::\n:::\n\n\n- Remove `name`\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_all <- data_all |> \n  select(-name)\n```\n:::\n\n\n-----\n\nFinally, let's make and save our training and validation sets.  If we were doing model building for prediction we would also need a test set but we will focus this unit on just selecting the best model but not rigorously evaluating it.\n\n- Let's use a 75/25 split, stratified on `mpg`\n- Don't forget to set a seed in case you need to re-split again in the future!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20110522) \n\nsplits <- data_all |> \n  initial_split(prop = .75, strata = \"mpg\")\n\nsplits |> \n  analysis() |> \n  glimpse() |> \n  write_csv(here::here(path_data, \"auto_trn.csv\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 294\nColumns: 8\n$ mpg          <fct> high, high, high, high, high, high, high, high, high, hig…\n$ cylinders    <dbl> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ displacement <dbl> 113.0, 97.0, 97.0, 110.0, 107.0, 104.0, 121.0, 97.0, 140.…\n$ horsepower   <dbl> 95, 88, 46, 87, 90, 95, 113, 88, 90, 95, 86, 90, 70, 76, …\n$ weight       <dbl> 2372, 2130, 1835, 2672, 2430, 2375, 2234, 2130, 2264, 222…\n$ acceleration <dbl> 15.0, 14.5, 20.5, 17.5, 14.5, 17.5, 12.5, 14.5, 15.5, 14.…\n$ year         <dbl> 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 7…\n$ origin       <fct> japanese, japanese, european, european, european, europea…\n```\n\n\n:::\n\n```{.r .cell-code}\nsplits |> \n  assessment() |> \n  glimpse() |> \n  write_csv(here::here(path_data, \"auto_val.csv\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 98\nColumns: 8\n$ mpg          <fct> low, low, low, low, low, low, low, low, low, low, low, lo…\n$ cylinders    <dbl> 8, 8, 8, 8, 8, 6, 6, 6, 6, 8, 8, 4, 4, 4, 8, 8, 8, 4, 8, …\n$ displacement <dbl> 350, 304, 302, 440, 455, 198, 200, 225, 250, 400, 318, 14…\n$ horsepower   <dbl> 165, 150, 140, 215, 225, 95, 85, 105, 100, 175, 150, 72, …\n$ weight       <dbl> 3693, 3433, 3449, 4312, 4425, 2833, 2587, 3439, 3329, 446…\n$ acceleration <dbl> 11.5, 12.0, 10.5, 8.5, 10.0, 15.5, 16.0, 15.5, 15.5, 11.5…\n$ year         <dbl> 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 72, 7…\n$ origin       <fct> american, american, american, american, american, america…\n```\n\n\n:::\n:::\n\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n#### Question: Any concerns about using this training-validation split?\n\n::: {.cell}\n\n```{.html .cell-code  code-fold=\"true\" code-summary=\"Show Answer\"}\nThese sample sizes are starting to get a little small.  Fitted models will have \nhigher variance with only 75% (N = 294) observations and performance in validation \n(with only N = 98 observations) may not be a very precise estimate of true validation \nerror.  We will learn more robust methods in the unit on resampling.\n```\n:::\n\n:::\n\n-----\n\n### Modeling EDA\n\nLet's do some quick modeling EDA to get a sense of the data.  \n\n- We will keep it quick and dirty.\n- First, we should make a function to class Cars since we may open it frequently\n- Even though its simple, still better this way (e.g., what if we decide to change how to handle classing - will only need to make that change in one place!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass_cars <- function(d) { \n  d |> \n    mutate(mpg = factor(mpg, levels = c(\"low\", \"high\")),\n           origin = factor (origin))\n}\n```\n:::\n\n\n-----\n\n- Open train (we don't need validate for modeling EDA)\n- [We are pretending this is a new script....]\n- Class the predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn <- read_csv(here::here(path_data, \"auto_trn.csv\"),\n                     col_type = cols()) |> \n  class_cars() |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 294\nColumns: 8\n$ mpg          <fct> high, high, high, high, high, high, high, high, high, hig…\n$ cylinders    <dbl> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ displacement <dbl> 113.0, 97.0, 97.0, 110.0, 107.0, 104.0, 121.0, 97.0, 140.…\n$ horsepower   <dbl> 95, 88, 46, 87, 90, 95, 113, 88, 90, 95, 86, 90, 70, 76, …\n$ weight       <dbl> 2372, 2130, 1835, 2672, 2430, 2375, 2234, 2130, 2264, 222…\n$ acceleration <dbl> 15.0, 14.5, 20.5, 17.5, 14.5, 17.5, 12.5, 14.5, 15.5, 14.…\n$ year         <dbl> 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 7…\n$ origin       <fct> japanese, japanese, european, european, european, europea…\n```\n\n\n:::\n:::\n\n\n-----\n\n- Bar plot for outcome\n  - Outcome is balanced\n  - Unbalanced outcomes can be more complicated (more on this later)\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> plot_bar(\"mpg\")\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-cars-9-1.png){width=672}\n:::\n:::\n\n\n-----\n\n- Grouped (by `mpg`) box/violin plots for numeric predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  select(where(is.numeric)) |> \n  names() |> \n  map(\\(name) plot_grouped_box_violin(df = data_trn, x = \"mpg\", y = name)) |> \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-cars-10-1.png){width=672}\n:::\n:::\n\n\n-----\n\n- Grouped barplot for origin\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  plot_grouped_barplot_percent(x = \"origin\", y = \"mpg\")\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-cars-11-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  plot_grouped_barplot_percent(x = \"mpg\", y = \"origin\")\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-cars-12-1.png){width=672}\n:::\n:::\n\n\n-----\n\n- Correlation plots for numeric\n- Can dummy code categorical predictors to examine correlations (point biserial, phi) including them\n- We will do this manually using tidyverse here.\n- Set american to be reference level (its the first level)\n- `cylinders`, `displacement`, `horsepower`, and `weight` are all essentially the same construct (big, beefy car!)\n- All are strongly correlated with `mpg`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> \n  mutate(mpg_high = if_else(mpg == \"high\", 1, 0), #<1>\n         origin_japan = if_else(origin == \"japanese\", 1, 0),\n         origin_europe = if_else(origin == \"european\", 1, 0)) |> \n  select(-origin, -mpg) |>  #<2>\n  cor() |> \n  corrplot::corrplot.mixed() \n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-cars-13-1.png){width=672 height=3in}\n:::\n:::\n\n1. If manually coding a binary outcome variable, best practice is to set the positive class to be 1\n2. Remove `origin` and `mpg` after converting to dummy-coded features\n\n-----\n\n## Logistic Regression - Model Building\nNow that we understand our data a little better, let's build some models\n\\\n\\\nLet's also try to simultaneously pretend that we: \n\n1. Are building and selecting a best prediction model that will be evaluated with some additional held out test data (how bad would it have been to split into three sets??!!)\n2. Have an explanatory question about production `year` - Are we more likely to have efficient cars more recently because of improvements in \"technology\", above and beyond broad car characteristics (e.g., the easy stuff like `weight`, `displacement`, etc.)\n\\\n\\\n\nTo be clear, prediction and explanation goals are often separate (though prediction is an important foundation explanation)\n\n-----\n\nEither way, we need a validation set\n\n- Open it and class variables\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val <- read_csv(here::here(path_data, \"auto_val.csv\"),\n                     col_type = cols()) |> \n  class_cars() |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 98\nColumns: 8\n$ mpg          <fct> low, low, low, low, low, low, low, low, low, low, low, lo…\n$ cylinders    <dbl> 8, 8, 8, 8, 8, 6, 6, 6, 6, 8, 8, 4, 4, 4, 8, 8, 8, 4, 8, …\n$ displacement <dbl> 350, 304, 302, 440, 455, 198, 200, 225, 250, 400, 318, 14…\n$ horsepower   <dbl> 165, 150, 140, 215, 225, 95, 85, 105, 100, 175, 150, 72, …\n$ weight       <dbl> 3693, 3433, 3449, 4312, 4425, 2833, 2587, 3439, 3329, 446…\n$ acceleration <dbl> 11.5, 12.0, 10.5, 8.5, 10.0, 15.5, 16.0, 15.5, 15.5, 11.5…\n$ year         <dbl> 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 72, 7…\n$ origin       <fct> american, american, american, american, american, america…\n```\n\n\n:::\n:::\n\n\n-----\n\nLet's predict high `mpg` from car beefiness and `year`\n\\\n\\\nWe likely want to combine the beefy variables into one construct/factor (`beef`)\n\n  - We can use PCA to extract one factor\n  - Input variables should be centered and scaled for PCA.  Easy peasy\n  - PCA will be calculated with `data_trn` during `prep()`.  These statistics from `data_trn` will be used to `bake()`\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- recipe(mpg ~ ., data = data_trn) |> #<1> \n  step_pca(cylinders, displacement, horsepower, weight, \n           options = list(center = TRUE, scale. = TRUE), \n           num_comp = 1, \n           prefix = \"beef_\") #<2>\n```\n:::\n\n1. Using `.` now in the recipe to leave all variables in the feature  matrix.  We can later select the ones we want use for prediction\n2. We will have one PCA component, it will be called `beef_1`\n\n-----\n\nNow, `prep()` the recipe and `bake()` some feature for training and validation sets\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(data_trn)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n:::\n\n\n-----\n\n- Lets skim our training features\n- We can see there are features in there we won't use\n- `mpg`, `year`, and `beef_1` look good\n\n::: {.cell}\n\n```{.r .cell-code}\nfeat_trn |> skim_all()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |         |\n|:------------------------|:--------|\n|Name                     |feat_trn |\n|Number of rows           |294      |\n|Number of columns        |5        |\n|_______________________  |         |\n|Column type frequency:   |         |\n|factor                   |2        |\n|numeric                  |3        |\n|________________________ |         |\n|Group variables          |None     |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate| n_unique|top_counts                 |\n|:-------------|---------:|-------------:|--------:|:--------------------------|\n|origin        |         0|             1|        3|ame: 187, jap: 56, eur: 51 |\n|mpg           |         0|             1|        2|low: 147, hig: 147         |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|  mean|   sd|    p0|   p25|   p50|  p75|  p100| skew| kurtosis|\n|:-------------|---------:|-------------:|-----:|----:|-----:|-----:|-----:|----:|-----:|----:|--------:|\n|acceleration  |         0|             1| 15.54| 2.64|  8.00| 14.00| 15.50| 17.0| 24.80| 0.24|     0.58|\n|year          |         0|             1| 75.94| 3.68| 70.00| 73.00| 76.00| 79.0| 82.00| 0.03|    -1.17|\n|beef_1        |         0|             1|  0.00| 1.92| -2.36| -1.64| -0.85|  1.4|  4.36| 0.63|    -0.96|\n\n\n:::\n:::\n\n\n-----\n\n- Fit the model configuration in train\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lr_2 <- \n  logistic_reg() |> #<1> \n  set_engine(\"glm\") |> #<2>\n  fit(mpg ~ beef_1 + year, data = feat_trn) #<3>\n```\n:::\n\n1. category of algorithm is logistic regression\n2. Set engine to be generalized linear model.  No need to `set_mode(\"classification\") because logistic regression glm is only used for classification\n3. Notice that we explicitly indicate which features to use because our feature matrix has more than just these two in it because we used `.` in our recipe and our datasets have more columns.\n\n-----\n\nLet's look at the logistic model and its parameter estimates from train\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lr_2 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -23.4      5.55       -4.23 2.38e- 5\n2 beef_1        -2.49     0.316      -7.87 3.49e-15\n3 year           0.292    0.0718      4.07 4.71e- 5\n```\n\n\n:::\n:::\n\n\n\n\\\n\\\n\n$Pr(high|(beef, year)) = \\frac{e^{b0 + b1*beef + b2*year}}{1 + e^{b0 + b1*beef  + b2*year}}$\n\n-----\n\nUse `predict()` to get $Pr(high|year)$ (**or predicted class**, more on that later) directly from the model for any data\n\n- First make a dataframe for feature values for prediction\n- We will use this for a plot\n- Get probabilities for a range of years (incrementing by .1 year)\n- Hold beef constant at its mean\n\n::: {.cell}\n\n```{.r .cell-code}\np_high <- \n  tibble(year = seq(min(feat_val$year), max(feat_val$year), .1),\n         beef_1 = mean(feat_val$beef_1)) \n```\n:::\n\n\n-----\n\n`predict()` will:\n\n- Give us probabilities for high and low (`type = \"prob\"`).  We will select out the low columns\n- Also provides the 95% conf int around these probabilities (`type = \"conf_int\"`)\n- Ambiguity is removed by their choice to label by the actual outcome labels (Nice!)\n- We will bind these predictions to the dataframe we made above\n\n::: {.cell}\n\n```{.r .cell-code}\np_high <- p_high |> \n  bind_cols(predict(fit_lr_2, new_data = p_high, type = \"prob\")) |> \n  bind_cols(predict(fit_lr_2, new_data = p_high, type = \"conf_int\")) |> \n  select(-.pred_low, -.pred_lower_low, -.pred_upper_low) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 121\nColumns: 5\n$ year             <dbl> 70.0, 70.1, 70.2, 70.3, 70.4, 70.5, 70.6, 70.7, 70.8,…\n$ beef_1           <dbl> 0.008438869, 0.008438869, 0.008438869, 0.008438869, 0…\n$ .pred_high       <dbl> 0.04733721, 0.04867270, 0.05004389, 0.05145161, 0.052…\n$ .pred_lower_high <dbl> 0.01495325, 0.01557265, 0.01621651, 0.01688572, 0.017…\n$ .pred_upper_high <dbl> 0.1398943, 0.1419807, 0.1440989, 0.1462495, 0.1484331…\n```\n\n\n:::\n:::\n\n\n-----\n\nHere is a quick look at the head\n\n::: {.cell}\n\n```{.r .cell-code}\np_high |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n   year  beef_1 .pred_high .pred_lower_high .pred_upper_high\n  <dbl>   <dbl>      <dbl>            <dbl>            <dbl>\n1  70   0.00844     0.0473           0.0150            0.140\n2  70.1 0.00844     0.0487           0.0156            0.142\n3  70.2 0.00844     0.0500           0.0162            0.144\n4  70.3 0.00844     0.0515           0.0169            0.146\n5  70.4 0.00844     0.0529           0.0176            0.148\n6  70.5 0.00844     0.0544           0.0183            0.151\n```\n\n\n:::\n:::\n\n\n-----\n\nCan plot this relationship using the predicted probabilities\n\n- plot both prediction line\n- and observed Y (with a little jitter)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = feat_val, aes(x = year, y = as.numeric(mpg) - 1), \n             position = position_jitter(height = 0.05, width = 0.05)) +\n  geom_line(data = p_high, mapping = aes(x = year, y = .pred_high)) +\n  geom_ribbon(data = p_high, \n              aes(x = year, ymin = .pred_lower_high, ymax = .pred_upper_high), \n              linetype = 2, alpha = 0.1) +\n  scale_x_continuous(name = \"Production Year (19XX)\", breaks = seq(70,82,2)) +\n  ylab(\"Pr(High MPG)\")\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-lr-6-1.png){width=672}\n:::\n:::\n\n\n-----\n\nCan plot odds instead of probabilities using odds equation and coefficients from the model\n\n- $odds = \\frac{Pr(positive\\:class|x_1)} {1 - Pr(positive\\:class|x_1)}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_high <- p_high |> \n  mutate(.odds_high = .pred_high / (1 - .pred_high),\n         .odds_lower_high = .pred_lower_high / (1 - .pred_lower_high),\n         .odds_upper_high = .pred_upper_high / (1 - .pred_upper_high))\n```\n:::\n\n\n-----\n\n- NOTE: Harder to also plot raw data as scatter plot when plotting odds vs. probabilities.  Maybe use second Y axis?\n\n::: {.cell}\n\n```{.r .cell-code}\np_high |> \n  ggplot() +\n    geom_line(mapping = aes(x = year, y = .odds_high)) +\n    geom_ribbon(aes(x = year, ymin = .odds_lower_high, ymax = .odds_upper_high), \n                linetype = 2, alpha = 0.1) +\n    scale_x_continuous(name = \"Production Year (19XX)\", breaks = seq(70, 82, 2)) +\n    ylab(\"Odds(High MPG)\")\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n-----\n\nOdds ratio for year:\n\n- $Odds\\:ratio  = e^{c*\\beta_1}$\n\\\n\\\n\nGet the parameter estimate/coefficient from the model\n\n- For every one year increase (holding beef constant), the odds of a car being categorized as high mpg increase by a factor of 1.3394301\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(get_estimate(fit_lr_2, \"year\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.33943\n```\n\n\n:::\n:::\n\n\n\n-----\n\nYou can do this for other values of c as well (not really needed here because a 1 year unit makes sense)\n\n- $Odds\\:ratio  = e^{c*\\beta_1}$\n- For every 10 year increase (holding beef constant), the odds of a car being categorized as high mpg increase by a factor of 18.5866278\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(10 * get_estimate(fit_lr_2, \"year\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18.58663\n```\n\n\n:::\n:::\n\n\n-----\n\nTesting parameter estimates\n\n- `tidy()` provides z-tests for the parameter estimates\n- This is NOT the recommended statistical test from 610\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lr_2 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -23.4      5.55       -4.23 2.38e- 5\n2 beef_1        -2.49     0.316      -7.87 3.49e-15\n3 year           0.292    0.0718      4.07 4.71e- 5\n```\n\n\n:::\n:::\n\n\n-----\n\nThe preferred test for parameter estimates from logistic regression is the likelihood ratio test\n\n- You can get this using `Anova()` from the `car` package (as you learn in 610/710)\n- The glm object is returned using $fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::Anova(fit_lr_2$fit, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\n       LR Chisq Df Pr(>Chisq)    \nbeef_1  239.721  1  < 2.2e-16 ***\nyear     20.317  1  6.562e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n-----\n\nAccuracy is a very common performance metric for classification models\n\\\n\\\nHow accurate is our two feature model?\n\n- Note the use of `type = \"class\"`\n\\\n\\\n\nWe can calculate accuracy in the training set.  This may be somewhat overfit and optimistic!\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_trn$mpg, predict(fit_lr_2, feat_trn, type = \"class\")$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9217687\n```\n\n\n:::\n:::\n\n\n-----\n\nAnd in the validation set (better estimate of performance in new data.  Though if we use this set multiple times to select a best configuration,  it will become overfit eventually too!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_val$mpg, predict(fit_lr_2, feat_val, type = \"class\")$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8877551\n```\n\n\n:::\n:::\n\n\\\n\\\nYou can see some evidence of over-fitting to the training set in that model performance is a bit lower in validation\n\n-----\n\nLet's take a look at the decision boundary for this model in the validation set\n\\\n\\\nWe need a function to plot the decision boundary **because we will use it repeatedly** to compare decision boundaries across statistical models\n\n- Displayed here as another example of a function that uses quoted variable names\n- This function is useful for book examples with two features.  \n- Only can be used with two features, so not that useful in real life!\n- Not included in my function scripts (just here to helpy you understand the material)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_decision_boundary <- function(data, model, x_names, y_name, n_points = 100) {\n  \n  preds <- crossing(X1 = seq(min(data[[x_names[1]]]), \n                                 max(data[[x_names[1]]]), \n                                 length = n_points),\n                   X2 = seq(min(data[[x_names[2]]]), \n                                 max(data[[x_names[2]]]), \n                                 length = n_points))\n  names(preds) <- x_names\n  preds[[y_name]] <- predict(model, preds)$.pred_class\n  preds[[y_name]] <- as.numeric(preds[[y_name]])-1\n  \n  ggplot(data = data, \n         aes(x = .data[[x_names[1]]], \n             y = .data[[x_names[2]]], \n             color = .data[[y_name]])) +\n    geom_point(size = 2, alpha = .5) +\n    geom_contour(data = preds, \n                 aes(x = .data[[x_names[1]]], \n                     y = .data[[x_names[2]]], \n                     z = .data[[y_name]]), \n                 color = \"black\", breaks = .5, linewidth = 2) +\n    labs(x = x_names[1], y = x_names[2], color = y_name) +\n    coord_obs_pred()\n}\n```\n:::\n\n\n-----\n\nLogistic regression produces a linear decision boundary when you consider a scatter plot of the points by the two features.  \n\n- Points on one side of the line are assigned to one class and points on the other side of the line are assigned to the other class.  \n- This decision boundary would be a plane if there were three features  Harder to visualize in higher dimensional space.\n- We will contrast this decision boundary from logistic regression with other statistical algorithms in a bit.\n\\\n\\\n\nHere is the decision boundary for this model in both train and validation sets\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- feat_trn |> \n  plot_decision_boundary(fit_lr_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\np_val <- feat_val |> \n  plot_decision_boundary(fit_lr_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\ncowplot::plot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-lr-14-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\nWhat if you wanted to try to improve our predictions?\n\n- You could find the best set of covariates to test the effect of year. [Assuming the best covariates are those that account for the most variance in mpg]?\n- For either prediction or explanation, you need to find this best model\n\\\n\\\n\nWe can compare model performance in validation set to find this best model\n\n- We can use that one for our prediction goal\n- We can test the effect of year in that model for our explanation goal\n  - This is a principled way to decide on the best model for our explanatory goal (vs. p-hacking)\n  - We get to explore and we end up with the best model to provide our focal test\n  \n-----\n\nLet's quickly fit another model we might have considered.   \n\n- This model will contain the 4 variables from our PCA but as individual features rather than one PCA component score (`beef`)\n- Make features for this model \n  - No feature engineering needed because raw variables are all numeric already)\n  - We will give the features dfs new names to retain the old features that included `beef_1`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_raw <- recipe(mpg ~ ., data = data_trn) \n\nrec_raw_prep <- rec_raw |> \n  prep(data_trn)\n\nfeat_raw_trn <- rec_raw_prep |> \n  bake(data_trn)\n\nfeat_raw_val <- rec_raw_prep |> \n  bake(data_val)\n```\n:::\n\n\n-----\n\nFit PCA features individually + year\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lr_raw <- \n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(mpg ~ cylinders + displacement + horsepower + weight + year, \n      data = feat_raw_trn)\n```\n:::\n\n\n----- \n\nWhich is the best prediction model?\n\n- Beef PCA\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_val$mpg, predict(fit_lr_2, feat_val)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8877551\n```\n\n\n:::\n:::\n\n\n- Individual raw features from PCA\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_raw_val$mpg, predict(fit_lr_raw, feat_raw_val)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8673469\n```\n\n\n:::\n:::\n\n\\\n\\\nThe PCA beef model fits best.  The model with individual features likely increases overfitting a bit but doesn't yield a reduction in bias because all the other variables are so highly correlated.\n\n-----\n\n`fit_lr_2` is your choice for best model for prediction (at least it is descriptively better)\n\\\n\\\nIf you had an explanatory question about `year`, how would you have chosen between these two tests of `year` in these two different but all reasonable models\n\n- You might have chose the model with individual features because the effect of year is stronger.  \n- That is NOT the model that comes closes to the DGP.  \n- We believe the appropriate model is the beef model that has higher overall accuracy!\n- This is a start for us to start to consider the use of resampling methods to make decisions about how to best pursue explanatory goals.\n- Could you now test your `year` effect in the full sample?  Let's discuss.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::Anova(fit_lr_2$fit, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\n       LR Chisq Df Pr(>Chisq)    \nbeef_1  239.721  1  < 2.2e-16 ***\nyear     20.317  1  6.562e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::Anova(fit_lr_raw$fit, type = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\n             LR Chisq Df Pr(>Chisq)    \ncylinders      0.4199  1    0.51697    \ndisplacement   1.3239  1    0.24990    \nhorsepower     6.4414  1    0.01115 *  \nweight        15.6669  1  7.553e-05 ***\nyear          27.0895  1  1.942e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n-----\n\n## K Nearest Neighbors - A Conceptual Overview\n\nLet's switch gears to a non-parametric method we already know - KNN.  \n\n- KNN can be used as a classifier as well as for regression problems\n\n- KNN tries to determine conditional class possibilities for any set of features by looking at observed classes for similar values of for these features in the training set\n\n-----\n\nThis figure illustrates the application of 3-NN to a small sample training set (N = 12) with 2 predictors \n\n- For test observation X in the left panel, we would predict class = blue because blue is the majority class (highest probability) among the 3 nearest training observations\n- If we calculated these probabilities for all possible combinations of the two predictors in the training set, it would yield the decision boundaries depicted in the right panel\n\n![](figs/unit4_knn_example.png){height=4in}\n\n-----\n\nKNN can produce complex decision boundaries\n\n- This makes it flexible (can reduce bias)\n- This makes it susceptible to variance/overfitting problems\n\n\\\n\\\nRemember that we can control this bias-variance trade-off with K.  \n\n- As K increases, variance reduces (but bias may increase).  \n- As K decreases, bias may be reduced but variance increases. \n- Choose a K that produces good performance in new (validation) data\n\n-----\n\nThese figures depict the KNN (and Bayes classifier) decision boundaries for the earlier simulated 2 class problem with `X1` and `X2`\n\n- K = 10 appears to provide the sweet spot b/c it closely approximates the Bayes decision boundary\n- Of course, you wouldn't know the true Bayes decision boundary if the data were real (not simulated)\n- But K = 10 would also yield the lowest test error (which is how it should be chosen)\n  - Bayes classifier test error: .1304\n  - K = 10 test error: .1363\n  - K = 1 test error: .1695\n  - K = 100 test err: .1925\n\n\n![](figs/unit4_knn_k.png){height=4in}\n\n![](figs/unit4_knn_k_2.png){height=4in}\n\n-----\n\nYou can NOT make the decision about K based on training error\n\\\n\\\nThis figure depicts training and test error for simulated data example as function of 1/K\n\n- Training error decreases as 1/K increases.  At 1 (K=1) training error is 0\n- Test error shows expected inverted U\n\n  - For high K (left side), error is high because of high variance\n  - As move right (lower K), variance is reduced rapidly with little increase in bias.  Error is reduced.\n  - Eventually, there is diminishing return from reducing variance but bias starts to increase rapidly.  Error increases again.\n\n![](figs/unit4_knn_error.png){height=4in}\n\n-----\n\n## A Return to Cars - Now with KNN\n\nLet's demonstrate KNN using the Cars dataset\n\n- Calculate `beef_1` PCA component\n- Scale both features\n- Make train and validation feature matrices\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- recipe(mpg ~ ., data = data_trn) |> \n  step_pca(cylinders, displacement, horsepower, weight,\n           options = list(center = TRUE, scale. = TRUE), \n           num_comp = 1, prefix = \"beef_\") |> \n  step_scale(year, beef_1)\n\nrec_prep <- rec |> \n  prep(data = data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(data_trn)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n:::\n\n\n-----\n\nFit models with varying K.  \n\n- K = 1\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_1nn_2 <- \n  nearest_neighbor(neighbors = 1) |> \n  set_engine(\"kknn\") |>\n  set_mode(\"classification\") |>  #<1>\n  fit(mpg ~ beef_1 + year, data = feat_trn)\n```\n:::\n\n1. Notice that we now use `set_mode(\"classification\")`\n\n- K = 5\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_5nn_2 <- \n  nearest_neighbor(neighbors = 5) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |>   \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n```\n:::\n\n\n- K = 10\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_10nn_2 <- \n  nearest_neighbor(neighbors = 10) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |>   \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n```\n:::\n\n\n\n- K = 20\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_20nn_2 <- \n  nearest_neighbor(neighbors = 20) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |>   \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n```\n:::\n\n\n-----\n\nOf course, training accuracy goes down with decreasing flexibility as k increases\n\n- K = 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_trn$mpg, predict(fit_1nn_2, feat_trn)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n- K = 5\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_trn$mpg, predict(fit_5nn_2, feat_trn)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9489796\n```\n\n\n:::\n:::\n\n\n- K = 10\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_trn$mpg, predict(fit_10nn_2, feat_trn)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9455782\n```\n\n\n:::\n:::\n\n\n- K = 20\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_trn$mpg, predict(fit_20nn_2, feat_trn)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9285714\n```\n\n\n:::\n:::\n\n\n-----\n\nIn contrast, validation accuracy first increases and then eventually decreases as k increase.\n\n- K = 1\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_val$mpg, predict(fit_1nn_2, feat_val)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8163265\n```\n\n\n:::\n:::\n\n\n- K = 5\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_val$mpg, predict(fit_5nn_2, feat_val)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.877551\n```\n\n\n:::\n:::\n\n\n- K = 10 (preferred based on validation accuracy)\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_val$mpg, predict(fit_10nn_2, feat_val)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8877551\n```\n\n\n:::\n:::\n\n\n- K = 20\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_val$mpg, predict(fit_20nn_2, feat_val)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8673469\n```\n\n\n:::\n:::\n\n\n-----\n\nLet's look at the decision boundaries for 10-NN in training and validation sets\n\n- A very complex decision boundary\n- Clearly trying hard to segregate the points\n- In Ames, the relationships were non-linear and therefore KNN did much better than the linear model\n- Here, the decision boundary is pretty linear so the added flexibility of KNN doesn't get us much.  \n  - Maybe we gain a little in bias reduction but lose a little in overfitting\n  - Ends up performing comparable to logistic regression (a generalized linear model)\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- feat_trn |> \n  plot_decision_boundary(fit_10nn_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\")\n\np_val <- feat_val |> \n  plot_decision_boundary(fit_10nn_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\")\n\ncowplot::plot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-knn-9-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: How do you think the parametric logistic regression compares to the non-parametric KNN with respect to explanatory goals?  Consider our (somewhat artificial) question about the effect of `year`.\n\n::: {.cell}\n\n```{.html .cell-code  code-fold=\"true\" code-summary=\"Show Answer\"}\nThe logistic regression provides coefficients (parameter estimates) that can be \nused to describe changes in probability, odds and odds ratio associated with \nchange in year.  These parameter estimates can be tested via inferential procedures.  \nKNN does not provide any parameter estimates.  With KNN, we can visualize decision boundary \n(only in 2 or three dimensions) or the predicted outcome by any feature, controlling for \nother features but these relationships may be complex in shape.  \nOf course, if the relationships are complex, we might not want to hide that.  \nWe will learn more about feature importance for explanation in a later unit.\n```\n:::\n\n:::\n\n-----\n\nPlot of probability of high_mpg by year, holding `beef_1` constant at its mean based on 10-NN\n\n- Get $Pr(high|year)$ holding beef constant at its mean\n- `predict()` returns probabilities for high and low.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_high <- \n  tibble(year = seq(min(feat_val$year), max(feat_val$year), .1),\n         beef_1 = mean(feat_val$beef_1)) \n\np_high <- p_high |> \n  bind_cols(predict(fit_10nn_2, p_high, type = \"prob\")) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 33\nColumns: 4\n$ year       <dbl> 18.99759, 19.09759, 19.19759, 19.29759, 19.39759, 19.49759,…\n$ beef_1     <dbl> 0.004400185, 0.004400185, 0.004400185, 0.004400185, 0.00440…\n$ .pred_low  <dbl> 0.96, 0.99, 0.99, 0.99, 0.97, 0.87, 0.81, 0.81, 0.81, 0.81,…\n$ .pred_high <dbl> 0.04, 0.01, 0.01, 0.01, 0.03, 0.13, 0.19, 0.19, 0.19, 0.19,…\n```\n\n\n:::\n:::\n\n\n-----\n\n- Plot it\n- In a later unit, we will learn about feature ablation that we can combine with model comparisons to potentially test predictor effects in non-parametric models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = feat_val, aes(x = year, y = as.numeric(mpg) - 1), \n             position = position_jitter(height = 0.05, width = 0.05)) +\n  geom_line(data = p_high, mapping = aes(x = year, y = .pred_high)) +\n  scale_x_continuous(name = \"Production Year (19XX)\", \n                     breaks = seq(0, 1, length.out = 7), \n                     labels = as.character(seq(70, 82, length.out = 7))) +\n  ylab(\"Pr(High MPG)\")\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-knn-11-1.png){width=672}\n:::\n:::\n\n\n-----\n\n## Linear Discriminant Analysis\nLDA models the distributions of the Xs separately for each class\n\\\n\\\nThen uses Bayes theorem to estimate $Pr(Y = k | X)$ for each k and assigns the  observation to the class with the highest probability\n\n$Pr(Y = k|X) = \\frac{\\pi_k * f_k(X)}{\\sum_{l = 1}^{K} f_l(X)}$\n\nwhere\n\n- $\\pi_k$ is the prior probability that an observation comes from class k (estimated from frequencies of k in training)\n- $f_k(X)$ is the density function of X for an observation from class k\n  -  $f_k(X)$ is large if there is a high probability that an observation in class k has that set of values for X and small if that probability is low\n  - $f_k(X)$ is difficult to estimate unless we make some simplifying assumptions (i.e., X is multivariate normal and common covariance matrix ($\\sum$) across K classes)\n  - With these assumptions, we can estimate $\\pi_k$, $\\mu_k$, and $\\sigma^2$ from the training set and calculate $Pr(Y = k|X)$ for each k\n\n-----\n\nWith a single feature, the probability of any class k, given X is:\n\n- $Pr(Y = k|X) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2)}{\\sum_{l=1}^{K}\\pi_l\\frac{1}{2\\sigma^2}\\exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)}$\n\n- LDA is a parametric model, but is it interpretable?\n\n-----\n\nApplication of LDA to Cars data set with two predictors\n\\\n\\\nNotice that LDA produces linear decision boundary (see @ISL for formula for discriminant function derived from the probability function on last slide)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- recipe(mpg ~ ., data = data_trn) |> \n  step_pca(cylinders, displacement, horsepower, weight, \n           options = list(center = TRUE, scale. = TRUE), \n           num_comp = 1, \n           prefix = \"beef_\")\n\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(data_trn)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n:::\n\n\n-----\n\n- Need to load the `discrm` package\n- Lets look at how the function is called\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(discrim, exclude = \"smoothness\")\n\ndiscrim_linear() |> \n  set_engine(\"MASS\") |> \n  translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Discriminant Model Specification (classification)\n\nComputational engine: MASS \n\nModel fit template:\nMASS::lda(formula = missing_arg(), data = missing_arg())\n```\n\n\n:::\n:::\n\n\n-----\n\nFit the LDA in train\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lda_2 <- \n  discrim_linear() |> \n  set_engine(\"MASS\") |> \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n```\n:::\n\n\n-----\n\nAccuracy and decision boundary\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_val$mpg, predict(fit_lda_2, feat_val)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8469388\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- feat_trn |> \n  plot_decision_boundary(fit_lda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\np_val <- feat_val |> \n  plot_decision_boundary(fit_lda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\ncowplot::plot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-da-4-1.png){width=672}\n:::\n:::\n\n\n-----\n\n## Quadratic Discriminant Analysis\n\nQDA relaxes one restrictive assumption of LDA\n  \n- Still required multivariate normal X\n- **But it allows each class to have its own $\\sum$**\n- This makes it:\n  - More flexible\n  - Able to model non-linear decision boundaries (see formula for discriminant in @ISL)\n  - But requires substantial increase in parameter estimation (more potential to overfit)\n\n-----\n\nApplication of RDA (Regularized Discriminant Analysis) algorithm to Car data set with two features\n\n- The algorithm that is available in tidymodels is actually a regularized discriminant analysis, `rda()` from the `klaR` package\n- There are two hyperparameters, `frac_common_cov` and `frac_identity`, that can each vary between 0 - 1 \n  - When `frac_common_cov = 1` and `frac_identity = 0`, this is an LDA\n  - When `frac_common_cov = 0` and `frac_identity = 0`, this is a QDA\n  - These hyperparameters can be tuned to different values to improve the fit dependent on the true DGP\n  - More on hyperparameter tuning in unit 5\n  - This is a flexible algorithm that likely replaces the need to fit separate LDA and QDA models\n- see https://discrim.tidymodels.org/reference/discrim_regularized.html\n\n-----\n\nHere is a true QDA using `frac_common_cov = 1` and `frac_identity = 0`\n\n::: {.cell}\n\n```{.r .cell-code}\ndiscrim_regularized(frac_common_cov = 0, frac_identity = 0) |> \n  set_engine(\"klaR\") |> \n  translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRegularized Discriminant Model Specification (classification)\n\nMain Arguments:\n  frac_common_cov = 0\n  frac_identity = 0\n\nComputational engine: klaR \n\nModel fit template:\nklaR::rda(formula = missing_arg(), data = missing_arg(), lambda = 0, \n    gamma = 0)\n```\n\n\n:::\n:::\n\n\n-----\n\n- Now fit it\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_qda_2 <- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) |> \n  set_engine(\"klaR\") |> \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n```\n:::\n\n\n-----\n\nAccuracy and decision boundary\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_val$mpg, \n             predict(fit_qda_2, feat_val)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.877551\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- feat_trn |> \n  plot_decision_boundary(fit_qda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\np_val <- feat_val |> \n  plot_decision_boundary(fit_qda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\ncowplot::plot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/u4-da-7-1.png){width=672}\n:::\n:::\n\n\n-----\n\n## Comparisons between these four classifiers\n\n- Both logistic and LDA are linear functions of X and therefore produce linear decision boundaries\n\n- LDA makes additional assumptions about X (multivariate normal and common $\\sum$) beyond logistic regression.  Relative performance is based on the quality of this assumption\n\n- QDA relaxes the LDA assumption about common $\\sum$ (and RDA can relax it partially)\n  - This also allows for nonlinear decision boundaries including 2-way interactions among features\n  - QDA is therefore more flexible, which means possibly less bias but more potential for overfitting\n\n- Both QDA and LDA assume multivariate normal X so *may* not accommodate categorical predictors very well.  Logistic and KNN do accommodate categorical predictors\n\n- KNN is non-parametric and therefore the most flexible\n  - Can also handle interactions and non-linear effects natively (with feature engineering)\n  - Increased overfitting, decreased bias?\n  - Not very interpretable.  But LDA/QDA, although parametric, aren't as interpretable as logistic regression\n\n- Logistic regression fails when classes are perfectly separated (but does that ever happen?) and is less stable when classes are well separated\n\n- LDA, KNN, and QDA naturally accommodate more than two classes  \n  - Logistic requires additional tweak (**Briefly describe: multiple one vs other classes models approach**)\n  \n- Logistic regression requires relatively large sample sizes.  LDA may perform better with smaller sample sizes if assumptions are met\n\n-----\n  \n## A quick tour of many classifiers\n\nThe Cars dataset had strong predictors and a mostly linear decision boundary for the two predictors we considered\n\n- This will not be true in many cases\n- Let's consider a more complex two predictor decision boundary in the **circle** dataset from the `mlbench` package (lots of cool datasets for ML)\n- This will hopefully demonstrate that the key is to have a algorithm that can model the DGP\n- There is NO best algorithm\n- The best algorithm depends on \n  - The DGP\n  - The goal (prediction vs. explanation)\n\n-----\n\nThis will also demonstrate the power of tidymodels to allow us to fit many different statistical algorithms which all have their own syntax using a common syntax provided by tidymodels.  \n\n- This example has been adapted to tidymodels from a demonstration by [Michael Hahsler](https://michael.hahsler.net/SMU/EMIS7332/R/viz_classifier.html)\n- Simulate train and test data\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlbench, include.only = \"mlbench.circle\")\n\nset.seed(20140102)\ndata_trn <- as_tibble(mlbench.circle(200)) |> \n  rename(x_1 = x.1, x_2 = x.2) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 200\nColumns: 3\n$ x_1     <dbl> -0.514721263, 0.763312056, 0.312073042, 0.162981535, -0.320294…\n$ x_2     <dbl> 0.47513532, 0.65803777, -0.89824011, 0.38680494, -0.47313964, …\n$ classes <fct> 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2,…\n```\n\n\n:::\n\n```{.r .cell-code}\ntest <- as_tibble(mlbench.circle(200)) |> \n  rename(x_1 = x.1, x_2 = x.2)\n```\n:::\n\n\n-----\n\n- Plot train data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn |> ggplot(aes(x = x_1, y = x_2, color = classes)) +\n  geom_point(size = 2, alpha = .5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n-----\n\n### Logistic Regression\n\n- Fit\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lr_bench <- \n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(classes ~ x_1 + x_2, data = data_trn)\n```\n:::\n\n\n-----\n\n- Accuracy and Decision Boundary\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(test$classes, \n             predict(fit_lr_bench, test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.62\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- data_trn |> \n  plot_decision_boundary(fit_lr_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\np_test <- test |> \n  plot_decision_boundary(fit_lr_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-29-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n### KNN with K = 5 (somewhat arbitrary default)\n\n- Fit\n- Note same syntax as logistic regression (and for all others!)\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_knn_bench <- \n  nearest_neighbor() |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |> \n  fit(classes ~ x_1 + x_2, data = data_trn)\n```\n:::\n\n\n-----\n\n- Accuracy and Decision Boundary\n- Note same syntax as logistic regression (and for all others!)\n- See what KNN can do relative to LR when the boundaries are non-linear!\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(test$classes, \n             predict(fit_knn_bench, test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.985\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- data_trn |> \n  plot_decision_boundary(fit_knn_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) + \n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\np_test <- test |> \n  plot_decision_boundary(fit_knn_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-30-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n### Linear Discriminant Analysis\n\n- Fit\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lda_bench <- \n  discrim_linear() |> \n  set_engine(\"MASS\") |> \n  fit(classes ~ x_1 + x_2, data = data_trn)\n```\n:::\n\n\n-----\n\n- Accuracy and Decision Boundary\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(test$classes, \n             predict(fit_lda_bench, test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.62\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- data_trn |> \n  plot_decision_boundary(fit_lda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\np_test <- test |> \n  plot_decision_boundary(fit_lda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-31-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n### Regularized Discriminant Analysis\n\n- Fit\n- Letting `rda()` select optimal hyperparameter values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_rda_bench <- \n  discrim_regularized() |> \n  set_engine(\"klaR\") |>\n  fit(classes ~ x_1 + x_2, data = data_trn)\n```\n:::\n\n\n-----\n\n- Accuracy and Decision Boundary\n- Less overfitting?\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(test$classes, \n             predict(fit_rda_bench, test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.985\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- data_trn |> \n  plot_decision_boundary(fit_rda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) + \n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\np_test <- test |> \n  plot_decision_boundary(fit_rda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) + \n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-32-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n### Naive Bayes Classifier\n\nThe Naïve Bayes classifier is a simple probabilistic classifier which is based on Bayes theorem\n\n- But, we assume that the predictor variables are conditionally independent of one another given the response value\n- This algorithm can be fit with either `klaR` or `naivebayes` engines\n- There are many [tutorials](https://uc-r.github.io/naive_bayes) available on this classifier\n\n- Fit it\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_bayes_bench <- \n  naive_Bayes() |> \n  set_engine(\"naivebayes\") |>\n  fit(classes ~ x_1 + x_2, data = data_trn)\n```\n:::\n\n\n-----\n\n- Accuracy and Decision Boundary\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(test$classes, \n             predict(fit_bayes_bench, test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.99\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- data_trn |> \n  plot_decision_boundary(fit_bayes_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\np_test <- test |> \n  plot_decision_boundary(fit_bayes_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-33-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n### Random Forest\n\nRandom Forest is a variant of decision trees\n\n- It uses **bagging** which involves resampling the data to produce many trees and then aggregating across trees for the final classification\n- We will discuss Random Forest in a later unit\n- Here we fit it with defaults for its hyperparameters\n  \n- Fit it\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_rf_bench <- \n  rand_forest() |> \n  set_engine(\"ranger\") |> \n  set_mode(\"classification\") |> \n  fit(classes ~ x_1 + x_2, data = data_trn)\n```\n:::\n\n\n-----\n\n- Accuracy and Decision Boundary\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(test$classes, \n             predict(fit_rf_bench, test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.985\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- data_trn |> \n  plot_decision_boundary(fit_rf_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\np_test <- test |> \n  plot_decision_boundary(fit_rf_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-34-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n### Neural networks\n\nIt is easy to fit a [single layer neural network](https://parsnip.tidymodels.org/reference/mlp.html) \n\n- We do this below with varying number of hidden units and all other hyperparameters set to defaults\n- We will have a unit on this later in the semester\n\n-----\n\n#### Single Layer NN with 1 hidden unit:\n\n- Fit\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_nn1_bench <- \n mlp(hidden_units = 1) |> \n  set_engine(\"nnet\") |> \n  set_mode(\"classification\") |> \n  fit(classes ~ x_1 + x_2, data = data_trn)\n```\n:::\n\n\n-----\n\n- Accuracy and Decision Boundary\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(test$classes, \n             predict(fit_nn1_bench, test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.66\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- data_trn |> \n  plot_decision_boundary(fit_nn1_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\np_test <- test |> \n  plot_decision_boundary(fit_nn1_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-35-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n#### Single Layer NN with 2 hidden units\n\n- Fit\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_nn2_bench <- \n mlp(hidden_units = 2) |> \n  set_engine(\"nnet\") |> \n  set_mode(\"classification\") |> \n  fit(classes ~ x_1 + x_2, data = data_trn)\n```\n:::\n\n\n-----\n\n- Accuracy and Decision Boundary\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(test$classes, \n             predict(fit_nn2_bench, test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.765\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- data_trn |> \n  plot_decision_boundary(fit_nn2_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) + \n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\np_test <- test |> \n  plot_decision_boundary(fit_nn2_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-36-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n#### Single Layer NN with 5 hidden units\n\n- Fit\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_nn5_bench <- \n mlp(hidden_units = 5) |> \n  set_engine(\"nnet\") |> \n  set_mode(\"classification\") |> \n  fit(classes ~ x_1 + x_2, data = data_trn)\n```\n:::\n\n\n-----\n\n- Accuracy and Decision Boundary\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(test$classes, \n             predict(fit_nn5_bench, test)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.96\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_train <- data_trn |> \n  plot_decision_boundary(fit_nn5_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\np_test <- test |> \n  plot_decision_boundary(fit_nn5_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400) +\n  coord_obs_pred()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n```{.r .cell-code}\ncowplot::plot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-37-1.png){width=672 height=3in}\n:::\n:::\n\n\n-----\n\n## Discussion\n\n\n### Anouncements\n\n- Quiz performance - 95%\n- Please meet with TA or me if you can't generate predictions from your models\n- Room 121 again in two weeks\n- And the winner is.....\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_csv(here::here(path_data, \"lunch_004.csv\")) |> \n  print_kbl()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): name\ndbl (1): acc_test\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; \"><table class=\"table table-striped table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> name </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> acc_test </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> rf </td>\n   <td style=\"text-align:right;\"> 0.80 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> wang </td>\n   <td style=\"text-align:right;\"> 0.80 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Phipps </td>\n   <td style=\"text-align:right;\"> 0.80 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> dong </td>\n   <td style=\"text-align:right;\"> 0.79 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> li </td>\n   <td style=\"text-align:right;\"> 0.79 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> jiang </td>\n   <td style=\"text-align:right;\"> 0.78 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Shea </td>\n   <td style=\"text-align:right;\"> 0.77 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> simental </td>\n   <td style=\"text-align:right;\"> 0.77 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> cherry </td>\n   <td style=\"text-align:right;\"> 0.77 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> crosby </td>\n   <td style=\"text-align:right;\"> 0.77 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.77 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.76 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.72 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.69 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0.66 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n-----\n\n### Probability, odds, and log-odds\n\n- DGP on probability\n- what is irreducible error for classification?\n- DGP on X1 - draw it with varying degrees of error  \n\n-----\n\n- DGP and error on two features\n\n\n![](figs/unit4_two_class_decision_boundry.png){height=4in}\n\n-----\n\n- Probability vs. odds vs. log-odds\n- How to interpret parameter estimates (effects of X)\n\n-----\n\n### Comparisons across algorithms\n\nLogistic regression models DGP condition probabilities using logistic function\n\n- Get parameter estimates for effects of X \n- Makes strong assumptions shape of DGP - linear on log-odds(Y)\n- Yields linear decision boundary\n- Better for binary outcomes but can do more than two levels\n- Needs numeric features but can dummy code categorical variables (as with lm)\n- Problems when classes are fully separable (or even mostly separable)\n\n-----\n\nLDA uses Bayes theorem to estimate condition probability\n\n- LDA models the distributions of the Xs separately for each class\n- Then uses Bayes theorem to estimate $Pr(Y = k | X)$ for each k and assigns the  observation to the class with the highest probability\n\n$Pr(Y = k|X) = \\frac{\\pi_k * f_k(X)}{\\sum_{l = 1}^{K} f_l(X)}$\n\nwhere\n\n- $\\pi_k$ is the prior probability that an observation comes from class k (estimated from frequencies of k in training)\n- $f_k(X)$ is the density function of X for an observation from class k\n  -  $f_k(X)$ is large if there is a high probability that an observation in class k has that set of values for X and small if that probability is low\n  - $f_k(X)$ is difficult to estimate unless we make some simplifying assumptions\n  - X is multivariate normal\n  - Common covariance matrix ($\\sum$) across K classesj\n  - With these assumptions, we can estimate $\\pi_k$, $\\mu_k$, and $\\sigma^2$ from the training set and calculate $Pr(Y = k|X)$ for each k\n  \n-----\n\n- Parametric model but parameters not useful for interpretation of effects of X\n- Linear decision boundary  \n- Assumptions about multivariate normal X and common $\\sum$\n- Dummy features may not work well given assumption?\n- May require smaller sample sizes to fit than logistic regression if assumptions met\n- Can natively handle more than two level for outcome  \n  \n----- \n  \nQDA relaxes one restrictive assumption of LDA\n  \n- Still required multivariate normal X\n- **But it allows each class to have its own $\\sum$**\n- This makes it:\n  - More flexible\n  - Able to model non-linear decision boundaries including 2-way intearctions (see formula for discriminant in @ISL)\n  - how handle interactions by relaxing common $\\sum$? \n  - But requires substantial increase in parameter estimation (more potential to overfit)\n- Still problems with dummy features\n- Can natively handle more than 2 levels of outcome like LDA\n- Compare to LDA and Logisitic Regression on bias-variance trade off?\n\\\n\\\nRDA may be better than both LDA and QDA?  More on idea of blending after elastic net\n\n-----\n\nKNN works similar to regression\n\n- But now looks at percentage of observations for each class among nearest neighbours to estimate conditional probabilities\n- Doesn't make assumptions about Xs or $\\sum$ for LDA and/or QDA\n- Doesn't not limited to linear decision boundaries like logistic and LDA\n- Very flexible - low bias but high variance?\n- K can be adjusted to impact bias-variance trade-off\n- KNN can handle more than two level outcomes natively\n\n-----\n\n## Categorical predictors\n\n- All algorithms so far require numeric features\n- Ordinal can be made numeric sometimes by just substituting ordered vector (i.e. 1, 2, 3, etc)\n- Nominal needs something more\n- Our go to method is Dummy features\n  - What is big problem with dummy features?\n  - Collapsing levels?\n  - Also issue of binary scores for LDA/QDA\n  \n- Target encoding example\n  - Country of origin for car example (but maybe think of many countries?)\n  - Why not data leakage?\n  - Problems with step_mutate()\n  - Can manually do it with our current resampling\n  - See [embed package](https://embed.tidymodels.org/)\n \n-----\n\n### Interactions in LDA and QDA\n\n- Simulate multivariate normal distribution for X (`x1` and `x2`) using MASS package\n- Separately for trn and val\n- NOTE: I first did this with uniform distributions on X and the models fit more poorly.  Why?\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5433)\nmeans <- c(0, 0)\nsigma <- diag(2) * 100\ndata_trn <- MASS::mvrnorm(n = 300, mu = means, Sigma = sigma) |>  \n    magrittr::set_colnames(str_c(\"x\", 1:length(means))) |>  \n    as_tibble()\n\ndata_val <- MASS::mvrnorm(n = 3000, mu = means, Sigma = sigma) |>  \n    magrittr::set_colnames(str_c(\"x\", 1:length(means))) |>  \n    as_tibble()\n```\n:::\n\n\n-----\n\n- Write function for interactive DGP based on `x1` and `x2`\n- Will map this over rows of d\n- Can specify any values for b\n- b[4] will be interaction parameter estimate\n\n::: {.cell}\n\n```{.r .cell-code}\nb <- c(0, 0, 0, .5)\n\ncalc_p <- function(x, b){\n   exp(b[1] + b[2]*x$x1 + b[3]*x$x2 + b[4]*x$x1*x$x2) /\n     (1 + exp(b[1] + b[2]*x$x1 + b[3]*x$x2 + b[4]*x$x1*x$x2))\n}\n```\n:::\n\n\n-----\n\n- Add p and then observed classes to trn and val \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn <- data_trn |> \n  mutate(p = calc_p(data_trn, b)) |> \n  mutate(y = rbinom(nrow(data_trn), 1, p),\n         y = factor(y, levels = 0:1, labels = c(\"neg\", \"pos\")))\n\nhead(data_trn, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n        x1     x2        p y    \n     <dbl>  <dbl>    <dbl> <fct>\n 1   9.85   -5.53 1.52e-12 neg  \n 2  -2.48    5.64 9.21e- 4 neg  \n 3  -4.74  -13.7  1.00e+ 0 pos  \n 4  -7.02   12.6  6.09e-20 neg  \n 5   0.942  -3.48 1.63e- 1 pos  \n 6   0.151   2.78 5.52e- 1 neg  \n 7   7.74   -6.84 3.24e-12 neg  \n 8  -4.85  -10.1  1.00e+ 0 pos  \n 9  -0.937   2.03 2.78e- 1 neg  \n10 -16.5     6.08 1.83e-22 neg  \n```\n\n\n:::\n\n```{.r .cell-code}\ndata_val <- data_val |> \n  mutate(p = calc_p(data_val, b)) |> \n  mutate(y = rbinom(nrow(data_val), 1, p),\n         y = factor(y, levels = 0:1, labels = c(\"neg\", \"pos\")))\n```\n:::\n\n\n-----\n\n- Lets look at what an interactive DGP looks like for two features and a binary outcome\n- Parameter estimates set up a \"cross-over\" interaction\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val |> \n  ggplot(mapping = aes(x = x1, y = x2, color = y)) +\n    geom_point(size = 2, alpha = .5)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n-----\n\n- Fit models in trn \n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lda <- \n  discrim_linear() |> \n  set_engine(\"MASS\") |> \n  fit(y ~ x1 + x2, data = data_trn)\n\nfit_lda_int <- \n  discrim_linear() |> \n  set_engine(\"MASS\") |> \n  fit(y ~ x1 + x2 + x1*x2, data = data_trn)\n\nfit_qda <- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) |> \n  set_engine(\"klaR\") |> \n  fit(y ~ x1 + x2, data = data_trn)\n\nfit_qda_int <- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) |> \n  set_engine(\"klaR\") |> \n  fit(y ~ x1 + x2 + x1*x2, data = data_trn)\n```\n:::\n\n\n-----\n\n- Additive LDA model decision boundary and performance in val\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val |> \n  plot_decision_boundary(fit_lda, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\n-----\n\n-  Interactive LDA model decision boundary and performance in val\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val |> \n  plot_decision_boundary(fit_lda_int, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400) \n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\n-----\n\n- Additive QDA model decision boundary\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val |> \n  plot_decision_boundary(fit_qda, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400)\n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\n\n- Interactive QDA model decision boundary\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val |> \n  plot_decision_boundary(fit_qda_int, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400) \n```\n\n::: {.cell-output-display}\n![](004_classification_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n \n- Costs for QDA vs. LDA intearctive in this example and more generally with more features?\n- What if you were using RDA, which can model the full range of models between linear and quadratic?\n\n\n\n\n-----\n\nPCA\n\n- https://setosa.io/ev/principal-component-analysis/:w\n- https://www.cs.cmu.edu/~elaw/papers/pca.pdf\n",
    "supporting": [
      "004_classification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}