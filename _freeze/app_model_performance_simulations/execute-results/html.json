{
  "hash": "b97216d92ef35f652b3f76eb987ed29c",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques\n\n\n\n## General Setup\n\nLoad libraries\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ntheme_set(theme_classic())\n```\n:::\n\n\n## DGP\nFunction to generate `n_obs` of simulated observations\n\n* DGP is linear on all `x` + normal error with sd = `irr_err`\n* y is simple sum such that all coefficients = 1\n* features are correlated based on `sigma`\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_DGP <- function (n_obs, n_features, irr_err, mu, sigma){\n\n  x <- MASS::mvrnorm(n_obs, mu = mu, Sigma = sigma) |> \n    magrittr::set_colnames(str_c(\"x\", 1:n_features)) |> \n    as_tibble()\n  \n  x |> \n     mutate(y = rowSums(t(t(x)*b)) + rnorm(n_obs, \n                                        mean = 0, \n                                        sd = irr_err))\n}\n```\n:::\n\n\n\n## Simulation settings\n\n::: {.cell}\n\n```{.r .cell-code}\nn_sims <- 1000  # number of simulations\nn_obs <- 300 # number of observations\n\nn_features <- 20\nmu <- rep(0, n_features)\nsigma <- matrix(.3, nrow = n_features, ncol = n_features)\ndiag(sigma) <- 1\nirr_err <- 10\nb <- rep(0.5, n_features) # no b0 so set to 0\n  \nset.seed(123456)\n# first call so that we can set up recipe\ndf <- simulate_DGP(n_obs, n_features, irr_err, mu, sigma) \nrec <- recipe(y ~ ., data = df)\n\nrmse_combined = NULL  # to store RMSE for all methods\n```\n:::\n\n\n\n## Get simulation dfs\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfs <- rep(n_obs, n_sims) |>\n  map(\\(n_obs) simulate_DGP(n_obs, n_features, irr_err, mu, sigma))\n```\n:::\n\n\n\n## What is the TRUE model performance\n\nThe irreducible error is set to 10 but our model will have some reducible error too so the true performance of our model will be worse than 10\n\n* The DGP is linear \n* but we only have 300 observations \n* and we have 20 features\n\nLets fit many models of n = 300 (the size of our final model) and assess performance in really big samples of new data (ah the luxury of simulated data!)\n\n::: {.cell}\n\n```{.r .cell-code}\n# models based on full n for each simulation run\nmodels <- dfs |> \n  map(\\(df) linear_reg() |> fit(y ~ ., data = df))\n\n# big samples of held out data for high precision assessment of models\n# 1 for each model\nouts <- rep(10000, n_sims) |> \n  map(\\(n_obs) simulate_DGP(n_obs, n_features, irr_err, mu, sigma))\n\n# list of predictions for out from each model\npreds <- map2(models, outs, \\(model, out) predict(model, out))\n\n# get mean rmse in big held out data set for 1000 full n models\n# should be very precise\nrmse_true <- map2_dbl(outs, preds, \\(out, pred)  rmse_vec(out$y,\n                                                      pred$.pred)) |> \n  mean()\n\nmessage(\"True RMSE = \", rmse_true)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrue RMSE = 10.3728762850761\n```\n\n\n:::\n:::\n\n\nParallel processing for resampling methods with `fit_resamples()`\n\n::: {.cell}\n\n```{.r .cell-code}\ncl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))\ndoParallel::registerDoParallel(cl)\n```\n:::\n\n\n## Validation set\n\n### 80/20 split\n\n\nHere we simulate repeated use of the validation set approach to assess our model performance\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_combined <- dfs |> \n  map(\\(df) validation_split(df, prop = .80)) |>   # validation set split\n  map(\\(split) linear_reg() |> fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |> \n  map(\\(fits) collect_metrics(fits, summarise = TRUE)) |> \n  list_rbind() |> \n  mutate(method = \"val_set_80\") |>       # label results in df\n  bind_rows(rmse_combined)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `validation_split()` was deprecated in rsample 1.2.0.\nℹ Please use `initial_validation_split()` instead.\n```\n\n\n:::\n:::\n\n\n## 50/50 split\n\nHere we simulate repeated use of the validation set approach to assess our model performance\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_combined <- dfs |> \n  map(\\(df) validation_split(df, prop = .50)) |>   # validation set split\n  map(\\(split) linear_reg() |> fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |> \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |> \n  list_rbind() |> \n  mutate(method = \"val_set_50\") |>       # label results in df\n  bind_rows(rmse_combined)\n```\n:::\n\n\n## K-fold\n\n### Simple 5-fold\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_combined <- dfs |> \n  map(\\(df) vfold_cv(df, v = 5)) |>   # 5-fold\n  map(\\(split) linear_reg() |> fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |> \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |> \n  list_rbind() |> \n  mutate(method = \"5-fold\") |>       # label results in df\n  bind_rows(rmse_combined)\n```\n:::\n\n\n### Simple 10-fold\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_combined <- dfs |> \n  map(\\(df) vfold_cv(df, v = 10)) |>   # 10-fold\n  map(\\(split) linear_reg() |> fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |> \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |> \n  list_rbind() |> \n  mutate(method = \"10-fold\") |>       # label results in df\n  bind_rows(rmse_combined)\n```\n:::\n\n\n\n### 3x 10-Fold\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_combined <- dfs |> \n  map(\\(df) vfold_cv(df, v = 10, repeats = 3)) |>   # 3x10-fold\n  map(\\(split) linear_reg() |> fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |> \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |> \n  list_rbind() |> \n  mutate(method = \"3x10-fold\") |> # label results in df\n  bind_rows(rmse_combined)\n```\n:::\n\n\n\n\n## Bootstrap Resampling\n\n### 10 resamples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_combined <- dfs |> \n  map(\\(df) bootstraps(df, times = 10)) |>   # 10 boots\n  map(\\(split) linear_reg() |> fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |> \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |> \n  list_rbind() |> \n  mutate(method = \"boot_10\") |>       # label results in df\n  bind_rows(rmse_combined)\n```\n:::\n\n\n### 100 resamples\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_combined <- dfs |> \n  map(\\(df) bootstraps(df, times = 100)) |>   # 100 boots\n  map(\\(split) linear_reg() |> fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |> \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |> \n  list_rbind() |> \n  mutate(method = \"boot_100\") |>       # label results in df\n  bind_rows(rmse_combined)\n```\n:::\n\n\n### 1000 resamples\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_combined <- dfs |> \n  map(\\(df) bootstraps(df, times = 1000)) |>   # 1000 boots\n  map(\\(split) linear_reg() |> fit_resamples(resamples = split, \n                                              preprocessor = rec,\n                                              metrics = metric_set(rmse))) |> \n  map(\\(fits) collect_metrics(fits, summarize = TRUE)) |> \n  list_rbind() |> \n  mutate(method = \"boot_1000\") |>       # label results in df\n  bind_rows(rmse_combined)\n```\n:::\n\n\n\n## Summarize\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_combined |> \n  group_by(method) |> \n  summarize(rmse_mean = mean(mean),\n            rmse_sd = sd(mean),\n            n = n())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 4\n  method     rmse_mean rmse_sd     n\n  <chr>          <dbl>   <dbl> <int>\n1 10-fold         10.4   0.442  1000\n2 3x10-fold       10.4   0.436  1000\n3 5-fold          10.5   0.455  1000\n4 boot_10         10.7   0.491  1000\n5 boot_100        10.7   0.451  1000\n6 boot_1000       10.7   0.449  1000\n7 val_set_50      10.8   0.672  1000\n8 val_set_80      10.5   0.985  1000\n```\n\n\n:::\n:::\n\n\n\n##  Plot sampling distributions\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_combined |> \n  ggplot(aes(x = mean, color = method)) + \n  geom_density() +\n  geom_vline(aes(xintercept = mean(rmse_true)),\n            color = \"blue\", linetype = \"dashed\", linewidth = 1)\n```\n\n::: {.cell-output-display}\n![](app_model_performance_simulations_files/figure-html/a102-plot-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "app_model_performance_simulations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}