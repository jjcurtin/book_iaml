{
  "hash": "e8d8b8a07178d24bc3e0423927c67482",
  "result": {
    "engine": "knitr",
    "markdown": "---\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Principal Components Analysis  { .unnumbered}\n\n##  Overview\n\n### General Information\n\nTo start, the introductory sections about PCA from [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis) provide a nice orientation and summary of the technique.\n\nIn short, PCA linearly transforms our data (our `x`s to be precise) onto a new coordinate system such that the directions/axes (principal components) capturing the largest variation in the data can be easily identified.\n\nA set of up to `p` principal components can be derived from `p` raw variables (e.g. predictors) such that the first principal component is a linear combination of the `p` raw variables that accounts for the most variance across those variables.   The 2nd prinicipal component is a similar linear combination of the raw variables that is orthogonal (uncorrleated) with the first component and accounts for the largest proportionn of remaining variance among the raw variables.  PCA continues to derive additional components that are each orthogonal to the previous components and account for successively less variance.\n\nThese new components represent a new coordinate system within which to represent score on our observations.   This new system is a rotation of the original coordinate system that consists of orthogonal axes (e.g., x, y, z) defined by our raw variables.\n\nWe can use PCA to\n\n- Understand the structure of our data\n- Transform our raw (correlated) variables into a set of uncorrelated features\n- Most importantly, retain a large portion of the the variance from the `p` original variables with << `p` principal components.  This last benefit may allow us to fit lower variance prediction models (due to less overfitting) without increasing model bias by much.\n\n### Applications in machine learning\n\nWhen we discuss PCA in the machine learning world, we consider it an example of an unsupervised machine learning approach.  \n\n- It is applied to the raw predictors (the `x`s, ignoring `y`).  \n- We use it to reduce the dimensionality of our features to minimize overfitting that can contribute to model variance.\n\n## A Two Dimensional Example\n\nThis example is based loosely on a tutorial and demonstration data developed by [Lindsay Smith](https://www.iro.umontreal.ca/~pift6080/H09/documents/papers/pca_tutorial.pdf)\n\nFor a second example that extends to 3D space, see this [website](https://setosa.io/ev/principal-component-analysis/)\n\n--------------------------------------------------------------------------------\n\nLet's start with a toy dataset for two variables (e.g., predictors), `x1` and `x2` and a sample size of 10.  We will work in two dimensions to make the example easier to visualize but the generalization to p dimensions (where p = number of variables) is not difficult.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nd <- tibble(x1 = c(2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1),\n            x2 = c(2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9))\nd |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 Ã— 2\n      x1    x2\n   <dbl> <dbl>\n 1   2.5   2.4\n 2   0.5   0.7\n 3   2.2   2.9\n 4   1.9   2.2\n 5   3.1   3  \n 6   2.3   2.7\n 7   2     1.6\n 8   1     1.1\n 9   1.5   1.6\n10   1.1   0.9\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\nHere is a scatterplot of the dataset.  I've also added a green dot (that is not part of the dataset) at the mean of `x1` and `x2`.  This will be the point of rotation for the dataset as we attempt to find a new coordinate system (not defined by `x1` and `x2`), where the dataset's variance is maximized across the new axes (the principal components) and the observations are uncorrelated in this new coordinate system.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_classic())\nd |> ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  geom_point(data = tibble(x1=mean(d$x1), x2 = mean(d$x2)), \n             size = 2, color = \"green\") +\n  xlim(-1,4) +\n  ylim(-1,4) +\n  coord_fixed()\n```\n\n::: {.cell-output-display}\n![](app_pca_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\nThe first step in the process is to center both `x1` and `x2` such that their means are zero.  This moves the green point to the origin.  This will make it easier to rotate the data around that point (the definitions of the new principal components will be defined as a linear combination of the original variables but there is not offset/intercept in those transformation formulas).  \n\nI've left the green point at the mean of `x1c` and `x2c` for this plot to reinforce the impact of centering.  I will remove it from later figures.  I have also drawn the true axes in blue to make the original coordinate system defined by `x1` and `x2` salient.  PCA will rotate this coordinate system to achieve its goals.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d |> \n  mutate(x1c = x1 - mean(x1), x2c = x2 - mean(x2))\n\nplot <- d |> \n  ggplot(aes(x = x1c, y = x2c)) +\n    geom_point() +\n    geom_hline(yintercept = 0, color = \"blue\") +\n    geom_vline(xintercept = 0, color = \"blue\") +\n    xlim(-2.5, 2.5) +\n    ylim(-2.5, 2.5) +\n    coord_fixed()\n\nplot + \n  geom_point(data = tibble(x1c=mean(d$x1c), x2c = mean(d$x2c)), \n               size = 2, color = \"green\")\n```\n\n::: {.cell-output-display}\n![](app_pca_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\nNOTE: It is sometimes useful to also scale the original variables (i.e.,, set their standard deviations = 1).  \n\n- This may be important if the variables have very difference variances.\n- If you dont scale the variables, the variables with the large variances will have more influence on the rotation than those with smaller variances.  \n- If this is not desirable, scale the variables as well as center them.  \n- However, do know that sometimes variances are larger because of noise and if you scale, you will magnify that noise. \n\n--------------------------------------------------------------------------------\n\nOur goal now is to find the axes of the new coordinate system.   The first axis (the first principal component) will be situated such it maximizes the variance in the data across its span.  Imagine fittinng all possible lines through the green dot and choosing the line that moves along the widest spread of the data.  That line (displayed in green below) will be the first axis of the new coordinate system and projections of the points onto that axis will represent the scores for each observation on our first principal component defined by this axis.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](app_pca_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\nThis axis associated with the first principal component is similar to a regression line but not identical.\n\n- The red regression line (below) was fit to minimize the sum of the squared errors when predicting the outcome (in this instance when regressing `x2c` on `x1c`) from a predictor (in this instance, `x1c`.  These errors are the vertical distances from the red line to the points.\n\n- In contrast, the green line maximized variance across that PC1 dimension.  As a result, it also minimized deviations around the line.  However, thsee deviations are the squared perpendicular distances between the green line and the points.  These distances go up and to the left and down and to the right from the green line to the points rather than vertical.   They are not the same line!\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](app_pca_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\nTo find the next principal component, we need to find a new axis that is perpendicular to the first component and in the direction that accounts for the largest proportion of the remaining variance in the dataset. \n\nIn our example with only two variables, there is only one direction remaining that is perpendicular/orthogonal with `PC1` because we are in two dimensional space given only two original variables.\n\nHowever, if we were in a higher dimensional space with p > 2 variables, this next component could follow the direction of maximal remaining variance in a direction orthogonal to `PC1`.  Subsequent components up to the pth component given p variables would each be orthogonal to all previous components and in the direction of maximal variance.\n\nWe have added this second component (orange) to our 2 dimensional example below.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](app_pca_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\nThese two components define a new coordinate system within which to measure/score our observations.  The new axes of this system are the PCs.  This new coordinate system is a rotation of our original system that was previously defined in axes based on `x1` and `x2`.  \n\nHere is a figure displaying the data in this new coordinate system. Notice the group of four points to the left (three vertical and one further to the left).  Those same points were previously in the top right quadrant of the coordinate system defined by `x1c` and `x2c`.\n\n- Scores on each of the two components are obtained by projecting the observations onto the associated axis for that component.\n- The data show the highest variance over `PC1` (the x axis) and the next highest variance over `PC2` (the y axis) \n- When the observations are scored/defined on these PCs, `PC1` and `PC2` are now new, uncorrelated features we can use to describe the data.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](app_pca_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n## PCA using Eigenvectors and Eigenvalues\n\nWe can derive the new coordinate system that maximizes the variance on `PC1`, and then `PC2`, etc by doing an eigen decomposition of the covariance matrix (or correlation matrix if the x's are to be scaled).  When applied to a p x p covariance matrix, this yields `p` pairs of [eigen vectors and eigen values](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).  Complete explanation of this process is beyond the scope of this tutorial but the interested reader can consult [Linear Algebra and its Applications by Gilbert Strang](https://rksmvv.ac.in/wp-content/uploads/2021/04/Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf).\n\nIn short, the eigen vectors represent the new axes for the principal components and the eigen values indicate the variances of the principal components.  \n\n-------------------------------------------------------------------------------\n\nHere is an eigen decomposition of the covariance for our toy data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nei <- d |> \n  select(x1c, x2c) |>\n  cov() |> \n  eigen(symmetric = TRUE)\n```\n:::\n\n\n\n\n- **Eigenvectors**.  These are unit vectors that point in the direction of the axes of the new coordinate system.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nei$vectors\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]       [,2]\n[1,] 0.6778734 -0.7351787\n[2,] 0.7351787  0.6778734\n```\n\n\n:::\n:::\n\n\n\n\nThis figure plots these two vectors on our original coordinate system defined by `x1c` and `x2c`.  Note that these vectors map on the new axes we demonstrated earlier.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot +\n  annotate(\"segment\", x = 0, y = 0, xend = ei$vectors[1,1], yend = ei$vectors[2,1], \n              color = \"green\") + \n  annotate(\"segment\", x = 0, y = 0, xend = ei$vectors[1,2], yend = ei$vectors[2,2], \n              color = \"green\")\n```\n\n::: {.cell-output-display}\n![](app_pca_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n- **Eigenvalues**. These are the variances associated with the principal components\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nei$values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.2840277 0.0490834\n```\n\n\n:::\n:::\n\n\n\n\nThe eigen decomposition parses the complete variance in the original variables such that `PC1` has the most variance, `PC2`, the secondmost, etc.  The full set of the PCs will contain all the variance of the original variables.\n\nIn our example:\n\n- Variance was originally split across `x1` and `x2`.  \n- `PC1` now contains most of the variance in the dataset (see eigenvalues above)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar(d$x1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6165556\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(d$x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7165556\n```\n\n\n:::\n:::\n\n\n\n\nAll variance accounted for both in original variables and new PCs\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nei$values[1] + ei$values[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.333111\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(d$x1) + var(d$x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.333111\n```\n\n\n:::\n:::\n\n\n\n\n## Stable Compuation of Princippal Components\n\nIt is more numerically stable to get the principal components using singular vector decomposition (`svd()` in R) than eigen decomposition.  \n\nIn base R, `prcomp()` uses svd and also directly calculates the PCs.\n\n- See [help](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp)\n- We pass in our raw variables.\n- By default, the raw variables are centered by not scaled.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca <- prcomp(d |> select(x1, x2))\n```\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\nWe can get the vectors associated with the new coordinate system from `$rotation`  Note that direction of the PCs is arbitrary (e.g., PCs are opposite direction from the solution using `eigen()` with these data)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nei$vectors\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]       [,2]\n[1,] 0.6778734 -0.7351787\n[2,] 0.7351787  0.6778734\n```\n\n\n:::\n\n```{.r .cell-code}\npca$rotation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          PC1        PC2\nx1 -0.6778734  0.7351787\nx2 -0.7351787 -0.6778734\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n`$sdev` returns the square root of the eigenvalues.  This represents the standard deviations of the PCs\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca$sdev\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.1331495 0.2215477\n```\n\n\n:::\n\n```{.r .cell-code}\n# square for variances\npca$sdev ^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.2840277 0.0490834\n```\n\n\n:::\n\n```{.r .cell-code}\n# compare to eigenvalues\nei$values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.2840277 0.0490834\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n`$x` contains the new scores on the PCs for the dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca$x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              PC1         PC2\n [1,] -0.82797019  0.17511531\n [2,]  1.77758033 -0.14285723\n [3,] -0.99219749 -0.38437499\n [4,] -0.27421042 -0.13041721\n [5,] -1.67580142  0.20949846\n [6,] -0.91294910 -0.17528244\n [7,]  0.09910944  0.34982470\n [8,]  1.14457216 -0.04641726\n [9,]  0.43804614 -0.01776463\n[10,]  1.22382056  0.16267529\n```\n\n\n:::\n:::\n\n\n\n\nAs expected, they are uncorrelated\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(cor(pca$x[,1], pca$x[,2]), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n\n## Using PCA for Dimensionality Reduction\n\nIn our example, `PC1` and `PC2`\n\n- Contain all the variance from `x1` and `x2`\n- Are orthogonal \n\nBut when using PCA for dimensionality reduction, we wanted to use the variance of our variables in few dimensions (with fewer features) for prediction to reduce overfitting.\n\n- Most of variance from the full dataset is now in `PC1`\n- We can use `PC1` as a feature rather than both `x1` and `x2`.\n\n## PCA in Tidymodels\n\nWe can use PCA for dimensionality reduction as part of our feature engineering.\n\n- `step_pca()` uses `prcomp()`\n- See [help](https://recipes.tidymodels.org/reference/step_pca.html)\n- Default is center = false and scale = false.  You definitely want to center and maybe scale predictors in a previous recipe step before using `step_pca()`\n- You can choose number of components to retain by specifying the exact number (`num_comp = `) or by indicating the minimum variance retained across PCs (`threshold = `)",
    "supporting": [
      "app_pca_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}