{
  "hash": "f9eb865ebf44bd0e9a898ff9b4ce9e7a",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n# Explanatory Approaches \n\n## Overview of Unit\n\n### Learning Objectives\n\n- Use of feature ablation to statisticall compare model configurations\n  - Frequentist correlated t-test using CV\n  - Bayesian estimation for model comparisons\n    - ROPE\n\n- Feature importance metrics for explanation\n  - Model specific vs. model agnostic approaches\n  - Permutation feature importance\n  - Shapley values (SHAP)\n    - local importance\n    - global importance\n\n- Visual approaches for explanation\n  - Partial Dependence plots\n  - Accumulated Local Effects (ALE) plots\n \n-----\n\n### Readings\n\n- @Benavoli2017 [paper](pdfs/benavoli_2017_bayesian_t_test.pdf):  Read pages 1-9 that describe the correlated t-test and its limitations.\n- @Kruschke2018 [paper](pdfs/kruschke2018a.pdf): Describes Bayesian estimation and the ROPE (generally, not in the context of machine learning and model comparisons)\n- @IML [Chapter 3 - Interpretability](https://christophm.github.io/interpretable-ml-book/interpretability.html)\n- @IML [Chapter 6 - Model-Agnostic Methods](https://christophm.github.io/interpretable-ml-book/agnostic.html)\n- @IML [Chapter 8 - Global Model Agnostic Methods](https://christophm.github.io/interpretable-ml-book/global-methods.html): Read setions 8.1, 8.2, 8.3, and 8.5\n- @IML [Chapter 9 - Local Model-Agnostic Methods](https://christophm.github.io/interpretable-ml-book/shapley.html): Read section 9.5\n\n\\\n\\\nPost questions to the readings channel in Slack\n\n-----\n\n## Lecture Videos\n\n- [Introduction to Model Comparisons](https://mediaspace.wisc.edu/media/iaml+unit+11-1/1_sz7we4by) ~ 23 mins\n- [The Nadeau & Bengio Correlated t-test for Model Comparisons](https://mediaspace.wisc.edu/media/iaml+unit+11-2/1_0iykku1l) ~ 10 mins\n- [Bayesian Estimation for Model Comparisons](https://mediaspace.wisc.edu/media/iaml+unit+11-3/1_mvczl7cn) ~ 28 mins\n- [Permutation Feature Importance]()\n- [SHAP Feature Importance]()\n- [Visual Approaches to Understand Models]()\n\n\\\n\\\nPost questions to the video-lectures channel in Slack\n\n-----\n\n## Application Assignment\n\n- data: [admissions.csv](homework/unit_9/admissions.csv)\n- [assignment qmd](homework/unit_9/hw_unit_9_explanatory.Rmd)\n- [solution html]()\n\n\\\n\\\nPost questions to application-assignments Slack channel\n\\\n\\\nSubmit the application assignment [here](https://canvas.wisc.edu/courses/395546/assignments/2187693) and complete the [unit quiz](https://canvas.wisc.edu/courses/395546/quizzes/514055) by 8 pm on Wednesday, April 10th \n\n-----\n\n## Model Comparisons & Feature Ablation\n\nIn 610/710, you learned to think about the tests of specific parameter estimates as model comparisons of models that did vs. did not include the specific feature(s) in the model\n\\\n\\\nModel comparisons can be used in a similar way for explanatory goals with machine learning\n\n-----\n\nThis can be done for a single feature (e.g., $x_3$)\n\n  - compact model:  $y = b_0 + b_1*x_1 + b_2*x_2$\n  - full (augmented) model: $y = b_0 + b_1*x_1 + b_2*x_2 + b_3*x_3$\n  - The comparison of these two models is equivalent to the test of $H_0: b_3 = 0$\n\\\n\\\nThis can also involve sets of features if you hypothesis involves the effect of a set of features\n\n  - All features that represent a categorical predictor\n  - Set of features that represent some broad construct (e.g., psychiatric illness represented by symptoms counts for all of the major psychiatric diagnoses)\n\\\n\\\nThis technique of comparing two nested models (i.e. feature set for the compact model is a subset of the feature set for the full/augmented model) is often called **feature ablation** in the machine learning world\n\n-----\n\nModel comparisons can also be done between model configurations that differ by characteristics other than their features (e.g., statistical algorithm)\n\\\n\\\nModel comparisons can be useful to determine the best available model configuration to use for a prediction goal\n\n- In some instances, it is OK to simply choose the descriptively better performing model configuration (e.g., better validation set or resampled performance estimate)\n- However, if the descriptively better performing model has other disadvantages (e.g., more costly to implement) you might want to only use it if you had rigorously demonstrated that it likely better for all new data.\n\n-----\n\nIn this unit, we will learn two approaches to statistically compare models\n\n- Traditional frequentist (NHST) approach using a variant of the t-test to accommodate correlated observations\n- A Bayesian alternative to the t-test\n\n\\\n\\\nWe will compare model nested model configurations formed by feature ablation (i.e., full and compact models will differ by features included)\n\\\n\\\nHowever, nothing would be different when implementing these comparison methods if these model configurations different by other characteristics such as statistical algorithm\n\n---\n\n## An Empirical Example of Feature Ablation\n\nThe context for our example will be the Cleveland heart disease dataset\n\\\n\\\nWe will imagine we have developed a new diagnostic screener for heart disease based on an exercise test protocol\n\\\n\\\nWe want to demonstrate the incremental improvement in our screening for heart disease using features from this test vs. other readily available characteristics about the patients\n\n-----\n\nOur exercise test protocol yields four scores ($exer\\_*$) that we use in combination to predict the probability of heart disease in the patient\n\n- Max heart rate during the exercise test\n- Experience of angina during the test\n- Slope of the peak exercise ST segment (don't ask me what that is!  ;-)\n- ST depression induced by exercise test relative to rest\n\n\\\n\\\nWe also have many other demographic and physical characteristics that we want to \"control\" for when evaluating the performance of our test\n\n- I use control in both its senses.  It likely helps to have these covariates b/c they reduce error in the outcome\n- I also want to demonstrate that our test has incremental predictive validity above these other characteristics which are already available for screening without my complicated test\n\n-----\n\n\n\n\n\nLet's open the data set and do some basic cleaning\n\n- Note there were some complex issues to deal with\n- Good examples of code to resolve those issues\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_all <- read_csv(here::here(path_data, \"cleveland.csv\"), col_names = FALSE, \n                     na = \"?\", col_types = cols()) |> \n  rename(age = X1,\n         sex = X2,\n         cp = X3,\n         rest_bp = X4,\n         chol = X5,\n         fbs = X6,\n         rest_ecg = X7,\n         exer_max_hr = X8,\n         exer_ang = X9,\n         exer_st_depress = X10,\n         exer_st_slope = X11,\n         ca = X12,\n         thal = X13,\n         disease = X14) |> \n  mutate(disease = fct(if_else(disease == 0, \"no\", \"yes\"),\n                       levels = c(\"yes\", \"no\")), # pos event first\n         sex = fct(if_else(sex == 0, \"female\", \"male\"), \n                   levels = c(\"female\", \"male\")),\n         fbs = fct(if_else(fbs == 0, \"normal\", \"elevated\"),\n                   levels = c(\"normal\", \"elevated\")),\n         exer_ang = fct(if_else(exer_ang == 0, \"no\", \"yes\"),\n                           levels = c(\"no\", \"yes\")),\n         exer_st_slope = fct_recode(as.character(exer_st_slope), \n                                       upslope = \"1\", \n                                       flat = \"2\",\n                                       downslope = \"3\"),\n         cp = fct_recode(as.character(cp), \n                            typ_ang = \"1\", \n                            atyp_ang = \"2\", \n                            non_anginal = \"3\", \n                            non_anginal = \"4\"),\n         rest_ecg = fct_recode(as.character(rest_ecg), \n                                  normal = \"0\", \n                                  wave_abn = \"1\", \n                                  ventric_hypertrophy = \"2\"),\n         thal = fct_recode(as.character(thal), \n                              normal = \"3\", \n                              fixeddefect = \"6\", \n                              reversabledefect = \"7\"))  \n```\n:::\n\n\n-----\n\nSkim it to make sure we didnt break anything during our cleaning!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_all |> skim_some()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |         |\n|:------------------------|:--------|\n|Name                     |data_all |\n|Number of rows           |303      |\n|Number of columns        |14       |\n|_______________________  |         |\n|Column type frequency:   |         |\n|factor                   |8        |\n|numeric                  |6        |\n|________________________ |         |\n|Group variables          |None     |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                  |\n|:-------------|---------:|-------------:|:-------|--------:|:---------------------------|\n|sex           |         0|          1.00|FALSE   |        2|mal: 206, fem: 97           |\n|cp            |         0|          1.00|FALSE   |        3|non: 230, aty: 50, typ: 23  |\n|fbs           |         0|          1.00|FALSE   |        2|nor: 258, ele: 45           |\n|rest_ecg      |         0|          1.00|FALSE   |        3|nor: 151, ven: 148, wav: 4  |\n|exer_ang      |         0|          1.00|FALSE   |        2|no: 204, yes: 99            |\n|exer_st_slope |         0|          1.00|FALSE   |        3|ups: 142, fla: 140, dow: 21 |\n|thal          |         2|          0.99|FALSE   |        3|nor: 166, rev: 117, fix: 18 |\n|disease       |         0|          1.00|FALSE   |        2|no: 164, yes: 139           |\n\n\n**Variable type: numeric**\n\n|skim_variable   | n_missing| complete_rate|  p0|  p100|\n|:---------------|---------:|-------------:|---:|-----:|\n|age             |         0|          1.00|  29|  77.0|\n|rest_bp         |         0|          1.00|  94| 200.0|\n|chol            |         0|          1.00| 126| 564.0|\n|exer_max_hr     |         0|          1.00|  71| 202.0|\n|exer_st_depress |         0|          1.00|   0|   6.2|\n|ca              |         4|          0.99|   0|   3.0|\n\n\n:::\n:::\n\n\n-----\n\nThe dataset is not that large and we have a decent number of features so we will build models using regularized logistic regression (glmnet)\n\\\n\\\nIt would be better to actually do some exploration to build the best compact model first but we will skip that part of the analysis here\n\n----- \n\nWe are using glmnet so we need to find the optimal set of hyperparameter values for this model configuration\n\\\n\\\nLet's select/tune hyperparameters using bootstrap resampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456)\nsplits_boot <- data_all |> \n  bootstraps(times = 100, strata = \"disease\")\n```\n:::\n\n\\\n\\\nAnd here is a grid of hyperparameters to tune\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_glmnet <- expand_grid(penalty = exp(seq(-8, 2, length.out = 300)),\n                           mixture = c(0, .025, .05, .1, .2, .4, .6, .8, 1))\n```\n:::\n\n\n-----\n\nHere is a feature engineering recipe for the full model with all features that is appropriate for glmnet\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_full <- recipe(disease ~ ., data = data_all) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) |> \n  step_normalize(all_predictors())\n```\n:::\n\n\n-----\n\nNow we tune the model configuration\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_full <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(),\n                 mixture = tune()) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_full,\n              resamples = splits_boot,\n              grid = grid_glmnet,\n              metrics = metric_set(accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/011/\",\n  file = \"fits_full\")\n```\n:::\n\n\n-----\n\nLet's check how the models perform (bootstrap resampled accuracy) with various values for the hyperparameters\n\\\n\\\nHere is accuracy by log penalty (lambda) and mixture (alpha)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_full |> \n  plot_hyperparameters(hp1 = \"penalty\", hp2 = \"mixture\", \n                       metric = \"accuracy\", log_hp1 = TRUE)\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: Does the glmnet (regularized logistic regresssion) outperform a simple logistic regression?  How would these two algorithms compare?\n\n::: {.cell}\n\n```{.html .cell-code  code-fold=\"true\" code-summary=\"Show Answer\"}\nRemember that the linear model is a special case of glmnet where the penalty = 0.  \nHere we are looking log penalty down to exp(-8) = 0.0003354626.  That is pretty \nclose to 0 so an approximation of how the standard logistic regression would \nperform.  glmnet is about 2% more accurate with its optimal hyperparameter values\n```\n:::\n\n:::\n\n-----\n\nHere are the optimal hyperparameter values for the top 10 configurations\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(fits_full) |> \n  arrange(desc(mean)) |> \n  print(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,700 × 8\n   penalty mixture .metric  .estimator  mean     n std_err\n     <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl>\n 1   0.279     0.1 accuracy binary     0.823   100 0.00277\n 2   0.270     0.1 accuracy binary     0.823   100 0.00277\n 3   0.261     0.1 accuracy binary     0.823   100 0.00273\n 4   0.403     0.1 accuracy binary     0.823   100 0.00287\n 5   0.476     0.1 accuracy binary     0.823   100 0.00288\n 6   0.252     0.1 accuracy binary     0.823   100 0.00272\n 7   0.509     0.1 accuracy binary     0.823   100 0.00281\n 8   0.364     0.1 accuracy binary     0.823   100 0.00280\n 9   0.228     0.1 accuracy binary     0.823   100 0.00278\n10   0.389     0.1 accuracy binary     0.823   100 0.00283\n   .config                \n   <chr>                  \n 1 Preprocessor1_Model1102\n 2 Preprocessor1_Model1101\n 3 Preprocessor1_Model1100\n 4 Preprocessor1_Model1113\n 5 Preprocessor1_Model1118\n 6 Preprocessor1_Model1099\n 7 Preprocessor1_Model1120\n 8 Preprocessor1_Model1110\n 9 Preprocessor1_Model1096\n10 Preprocessor1_Model1112\n# ℹ 2,690 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\nAnd here is the best\n\n::: {.cell}\n\n```{.r .cell-code}\n(hp_best_full <- select_best(fits_full, n = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  penalty mixture .config                \n    <dbl>   <dbl> <chr>                  \n1   0.279     0.1 Preprocessor1_Model1102\n```\n\n\n:::\n:::\n\n\n-----\n\nLet's fit this model to the full sample \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_full_prep <- rec_full |> \n  prep(data_all)\n\nfeat_full <- rec_full_prep |> \n  bake(data_all)\n\nfit_full <- \n  logistic_reg(penalty = hp_best_full$penalty,\n               mixture = hp_best_full$mixture) |> \n  set_engine(\"glmnet\") |> \n  fit(disease ~ ., data = feat_full)\n```\n:::\n\n\n-----\n\nHere are two ways to quantify model performance\n\n- How does the optimal model configuration perform in the full dataset\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_full$disease, predict(fit_full, feat_full)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8382838\n```\n\n\n:::\n:::\n\n\n- And how does the optimal model configuration perform in held-out data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(fits_full) |> \n  arrange(desc(mean)) |> \n  print(n = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,700 × 8\n  penalty mixture .metric  .estimator  mean     n std_err\n    <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl>\n1   0.279     0.1 accuracy binary     0.823   100 0.00277\n  .config                \n  <chr>                  \n1 Preprocessor1_Model1102\n# ℹ 2,699 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\nWe don't care too much about objective performance in our situation.  That is not our question.\n\n- Same sample accuracy is comparable to what you would normally have if you weren't in a machine learning context (except that we did use held-out data to determine the optimal hyperparameter values)\n- Bootstrapped accuracy will remove bias due to overfitting the training data from our performance estimate because it is prediction in new data vs. same sample\n- K-fold would be less biased than bootstrap if we cared about performance of the overall model\n- Some optimization bias remains using either k-fold or bootstrapped accuracy to both select hyperparameters AND assess performance of best model\n- If we really cared about overall performance, we should have held back a test set (or done nested CV)\n\n-----\n\nOur focal question is NOT how well our full model does.  \n\\\n\\\nInstead, we want to know how much do scores from our exercise test improve prediction above baseline (with baseline being all other available characteristics).  This is a relative comparison, not based on absolute performance.\n\\\n\\\nTo quantify this relative comparison, we need to use **feature ablation** to form a compact/reduced model that does not include the features associated with our exercise test\n\n-----\n \nHere is a recipe to feature engineer features associated with this compact model\n\n- We can start with the full recipe and add one more step to remove the features we want to evaluate\n- `step_rm(contains(\"exer_\")`\n- All previous recipe steps remain the same\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_compact <- rec_full |> \n  step_rm(contains(\"exer_\"))\n```\n:::\n\n\n-----\n\nWe need to select/tune hyperparameters for this new model\n\\\n\\\nIt has different complexity than the full model so it might need different (less?) regularization for optimal performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_compact <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(),\n                 mixture = tune()) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_compact,   # use recipe for compact model\n              resamples = splits_boot,\n              grid = grid_glmnet,\n              metrics = metric_set(accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/011/\",\n  file = \"fits_compact\")\n```\n:::\n\n\n-----\n\nConfirm that we found a good set of hyperparameters\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_compact |> \n  plot_hyperparameters(hp1 = \"penalty\", hp2 = \"mixture\", metric = \"accuracy\", \n                       log_hp1 = TRUE)\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(fits_compact) |> \n  arrange(desc(mean)) |> \n  print(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,700 × 8\n   penalty mixture .metric  .estimator  mean     n std_err\n     <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl>\n 1  0.0362     1   accuracy binary     0.784   100 0.00318\n 2  0.0277     1   accuracy binary     0.784   100 0.00299\n 3  0.0350     1   accuracy binary     0.784   100 0.00318\n 4  0.0598     0.4 accuracy binary     0.784   100 0.00303\n 5  0.0339     1   accuracy binary     0.784   100 0.00310\n 6  0.0296     1   accuracy binary     0.784   100 0.00310\n 7  0.102      0.2 accuracy binary     0.784   100 0.00280\n 8  0.0988     0.2 accuracy binary     0.784   100 0.00279\n 9  0.0328     1   accuracy binary     0.784   100 0.00310\n10  0.0579     0.4 accuracy binary     0.784   100 0.00299\n   .config                \n   <chr>                  \n 1 Preprocessor1_Model2541\n 2 Preprocessor1_Model2533\n 3 Preprocessor1_Model2540\n 4 Preprocessor1_Model1656\n 5 Preprocessor1_Model2539\n 6 Preprocessor1_Model2535\n 7 Preprocessor1_Model1372\n 8 Preprocessor1_Model1371\n 9 Preprocessor1_Model2538\n10 Preprocessor1_Model1655\n# ℹ 2,690 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n(hp_best_compact <- select_best(fits_compact, n = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  penalty mixture .config                \n    <dbl>   <dbl> <chr>                  \n1  0.0362       1 Preprocessor1_Model2541\n```\n\n\n:::\n:::\n\n\n-----\n\nFit this compact model configuration to the full dataset\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_compact_prep <- rec_compact |> \n  prep(data_all)\n\nfeat_compact <- rec_compact_prep |> \n  bake(data_all) \n\nfit_compact <- \n  logistic_reg(penalty = hp_best_compact$penalty,\n               mixture = hp_best_compact$mixture) |> \n  set_engine(\"glmnet\") |> \n  fit(disease ~ ., data = feat_compact)\n```\n:::\n\n\n-----\n\nHere are accuracies for full and compact models in full sample where models were fit\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_vec(feat_full$disease, predict(fit_full, feat_full)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8382838\n```\n\n\n:::\n\n```{.r .cell-code}\naccuracy_vec(feat_compact$disease, predict(fit_compact, feat_compact)$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7986799\n```\n\n\n:::\n:::\n\n\n-----\n\nHere are accuracies for these two models assessed by bootstrap resampling\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(fits_full) |> \n  arrange(desc(mean)) |> \n  slice(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 8\n  penalty mixture .metric  .estimator  mean     n std_err\n    <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl>\n1   0.279     0.1 accuracy binary     0.823   100 0.00277\n  .config                \n  <chr>                  \n1 Preprocessor1_Model1102\n```\n\n\n:::\n\n```{.r .cell-code}\ncollect_metrics(fits_compact) |> \n  arrange(desc(mean)) |> \n  slice(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 8\n  penalty mixture .metric  .estimator  mean     n std_err\n    <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl>\n1  0.0362       1 accuracy binary     0.784   100 0.00318\n  .config                \n  <chr>                  \n1 Preprocessor1_Model2541\n```\n\n\n:::\n:::\n\n\n-----\n\nThe compact model is less accurate but.....\n\n- A simple descriptive comparison is not sufficient to justify the use of a costly test\n- We need to be more confident that the test really improves screening in all possible held out samples from our dataset\n- And by how much?\n- How can we compare these two models?\n\n-----\n\n## Nadeau and Bengio (2003) Correlated t-test\n\nWe have 100 bootstrapped accuracies for each model.\n\\\n\\\nCould we compare these?\n\n- We used the bootstrap but if we wanted a less biased estimate we could (should?) have used k-fold\n- We could do 10 repeats of 10-fold to still obtain 100 performance estimates for each model configuration\n\n\\\n\\\nEither way, we have the same 100 held-out samples (we used the same splits) for both compact and full models so these two sets of accuracies (for each of the two models) should be considered paired/repeated.   \n\n  - Not a problem, we could use paired samples t-test\n  - Easiest to think about this paired test as testing if the differences in accuracy for each of the 100 held out sets == 0.  That removes the lack of independence from using the sample 100 held-out sets twice\n\n\\\n\\\nBUT these 100 differences are still not independent\n\n- Each have been estimated using models that were fit with overlapping observations (the held in sets were fit with many of the same observations for each of the k-1 held in folds)\n- If we ignore this violation and simply do a paired-samples t-test, we will have inflation of alpha\n\n-----\n\n@Nadeau2003 ([see pdf](pdfs/nadeauC2003.pdf)) and @Bouckaert2003 ([see pdf](pdfs/bouckaertR2003.pdf)) have explored the degree of dependence among performance estimates using resampling.   \n\\\n\\\nThis was originally done for repeated random train/test splits (e.g., 90/10 splits) but is now also used when doing repeated k-fold.\n\\\n\\\nThe classic paired t-test has the following formula\n\n$t = \\frac{\\overline{x} - 0}{\\sqrt{\\hat{\\sigma^2}*\\frac{1}{n}}}$\n\\\n\\\nThe standard error for the difference (denominator of the t-statistic formula) is too small\n\n-----\n\nNadeau and Benigo adjusted it by $\\frac{\\rho}{1-\\rho}$ where $\\rho = \\frac{n_{test}}{n_{total}}$ or equivalent $\\frac{1}{k}$\n\\\n\\\nThis adjustment yields:\n\n$t = \\frac{\\overline{x} - 0}{\\sqrt{\\hat{\\sigma^2}*(\\frac{1}{n} + \\frac{\\rho}{1-\\rho})}}$\n\n-----\n\nLet's perform this correlated t-test to compare our compact and full models\n\\\n\\\nWe first need to evaluate both models using 10x 10-fold CV splits. \n\\\n\\\nImportant that these are the SAME splits for both model configurations\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2468)\nsplits_cv <-\n  data_all |> \n  vfold_cv(v = 10, repeats = 10, strata = \"disease\")\n```\n:::\n\n\n-----\n\nLets get k-fold accuracy for the full model\n\n- best penalty = 0.2787076\n- best mixture = 0.1\n\n::: {.cell}\n\n```{.r .cell-code}\n# NOTE:  I cant get this to accept anything but the actual numeric values for \n# penalty and mixture. Not sure why.   If no one else solves this problem, \n# I will simplify to a reprex and submit a bug report at some point\ncv_full <- \n  logistic_reg(penalty = 0.2787076, \n               mixture = 0.1) |> \n  set_engine(\"glmnet\") |> \n  fit_resamples(preprocessor = rec_full,  \n            resamples = splits_cv,\n            metrics = metric_set(accuracy))\n```\n:::\n\n\n-----\n\nHere is\n\n- mean performance over 100 held out folds\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(cv_full, summarize = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.826   100 0.00687 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n- and here we pull the 100 metrics\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(cv_full, summarize = FALSE)$.estimate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] 0.8709677 0.8064516 0.8709677 0.8387097 0.9333333 0.7333333 0.9000000\n  [8] 0.6333333 0.8666667 0.7586207 0.8387097 0.8064516 0.7741935 0.7741935\n [15] 0.8333333 0.8000000 0.8333333 0.8333333 0.8666667 0.8965517 0.9032258\n [22] 0.7419355 0.8387097 0.8387097 0.7666667 0.8666667 0.9000000 0.8333333\n [29] 0.8333333 0.7241379 0.7419355 0.9032258 0.8064516 0.8064516 0.8666667\n [36] 0.9000000 0.9000000 0.8000000 0.8000000 0.7586207 0.8064516 0.8064516\n [43] 0.9354839 0.9032258 0.6333333 0.8000000 0.8333333 0.8666667 0.8000000\n [50] 0.8965517 0.8709677 0.8064516 0.7741935 0.9032258 0.8666667 0.8666667\n [57] 0.8333333 0.8666667 0.6666667 0.7931034 0.6129032 0.9032258 0.8387097\n [64] 0.8387097 0.9000000 0.8000000 0.8000000 0.8333333 0.8333333 0.8965517\n [71] 0.7419355 0.8064516 0.9354839 0.7741935 0.9333333 0.6333333 0.9000000\n [78] 0.7666667 0.8000000 0.8965517 0.8709677 0.8709677 0.8709677 0.8064516\n [85] 0.8666667 0.8333333 0.7666667 0.8333333 0.8333333 0.7586207 0.8064516\n [92] 0.8064516 0.8709677 0.7741935 0.9666667 0.8000000 0.7333333 0.8666667\n [99] 0.8333333 0.8965517\n```\n\n\n:::\n:::\n\n\n-----\n\nNow get k-fold accuracy for the compact model with\n\n- best penalty = 0.0362351\n- best mixture = 1\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_compact <- \n  logistic_reg(penalty = .03623515,\n               mixture = 1) |> \n  set_engine(\"glmnet\") |> \n  fit_resamples(preprocessor = rec_compact,  \n            resamples = splits_cv,\n            metrics = metric_set(accuracy))\n```\n:::\n\n\n-----\n\nMean performance of compact model\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(cv_compact, summarize = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.784   100 0.00772 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nand the 100 metrics\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(cv_compact, summarize = FALSE)$.estimate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] 0.7741935 0.7741935 0.9354839 0.6774194 0.8333333 0.8000000 0.8000000\n  [8] 0.6333333 0.8000000 0.7586207 0.8709677 0.8064516 0.7096774 0.6451613\n [15] 0.8333333 0.7000000 0.8000000 0.8000000 0.7333333 0.8620690 0.8387097\n [22] 0.6774194 0.8064516 0.8064516 0.7666667 0.8000000 0.8333333 0.7333333\n [29] 0.9000000 0.7586207 0.6451613 0.8709677 0.6774194 0.8064516 0.7333333\n [36] 0.8000000 0.8000000 0.7666667 0.8333333 0.8275862 0.9354839 0.7741935\n [43] 0.8709677 0.8387097 0.5666667 0.6666667 0.7666667 0.8333333 0.8000000\n [50] 0.7586207 0.8064516 0.8064516 0.6451613 0.9032258 0.7666667 0.7666667\n [57] 0.8000000 0.9333333 0.7000000 0.7931034 0.7096774 0.7741935 0.6451613\n [64] 0.8064516 0.9666667 0.7000000 0.7000000 0.8333333 0.8000000 0.8620690\n [71] 0.7419355 0.8709677 0.8709677 0.7741935 0.8333333 0.6666667 0.8333333\n [78] 0.7333333 0.6666667 0.8620690 0.8064516 0.7419355 0.8709677 0.6774194\n [85] 0.8333333 0.7666667 0.7666667 0.8333333 0.6666667 0.8275862 0.7096774\n [92] 0.7741935 0.8387097 0.7741935 0.8666667 0.8333333 0.7333333 0.8333333\n [99] 0.7000000 0.9310345\n```\n\n\n:::\n:::\n\n\n-----\n\nDefine a function for Nadeau and Bengio (2003) correlated t-test\n\n::: {.cell}\n\n```{.r .cell-code}\n# included in fun_ml.R\nnb_correlated_t_test <- function(cv_fits_full, cv_fits_compact, k = 10){\n\n    cv_metrics_full <- collect_metrics(cv_fits_full, summarize = FALSE)$.estimate\n    cv_metrics_compact <- collect_metrics(cv_fits_compact, summarize = FALSE)$.estimate\n    diffs <- cv_metrics_full - cv_metrics_compact\n    n <- length(diffs)\n    mean_diff <- mean(diffs)\n    var_diffs <- var(diffs)\n    proportion_test <- 1 / k\n    proportion_train <- 1 - proportion_test\n    correction <- (1 / n) + (proportion_test / proportion_train)\n    se = sqrt(correction * var_diffs)\n\n    t = abs(mean_diff/se)\n    p_value <- 2 * pt(t, n - 1, lower.tail = FALSE)\n    tibble(mean_diff = mean_diff, se = se, t = t, df = n - 1, p_value = p_value)\n}\n```\n:::\n\n\n----- \n\nCalculate the t-test.  \n\\\n\\\nIn this instance we likely want a one-tailed test (though of course, that should have been planned in advanced and ideally pre-registered!).   \n\\\n\\\nMy function returns a two-tailed p-value so we should cut it in half.\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_correlated_t_test(cv_fits_full = cv_full, \n                     cv_fits_compact = cv_compact, \n                     k = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  mean_diff     se     t    df p_value\n      <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1    0.0424 0.0228  1.86    99  0.0653\n```\n\n\n:::\n:::\n\n\nThe improvement in prediction accuracy associated with the use of our exercise test protocol is significant (one-tailed).\n\\\n\\\nThese data are unlikely if the full and compact models had the same accuracy.  \n\n-----\n\n## Bayesian estimation for model comparisons\n\n@Benavoli2017 critique the many shortcomings wrt the frequentist approach, and I must admit, I am mostly convinced\n\n- NHST does not provide the probabilities of the null and alternative hypotheses.   \n  - That is what we want\n  - NHST gives us the probability of our data given the null\n- NHST focuses on a point-wise comparison (no difference) that is almost never true.\n- NHST yields no information about the null hypothesis (i.e., when we fail to reject)\n- The inference depends on the sampling and testing intention (think about Bonferonni correction)\n\n-----\n\nThey suggest to use Bayesian parameter estimation as alternative to the t-test.  Bayesian estimation has now been included in tidymodels in the `tidyposterior` package using the `perf_mod()` function.  \n\\\n\\\nYou can (and should!) read more about this implementation of Bayesian Estimation in the associated [vignette](https://tidyposterior.tidymodels.org/articles/Getting_Started.html) AND by reading the help materials on `perf_mod()`\n\n-----  \n\nUsing this approach, we will estimate the posterior probability for values associated with specific parameters of interest.  For our goals, we will care about estimates of three parameters\n\n- The accuracy of the full model\n- The accuracy of the compact model\n- The difference in accuracies between these two models.\n\n-----\n\nWe want to determine the posterior probabilities associated with ranges of values for each of these three model performance parameters estimates.   We can then use these posterior probability distributions to determine that probability that the accuracy of the full model is greater than the accuracy of the compact model.  \n\nIn addition, we can also determine if the increased accuracy of the full model is meaningful (i.e., practically important).\n\nTo accomplish this latter goal, we will: \n\n- Specify a Region of Practical Equivalence (a better alternative to the point-wise null in NHST)\n  - I will define classifiers whose performance are within +-1% as equivalent (not meaningfully different from each other) for our example   \n  - Not worth the effort if my test doesn't improve screening accuracy by at least this\n\n----- \n\nTo estimate posterior probabilities for these three parameter estimates, we need to \n\n- set prior probabilities for these parameter estimates.  These should be broad/uninformative in most instances unless you have substantial prior information about credible values.  \n- Collect data on these estimates.  This will be the same as before - the 100 estimates of accuracy using 10x10 fold CV for both the full and compact models.\n\n\\\n\\\nUsing these priors and these data, we can derive the posterior probabilities for our three performance estimates\n\n----- \n\nLets do this step by step.  We will use the `tidyposterior` package.  It in not included when we load `tidymodels` so we will load it now\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyposterior)\n```\n:::\n\n\n-----\n\nWe need to make a dataframe of our 100 performance estimates for the full and compact models.  Here is the code to do this using our previous resamples of our models\n\n- Make dataframes of the accuracies from the full model and the compact model\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_full <-\n  collect_metrics(cv_full, summarize = FALSE) |>\n  filter(.metric == \"accuracy\") |>\n  select(id, id2, full = .estimate) |> \n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 3\n   id       id2     full\n   <chr>    <chr>  <dbl>\n 1 Repeat01 Fold01 0.871\n 2 Repeat01 Fold02 0.806\n 3 Repeat01 Fold03 0.871\n 4 Repeat01 Fold04 0.839\n 5 Repeat01 Fold05 0.933\n 6 Repeat01 Fold06 0.733\n 7 Repeat01 Fold07 0.9  \n 8 Repeat01 Fold08 0.633\n 9 Repeat01 Fold09 0.867\n10 Repeat01 Fold10 0.759\n# ℹ 90 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_compact <-\n     collect_metrics(cv_compact, summarize = FALSE) |>\n     filter(.metric == \"accuracy\") |>\n     select(id, id2, compact = .estimate)\n```\n:::\n\n\n-----\n\nNow we need to join these dataframes, matching on repeat and fold ids\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresamples <- accuracy_full |> \n  full_join(accuracy_compact, by = c(\"id\", \"id2\")) |> \n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 4\n   id       id2     full compact\n   <chr>    <chr>  <dbl>   <dbl>\n 1 Repeat01 Fold01 0.871   0.774\n 2 Repeat01 Fold02 0.806   0.774\n 3 Repeat01 Fold03 0.871   0.935\n 4 Repeat01 Fold04 0.839   0.677\n 5 Repeat01 Fold05 0.933   0.833\n 6 Repeat01 Fold06 0.733   0.8  \n 7 Repeat01 Fold07 0.9     0.8  \n 8 Repeat01 Fold08 0.633   0.633\n 9 Repeat01 Fold09 0.867   0.8  \n10 Repeat01 Fold10 0.759   0.759\n# ℹ 90 more rows\n```\n\n\n:::\n:::\n\n\n-----\n\nNow we can use `perf_mod()` to derive the posterior probabilites for the accuracy of each of these two models\n\n- We need to specify a model with parameters in formula.  Here we indicate that we have a multi-level model with repeated observation of accuracy across folds (id2) nested within repeats (id).  This handles dependence associated with repeated observations of accuracy using similar models in k-fold cv.\n\n- We are interested in the intercept from this model listed in formula.  The intercept value will represent the accuracy estimate for each model.\n\n- The default for `perf_mod()` will be to constrain the variances of the intercept parameter estimate to be the same across models.  This may be fine for some performance metrics (e.g., rmse) but for binary accuracy the variance is dependent on the mean.  Therefore we allow these variances to be different using `hetero_var = TRUE`\n\n- In some instances (e..g., rmse), we may want to allow the errors in our model to be something other than Gaussian (though this is often a reasonable assumption by the central limit theorem).  You can change the `family` for the errors if needed.  See vignette and help on `perf_mod()`.  Here, we use the default Gaussian distribution.\n\n- This is an iterative process using a Markov chain Monte Carlo method (Hamilton Monte Carlo) so we need to set a seed (for reproducibility), and the number of iterations and chains (beyond the scope of this course to dive into this method).  I provide default values for `iter` and `chains` because you may need to increase these in some instances for the method to converge on valid values.  You can often address converge and other warnings by increasing `iter`, `chains` or `adapt_delta`.  You can read more about these warnings and issues [here](https://mc-stan.org/misc/warnings.html), [here](https://mc-stan.org/rstanarm/reference/adapt_delta.html#:~:text=For%20the%20No%2DU%2DTurn,not%20set%20to%20%22sampling%22%20.), [here](https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup), and [here](http://singmann.org/hierarchical-mpt-in-stan-i-dealing-with-convergent-transitions-via-control-arguments/) to start.\n\n-----\n\nHere is the code\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\npp <- cache_rds(\n  expr = {\n    perf_mod(resamples, \n            formula = statistic ~ model + (1 | id2/id),\n            # defaults but may require increases\n            iter = 2000, chains = 4,  \n            # for more Gaussian distribution of accuracy\n            transform = tidyposterior::logit_trans,\n            hetero_var = TRUE, # for accuracy\n            family = gaussian, # default but could change depending on DV\n            # increase adapt_delta (e.g., .99, .999) to \n            # fix divergent transitions\n            adapt_delta = .99)  \n  },\nrerun = rerun_setting,\ndir = \"cache/011/\",\nfile = \"pp\")\n```\n:::\n\n\n-----\n\nIn contrast to the NHST approach, we now have what we really want - posterior probabilities. Lets look at them\n\nWe can view the posterior probability distributions using an autoplot method for perf_mod objects. \n\n- These density plots tell how probable various values are for the accuracy of each model\n\n- The probabilities associated with any region of the curve is equal to the area under that curve for that region.  This will tell you the probability associated with that range of values for accuracy.\n\n- You can easily see in this instance that the probable values for accuracy are higher generally for full model than the compact model\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp |> autoplot()\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n-----\n\nYou will likely want to publish a figure showing these posterior probability distributions so you may want to fine tune the plots.  Here are some code options using ggplot\n\nHere is the same density plots using ggplot so you can now edit to adjust as you like\n\n::: {.cell}\n\n```{.r .cell-code}\npp |> \n  tidy(seed = 123) |> \n  mutate(model = fct_inorder(model)) |>\n  ggplot() + \n  geom_density(aes(x = posterior, color = model) )\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n-----\n\nWe are actually sampling from the posterior distribution so it might make more sense to display these as histograms rather than density plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp |> \n  tidy(seed = 123) |> \n  mutate(model = fct_inorder(model)) |>\n  ggplot() + \n  geom_histogram(aes(x = posterior, fill = model), color = \"white\", alpha = 0.4,\n                 bins = 50, position = \"identity\") \n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n-----\n\nOr maybe you want to facet the histograms if the overlap is difficulty to view\n\n::: {.cell}\n\n```{.r .cell-code}\npp |> \n  tidy(seed = 123) |> \n  mutate(model = fct_inorder(model)) |>\n  ggplot(aes(x = posterior)) + \n  geom_histogram(color = \"white\", fill = \"blue\", bins = 30) + \n  facet_wrap(~ model, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n-----\n\nWe can also calculate the 95% Higher Density Intervals (aka, 95% Credible Intervals; the Bayesian alternative to the 95% Confidence Intervals) for the accuracy of each model.  This is the range of parameter estimate values that include 95% of the credible values.  Kruschke described this in the [assigned reading](pdfs/pkruschke2018a.pdf.pdf).\n\n::: {.cell}\n\n```{.r .cell-code}\npp |> tidy(seed = 123) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  model    mean lower upper\n  <chr>   <dbl> <dbl> <dbl>\n1 compact 0.796 0.780 0.811\n2 full    0.837 0.824 0.850\n```\n\n\n:::\n:::\n\n\n-----\n\nBut what we really want is derive the posterior probability for the difference in accuracy between the two models.  This will let us determine credible values for the magnitude of the difference and determine if this difference is meaningful.\n\nWe said early that we would define a ROPE of +-.01 around zero.  The models are only meaningful different if their accuracies differ by at least 1%\n\nLets visualize the posterior probability distribution for the difference along with this ROPE using the built in autoplot function\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp |> contrast_models(seed = 4) |> autoplot(size = .01)\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n-----\n\nWe could make more pretty plots directly in ggplot\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp |> \n  contrast_models(seed = 4) |> \n  ggplot() +\n  geom_density(aes(x = difference), color = \"blue\")+\n  geom_vline(aes(xintercept = -.01), linetype = \"dashed\") + \n  geom_vline(aes(xintercept = .01), linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\nor my prefered histogram\n\n::: {.cell}\n\n```{.r .cell-code}\npp |> \n  contrast_models(seed = 4) |> \n  ggplot(aes(x = difference)) + \n  geom_histogram(bins = 50, color = \"white\", fill = \"blue\")+\n  geom_vline(aes(xintercept = -.01), linetype = \"dashed\") + \n  geom_vline(aes(xintercept = .01), linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n-----\n\nBut perhaps most important, lets calculate the probability that the full model is more accurate than the compact model\n\n- The mean increase in accuracy is 4.13%\n- The 95% HDI is 2.94% - 5.35%\n- The probability that the full model is meaningfully higher than the compact model (i.e., what proportion of the credible values are above the ROPE) is 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp |> contrast_models(seed = 4) |> summary(size = .01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n  contrast        probability   mean  lower  upper  size pract_neg pract_equiv\n  <chr>                 <dbl>  <dbl>  <dbl>  <dbl> <dbl>     <dbl>       <dbl>\n1 full vs compact           1 0.0413 0.0294 0.0535  0.01         0     0.00025\n  pract_pos\n      <dbl>\n1      1.00\n```\n\n\n:::\n:::\n\n\nAlternatively, using the approach proposed by Kruschke (2018), you can conclude that the full model is meaningfully better than the compact model if the 95% HDI is fully above the ROPE.  In this instance it is (.0294- .0535 is all above .01)\n\n-----\n\nFinally, in some instances, you may not want to use the ROPE.   \n\n- Instead, you may simply want the posterior probability that the full model performs better than the compact model.   \n- This is probability is provided in the `probability` column of the table.\n- You can also set the size of the ROPE to 0 (though  not necessary)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp |> contrast_models(seed = 4) |> summary(size = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n  contrast        probability   mean  lower  upper  size pract_neg pract_equiv\n  <chr>                 <dbl>  <dbl>  <dbl>  <dbl> <dbl>     <dbl>       <dbl>\n1 full vs compact           1 0.0413 0.0294 0.0535     0        NA          NA\n  pract_pos\n      <dbl>\n1        NA\n```\n\n\n:::\n:::\n\n\n-----\n\n## Feature Importance\n\nThere as been increasing focus on improving the interpretability of machine learning models that we are using.  There are numerous reasons to want to better understand why our models make the predictions that they do.  \n\n- The growing set of tools to interpret our models can help address our **explanatory questions**\n- But they can also help us **find errors** in our models\n- And they can **detect possible bias** (we will focus explicitly on algorithmic bias in later units)\n\n-----\n\nFeature importance metrics are an important tool to better understand how our models work.\n\\\n\\\nThese metrics help us understand which features in our models contribute most to the predictions that the model makes.  \n\\\n\\\nFor some models, interpretation and identification of important features is easy.  \n\\\n\\\nFor example, if we standardize the features in a glm or glmnet model, we can interpret the absolute magnitude of the parameter estimates (i.e., the coefficients) as an index of the global (i.e., across all observations) importance of each feature.  \n\n- You can use the [vip]() package to extract these **model-specific** feature importance metrics, but you can often just get them directly from the model as well\n- More info on the use of vip package is available [elsewhere](https://cran.r-project.org/web/packages/vip/vignettes/vip.html)\n\n-----\n\nBut for other models, we need different approaches.  \n\\\n\\\nThere are many **model-agnostic** (i.e., can be used across all statistical algorithms) approaches to quantify the importance of a feature, but we will focus on two:  \n\n- Permutation Feature Importance\n- Shapley Values\n\n-----\n\nWe follow [recommendations from the tidymodels folks](https://www.tmwr.org/explain) and use the DALEX and DALEXtra packages for model agnostic approaches to feature importance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DALEX, exclude= \"explain\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWelcome to DALEX (version: 2.4.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\nAdditional features will be available after installation of: ggpubr.\nUse 'install_dependencies()' to get all suggested dependencies\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(DALEXtra)\n```\n:::\n\n\n-----\n\nLets first get some coding issues accomplished before we dig into the details of the two feature importance metrics\n\\\n\\\nTo calculate these importance metrics, we will need access to the raw features and outcome. Lets get those again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeat_full <-  rec_full_prep |> \n  bake(data_all)\n```\n:::\n\n\n-----\n\nWe will need to have a df for the features (without the outcome) and a separate vector for the outcome\n\n- features are easy.  Just select out the outcome\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- feat_full |> select(-disease)\n```\n:::\n\n\n- For outcome, we need to select it first, convert to 0/1, and then pull the vector out of the dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- feat_full |> \n  select(disease) |>\n  mutate(disease = if_else(disease == \"yes\", 1, 0)) |> \n  pull(disease)\n```\n:::\n\n\n-----\n\nWe also need a specific predictor function that will work with the DALEX package\n\nWe will write a custom function that \"wraps\" around our tidymodels `predict()` function\n\n- It needs two parameters named `model` and `newdata` and it needs to return a single column of probabilites for classification problems\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_wrapper <- function(model, newdata) {\n  predict(model, newdata, type = \"prob\") |> \n    pull(.pred_yes)\n}\n```\n:::\n\n\n-----\n\nWe will also need an `explainer` object based on our model and data.  The `explain_tidymodels()` function in DALEXtra will create this object for us.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexplain_full <- explain_tidymodels(fit_full, # our model object \n                                   data = x, # features without outcome\n                                   y = y, # outcome\n                                   # our predictor function\n                                   predict_function = predict_wrapper)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPreparation of a new explainer is initiated\n  -> model label       :  model_fit  (  default  )\n  -> data              :  303  rows  17  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  303  values \n  -> predict function  :  predict_function \n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package parsnip , ver. 1.2.0 , task classification (  default  ) \n  -> predicted values  :  numerical, min =  0.1257285 , mean =  0.4587467 , max =  0.9100119  \n  -> residual function :  residual_function \n  -> residuals         :  numerical, min =  0 , mean =  0 , max =  0  \n  A new explainer has been created!  \n```\n\n\n:::\n:::\n\n\n-----\n\nFinally, we need to define a custom function for our performance metric\n\n- It needs to have two parameters: `observed` and `predicted`\n- We can create a wrapper function around `accuracy_vec()` to fit these needs\n- For accuracy, we need to transform the predicted probabilites from our prediction function to class predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy_wrapper <- function(observed, predicted) {\n  observed <- fct(if_else(observed == 1, \"yes\", \"no\"),\n                  levels = c(\"yes\", \"no\"))\n  predicted <- fct(if_else(predicted > .5, \"yes\", \"no\"), levels  = c(\"yes\", \"no\"))\n  accuracy_vec(observed, predicted)\n}\n```\n:::\n\n\\\n\\\nWe are now ready to calculate feature importance metrics\n\n-----\n\n### Permutation Feature Importance\n\nThe first model agnostic approach to calculating feature important is called Permutation Feature Importance\n\\\n\\\nThis approach is very straight forward.  This approach says - if we want to calculate the importance of any specific feature, we can compare our performance metric using the original features to the performance metric we get if we permute (i.e., shuffle) the values for the feature we are evaluating.  \n\\\n\\\nBy randomly shuffling the values for the feature, we break the relationship between that feature and the outcome so it no longer contributes to the predictions.   If performance doesn't change much, then that feature is not important.  If performance goes down a lot, the feature is important.  \n\n- The function can provide `raw` performance (will give us performance for the non-permuted model and then performance for the model with each feature permuted, one at a time) \n- `difference` performance measure, which is the difference between the permuted model and the non-permuted mode, separately for each feature\n- `ratio` performance measure, which is ($\\frac{permuted}{original}$), separately for each feature\n\n-----\n\nTo calculate accuracy after permuting each feature, we use `model_parts()`.  We pass in \n\n- our explainer object\n- set the type (`raw` in this example)\n- indicate our custom accuracy function\n- set B to indicate number of permutations to perform\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456)\nimp_permute <- model_parts(explain_full, \n                               type = \"raw\", \n                               loss_function = accuracy_wrapper,\n                               B = 100)\n```\n:::\n\n\n-----\n\nLets look at what this function returns\n\n- the first row contains the accuracy for the full model (with no features permuted)\n- last row is a baseline models (performance with all features permuted)\n- Other row show the accuracy of the model when that specific feature is permuted\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_permute\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                       variable mean_dropout_loss     label\n1                  _full_model_         0.8382838 model_fit\n2                            ca         0.7991419 model_fit\n3         thal_reversabledefect         0.8045875 model_fit\n4                  exer_ang_yes         0.8177888 model_fit\n5                   exer_max_hr         0.8309241 model_fit\n6               exer_st_depress         0.8314521 model_fit\n7                cp_non_anginal         0.8325083 model_fit\n8            exer_st_slope_flat         0.8333333 model_fit\n9                      sex_male         0.8335974 model_fit\n10             thal_fixeddefect         0.8382508 model_fit\n11                         chol         0.8382838 model_fit\n12                 fbs_elevated         0.8382838 model_fit\n13            rest_ecg_wave_abn         0.8382838 model_fit\n14      exer_st_slope_downslope         0.8382838 model_fit\n15                  cp_atyp_ang         0.8386139 model_fit\n16                      rest_bp         0.8388119 model_fit\n17 rest_ecg_ventric_hypertrophy         0.8401650 model_fit\n18                          age         0.8436964 model_fit\n19                   _baseline_         0.5059406 model_fit\n```\n\n\n:::\n:::\n\n\n-----\n\nWe can use the built in plot function from DALEX to display this\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(imp_permute)\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\n-----\n\nOr we can plot it directly.  Here is an example from the [tidymodels folks](https://www.tmwr.org/explain)    \n\n::: {.cell}\n\n```{.r .cell-code}\nfull_model <- imp_permute |>  \n    filter(variable == \"_full_model_\")\n  \nimp_permute |> \n  filter(variable != \"_full_model_\",\n         variable != \"_baseline_\") |> \n  mutate(variable = fct_reorder(variable, dropout_loss)) |> \n  ggplot(aes(dropout_loss, variable)) +\n  geom_vline(data = full_model, aes(xintercept = dropout_loss),\n             linewidth = 1.4, lty = 2, alpha = 0.7) +\n  geom_boxplot(fill = \"#91CBD765\", alpha = 0.4) +\n  theme(legend.position = \"none\") +\n  labs(x = \"accuracy\", \n       y = NULL,  fill = NULL,  color = NULL)\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n\n-----\n\nWe can also permute a set of features to quantify the contribution of the full set\n\\\n\\\nThis is what we would want for our example, were we want to know the contribution of the four features that represent our exercise test.\n\\\n\\\nTo do this, we pass in a list of vectors of the groups.  Here we provide just one group that we name `exer_test`\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456)\nimp_permute_group <- model_parts(explain_full, \n                               type = \"raw\", \n                               loss_function = accuracy_wrapper,\n                               B = 100,\n                               variable_groups = list(exer_test = \n                                                        c(\"exer_ang_yes\",\n                                                          \"exer_max_hr\",\n                                                          \"exer_st_depress\", \n                                                          \"exer_st_slope_downslope\")))\n```\n:::\n\n\n-----\n\nThe results show that permuting these four features as a set drops accuracy from 0.838 to 0.738\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_permute_group\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      variable mean_dropout_loss     label\n1 _full_model_         0.8382838 model_fit\n2    exer_test         0.7378218 model_fit\n3   _baseline_         0.5119472 model_fit\n```\n\n\n:::\n:::\n\n\n-----\n\n### Shapley Values\n\nShapley values provide insight on the importance of any feature to the prediction for a single observation - often called **local importance** (vs. global importance as per the permutation feature importance measure above).\n\\\n\\\nShapley values can also be used to index global importance by averaging the local shapley values for a feature across all (or a random sample) of the observations.\n\n-----\n\nShapley values are derived from Coalition Game Theory.  \n\\\n\\\nThey provide the average marginal contribution to prediction (for a single observation) of a feature value across all possible coalitions of features (combinations of sets of features from the null set to all other features).\n\\\n\\\n@IML provides a [detailed account](https://christophm.github.io/interpretable-ml-book/shapley.html#shapley) of the theory behind these values and how they are calculated which I will not reproduce here.\n\n-----\n\nLets calculate Shapley Values for the first observation in our dataset\n\nTheir features values were\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_num <- 1\nx1 <- x |> \n  slice(obs_num) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1\nColumns: 17\n$ age                          <dbl> 0.9471596\n$ rest_bp                      <dbl> 0.756274\n$ chol                         <dbl> -0.2644628\n$ exer_max_hr                  <dbl> 0.01716893\n$ exer_st_depress              <dbl> 1.085542\n$ ca                           <dbl> -0.7099569\n$ sex_male                     <dbl> 0.6850692\n$ cp_atyp_ang                  <dbl> -0.44382\n$ cp_non_anginal               <dbl> -1.772085\n$ fbs_elevated                 <dbl> 2.390484\n$ rest_ecg_wave_abn            <dbl> -0.115472\n$ rest_ecg_ventric_hypertrophy <dbl> 1.021685\n$ exer_ang_yes                 <dbl> -0.69548\n$ exer_st_slope_flat           <dbl> -0.9252357\n$ exer_st_slope_downslope      <dbl> 3.658449\n$ thal_fixeddefect             <dbl> 3.972541\n$ thal_reversabledefect        <dbl> -0.7918057\n```\n\n\n:::\n:::\n\n\nAnd we can get Shapley values using `predict_parts()`\n\n::: {.cell}\n\n```{.r .cell-code}\nsv <- predict_parts(explain_full, \n                    new_observation = x1,\n                    type = \"shap\",\n                    B = 25)\n```\n:::\n\n\n-----\n\nThere is a built in plot function for shap values\n\\\n\\\nFor this first observation\n\n- The values for each feature are listed in the left margin\n- Bars to the right (e.g., sex_male) indicate that their feature value increases their probability of disease\n- Bars to the left indicate that their feature value decreases their probability of disease  \n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sv)\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-60-1.png){width=672}\n:::\n:::\n\n\n-----\n\nWe can use these Shapley values for the local importance of the features for each observation to calculate the global importance of these features.  \n\\\n\\\nFeatures that have big absolute Shapley values on average across observation are more important.   Let's calculate this.\n\\\n\\\nFirst we need a function to get shapley values for each observation (along with the feature values for a nicer plot)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_shaps <- function(df1){\n  predict_parts(explain_full, \n                new_observation = df1,\n                type = \"shap\",\n                B = 25) |> \n    filter(B == 0) |> \n    select(variable_name, variable_value, contribution) |> \n    as_tibble()\n}\n```\n:::\n\n\n-----\n\nAnd then we can map this function over observations to get the Shapley values for each observation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlocal_shaps <- cache_rds(\n  expr = {\n    x |>\n      slice_sample(prop = 1/5) |> # take random sample to reduce computation time\n      mutate(id = row_number()) |>\n      nest(.by = id, .key = \"dfs\") |>   # nest a dataframe for each observation\n      mutate(shaps = map(dfs, \\(df1) get_shaps(df1))) |> \n      select(-dfs) |>\n      unnest(shaps)\n  },\n  rerun = rerun_setting,\n  dir = \"cache/011\",\n  file = \"local_shaps\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlocal_shaps |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n     id variable_name  variable_value contribution\n  <int> <chr>          <chr>                 <dbl>\n1     1 age            1.058             0.00704  \n2     1 ca             -0.71            -0.0486   \n3     1 chol           1.281             0.0000232\n4     1 cp_atyp_ang    -0.4438           0.00286  \n5     1 cp_non_anginal 0.5624            0.0107   \n6     1 exer_ang_yes   -0.6955          -0.0362   \n```\n\n\n:::\n:::\n\n\n**Programming note**:  This code demonstrates another nice R programming technique using `nest()` and `unnest()` in combination with `map()` and list-columns. For more info, see [this chapter](https://r4ds.had.co.nz/many-models.html) in @RDS and the vignette on nesting (`vignette(\"nest\")`).\n\n-----\n\nNow that we have Shapley values for all observations, we can calculate the mean absolute Shapley value across observations and plot it.\n\n- Across all observations, `ca` contributes to an average change of .06 from the mean predicted probability of disease.\n- One of the features from our exercise test, `exer_ang_yes`, contributes about .05 change from mean predicted probability of disease.\n- The other `exer_` features are not far behind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlocal_shaps |>\n  mutate(contribution = abs(contribution)) |>\n  group_by(variable_name) |>\n  summarize(mean_shap = mean(contribution)) |>\n  arrange(desc(mean_shap)) |>\n  mutate(variable_name = factor(variable_name),\n         variable_name = fct_reorder(variable_name, mean_shap)) |>\n  ggplot(aes(x = variable_name, y = mean_shap)) +\n  geom_point() +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-64-1.png){width=672}\n:::\n:::\n\n\n-----\n\nFor a more advanced plot (a sina plot; not displayed here) we could superimpose the individual local Shapley values and color them based on the feature score.   \n\\\n\\\nThis would allow us to show the direction of the relationship between the Shapley values and feature values.  \n\\\n\\\nSee FIGURE 9.26 in @IML for an example of this type of plot.\n\n-----\n\nShapley values are attractive relative to other approaches because\n\n- They have a solid theoretical basis\n- Sound statistical properties (Efficiency, Symmetry, Dummy and Additivity - see @IML)\n- Can provided a unified perspective across both local and global importance.\n\n-----\n\nHowever, they can be VERY time consuming to calculate (particularly if you want to use them for global importance such that you need them for all/many observations).  \n\nThere are computational shortcuts available but even those can be very time consuming in some instances (though XGBoost has a very fast implementation that we use regularly).\n\n(Note that for decision tree based algorithms SHAP provides a more computationally efficient way to estimate Shapley values - see [section 9.6](https://christophm.github.io/interpretable-ml-book/shap.html) in @IML for more detail.)\n\n-----\n\n## Visual Approaches to Understand our Models\n\nWe can also learn about how our features are used to make predictions in our models using visual approaches.   \n\\\n\\\nThere are two key plots that we can use:\n\n- Partial Dependence (PD) Plots\n- Accumulated Local Effects (ALE) Plots\n\n-----\n\n### Partial Dependence (PD) Plots\n\nThe Partial dependence (PD) plot displays the marginal effect of a target feature or combination of features on the predictions from a model.  \n\\\n\\\nIn essence, the prediction for any value of a target feature is the average prediction across cases if we set all cases to have that value for the target feature but their observed values for all other features.\n\\\n\\\nWe can use PD plots to understand whether the relationship between a target feature and the outcome is linear, monotonic, or more complex.  It may also help us visualize and understand if interactions between features exist (if we make a PD plot for two target features).  \n\n-----\n\nThe PD Plot is attractive because \n\n- it is easy to understand (prediction for each feature value averaged across observed values for all other features)\n- if the target feature is uncorrelated with all other features, its interpretation is clear, it is how the average prediction changes as the target features changes values.\n- it is computationally easy to implement\n- it has a causal (for the model, not the real world!) interpretation. This is what happens to the predciction if we manipulate the values of the target feature but hold all other features constant at their observed values.\n\n-----\n\nHowever:\n\n- The assumption that the target feature is not correlated with the other features is likely unrealistic in many/most instances\n- This plot (but also other plot methods) are limited to 1 - 2 features in combination.\n- It may hide effects when interactions exist\n\n-----\n\n### Accumulated Local Effects (ALE) Plots\n\nIf the features are correlated, the partial dependence plot should not be used because the plots will otherwise be based on combinations of the target feature and other features that may never occur (given the feature correlations).\n\\\n\\\nMolnar describes how this problem of correlated features and unrealistic combinations of features can be solved by M-Plots that plot the average effect of a target feature using the conditional values on other features (i.e., only using realistic values for the other features based on their correlations with the target feature).  Unfortunately, this too is sub-optimal because it will confound the effect of the target feature with the effects of the other features that are correlated with it.\n\n-----\n\nAccumulated Local Effects (ALE) plots also use conditional values of other features to solve the correlated features problem.  However, ALE plots solve the confounding problem by calculating **differences** in predictions associated with changes in the target feature rather than average predictions for each value of that target feature. These differences hold the other features values (mostly) constant to remove their effects.  \n\\\n\\\nALE plots are the preferred plot in situations where  you expect your target feature to be correlated with other features (which is likely most situations.)\n\n-----\n\nWe will use the `DALEX` package again to make these PD and ALE plots.\n\\\n\\\nIt will require the explainer object we created earlier for feature importance\n\\\n\\\nOtherwise, the code is very straight-forward.  Here we get the predicted values for an ALE plot to examine the effect of one of the features from our exercise test (`exer_max_hr`) on disease probabilities. \n\nIf we wanted a PD plot, we could simply substitute `partial` for `accumulated`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nale <- model_profile(explainer = explain_full,\n                     type = \"accumulated\",\n                     variables = \"exer_max_hr\",\n                     N = NULL)  # to use full sample (default is 100)\n```\n:::\n\n\n-----\n\nThere is a default plot function for these plot object (or you could use the data in the object to make your own ggplot)\n\\\n\\\nThe probability of disease decreases as max hr increases in the exercise test\n\n::: {.cell}\n\n```{.r .cell-code}\nale |> plot()\n```\n\n::: {.cell-output-display}\n![](011_explanation_files/figure-html/unnamed-chunk-66-1.png){width=672}\n:::\n:::\n\n\n-----\n\n\n\n## Summary and Closing Thoughts\n\nWhen pursuing purely explanatory goals with machine learning methods, we can: \n\n- Use resampling with the full dataset to determine appropriate model configuration\n  - Best statistical algorithm\n  - Which covariates\n  - Other \"researcher degrees of freedom\" such as handling of outliers, transformations of predictors\n\n- Use permutation test for a test that is analogous to the linear model test of R^2^\n  - Useful if we have a question about our full set of features\n\n- Use model comparisons (Frequentist or Bayesian) in combination with feature ablation to test effect of feature or set of features\n\n- We can use feature importance measures (permutation or Shapley) to understand the contributions that various features make to prediction for an observation (local) or across observations (global)\n\n- We can explore the shape (and magnitude?) of relationships using PD and ALE plots\n\n- We can explore the strength of interactions among our features\n\n\n## Discussion\n\n### Annoucements\n\n- Review conceptual exam\n- RMD files\n\n\n### Questions\n\n#### ALE and PD plots\n- What is the difference between PD plot and ALE plot? And when should we use they for specific?\n\n- I am still confused about partial dependence plots and ale plots and when we want to use them. \n\n- I am confused about the question in the unit 9 quiz. when you have moderate or stronger correlations among your features, ALE and M-plots both could be used to visualize the effects of your features on the outcome. Based on the web book, we learned that M-plot is sub-optimal, which means we better use ALE right? and M-plots are not recommended. \n\n#### Prediction vs. explanation\n- I think it would be useful to talk about how to use these same models that we have been building for explanatory purposes rather than predictive ones. Is that when interpretability matters most? I feel like some of the reading on interpretability contradicted what we talked about earlier on in the semester about how in a predictive model, interpretability doesn't necessarily matter.\n\n- It seems that there is a trade-off regarding interpretability. How to make a good decision about the degree of the interpretability so that the system will be not manipulated when there is a misalignment/mismatch between the goals of the creator and the user? \n\n- If I understood correctly, interpretability/explainability is about the model predictions, while comprehensibility is about users' understanding of the explanations. Transparency is about the algorithms. These concepts are kind of confusing. It would be nice if you could help us distinguish them more conceptually. \n\n#### Permutation test\n\n- Mathematically, is there not a way to estimate what the accuracy would be even if there were not a relationship between predictors and outcome? Wouldn't the estimated accuracy simply be the proportion of observations in the dominant class? I guess I do not see why the permutation test is necessary, then.\n\n- shuffling a feature's values removes the relationship between that feature and the outcome, and I think the prediction error of the model increases. \n\n#### Bayesian estimation\n\n- In case of unit 9, K-fold is preferred, the question goes how to balance the choice between K-fold and bootstrap?\n\n- What's are prior posterior probabilities?\n\n- HDI and ROPE.\n\n- I'm still a little confused about HDI values. I understand that when HDI values are above the ROPE it tells us that our full model is meaningfully better than our compact model, but I don't understand what the HDI values themselves represent or how they are calculated.\n\n- If there’s no difference between our model’s performance and the performance of a similar model in a permuted dataset, (how) can we distinguish whether that’s because there’s no signal in our predictors or because our model failed to exploit that signal?\n\n- I am especially confused about Bayesian estimation for model comparisons. Does convergence mean that the best parameters for the model have been found, and why do we do this specifically with Bayesian estimation\n\n#### Shapley values\n\n- I think I just need a deep dive into shapely values\n\n- What are shapley values and how do we interpret them?\n\n- Revisiting Shapley values\n\n- Since Shapley values average across all possible coalitions, do they implicitly take into account interactions?\n\n#### Interations and H-Statistic\n\n- The IML book says “The H-statistic has a meaningful interpretation: The interaction is defined as the share of variance that is explained by the interaction.” But one thing I’m confused about: what variance are we talking about here (i.e., the share of variance in what)? \n\n- In particular, I'm confused about how to interpret the H-statistic in the context of looking at all 2-way interactions with a specific feature, since sometimes those interaction strengths with that feature seem to sum to more than the overall interaction strength for that feature (e.g., looking at figures 8.20 and 8.21 in the IML book), and I don't know how that would be. \n\n- After detecting interactions, what should we do as the next step? Is that possible to detect three-way interactions?\n\n- Can we go over more details about detecting interactions?\n\n- could you explain more about Friedman’s H-statistic\n",
    "supporting": [
      "011_explanation_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}