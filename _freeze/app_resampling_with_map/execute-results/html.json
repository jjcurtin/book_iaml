{
  "hash": "2d1c2c43f25cd417a4dcd9d15a2fd167",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Demonstration of using mapping to fit_resamples or tune_grid\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::%||%()   masks base::%||%()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.5     ✔ rsample      1.2.1\n✔ dials        1.2.1     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.3.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::%||%()     masks base::%||%()\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n```\n\n\n:::\n:::\n\n\n\nCreate a simple dataset and get 100 bootstrap samples\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tibble(x = rnorm(100), y = factor(rbinom(100, 1, 0.5)))\nset.seed(123456)\nsplits <- bootstraps(d, times = 100)\n```\n:::\n\n\n\n## fit_resmples done manually\n\nSet up a simple recipe for feature engineering for the logistic regression\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- recipe(y ~ x, data = d)\n```\n:::\n\n\nCreate a function to fit logistic regression to held-in training data\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lr <- function(held_in) {\n  logistic_reg() |> \n    set_engine(\"glm\") |> \n    set_mode(\"classification\") |> \n    fit(y ~ ., data = held_in)\n}\n```\n:::\n\n\n\nUse `map()` and list columns to save the individual steps for evaluating the model in each resample.  The following steps are done for EACH resample using `map()` or `map2()`\n\n- prep the recipe with held-in data\n- bake the recipe using `new_data = NULL` to get held-in features\n- bake the recipe using `new_data = assessment(split)` to get held-out features\n- fit the model using the held-in features\n- get predictions using the model with the held-out features\n- calculate the accuracy of the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels <- tibble(prep_recs = map(splits$splits, \n                                 \\(split) prep(rec, training = analysis(split))),\n                 held_ins = map2(splits$splits, prep_recs, \n                                \\(split, prep_rec) bake(prep_rec, new_data = NULL)),\n                 held_outs = map2(splits$splits, prep_recs, \n                                 \\(split, prep_rec) bake(prep_rec, \n                                                         new_data = assessment(split))),\n                 models = map(held_ins, \\(held_in) fit_lr(held_in)),\n                 predictions = map2(models, held_outs, \n                                    \\(model, held_out) predict(model, held_out, type = \"class\")),\n                 accuracy = map2_dbl(predictions, held_outs, \n                                     \\(pred, held_out) accuracy_vec(held_out$y, pred$.pred_class)))\n```\n:::\n\n\nThe pipline above creates a tibble with columns for each of the intermediate products and the accuracy of the model in each resample.  All but the last columns are list columns that can hold objects of any time.  The final column is a double column that holds the accuracy of the model in each resample.  That is why we used `map_dbl()` to create the accuracy column.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100\nColumns: 6\n$ prep_recs   <list> [x, y, double, numeric, factor, unordered, nominal, predi…\n$ held_ins    <list> [<tbl_df[100 x 2]>], [<tbl_df[100 x 2]>], [<tbl_df[100 x …\n$ held_outs   <list> [<tbl_df[36 x 2]>], [<tbl_df[36 x 2]>], [<tbl_df[38 x 2]>…\n$ models      <list> [0, 1, ~NULL, ~NULL, classification, TRUE, stats, formula…\n$ predictions <list> [<tbl_df[36 x 1]>], [<tbl_df[36 x 1]>], [<tbl_df[38 x 1]>…\n$ accuracy    <dbl> 0.4444444, 0.3333333, 0.3947368, 0.4054054, 0.4736842, 0.5…\n```\n\n\n:::\n:::\n\n\n\nWe can now look at accuracy across the 100 bootstraps.  For example, we can make a histogram using ggplot from the accuracy column in the models tibble\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels |> \n  ggplot(aes(accuracy)) +\n  geom_histogram(binwidth = 0.05)\n```\n\n::: {.cell-output-display}\n![](app_resampling_with_map_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nAnd we can summarize the min, max, mean, median and stdev of the accuracy column in the models tibble\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels |> \n  summarize(min = min(accuracy), \n            max = max(accuracy), \n            mean = mean(accuracy), \n            median = median(accuracy),\n            std_dev = sd(accuracy))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n    min   max  mean median std_dev\n  <dbl> <dbl> <dbl>  <dbl>   <dbl>\n1 0.323 0.618 0.442  0.434  0.0648\n```\n\n\n:::\n:::\n\n\nEasy peasy!  But remember, it is easier still using `fit_reamples()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_lr <-\n  logistic_reg() |> \n    set_engine(\"glm\") |> \n    fit_resamples(preprocessor = rec, \n                  resamples = splits, \n                  metrics = metric_set(accuracy))\n\nfits_lr |> collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.442   100 0.00648 Preprocessor1_Model1\n```\n\n\n:::\n\n```{.r .cell-code}\nfits_lr |> collect_metrics(summarize = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 5\n   id           .metric  .estimator .estimate .config             \n   <chr>        <chr>    <chr>          <dbl> <chr>               \n 1 Bootstrap001 accuracy binary         0.444 Preprocessor1_Model1\n 2 Bootstrap002 accuracy binary         0.333 Preprocessor1_Model1\n 3 Bootstrap003 accuracy binary         0.395 Preprocessor1_Model1\n 4 Bootstrap004 accuracy binary         0.405 Preprocessor1_Model1\n 5 Bootstrap005 accuracy binary         0.474 Preprocessor1_Model1\n 6 Bootstrap006 accuracy binary         0.5   Preprocessor1_Model1\n 7 Bootstrap007 accuracy binary         0.441 Preprocessor1_Model1\n 8 Bootstrap008 accuracy binary         0.472 Preprocessor1_Model1\n 9 Bootstrap009 accuracy binary         0.474 Preprocessor1_Model1\n10 Bootstrap010 accuracy binary         0.429 Preprocessor1_Model1\n# ℹ 90 more rows\n```\n\n\n:::\n:::\n\n\n\n## tune_grid done manually\n\n",
    "supporting": [
      "app_resampling_with_map_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}