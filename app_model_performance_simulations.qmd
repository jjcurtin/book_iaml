---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Appendix 2: Simulations of Model Performance Bias and Variance by Resampling Techniques { .unnumbered}


## General Setup

Load libraries
```{r}
#| message: false

library(tidyverse)
library(tidymodels)

theme_set(theme_classic())
options(pillar.sigfig = 5)
```

## DGP
Function to generate `n_obs` of simulated observations

* DGP is linear on all `x` + normal error with sd = `irr_err`
* y is simple sum such that all coefficients = 1
* features are correlated based on `sigma`
```{r}
simulate_DGP <- function (n_obs, n_features, irr_err, mu, sigma, b){

  x <- MASS::mvrnorm(n_obs, mu = mu, Sigma = sigma) |> 
    magrittr::set_colnames(str_c("x", 1:n_features)) |> 
    as_tibble()
  
  x |> 
     mutate(y = rowSums(t(t(x) * b)) + rnorm(n_obs, 
                                        mean = 0, 
                                        sd = irr_err))
}
```


## Simulation settings
```{r}
n_sims <- 1000  # number of simulations
n_obs <- 200 # number of observations

n_features <- 10
n_effects <- 2 # number of features with non-zero effects.  Only several to increase model variance
mu <- rep(0, n_features)

# correlated features to increase model variance
sigma <- matrix(.3, nrow = n_features, ncol = n_features)
diag(sigma) <- 1
irr_err <- 10
b <- c(rep(0.5, n_effects), rep(0, n_features - n_effects))
  
set.seed(123456)
# first call so that we can set up recipe
df <- simulate_DGP(n_obs, n_features, irr_err, mu, sigma, b) 
rec <- recipe(y ~ ., data = df)

rmse_combined = NULL  # to store RMSE for all methods
```

Parallel processing for resampling methods with `fit_resamples()`
```{r}
cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
```


## Get simulation dfs
```{r}
#| label: gen_dfs

dfs <- rep(n_obs, n_sims) |>
  map(\(n_obs) simulate_DGP(n_obs, n_features, irr_err, mu, sigma, b))
```


## What is the TRUE model performance

The irreducible error is set to 10 but our model will have some reducible error too so the true performance of our model will be worse than 10. 

* The DGP is linear 
* but we only have `r n_obs` observations 
* and we have 20 features

Lets fit many models of n = `r n_obs` (the size of our final model) and assess performance in really big samples of new data (ah the luxury of simulated data!) to get a "TRUE" estimate of how well our model trained with full N performs in a low variance (very big) sample of new data.
```{r}

# models based on full n for each simulation run
# use KNN with small K to get high variance models (to better reveal differences among resampling approaches)
models <- dfs |> 
  # map(\(df) linear_reg() |> fit(y ~ ., data = df))
  map (\(df) nearest_neighbor() |> 
             set_engine("kknn") |> set_mode("regression") |>
             fit(y ~ ., data = df))

# big samples of held out data for high precision assessment of models
# 1 for each model
outs <- rep(10000, n_sims) |> 
  map(\(n_obs) simulate_DGP(n_obs, n_features, irr_err, mu, sigma, b))

# list of predictions for out from each model
preds <- map2(models, outs, \(model, out) predict(model, out))

# get mean rmse in big held out data set for 1000 full n models
# should be very precise
rmse_true <- map2_dbl(outs, preds, \(out, pred)  rmse_vec(out$y,
                                                      pred$.pred)) |> 
  mean()

message("True RMSE = ", rmse_true)
```


## Validation set

### 80/20 split


Here we simulate repeated use of the validation set approach to assess our model performance
```{r}
#| label: val_set_80_20

rmse_combined <- dfs |> 
  map(\(df) validation_split(df, prop = .80)) |>   # validation set split
  map(\(split) nearest_neighbor() |>
               set_engine("kknn") |> 
               set_mode("regression") |>
               fit_resamples(resamples = split, 
                             preprocessor = rec,
                             metrics = metric_set(rmse))) |> 
  map(\(fits) collect_metrics(fits, summarize = TRUE)) |> 
  list_rbind() |> 
  mutate(method = "val_set_80") |>       # label results in df
  bind_rows(rmse_combined)
```

## 50/50 split

Here we simulate repeated use of the validation set approach to assess our model performance
```{r}
#| label: val_set_50_50

rmse_combined <- dfs |> 
  map(\(df) validation_split(df, prop = .50)) |>   # validation set split
  map(\(split) nearest_neighbor() |>
               set_engine("kknn") |> 
               set_mode("regression") |>
               fit_resamples(resamples = split, 
                             preprocessor = rec,
                             metrics = metric_set(rmse))) |> 
  map(\(fits) collect_metrics(fits, summarize = TRUE)) |> 
  list_rbind() |> 
  mutate(method = "val_set_50") |>       # label results in df
  bind_rows(rmse_combined)
```

## K-fold

### Simple 5-fold
```{r}
#| label: 5fold

rmse_combined <- dfs |> 
  map(\(df) vfold_cv(df, v = 5)) |>   # 5-fold
  map(\(split) nearest_neighbor() |>
               set_engine("kknn") |> 
               set_mode("regression") |>
               fit_resamples(resamples = split, 
                             preprocessor = rec,
                             metrics = metric_set(rmse))) |> 
  map(\(fits) collect_metrics(fits, summarize = TRUE)) |> 
  list_rbind() |> 
  mutate(method = "5-fold") |>       # label results in df
  bind_rows(rmse_combined)
```

### Simple 10-fold
```{r}
#| label: 10fold

rmse_combined <- dfs |> 
  map(\(df) vfold_cv(df, v = 10)) |>   # 10-fold
  map(\(split) nearest_neighbor() |>
               set_engine("kknn") |> 
               set_mode("regression") |>
               fit_resamples(resamples = split, 
                             preprocessor = rec,
                             metrics = metric_set(rmse))) |> 
  map(\(fits) collect_metrics(fits, summarize = TRUE)) |> 
  list_rbind() |> 
  mutate(method = "10-fold") |>       # label results in df
  bind_rows(rmse_combined)
```


### 3x 10-Fold
```{r}
#| label: 3x10fold

rmse_combined <- dfs |> 
  map(\(df) vfold_cv(df, v = 10, repeats = 3)) |>   # 3x10-fold
  map(\(split) nearest_neighbor() |>
               set_engine("kknn") |> 
               set_mode("regression") |>
               fit_resamples(resamples = split, 
                             preprocessor = rec,
                             metrics = metric_set(rmse))) |> 
  map(\(fits) collect_metrics(fits, summarize = TRUE)) |> 
  list_rbind() |> 
  mutate(method = "3x10-fold") |> # label results in df
  bind_rows(rmse_combined)
```


## Bootstrap Resampling

### 10 resamples

```{r}
#| label: boot10

rmse_combined <- dfs |> 
  map(\(df) bootstraps(df, times = 10)) |>   # 10 boots
  map(\(split) nearest_neighbor() |>
               set_engine("kknn") |> 
               set_mode("regression") |>
               fit_resamples(resamples = split, 
                             preprocessor = rec,
                             metrics = metric_set(rmse))) |> 
  map(\(fits) collect_metrics(fits, summarize = TRUE)) |> 
  list_rbind() |> 
  mutate(method = "boot_10") |>       # label results in df
  bind_rows(rmse_combined)
```

### 100 resamples
```{r}
#| label: boot100

rmse_combined <- dfs |> 
  map(\(df) bootstraps(df, times = 100)) |>   # 100 boots
  map(\(split) nearest_neighbor() |>
               set_engine("kknn") |> 
               set_mode("regression") |>
               fit_resamples(resamples = split, 
                             preprocessor = rec,
                             metrics = metric_set(rmse))) |> 
  map(\(fits) collect_metrics(fits, summarize = TRUE)) |> 
  list_rbind() |> 
  mutate(method = "boot_100") |>       # label results in df
  bind_rows(rmse_combined)
```


## Summarize
```{r}
rmse_combined |> 
  group_by(method) |> 
  summarize(rmse_mean = mean(mean),
            rmse_sd = sd(mean))
```



##  Plot sampling distributions
```{r}
rmse_combined |> 
  ggplot(aes(x = mean, color = method)) + 
  geom_density() +
  geom_vline(aes(xintercept = mean(rmse_true)),
            color = "blue", linetype = "dashed", linewidth = 1)
```
