{"title":"Map across resamples","markdown":{"yaml":{"output":"html_document","editor_options":{"chunk_output_type":"console"}},"headingText":"Map across resamples","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n`fit_resamples()` and `tune_grid()` are tidymodels functions that we use in combination with objects generated from a resampling function (e.g., `vfold_cv()`, `bootstraps()`, etc from the `rsample` package) to get held-out performance estimates for our models.  \n\n- `fit_resamples()` and `tune_grid()` use looping under the hood to accomplish their goals.  `fit_resamples()` loops over the splits generated by our resampling function to get held-out performance estiamtes for a single model configuration.  \n\n- `tune_grid()` also loops over these splits but includes an additional inner loop that loops over the values of our hyper-parameters in a hyper-parameter grid that we create.  \n\n--------------------------------------------------------------------------------\n\nIn this appendix, we are going to re-create the functionality of `fit_resamples()` and `tune_grid()` using the `map()` function from the `purrr` package.  This will help us understand how these functions work under the hood and give us a better understanding of how to use them.  It will also give us an alternative way to do resampling if we need it.\n\nSpecifically, with respect to using resampling to get held-out performance estimates, we will\n\n- Gain a better understanding of the resampling object that is returns from `bootstraps()` and `vfold_cv()`\n- Make more transparent how these functions incorporate loops as part of their computations\n- Be able to reproduce these computations, manually for when we can't use these functions directly.\n  - This might be if we want to loop over model configurations that are not defined by differece in hyperparameters but instead by other characteristics (e.g., different recipes, different statistical algorithms).  \n  - We want to use an algorigthm that is not supported by tidymodels (e.g., we are developing deep neural networks outside of tidymodels by directly using the keras package)\n- We want to do nested resampling (e.g., using `bootstraps()` to both select a best model configuration and evaluate that best configuration with the variance benefits that are offered by nested cv relative to using a single held out test set.\n\n--------------------------------------------------------------------------------\n\nMore generally, we will also gain a better understanding (and some code examples) of the use of list columns with iteration via `map()`.  \n\n- This has many other uses than just resampling.  \n- Wickham has writting about this in more detail [elsewhere](https://r4ds.had.co.nz/many-models.html).\n\n--------------------------------------------------------------------------------\n\nIn this appendix, we will work through four separate examples that build in complexity\n\n- First, we will reproduce `fit_resamples()` computations by using `map()` to loop over the splits and fit a single model to each split.  We use separate independent `maps()` for each step\n- Next, we will do this again but by wrapping all the steps into one function and using a single call to `map()` to loop that function over the splits.\n- The third example, will reproduce `tune_grid()` by using two nested `map()`s to loop over the splits and the hyper-parameter grid.  \n- The final example will demonstrate how to do nested resampling using `map()`  Nested resampling includes both an outer and inner loop across different out and inner splits.  There is also an innermost loop across values in a hyper-parameter grid (or any other grid that defines different model configurations) \n\n--------------------------------------------------------------------------------\n\n## Set up\n\nLets start all of these examples by....\n\n\nLoading libraries\n```{r}\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n\n--------------------------------------------------------------------------------\n\nCreating a simple data set that has 300 observationns and two predictors (`x1` and `x2`) and one outcome (`y`).  The outcome is a linear combination of the two predictors with some noise added. \n```{r}\nset.seed(123456)\nn_obs <- 300\nd <- tibble(x1 = rnorm(n_obs), x2 = rnorm(n_obs), y = 2*x1 + 3*x2 + rnorm(n_obs))\n```  \n\n--------------------------------------------------------------------------------\n\nGetting bootstrap resamples of d.  \n\n```{r}\nn_boots <- 50\nresamples <- bootstraps(d, times = n_boots)\n```\n\n`bootstraps()` \n\n  - Returns an object that two columns and `n_boots` (in this case 50) rows.  Each row is a bootstrap sample of the data.  The columns are:\n  - `splits` - contains a bootstrap sample of the data that includes held-in raw data and OOB held-out raw data subsamples\n  - `id` - name of the resample\n  \n```{r}\nresamples\n```\n\nThis resamples object is a tibble (as typical in the tidy framework).  \n\n- However, it uses \"list columns\" to hold the splits.  \n- In a typical tibble, the individual cells in any column contain numeric or character data.  However, this is not required.  A column in a tibble is just a list and therefore, the cells can contain any type of object.  \n-  The cells for the `splits` column in this tibble contain splits, a special object created by functions in the `rsample` package that hold resampled datasets.\n\nWe can extract the contents of one of these cells using the base R `[[]]` notation\n\n- The first resample used the full 300 observations from d (the last value)\n- It make a bootstrap resample of 300 observations from d (the first value) that we can use as held-in data for training models\n- There are 115 OOB observations for this split that we can use as held-out data\n\n```{r}\nresamples$splits[[1]]\n```\n\n--------------------------------------------------------------------------------\n\nWe will also need a recipe to create features from our raw data.  Here is a simple recipe that indicate that y will be models on all the other columns (`x1` and `x2`).  We will not do any other feature engineering to keep this example simple.\n\n```{r}\nrec <- recipe(y ~ ., data = d)\n```\n\nWe are now ready to start the first example\n\n## Using map() to replace fit_resamples() - step x step\n\nIn this first example, we will combine `map()` with the use of list columns to save all the intermediate products that are produced when fitting and evaluating a single model configuration for a simple linear regression across many (in this case 50) held-out sets created by bootstrapping.\n\n\nTo do this, we will need a function to fit linear models to held-in training data.  We can use the typical tidymodels code here.\n```{r}  \nfit_lm <- function(held_in) {\n  linear_reg() |> \n    set_engine(\"lm\") |> \n    fit(y ~ ., data = held_in)\n}\n```\n\n--------------------------------------------------------------------------------\n\nThen we use `map()` and list columns to save the individual steps for evaluating the model in each split/resample.  The following steps are done for EACH resample using `map()` or `map2()`\n\n- Prep the recipe with held-in data (in `prep_recs` column)\n- Bake the recipe using `new_data = NULL` to get held-in features (in `held_ins`)\n- Bake the recipe using `new_data = assessment(split)` to get held-out features (in `held_outs`)\n- Fit the model using the held-in features (in `models`)\n- Get predictions using the model with the held-out features (in `predictions`)\n- Calculate the rmse of the model (in `rmses`)\n\n```{r}\nresamples_ex1 <- resamples |> \n  mutate(prep_recs = map(splits, \n                         \\(split) prep(rec, training = analysis(split))),\n         held_ins = map2(resamples$splits, prep_recs, \n                         \\(split, prep_rec) bake(prep_rec, new_data = NULL)),\n         held_outs = map2(resamples$splits, prep_recs, \n                          \\(split, prep_rec) bake(prep_rec, \n                                                  new_data = assessment(split))),\n         models = map(held_ins, \n                      \\(held_in) fit_lm(held_in)),\n         predictions = map2(models, held_outs, \n                            \\(model, held_out) predict(model, held_out)$.pred),\n         rmses = map2_dbl(predictions, held_outs, \n                           \\(pred, held_out) rmse_vec(held_out$y, pred)))\n```\n\n--------------------------------------------------------------------------------\n\nThe pipline above creates a tibble with columns for each of the intermediate products and the rmse/error of the model in each resample.  \n\n- All but the last columns are list columns that can hold objects of any time (e.g., prepped recipes, data frames, model objects).  \n- The final column is a double column that holds the numeric rmse of the model in each resample.  That is why we used `map_dbl()` to create the rmses column.\n```{r}\nresamples_ex1 |> glimpse()\n```\n\n--------------------------------------------------------------------------------\n\nWe can now look at rmses across the 50 bootstraps.  For example, we can make a histogram using ggplot from the errors column in the fits tibble\n```{r}\nresamples_ex1 |> \n  ggplot(aes(rmses)) +\n  geom_histogram(binwidth = 0.05)\n```\n\nAnd we can summarize the min, max, mean, median and stdev of the error column in the fits tibble\n```{r}\nresamples_ex1 |> \n  summarize(n = n(),\n            min = min(rmses), \n            max = max(rmses), \n            mean = mean(rmses), \n            median = median(rmses),\n            std_dev = sd(rmses))\n```\n\n--------------------------------------------------------------------------------\n\nEasy peasy!  \n\n- This first example has demonstrated the use of `map()` and list columns, which has many uses.\n- This first example also makes clear what `fit_resamples()` is doing.\n- It also gives you an alternative workflow in case you can't use `fit_resamples()`. \n  - This might occur if you are training deep neural networks with keras directly. \n  - In that instance, you might use the `fit()` function from keras to fit the model and then use `predict()` to get predictions.  But you could still use the functions from the `rsample` package to get a resampled object (e.g., using `bootstraps()`) and you could use performance metrics (e.g., `rmse_vec()`) from the yardstick package.\n\nBut remember, if you can (and in most instances, you can), it is easier to still using `fit_resamples()` to get your held-out performance estimates.\n```{r}\nresamples_tidy_ex1 <-\n  linear_reg() |> \n    set_engine(\"lm\") |> \n    fit_resamples(preprocessor = rec, \n                  resamples = resamples, \n                  metrics = metric_set(rmse))\n\nresamples_tidy_ex1 |> \n  collect_metrics()\n\nresamples_tidy_ex1 |> \n  collect_metrics(summarize = FALSE)\n```\n\n## Using map() to replace fit_resamples() - one function\n\nIf we wanted to generate the held-out error using resampling but didnt need/want the intermediate products, we could wrap all the steps in one function and just map that single function.  \n\n- We might do this if we are working with big datasets and saving all the intermediate products requires too much memory.\n- Of course, we could also blend the this example with the previous example to save some but not all the intermediate products.\n\n--------------------------------------------------------------------------------\n\nHere is a function that takes a split and a recipe and returns the rmse of the model fit to the held-in data and evaluated on the held-out data.  It does all the steps we did in the previous example but in one function.  We will use this function to replace all the intermediate steps in the previous example.\n\n```{r}\nfit_and_eval <- function(split, rec) {\n  # prep the recipe with held-in data\n  prep_rec <- prep(rec, training = analysis(split))\n  \n  # bake the recipe using new_data = NULL to pull out the held-in features\n  held_in <- bake(prep_rec, new_data = NULL)\n  \n  # bake the recipe using new_data = assessment(split) to get held-out features\n  held_out <- bake(prep_rec, new_data = assessment(split))\n  \n  # fit the model using the held-in features\n  model <- \n    linear_reg() |> \n    set_engine(\"lm\") |> \n    fit(y ~ ., data = held_in)\n  \n  # get predictions using the model with the held-out features\n  pred <- predict(model, held_out)$.pred\n  \n  # calculate the accuracy of the model\n  rmse_vec(held_out$y, pred)\n}\n```\n\n--------------------------------------------------------------------------------\n\nNow map this function over the splits to get a vector of rmse.  Same results, but not saving intermediate steps by using one function.\n```{r}\nresamples_ex2 <- resamples |> \n  mutate(errors = map_dbl(splits, \\(split) fit_and_eval(split, rec)))\n```\n\nHere is what the resamples_ex2 tibble now looks like\n```{r}\nresamples_ex2\n```\n\n--------------------------------------------------------------------------------\n\nAnd here we demo getting overall held-out performance details across the 50 bootstraps.\n\n```{r}\nresamples_ex2 |> \n  summarize(n = n(),\n            min = min(errors), \n            max = max(errors), \n            mean = mean(errors), \n            median = median(errors),\n            std_dev = sd(errors))\n```\n\n\n## Using two map()s to replace tune_grid()\n\nNow we can make this a bit more complicated by adding a grid of hyperparameters to tune.  \n\n- Lets keep it simple and tune only k in a knn model.  \n- To tune k, we would normally use `tune_grid()` but we can do it again with two loops using `map()`.  \n- We use an outer `map()` to loop over the resamples (as we did in the last two examples) and an inner `map()` to loop over the values of k \n\nLets start by setting up a grid of values of k to tune over.\n```{r}\ngrid_k = tibble(neighbors = c(3, 6, 9, 12, 15, 18))\n```\n\n---------------------------------------------------------------------------------\n\nWe will use a single function to repeatedly fit and eval over our grid of parameters.\n```{r}\neval_grid <- function(split, rec, grid_k) {\n \n  # get held-in and held-out features for split \n  # we calculate features inside the function where we fit all models across the grid \n  # because we want to make sure we only need to prep and bake the recipe once\n  # per split.  Otherwise, we would waste a lot of computational time.\n  prep_rec <- prep(rec, training = analysis(split))\n  held_in <- bake(prep_rec, new_data = NULL)\n  held_out <- bake(prep_rec, new_data = assessment(split))\n\n  # function to fit and eval model for a specific k using held-in/held-out\n  # for this split\n  fit_eval <- function(k, held_in, held_out) {\n    model <- \n      nearest_neighbor(neighbors = k) |>   \n        set_engine(\"kknn\") |>   \n        set_mode(\"regression\") |>  \n        fit(y ~ ., data = held_in)\n    \n    pred <- predict(model, held_out)$.pred\n   \n    # lets put k and rmse in a tibble and return that for each split \n    tibble(k = k, \n           rmse = rmse_vec(held_out$y, pred))\n  }\n  \n  # loop through grid_k and fit/eval model for each k \n  # this is the inner loop from tune_grid()\n  # use list_rbind() to bind the separate rows for each tibble into one larger tibble\n  grid_k$neighbors |> \n    map(\\(k) fit_eval(k, held_in, held_out)) |> \n    list_rbind()\n}\n```\n\n-------------------------------------------------------------------------------\n\nNow we map this function over the 50 bootstrap splits. \n\n- This is the outer loop from `tune_grid()`.  \n- `eval_grid()` will return a tibble with rows for each value of k.  We will save one tibble for each split in a list column called `rmses``. \n\n```{r}\nresamples_ex3 <- resamples |> \n  mutate(rmses = map(splits, \n              \\(split) eval_grid(split, rec, grid_k)))\n```\n\n--------------------------------------------------------------------------------\n\nThe `rmses` column contains the rmse for each value of k for each split/resample in a tibble.  Each tibble has 6 rows (one for each k) and 2 columns (k and rmse).\n```{r}\nresamples_ex3\n```\n\nLets take a look at one of these tibbles to make this structure clearer.  The same tibble format is saved for all fifty bootstrap splits (with different values for `rmse` of course!)\n\n```{r}\nresamples_ex3$rmses[[1]]\n```\n\n-----------------------------------------------------------------------------\n\nWe can `unnest()` the `rmses` column to get a tibble with one row for each k value in each resample.  \n\n- `unnest()` is used frequently when a list column contains tibbles and you want to combine those tibbles into more traditional tibble without the list column.  \n- No need to display the original splits column so we will select it out. \n```{r}\nresamples_ex3 |> \n  unnest(rmses) |> \n  select(-splits) |>\n  print(n = 30)\n```\n\n--------------------------------------------------------------------------------\n\nWe can pipe this unnested tibble into a `group_by()` and `summarize()` to get the median (or mean) across resamples and then arrange to find the best k \n\n```{r}\nresamples_ex3 |> \n  unnest(rmses) |> \n  select(-splits) |>\n  group_by(k) |> \n  summarize(n = n(),\n            mean_rmse = mean(rmse)) |> \n  arrange(mean_rmse)\n```\n\n-------------------------------------------------------------------------------\n\nThe example makes it clear that using resampling to tune a grid of hyperparameters is just a matter of looping over the resamples in an outer loop and looping over a grid of hyperparameters in an inner loop. \n\nYou might use this approach\n\n- if you wanted to select among model configurations that different by something other than hyper-parameters (e.g., different recipes, different statistical algorithms).  \n- Or if you wanted to fit models not supported by tidymodels.\n\n--------------------------------------------------------------------------------\n\nBut of course, when you can use it, using `tune_grid()` is easier because it takes care of the nested looping internally.\n```{r}\nresamples_tidy_ex3 <- \n  nearest_neighbor(neighbors = tune()) |>   \n    set_engine(\"kknn\") |>   \n    set_mode(\"regression\") |>  \n    tune_grid(preprocessor = rec, \n              resamples = resamples, \n              grid = grid_k, \n              metrics = metric_set(rmse))\n\nresamples_tidy_ex3 |> \n  collect_metrics() |> \n  arrange(mean)\n```\n\n## Using map()s to do nested cv\n\nNow lets do the most complicated version of this.   Nested resampling involves looping over outer splits where the held out data are test sets used to evaluate best model configurations for each outer split and the inner loop makes validation sets that are used to select the best model configuration for each outer fold.  However, if we are tuning a grid of hyperparameters, there is even a further nested loop inside the inner resampling loop to get performance metrics for each value of the hyperparameter in the validation sets.\n\n`rsample` supports creating a nested resampling object.  \n\n- You can specify different resampling for the inner and outer loops.  \n- k-fold for the outer loop and bootstraps for the inner loop is a common choice.\n- However, neither `fit_resamples()` or `tune_grid()` support nested resampling.  So we will always need to use nested `map()` to do this ourselves.\n\n--------------------------------------------------------------------------------\n\nFirst, lets make the nested resampling object and explore it a bit\n\n- We will use 5-fold cv for the outer loop\n- We will use 10 bootstraps for the inner loop\n- These numbers were selected only to make the computations faster.  You may need more splits.\n```{r}\nresamples_nested <- d |> \n  nested_cv(outside = vfold_cv(v = 5), inside = bootstraps(times = 10))\n```\n\nLets take a look at it.  \n\n- It has five rows for each of the five outer splits.  The outer splits are the k-folds.\n- The inner_resamples column contains the inner splits associated with each outer k-fold split.  Each inner split is a bootstrap resample with 10 bootstraps.\n\n```{r}\nresamples_nested\n```\n\nLets look a little more carefully at this structure because its critical to understanding nested cv. \n\n- Below we pull out and look at the first outer k-fold split. \n- The total sample size is 300 and the outer k-fold split has 60 observations held-out to eventually use as a test set and 240 held-in that will eventually be used as training data when we train models in the outer loop to get our final performance metrics for best values of k.  We will also use these held-in data in the inner loop.\n```{r}\nresamples_nested$splits[[1]]\n\n# I prefer to work with this resampled objects using base R notation\n# however you could do the same using piped tidy code\n# resamples_nested |> \n#   slice(1) |> \n#   pull(splits)\n```\n\nNow look below at the inner resamples for the first outer k-fold split.\n\n- For each outer k-fold split, we take the held-in data (240 observations in this case) and split it further, using the inner resampling method (10 bootstraps here).\n- Notice that each of the 10 bootstraps associated with the first outer k-fold split have 240 observation (because we used the held-in data from that outer k-fold split).  These bootstrap resamples will be used as training data for the inner loop of nested.  We will train models with various values of k with these data\n- Notice also that each of these 10 boostraps have held-out (OOB) observations that will be used as validation sets for the inner loop of nested.  We will use these held-out validation sets to calculate performance metrics for the models with different value of k.  This will let us select the best value of k for each outer split.\n\n```{r}\nresamples_nested$inner_resamples[[1]]\n```\n\n-------------------------------------------------------------------------------\n\nNow that we have a resamples object to use for nested cv, lets get some functions together to calculate rmses for inner validation sets and outer test sets\n\nWe will need a function again that takes a split, uses the recipe to get held-in/held-out features, and then fits and evaluates a model for each value of k in the grid.  This is the exact same function as we used earlier (because we are using knn again).   We include it below again for your review.\n\n- We will use this function twice.\n- In the inner loop of our nested cv, we will map this function over all 10 of our bootstraps associated with an outer split. \n- In the outer loop of our nested cv, we will map this function over all 5 of our outer splits using the best value of k that was identified for each outer split using its respective 10 bootstraps.\n```{r}\n#| eval: false\n\neval_grid <- function(split, rec, grid_k) {\n \n  # get held-in and held-out features for split \n  prep_rec <- prep(rec, training = analysis(split))\n  held_in <- bake(prep_rec, new_data = NULL)\n  held_out <- bake(prep_rec, new_data = assessment(split))\n\n  # function fit fit and eval model for a specific lambda and resample\n  fit_eval <- function(k, held_in, held_out) {\n    model <- \n      nearest_neighbor(neighbors = k) |>   \n        set_engine(\"kknn\") |>   \n        set_mode(\"regression\") |>  \n        fit(y ~ ., data = held_in)\n    \n    pred <- predict(model, held_out)$.pred\n    \n    # lets put k and rmse in a tibble and return that for each split \n    tibble(k = k, \n           rmse = rmse_vec(held_out$y, pred))\n  }\n  \n  # loop through grid_k and fit/eval model for each k \n  # this is the inner loop\n  # use list_rbind() to bind the separate rows for each tibble into one larger tibble\n  grid_k$neighbors |> \n    map(\\(k) fit_eval(k, held_in, held_out)) |> \n    list_rbind()\n}\n```\n\n--------------------------------------------------------------------------------\n\nTo loop eval_grid over the 10 bootstraps in the nested cv inner loop, we will need another simple function that applies our `eval_grid()` over each bootstrap within a set of 10 and binds the results into a single tibble.  \n```{r}\nbind_bootstraps <- function(bootstraps, rec, grid_k) {\n \n  bootstraps$splits |>  \n    map(\\(bootstrap_split) eval_grid(bootstrap_split, rec, grid_k)) |> \n    list_rbind()\n}\n```\n\n--------------------------------------------------------------------------------  \n\nNow we are ready to get performance metrics for each k in `grid_k` for each of the inner 10 bootstraps in each of the five outer k-folds splits. Remember that `bind_bootstraps()` just calls `eval_grid()` for each of the 10 bootstraps in the inner loop and binds the results.  \n\n```{r}\nresamples_nested_ex4 <- resamples_nested |> \n  mutate(inner_rmses = map(inner_resamples, \n                          \\(inner_resample) bind_bootstraps(inner_resample, \n                                                            rec, \n                                                            grid_k)))\n```\n\nLets look at what we just added to our resamples object\n\n- We now have a tibble with 60 rows and 2 columns associated with each set of 10 inner bootstrap splits.\n```{r}\nresamples_nested_ex4 \n```\n\nIf we look at one of them, we see\n\n- The 60 rows are because we have 10 boostraps for each of the 6 values of k in the grid.  So 10 * 6 = 60 rmses\n- we have columns for k and rmse\n```{r}\nresamples_nested_ex4$inner_rmses[[1]]\n```\n\n--------------------------------------------------------------------------------\n\nWe now need to calculate the best k for each of the sets of 10 boostraps (associated with one outer k-fold split). To do this, we average the 10 rmses together for each value of k and then choose the k with the lowest rmse.  We will write a function to do this because we can then map that function over each of the sets of 10 bootstraps associated with each outer split. \n\n```{r}\nget_best_k <- function(rmses) {\n  rmses |>  \n    group_by(k) |> \n    summarize(mean_rmse = mean(rmse)) |> \n    arrange(mean_rmse) |> \n    slice(1) |> \n    pull(k)\n}\n```\n\nAnd then map it across the sets of rmses for each outer k-fold splilt.\n\n```{r}\nresamples_nested_ex4 <- resamples_nested_ex4 |> \n  mutate(best_k = map_dbl(inner_rmses, \\(rmses) get_best_k(rmses)))\n```\n\nNow we have determined the best value of k for each of the 5 outer folds.  \n```{r}\nresamples_nested_ex4\n```\n\n--------------------------------------------------------------------------------\n\nWe can now move up to the outer k-fold splits.  We will use the best k for each outer fold to fit a model using the held-in data from the outer fold and then evaluate that model using the held-out test data.  Those test data were NEVER used before so the will allow us to see how that model with a best value of k (selected by bootstrap resampling) performs in new data\n\nWe can reuse the `eval_grid()` function we used before to fit and evaluate the model using the held-in data and the held-out test data.  But now we just use one value of k, the one that was best for each fold\n\n```{r}\nresamples_nested_ex4 <- resamples_nested_ex4 |> \n  mutate(test_rmse = map2(splits, best_k, \n                               \\(split, k) eval_grid(split, rec, tibble(neighbors = k))))\n```\n\n---------------------------------------------------------------------------------\n\nWe can pull the one row tibbles out of this final column and bind them together to get a tibble with one row for each outer fold.  We have retained the best value of k for each fold for our information (these don't need to be the same for each k-fold split).  \n```{r}\nresamples_nested_ex4$test_rmse |> \n  list_rbind() |> \n  mutate(outer_fold = 1:length(resamples_nested_ex4$test_rmse)) |> \n  relocate(outer_fold)\n```\n\nIf we want to know the true best k to use when we train a final model to implement using all our data, we need select it by using our inner reasampling method (10 boostraps) with all our data.  \n\nWe could then train our final implementation model by fitting a model to all our data using this value of k.\n\n\n## Final notes - parallel processing\n\nAll of these examples were done without the use of parallel processing.  `fit_resamples()` and `tune_grid()` can use a parallel backend if you set it up.  \n\n- You could modify our code to implement `map()` in parallel using `future_map()` from the `furrr` package.\n- For our `fit_resamples()` examples, there is only one `map()` or several independent ones.  None are nested.  Switch them all to `future_map()`. \n- For our `tune_grid()` example, there are two nested `maps()`.  In most instances, you will want to switch the outer loop `map()` to `future_map()` and leave the inner `map()` as is.\n- For our nested example, there are three nested `map()`s.  In most instances, you will want to switch that outermost `map()` (across outer splits) to `future_map()` and leave the inner two `map()`s as is. - See our [post](https://jjcurtin.github.io/book_dwvt/parallel_processing.html) on setting up parallel backends for more info on how to do that ","srcMarkdownNoYaml":"\n\n# Map across resamples { .unnumbered}\n\n`fit_resamples()` and `tune_grid()` are tidymodels functions that we use in combination with objects generated from a resampling function (e.g., `vfold_cv()`, `bootstraps()`, etc from the `rsample` package) to get held-out performance estimates for our models.  \n\n- `fit_resamples()` and `tune_grid()` use looping under the hood to accomplish their goals.  `fit_resamples()` loops over the splits generated by our resampling function to get held-out performance estiamtes for a single model configuration.  \n\n- `tune_grid()` also loops over these splits but includes an additional inner loop that loops over the values of our hyper-parameters in a hyper-parameter grid that we create.  \n\n--------------------------------------------------------------------------------\n\nIn this appendix, we are going to re-create the functionality of `fit_resamples()` and `tune_grid()` using the `map()` function from the `purrr` package.  This will help us understand how these functions work under the hood and give us a better understanding of how to use them.  It will also give us an alternative way to do resampling if we need it.\n\nSpecifically, with respect to using resampling to get held-out performance estimates, we will\n\n- Gain a better understanding of the resampling object that is returns from `bootstraps()` and `vfold_cv()`\n- Make more transparent how these functions incorporate loops as part of their computations\n- Be able to reproduce these computations, manually for when we can't use these functions directly.\n  - This might be if we want to loop over model configurations that are not defined by differece in hyperparameters but instead by other characteristics (e.g., different recipes, different statistical algorithms).  \n  - We want to use an algorigthm that is not supported by tidymodels (e.g., we are developing deep neural networks outside of tidymodels by directly using the keras package)\n- We want to do nested resampling (e.g., using `bootstraps()` to both select a best model configuration and evaluate that best configuration with the variance benefits that are offered by nested cv relative to using a single held out test set.\n\n--------------------------------------------------------------------------------\n\nMore generally, we will also gain a better understanding (and some code examples) of the use of list columns with iteration via `map()`.  \n\n- This has many other uses than just resampling.  \n- Wickham has writting about this in more detail [elsewhere](https://r4ds.had.co.nz/many-models.html).\n\n--------------------------------------------------------------------------------\n\nIn this appendix, we will work through four separate examples that build in complexity\n\n- First, we will reproduce `fit_resamples()` computations by using `map()` to loop over the splits and fit a single model to each split.  We use separate independent `maps()` for each step\n- Next, we will do this again but by wrapping all the steps into one function and using a single call to `map()` to loop that function over the splits.\n- The third example, will reproduce `tune_grid()` by using two nested `map()`s to loop over the splits and the hyper-parameter grid.  \n- The final example will demonstrate how to do nested resampling using `map()`  Nested resampling includes both an outer and inner loop across different out and inner splits.  There is also an innermost loop across values in a hyper-parameter grid (or any other grid that defines different model configurations) \n\n--------------------------------------------------------------------------------\n\n## Set up\n\nLets start all of these examples by....\n\n\nLoading libraries\n```{r}\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n\n--------------------------------------------------------------------------------\n\nCreating a simple data set that has 300 observationns and two predictors (`x1` and `x2`) and one outcome (`y`).  The outcome is a linear combination of the two predictors with some noise added. \n```{r}\nset.seed(123456)\nn_obs <- 300\nd <- tibble(x1 = rnorm(n_obs), x2 = rnorm(n_obs), y = 2*x1 + 3*x2 + rnorm(n_obs))\n```  \n\n--------------------------------------------------------------------------------\n\nGetting bootstrap resamples of d.  \n\n```{r}\nn_boots <- 50\nresamples <- bootstraps(d, times = n_boots)\n```\n\n`bootstraps()` \n\n  - Returns an object that two columns and `n_boots` (in this case 50) rows.  Each row is a bootstrap sample of the data.  The columns are:\n  - `splits` - contains a bootstrap sample of the data that includes held-in raw data and OOB held-out raw data subsamples\n  - `id` - name of the resample\n  \n```{r}\nresamples\n```\n\nThis resamples object is a tibble (as typical in the tidy framework).  \n\n- However, it uses \"list columns\" to hold the splits.  \n- In a typical tibble, the individual cells in any column contain numeric or character data.  However, this is not required.  A column in a tibble is just a list and therefore, the cells can contain any type of object.  \n-  The cells for the `splits` column in this tibble contain splits, a special object created by functions in the `rsample` package that hold resampled datasets.\n\nWe can extract the contents of one of these cells using the base R `[[]]` notation\n\n- The first resample used the full 300 observations from d (the last value)\n- It make a bootstrap resample of 300 observations from d (the first value) that we can use as held-in data for training models\n- There are 115 OOB observations for this split that we can use as held-out data\n\n```{r}\nresamples$splits[[1]]\n```\n\n--------------------------------------------------------------------------------\n\nWe will also need a recipe to create features from our raw data.  Here is a simple recipe that indicate that y will be models on all the other columns (`x1` and `x2`).  We will not do any other feature engineering to keep this example simple.\n\n```{r}\nrec <- recipe(y ~ ., data = d)\n```\n\nWe are now ready to start the first example\n\n## Using map() to replace fit_resamples() - step x step\n\nIn this first example, we will combine `map()` with the use of list columns to save all the intermediate products that are produced when fitting and evaluating a single model configuration for a simple linear regression across many (in this case 50) held-out sets created by bootstrapping.\n\n\nTo do this, we will need a function to fit linear models to held-in training data.  We can use the typical tidymodels code here.\n```{r}  \nfit_lm <- function(held_in) {\n  linear_reg() |> \n    set_engine(\"lm\") |> \n    fit(y ~ ., data = held_in)\n}\n```\n\n--------------------------------------------------------------------------------\n\nThen we use `map()` and list columns to save the individual steps for evaluating the model in each split/resample.  The following steps are done for EACH resample using `map()` or `map2()`\n\n- Prep the recipe with held-in data (in `prep_recs` column)\n- Bake the recipe using `new_data = NULL` to get held-in features (in `held_ins`)\n- Bake the recipe using `new_data = assessment(split)` to get held-out features (in `held_outs`)\n- Fit the model using the held-in features (in `models`)\n- Get predictions using the model with the held-out features (in `predictions`)\n- Calculate the rmse of the model (in `rmses`)\n\n```{r}\nresamples_ex1 <- resamples |> \n  mutate(prep_recs = map(splits, \n                         \\(split) prep(rec, training = analysis(split))),\n         held_ins = map2(resamples$splits, prep_recs, \n                         \\(split, prep_rec) bake(prep_rec, new_data = NULL)),\n         held_outs = map2(resamples$splits, prep_recs, \n                          \\(split, prep_rec) bake(prep_rec, \n                                                  new_data = assessment(split))),\n         models = map(held_ins, \n                      \\(held_in) fit_lm(held_in)),\n         predictions = map2(models, held_outs, \n                            \\(model, held_out) predict(model, held_out)$.pred),\n         rmses = map2_dbl(predictions, held_outs, \n                           \\(pred, held_out) rmse_vec(held_out$y, pred)))\n```\n\n--------------------------------------------------------------------------------\n\nThe pipline above creates a tibble with columns for each of the intermediate products and the rmse/error of the model in each resample.  \n\n- All but the last columns are list columns that can hold objects of any time (e.g., prepped recipes, data frames, model objects).  \n- The final column is a double column that holds the numeric rmse of the model in each resample.  That is why we used `map_dbl()` to create the rmses column.\n```{r}\nresamples_ex1 |> glimpse()\n```\n\n--------------------------------------------------------------------------------\n\nWe can now look at rmses across the 50 bootstraps.  For example, we can make a histogram using ggplot from the errors column in the fits tibble\n```{r}\nresamples_ex1 |> \n  ggplot(aes(rmses)) +\n  geom_histogram(binwidth = 0.05)\n```\n\nAnd we can summarize the min, max, mean, median and stdev of the error column in the fits tibble\n```{r}\nresamples_ex1 |> \n  summarize(n = n(),\n            min = min(rmses), \n            max = max(rmses), \n            mean = mean(rmses), \n            median = median(rmses),\n            std_dev = sd(rmses))\n```\n\n--------------------------------------------------------------------------------\n\nEasy peasy!  \n\n- This first example has demonstrated the use of `map()` and list columns, which has many uses.\n- This first example also makes clear what `fit_resamples()` is doing.\n- It also gives you an alternative workflow in case you can't use `fit_resamples()`. \n  - This might occur if you are training deep neural networks with keras directly. \n  - In that instance, you might use the `fit()` function from keras to fit the model and then use `predict()` to get predictions.  But you could still use the functions from the `rsample` package to get a resampled object (e.g., using `bootstraps()`) and you could use performance metrics (e.g., `rmse_vec()`) from the yardstick package.\n\nBut remember, if you can (and in most instances, you can), it is easier to still using `fit_resamples()` to get your held-out performance estimates.\n```{r}\nresamples_tidy_ex1 <-\n  linear_reg() |> \n    set_engine(\"lm\") |> \n    fit_resamples(preprocessor = rec, \n                  resamples = resamples, \n                  metrics = metric_set(rmse))\n\nresamples_tidy_ex1 |> \n  collect_metrics()\n\nresamples_tidy_ex1 |> \n  collect_metrics(summarize = FALSE)\n```\n\n## Using map() to replace fit_resamples() - one function\n\nIf we wanted to generate the held-out error using resampling but didnt need/want the intermediate products, we could wrap all the steps in one function and just map that single function.  \n\n- We might do this if we are working with big datasets and saving all the intermediate products requires too much memory.\n- Of course, we could also blend the this example with the previous example to save some but not all the intermediate products.\n\n--------------------------------------------------------------------------------\n\nHere is a function that takes a split and a recipe and returns the rmse of the model fit to the held-in data and evaluated on the held-out data.  It does all the steps we did in the previous example but in one function.  We will use this function to replace all the intermediate steps in the previous example.\n\n```{r}\nfit_and_eval <- function(split, rec) {\n  # prep the recipe with held-in data\n  prep_rec <- prep(rec, training = analysis(split))\n  \n  # bake the recipe using new_data = NULL to pull out the held-in features\n  held_in <- bake(prep_rec, new_data = NULL)\n  \n  # bake the recipe using new_data = assessment(split) to get held-out features\n  held_out <- bake(prep_rec, new_data = assessment(split))\n  \n  # fit the model using the held-in features\n  model <- \n    linear_reg() |> \n    set_engine(\"lm\") |> \n    fit(y ~ ., data = held_in)\n  \n  # get predictions using the model with the held-out features\n  pred <- predict(model, held_out)$.pred\n  \n  # calculate the accuracy of the model\n  rmse_vec(held_out$y, pred)\n}\n```\n\n--------------------------------------------------------------------------------\n\nNow map this function over the splits to get a vector of rmse.  Same results, but not saving intermediate steps by using one function.\n```{r}\nresamples_ex2 <- resamples |> \n  mutate(errors = map_dbl(splits, \\(split) fit_and_eval(split, rec)))\n```\n\nHere is what the resamples_ex2 tibble now looks like\n```{r}\nresamples_ex2\n```\n\n--------------------------------------------------------------------------------\n\nAnd here we demo getting overall held-out performance details across the 50 bootstraps.\n\n```{r}\nresamples_ex2 |> \n  summarize(n = n(),\n            min = min(errors), \n            max = max(errors), \n            mean = mean(errors), \n            median = median(errors),\n            std_dev = sd(errors))\n```\n\n\n## Using two map()s to replace tune_grid()\n\nNow we can make this a bit more complicated by adding a grid of hyperparameters to tune.  \n\n- Lets keep it simple and tune only k in a knn model.  \n- To tune k, we would normally use `tune_grid()` but we can do it again with two loops using `map()`.  \n- We use an outer `map()` to loop over the resamples (as we did in the last two examples) and an inner `map()` to loop over the values of k \n\nLets start by setting up a grid of values of k to tune over.\n```{r}\ngrid_k = tibble(neighbors = c(3, 6, 9, 12, 15, 18))\n```\n\n---------------------------------------------------------------------------------\n\nWe will use a single function to repeatedly fit and eval over our grid of parameters.\n```{r}\neval_grid <- function(split, rec, grid_k) {\n \n  # get held-in and held-out features for split \n  # we calculate features inside the function where we fit all models across the grid \n  # because we want to make sure we only need to prep and bake the recipe once\n  # per split.  Otherwise, we would waste a lot of computational time.\n  prep_rec <- prep(rec, training = analysis(split))\n  held_in <- bake(prep_rec, new_data = NULL)\n  held_out <- bake(prep_rec, new_data = assessment(split))\n\n  # function to fit and eval model for a specific k using held-in/held-out\n  # for this split\n  fit_eval <- function(k, held_in, held_out) {\n    model <- \n      nearest_neighbor(neighbors = k) |>   \n        set_engine(\"kknn\") |>   \n        set_mode(\"regression\") |>  \n        fit(y ~ ., data = held_in)\n    \n    pred <- predict(model, held_out)$.pred\n   \n    # lets put k and rmse in a tibble and return that for each split \n    tibble(k = k, \n           rmse = rmse_vec(held_out$y, pred))\n  }\n  \n  # loop through grid_k and fit/eval model for each k \n  # this is the inner loop from tune_grid()\n  # use list_rbind() to bind the separate rows for each tibble into one larger tibble\n  grid_k$neighbors |> \n    map(\\(k) fit_eval(k, held_in, held_out)) |> \n    list_rbind()\n}\n```\n\n-------------------------------------------------------------------------------\n\nNow we map this function over the 50 bootstrap splits. \n\n- This is the outer loop from `tune_grid()`.  \n- `eval_grid()` will return a tibble with rows for each value of k.  We will save one tibble for each split in a list column called `rmses``. \n\n```{r}\nresamples_ex3 <- resamples |> \n  mutate(rmses = map(splits, \n              \\(split) eval_grid(split, rec, grid_k)))\n```\n\n--------------------------------------------------------------------------------\n\nThe `rmses` column contains the rmse for each value of k for each split/resample in a tibble.  Each tibble has 6 rows (one for each k) and 2 columns (k and rmse).\n```{r}\nresamples_ex3\n```\n\nLets take a look at one of these tibbles to make this structure clearer.  The same tibble format is saved for all fifty bootstrap splits (with different values for `rmse` of course!)\n\n```{r}\nresamples_ex3$rmses[[1]]\n```\n\n-----------------------------------------------------------------------------\n\nWe can `unnest()` the `rmses` column to get a tibble with one row for each k value in each resample.  \n\n- `unnest()` is used frequently when a list column contains tibbles and you want to combine those tibbles into more traditional tibble without the list column.  \n- No need to display the original splits column so we will select it out. \n```{r}\nresamples_ex3 |> \n  unnest(rmses) |> \n  select(-splits) |>\n  print(n = 30)\n```\n\n--------------------------------------------------------------------------------\n\nWe can pipe this unnested tibble into a `group_by()` and `summarize()` to get the median (or mean) across resamples and then arrange to find the best k \n\n```{r}\nresamples_ex3 |> \n  unnest(rmses) |> \n  select(-splits) |>\n  group_by(k) |> \n  summarize(n = n(),\n            mean_rmse = mean(rmse)) |> \n  arrange(mean_rmse)\n```\n\n-------------------------------------------------------------------------------\n\nThe example makes it clear that using resampling to tune a grid of hyperparameters is just a matter of looping over the resamples in an outer loop and looping over a grid of hyperparameters in an inner loop. \n\nYou might use this approach\n\n- if you wanted to select among model configurations that different by something other than hyper-parameters (e.g., different recipes, different statistical algorithms).  \n- Or if you wanted to fit models not supported by tidymodels.\n\n--------------------------------------------------------------------------------\n\nBut of course, when you can use it, using `tune_grid()` is easier because it takes care of the nested looping internally.\n```{r}\nresamples_tidy_ex3 <- \n  nearest_neighbor(neighbors = tune()) |>   \n    set_engine(\"kknn\") |>   \n    set_mode(\"regression\") |>  \n    tune_grid(preprocessor = rec, \n              resamples = resamples, \n              grid = grid_k, \n              metrics = metric_set(rmse))\n\nresamples_tidy_ex3 |> \n  collect_metrics() |> \n  arrange(mean)\n```\n\n## Using map()s to do nested cv\n\nNow lets do the most complicated version of this.   Nested resampling involves looping over outer splits where the held out data are test sets used to evaluate best model configurations for each outer split and the inner loop makes validation sets that are used to select the best model configuration for each outer fold.  However, if we are tuning a grid of hyperparameters, there is even a further nested loop inside the inner resampling loop to get performance metrics for each value of the hyperparameter in the validation sets.\n\n`rsample` supports creating a nested resampling object.  \n\n- You can specify different resampling for the inner and outer loops.  \n- k-fold for the outer loop and bootstraps for the inner loop is a common choice.\n- However, neither `fit_resamples()` or `tune_grid()` support nested resampling.  So we will always need to use nested `map()` to do this ourselves.\n\n--------------------------------------------------------------------------------\n\nFirst, lets make the nested resampling object and explore it a bit\n\n- We will use 5-fold cv for the outer loop\n- We will use 10 bootstraps for the inner loop\n- These numbers were selected only to make the computations faster.  You may need more splits.\n```{r}\nresamples_nested <- d |> \n  nested_cv(outside = vfold_cv(v = 5), inside = bootstraps(times = 10))\n```\n\nLets take a look at it.  \n\n- It has five rows for each of the five outer splits.  The outer splits are the k-folds.\n- The inner_resamples column contains the inner splits associated with each outer k-fold split.  Each inner split is a bootstrap resample with 10 bootstraps.\n\n```{r}\nresamples_nested\n```\n\nLets look a little more carefully at this structure because its critical to understanding nested cv. \n\n- Below we pull out and look at the first outer k-fold split. \n- The total sample size is 300 and the outer k-fold split has 60 observations held-out to eventually use as a test set and 240 held-in that will eventually be used as training data when we train models in the outer loop to get our final performance metrics for best values of k.  We will also use these held-in data in the inner loop.\n```{r}\nresamples_nested$splits[[1]]\n\n# I prefer to work with this resampled objects using base R notation\n# however you could do the same using piped tidy code\n# resamples_nested |> \n#   slice(1) |> \n#   pull(splits)\n```\n\nNow look below at the inner resamples for the first outer k-fold split.\n\n- For each outer k-fold split, we take the held-in data (240 observations in this case) and split it further, using the inner resampling method (10 bootstraps here).\n- Notice that each of the 10 bootstraps associated with the first outer k-fold split have 240 observation (because we used the held-in data from that outer k-fold split).  These bootstrap resamples will be used as training data for the inner loop of nested.  We will train models with various values of k with these data\n- Notice also that each of these 10 boostraps have held-out (OOB) observations that will be used as validation sets for the inner loop of nested.  We will use these held-out validation sets to calculate performance metrics for the models with different value of k.  This will let us select the best value of k for each outer split.\n\n```{r}\nresamples_nested$inner_resamples[[1]]\n```\n\n-------------------------------------------------------------------------------\n\nNow that we have a resamples object to use for nested cv, lets get some functions together to calculate rmses for inner validation sets and outer test sets\n\nWe will need a function again that takes a split, uses the recipe to get held-in/held-out features, and then fits and evaluates a model for each value of k in the grid.  This is the exact same function as we used earlier (because we are using knn again).   We include it below again for your review.\n\n- We will use this function twice.\n- In the inner loop of our nested cv, we will map this function over all 10 of our bootstraps associated with an outer split. \n- In the outer loop of our nested cv, we will map this function over all 5 of our outer splits using the best value of k that was identified for each outer split using its respective 10 bootstraps.\n```{r}\n#| eval: false\n\neval_grid <- function(split, rec, grid_k) {\n \n  # get held-in and held-out features for split \n  prep_rec <- prep(rec, training = analysis(split))\n  held_in <- bake(prep_rec, new_data = NULL)\n  held_out <- bake(prep_rec, new_data = assessment(split))\n\n  # function fit fit and eval model for a specific lambda and resample\n  fit_eval <- function(k, held_in, held_out) {\n    model <- \n      nearest_neighbor(neighbors = k) |>   \n        set_engine(\"kknn\") |>   \n        set_mode(\"regression\") |>  \n        fit(y ~ ., data = held_in)\n    \n    pred <- predict(model, held_out)$.pred\n    \n    # lets put k and rmse in a tibble and return that for each split \n    tibble(k = k, \n           rmse = rmse_vec(held_out$y, pred))\n  }\n  \n  # loop through grid_k and fit/eval model for each k \n  # this is the inner loop\n  # use list_rbind() to bind the separate rows for each tibble into one larger tibble\n  grid_k$neighbors |> \n    map(\\(k) fit_eval(k, held_in, held_out)) |> \n    list_rbind()\n}\n```\n\n--------------------------------------------------------------------------------\n\nTo loop eval_grid over the 10 bootstraps in the nested cv inner loop, we will need another simple function that applies our `eval_grid()` over each bootstrap within a set of 10 and binds the results into a single tibble.  \n```{r}\nbind_bootstraps <- function(bootstraps, rec, grid_k) {\n \n  bootstraps$splits |>  \n    map(\\(bootstrap_split) eval_grid(bootstrap_split, rec, grid_k)) |> \n    list_rbind()\n}\n```\n\n--------------------------------------------------------------------------------  \n\nNow we are ready to get performance metrics for each k in `grid_k` for each of the inner 10 bootstraps in each of the five outer k-folds splits. Remember that `bind_bootstraps()` just calls `eval_grid()` for each of the 10 bootstraps in the inner loop and binds the results.  \n\n```{r}\nresamples_nested_ex4 <- resamples_nested |> \n  mutate(inner_rmses = map(inner_resamples, \n                          \\(inner_resample) bind_bootstraps(inner_resample, \n                                                            rec, \n                                                            grid_k)))\n```\n\nLets look at what we just added to our resamples object\n\n- We now have a tibble with 60 rows and 2 columns associated with each set of 10 inner bootstrap splits.\n```{r}\nresamples_nested_ex4 \n```\n\nIf we look at one of them, we see\n\n- The 60 rows are because we have 10 boostraps for each of the 6 values of k in the grid.  So 10 * 6 = 60 rmses\n- we have columns for k and rmse\n```{r}\nresamples_nested_ex4$inner_rmses[[1]]\n```\n\n--------------------------------------------------------------------------------\n\nWe now need to calculate the best k for each of the sets of 10 boostraps (associated with one outer k-fold split). To do this, we average the 10 rmses together for each value of k and then choose the k with the lowest rmse.  We will write a function to do this because we can then map that function over each of the sets of 10 bootstraps associated with each outer split. \n\n```{r}\nget_best_k <- function(rmses) {\n  rmses |>  \n    group_by(k) |> \n    summarize(mean_rmse = mean(rmse)) |> \n    arrange(mean_rmse) |> \n    slice(1) |> \n    pull(k)\n}\n```\n\nAnd then map it across the sets of rmses for each outer k-fold splilt.\n\n```{r}\nresamples_nested_ex4 <- resamples_nested_ex4 |> \n  mutate(best_k = map_dbl(inner_rmses, \\(rmses) get_best_k(rmses)))\n```\n\nNow we have determined the best value of k for each of the 5 outer folds.  \n```{r}\nresamples_nested_ex4\n```\n\n--------------------------------------------------------------------------------\n\nWe can now move up to the outer k-fold splits.  We will use the best k for each outer fold to fit a model using the held-in data from the outer fold and then evaluate that model using the held-out test data.  Those test data were NEVER used before so the will allow us to see how that model with a best value of k (selected by bootstrap resampling) performs in new data\n\nWe can reuse the `eval_grid()` function we used before to fit and evaluate the model using the held-in data and the held-out test data.  But now we just use one value of k, the one that was best for each fold\n\n```{r}\nresamples_nested_ex4 <- resamples_nested_ex4 |> \n  mutate(test_rmse = map2(splits, best_k, \n                               \\(split, k) eval_grid(split, rec, tibble(neighbors = k))))\n```\n\n---------------------------------------------------------------------------------\n\nWe can pull the one row tibbles out of this final column and bind them together to get a tibble with one row for each outer fold.  We have retained the best value of k for each fold for our information (these don't need to be the same for each k-fold split).  \n```{r}\nresamples_nested_ex4$test_rmse |> \n  list_rbind() |> \n  mutate(outer_fold = 1:length(resamples_nested_ex4$test_rmse)) |> \n  relocate(outer_fold)\n```\n\nIf we want to know the true best k to use when we train a final model to implement using all our data, we need select it by using our inner reasampling method (10 boostraps) with all our data.  \n\nWe could then train our final implementation model by fitting a model to all our data using this value of k.\n\n\n## Final notes - parallel processing\n\nAll of these examples were done without the use of parallel processing.  `fit_resamples()` and `tune_grid()` can use a parallel backend if you set it up.  \n\n- You could modify our code to implement `map()` in parallel using `future_map()` from the `furrr` package.\n- For our `fit_resamples()` examples, there is only one `map()` or several independent ones.  None are nested.  Switch them all to `future_map()`. \n- For our `tune_grid()` example, there are two nested `maps()`.  In most instances, you will want to switch the outer loop `map()` to `future_map()` and leave the inner `map()` as is.\n- For our nested example, there are three nested `map()`s.  In most instances, you will want to switch that outermost `map()` (across outer splits) to `future_map()` and leave the inner two `map()`s as is. - See our [post](https://jjcurtin.github.io/book_dwvt/parallel_processing.html) on setting up parallel backends for more info on how to do that "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":"html_document","warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["book.css"],"output-file":"app_resampling_with_map.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","bibliography":["refs.bib"],"callout-icon":false,"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}