{"title":"Advanced Performance Metrics","markdown":{"yaml":{"output":"html_document","editor_options":{"chunk_output_type":"console"}},"headingText":"Advanced Performance Metrics","containsRefs":false,"markdown":"\n\n::: {.content-visible unless-format=\"revealjs\"}\n:::\n::: {.content-visible when-format=\"revealjs\"}\n# IAML Unit 8: Advanced Performance Metrics\n:::\n\n\n## Learning Objectives\n\n- Understand costs and benefits of accuracy \n- Use of a confusion matrix\n- Understand costs and benefits of other performance metrics\n- The ROC curve and area under the curve\n- Model selection using other performance metrics\n- How to address class imbalance\n  - Selection of performance metric\n  - Selection of classification threshold\n  - Sampling and resampling approaches\n\n--------------------------------------------------------------------------------\n\n## Introduction\n\n```{r}\n#| include: false\n\n# set up environment.  Now hidden from view\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\noptions(conflicts.policy = \"depends.ok\")\n\nlibrary(xfun, include.only = \"cache_rds\")\n\n# parallel processing\nlibrary(future)\nplan(multisession, workers = parallel::detectCores(logical = FALSE))\n\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\npath_data <- \"./data\"\n\nrerun_setting <- FALSE \n``` \n\nIn this unit, we will again use the Cleveland heart disease dataset.\n\nHowever, I have modified this to make the outcome unbalanced such that Yes represents approximately 10% of the observations\n\nNow that we will calculate performance metrics beyond accuracy, the order of the levels of our outcome variable(`disease`) matters.  We will make sure that the positive class (event of interest; in this case yes for `disease`) is the **first level**.\n\n--------------------------------------------------------------------------------\n\nFirst, lets open and skim the raw data\n```{r}\ndata_all <- read_csv(here::here(path_data, \"cleveland_unbalanced.csv\"), \n                     col_names = FALSE, na = \"?\", \n                     col_types = cols()) |> \n  rename(age = X1,\n         sex = X2,\n         cp = X3,\n         rest_bp = X4,\n         chol = X5,\n         fbs = X6,\n         rest_ecg = X7,\n         exer_max_hr = X8,\n         exer_ang = X9,\n         exer_st_depress = X10,\n         exer_st_slope = X11,\n         ca = X12,\n         thal = X13,\n         disease = X14) \n\ndata_all |> skim_some()\n```\n\n--------------------------------------------------------------------------------\n\nCode categorical variables as factors with meaningful text labels (and no spaces)\n\n- NOTE the use of `disease = fct_relevel (disease, \"yes\")` to set yes as positive class (first level) for disease\n```{r}\ndata_all <- data_all |> \n  mutate(disease = factor(disease, levels = 0:4, \n                          labels = c(\"no\", \"yes\", \"yes\", \"yes\", \"yes\")),\n         disease = fct_relevel (disease, \"yes\"),\n         sex = factor(sex,  levels = c(0, 1), labels = c(\"female\", \"male\")),\n         fbs = factor(fbs, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_ang = factor(exer_ang, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_st_slope = factor(exer_st_slope, levels = 1:3, \n                                labels = c(\"upslope\", \"flat\", \"downslope\")),\n         cp = factor(cp, levels = 1:4, \n                     labels = c(\"typ_ang\", \"atyp_ang\", \"non_anginal\", \"non_anginal\")),\n         rest_ecg = factor(rest_ecg, levels = 0:2, \n                           labels = c(\"normal\", \"wave_abn\", \"ventric_hypertrophy\")),\n         thal = factor(thal, levels = c(3, 6, 7), \n                       labels = c(\"normal\", \"fixeddefect\", \"reversabledefect\")))\n\ndata_all |> skim_some()\n```\n\n--------------------------------------------------------------------------------\n\nDisease is now unbalanced\n```{r}\ndata_all |> tab(disease)\n```\n\n--------------------------------------------------------------------------------\n\nFor this example, we will evaluate our final model using a held out test set\n\n```{r}\nset.seed(20140102)\nsplits_test <- data_all |> \n  initial_split(prop = 2/3, strata = \"disease\")\n\ndata_trn <- splits_test |> \n  analysis()\n\ndata_test <- splits_test |> \n  assessment()\n```\n\n--------------------------------------------------------------------------------\n\nWe will be fitting a penalized logistic regression again (using glmnet)\n\nWe will do only basic feature engineering for this algorithm and to handle missing data\n\n```{r}\nrec <- recipe(disease ~ ., data = data_trn) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) |> \n  step_normalize(all_predictors())\n```\n\n--------------------------------------------------------------------------------\n\nWe tune/select best hyperparameter values via bootstrap resampling with the training data\n\n- get bootstrap splits\n- make grid of hyperparameter values\n```{r}\nsplits_boot <- data_trn |> \n  bootstraps(times = 100, strata = \"disease\")  \n\ngrid_glmnet <- expand_grid(penalty = exp(seq(-8, 3, length.out = 200)),\n                           mixture = seq(0, 1, length.out = 6))\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\n#| label: fits_glmnet\n\nfits_glmnet <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(accuracy))\n  },\n  dir = \"cache/008/\",\n  file = \"fits_glmnet\",\n  rerun = rerun_setting)\n```\n\n--------------------------------------------------------------------------------\n\nReview hyperparameter plot and best values for hyperparameters\n```{r}\nplot_hyperparameters(fits_glmnet, hp1 = \"penalty\", hp2 = \"mixture\", \n                     metric = \"accuracy\", log_hp1 = TRUE)\n```\n\n```{r}\nshow_best(fits_glmnet, n = 1, metric = \"accuracy\")\n```\n\n--------------------------------------------------------------------------------\n\nLet's fit this best model configuration to all of our training data and evaluate it in test\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\nfeat_trn <- rec_prep |> \n  bake(data_trn)\n\nfit_glmnet <-   \n  logistic_reg(penalty = select_best(fits_glmnet, metric = \"accuracy\")$penalty, \n               mixture = select_best(fits_glmnet, metric = \"accuracy\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\nAnd evaluate it by predicting into test\n```{r}\nfeat_test <- rec_prep |> \n  bake(data_test)\n\n(model_accuracy <- accuracy_vec(feat_test$disease, predict(fit_glmnet, feat_test)$.pred_class))\n```\n\n--------------------------------------------------------------------------------\n\nAccuracy is an attractive measure because it is:\n\n- Intuitive and widely understood\n- Naturally extends to multi-class scenarios\n- These are not trivial advantages in research or application\n\nHowever, accuracy has at least three problems in some situations\n\n- If the outcome is unbalanced, it can be misleading\n  - High performance from simply predicting the majority (vs. minority) class for all observations\n  - Need to anchor evaluation of accuracy to baseline performance based on the majority case percentage\n\n- If the outcome is unbalanced, selecting among model configurations with accuracy can be biased toward configurations that predict the majority class because that will yield high accuracy by itself even without any signal from the predictors\n\n- Regardless of outcome distribution, it considers false positives (FP) and false negatives (FN) equivalent in their costs\n  - This is often not the case\n\n--------------------------------------------------------------------------------\n\nOutcome distributions :\n\n- May start to be considered unbalanced at ratios of 1:5 (20% of cases in the infrequent class)\n- In many real life applications (e.g., fraud detection), imbalance ratios ranging from 1:1000 up to 1:5000 are not atypical\n\nWhen working with unbalanced datasets:\n\n- The class or classes with many observations are called the major or **majority class(es)**\n- The class with few observations (and there is typically just one) is called the minor or **minority class**.\n\nIn our example, the majority class is negative (no) for heart disease and the minority class is positive (yes) for heart disease\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: In our example, our model's accuracy in test seemed high but was it really performing as well as this seems?\n:::\n\n::: {.fragment .uwred}\nAlthough this test accuracy seems high, this is somewhat misleading.  A model that simply labeled everyone as negative for heart disease would achieve almost as high accuracy in our test data\n:::\n\n--------------------------------------------------------------------------------\n\n```{r}\n(model_accuracy <- accuracy_vec(feat_test$disease, \n                                predict(fit_glmnet, feat_test)$.pred_class))\n\nfeat_test |> tab(disease)\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: Perhaps more importantly, are the costs of false positives (screening someone as positive when they do not have heart disease) and false negatives (screening someone as negative when they do have heart disease) comparable for a preliminary screening method?\n:::\n\n::: {.fragment .uwred}\nProbably not.  A false positive might mean that we do more testing that is unnecessary and later find out they do not have heart disease.   This comes with some monetary cost and also likely some distress for the patient.   However, a false negative means we send the patient home thinking they are healthy and may suffer a heart attack or other bad outcome.  That seems worse if this is only a preliminary screen.\n:::\n\n--------------------------------------------------------------------------------\n\n## The Confusion Matrix and Related Performance Metrics\n\nAt a minimum, it seems important to consider these issues explicitly but accuracy is not sufficiently informative.\n\nThe first step to this more careful assessment is to construct a confusion matrix\n\n--------------------------------------------------------------------------------\n\n|               | **Ground Truth** |    |\n|:--------------|:--------- |:----------|\n| **Prediction**| Positive  | Negative  | \n| Positive      | TP        | FP        |\n| Negative      | FN        | TN        |\n\nDefinitions: \n\n- TP: True positive\n- TN: True negative\n- FP: False positive (Type 1 error/false alarm)\n- FN: False negative (Type 2 error/miss)\n\nPerfect classifier has all the observations on the diagonal from top left to bottom right\n\nThe two types of errors (on the other diagonal) may have different costs\n\nWe can now begin to consider these costs\n\n--------------------------------------------------------------------------------\n\nLets look at the confusion matrix associated with our model's performance in test\n\n- We will use `conf_mat()` to calculate the confusion matrix\n- There does NOT (yet?) seem to be a vector version (i.e., `conf_mat_vec()`)\n- Therefore, we have to build a tibble of `truth` and `estimate` to pass into `conf_mat()`\n- It is best to assign the result to an object (e.g., `cm`) because we will use it for a few different tasks\n\n```{r}\ncm <- tibble(truth = feat_test$disease,\n             estimate = predict(fit_glmnet, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n```\n\n--------------------------------------------------------------------------------\n\nLet's display the matrix\n\n- The columns are sorted based on the true value for the observation (i.e., ground truth)\n  - In this case, that is the patients' true disease status\n  - We can see it is unbalanced with most of the cases in the \"no\" column\n\n- The rows are sorted by our model's predictions\n\n- As we noted above, correct predictions fall on the top/left- bottom/right diagonal\n\n```{r}\ncm\n```\n\n--------------------------------------------------------------------------------\n\nTidy model's makes it easy to visualize this matrix in one of two types of plots\n\n- mosaic (the default)\n```{r}\n#| out-height: 3in\n\nautoplot(cm)\n```\n\n- heatmap\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n--------------------------------------------------------------------------------\n\nRegardless of the plot, you can now begin to see the issues with our model\n\n- It seems accurate for patients that do NOT have heart disease\n  - 368/381 correct\n- It is not very accurate for patients that DO have heart disease\n  - 30/47 correct\n\n- This differential performance was masked by our global accuracy measure because overall accuracy was weighted heavily toward accuracy for patients without heart disease given their much higher numbers\n\n--------------------------------------------------------------------------------\n\nWe can use this confusion matrix as the starting point for MANY other common metrics and methods for evaluating the performance of a classification model.  In many instances, the metrics come in pairs that are relevant for FP and FN errors\n\n- Sensitivity & Specificity\n- Positive and Negative Predictive Value (PPV, NPV)\n- Precision and Recall\n\n--------------------------------------------------------------------------------\n\nThere are also some single metric approaches (like accuracy) that may be preferred when the outcome is unbalanced\n\n- Balanced accuracy\n- F1 (and other F-scores)\n- Kappa\n\n--------------------------------------------------------------------------------\n\nThere are also graphical approaches are based on either sensitivity/specificity or precision/recall.  These are:\n\n  - The Receiver Operating Characteristic (ROC) curve\n  - The Precision/Recall Curve (not covered further in this unit)\n  - Each of these curves also yields a single metric that represents the area under the curve\n  \n--------------------------------------------------------------------------------\n\nThe best metric/method is a function both of your intended use and the class distributions\n\n- As noted, **accuracy** is widely understood but can be misleading when class distributions are highly unbalanced\n\n- **Sensitivity/Specificity** are common in literatures that consider diagnosis (clinical psychology/psychiatry, medicine)\n\n- **Positive/Negative predictive value** are key to consider with sensitivity/specificity when classes are unbalanced\n\n- **ROC curve and its auROC metric** provide nice summary visualization and metric when considering classification thresholds other than 50% (also common when classes are unbalanced or types of errors matter)\n\n--------------------------------------------------------------------------------\n\nHere are definitions of many of the most common metrics linked to the confusion matrix\n\n\n|               | **Ground Truth**  |   |\n|:--------------|:--------- |:----------|\n| **Prediction**| Positive  | Negative  | \n| Positive      | TP        | FP        |\n| Negative      | FN        | TN        |\n\n$Accuracy = \\frac{TN + TP}{TN + TP + FN + FP}$\n\n$Sensitivity\\:(Recall) = \\frac{TP}{TP + FN}$\n\n$Specificity = \\frac{TN}{TN + FP}$\n\n$Positive\\:Predictive\\:Value\\:(Precision) = \\frac{TP}{TP + FP}$\n\n$Negative\\:Predictive\\:Value = \\frac{TN}{TN + FN}$\n\n$Balanced\\:accuracy = \\frac{Sensitivity + Specificity}{2}$\n\n$F_\\beta$ score:\n\n- $F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$ (most common; harmonic mean of precision and recall)\n\n- $F_\\beta = (1 + \\beta^2) * \\frac{Precision * Recall}{(\\beta^2*Precision) + Recall}$\n\n- $F_\\beta$ was derived so that it measures the effectiveness of a classifier for someone who assigns $\\beta$ times as much importance to recall as precision\n\n\n--------------------------------------------------------------------------------\n\nIt is easy to get any of these performance metrics using `summary()` on the confusion matrix\n\nMany of the statistics generated are based on an understanding of which level is the positive level.  \n\n- Tidymodels (yardstick to be precise) will default to consider the first level the positive level.  \n- If this is not true, some statistics (e.g., sensitivity, specificity) will be incorrect (i.e., swapped). \n- You can override this default by setting the following parameter inside any function that is affected by the order of the classes\" `event_level = \"second\"`\n\n--------------------------------------------------------------------------------\n\nHere is summary in action!\n```{r}\ncm |> summary()\n```\n\n--------------------------------------------------------------------------------\n\nLet's consider some of what these metrics are telling us about our classifier by looking at the metrics and a confusion matrix plot\n\nLet's start with **sensitivity** and **specificity** and their arithmetic mean (**balanced accuracy**)\n\n- These are column specific accuracies\n- Focus is on truth (columns)\n- Focuses a priori on two types of patients that may walk into the clinic to use our classifier\n```{r}\ncm |> \n  summary() |> \n  filter(.metric == \"sens\" | .metric == \"spec\" | .metric == \"bal_accuracy\") |> \n  select(-.estimator)\n```\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: Can you link the numbers in the confusion matrix on the previous slide to Sensitivity and Specificity metrics\n:::\n\n::: {.fragment .uwred}\nSensivity is \"accuracy\" for the positive cases (in this instance, those with disease = yes).  \nSensitivity = 30 / (17 + 30)\n\nSpecificity is \"accuracy\" for the negative cases (disease = no).  \nSpecificity = 368 / (368 + 13)\n:::\n\n--------------------------------------------------------------------------------\n\nNow let's consider **positive predictive value** and **negative predictive value** \n\n- These are row specific accuracies\n- Focus is on model predictions (rows)\n- Focuses on the utility of the information/screening result provided from our classifier\n- Not typically reported alone but instead in combo with sensitivity/specificity and prevalence (see next pages)\n- Mosaic plot is better visualization for sensitivity/specificity (though I also like the numbers).  Not that useful for PPV/NPV\n- Use heatmap?\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n```{r}\ncm |> \n  summary() |>\n  filter(.metric == \"ppv\" | .metric == \"npv\") |> \n  select(-.estimator)\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: Can you link the numbers in the confusion matrix on the previous slide to PPV and NPV metrics\n:::\n\n::: {.fragment .uwred}\nPPV is \"accuracy\" for the positive predictions (in this instance, when the model predicts yes. \nPPV = 30 / (30 + 13)\n\nNPV is \"accuracy\" for the negative predictions (disease = no).  \nNPV = 368 / (368 + 17)\n:::\n\n--------------------------------------------------------------------------------\n\n- PPV and NPV are influenced by both sensitivity and specificity BUT ALSO prevalence.\n\n- This becomes important in unbalanced settings where prevalence of classes is not equal\n  - Your classifier's PPV will be lower, even with good sensitivity and specificity if the prevalence of the positive class is low   \n  - Conversely, your classifier's NPV will be lower, even with good sensitivity and specificity, if the prevalence of the negative class is low.\n\n- Prevalence also can vary by testing setting\n\n--------------------------------------------------------------------------------\n\nTests for many genetic disorders have very good sensitivity and specificity but their PPV (and NPV) vary widely by setting/patients tested\n\n- Test for multiple endocrine neoplasia type 2 (MEN2) based on mutations in RET\n  - Sensitivity = 98%\n  - Specificity = 99.9%\n\nMENS2 has prevalence of 1/30,000 in general population.  If using the test in the general population with 3 million people:\n\n- 100 will have the disease\n- 2,999,900 will not have the disease\n- Column accuracies (sensitivity and specificity) are high\n- PPV will be very BAD;  98/(3000 + 98) = 3.2%\n- Though NPV will be very near perfect! 2996900 / (2996900 + 2)\n\n|               | **Ground Truth**|     |\n|:--------------|:--------- |:----------|\n| **Prediction**| Positive  | Negative  | \n| Positive      | 98        |    3000   |\n| Negative      |  2        | 2996900   | \n\n--------------------------------------------------------------------------------\n\nHowever, MENS2 prevalence is high (1/5) among patients who present in a clinic with medullary thyroid carcinoma.  If we only used the test among 3 million of these patients \n\n- 600,000 will have the disease\n- 2,400,000 will NOT have the disease (still unbalanced by but much less)\n- Column accuracies (sensitivity and specificity) remain the same (98% and 99.9%)\n  - These are properties of the test/classifier\n- PPV is now much better; 588,000 / (2400 + 588,000) = 99.6%\n  \n|               | **Ground Truth**|     |\n|:--------------|:--------- |:----------|\n| **Prediction**| Positive  | Negative  | \n| Positive      | 588000    |    2400   |\n| Negative      |  12000    | 2397600   | \n\n--------------------------------------------------------------------------------\n\nNow think about \"accuracy\" of any specific COVID test\n\n- It dismayed me to see talk of accuracy\n  - The cost of the two types of errors is different!\n- Occasionally, there was talk of sensitivity and specificity\n- There was rarely/never discussion of PPV and NPV, which is what matters most when you are given your test result\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: How would the PPV and NPV change when we moved from testing only people with symptoms who presented at the hospital to testing everyone (e.g., all college students)?\n:::\n\n::: {.fragment .uwred}\nRelatively speaking, when testing someone with obvious COVID symptoms PPV would be high but NPV could be low.  Conversely, for our students PPV is likely low but NPV is likely high\n:::\n\n--------------------------------------------------------------------------------\n\nIn some instances, it may be more useful to focus on **precision** and **recall** rather than sensitivity and specificity.  The **F1 measure** is the harmonic mean of precision and recall\n\n- This is a row and column accuracy \n- Recall (sensitivity) focuses on how many true positive cases will we correctly identify\n- Precision (PPV) focuses on how accurate the prediction of \"positive\" will be (prevalence dependent)\n- This keeps the focus on positive cases\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n```{r}\ncm |> \n  summary() |> \n  filter(.metric == \"precision\" | .metric == \"recall\" | .metric == \"f_meas\") |> \n  select(-.estimator)\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: Can you link the numbers in the confusion matrix on the previous slide to Recall (Sensitivity) and Precision (PPV) metrics\n:::\n\n::: {.fragment .uwred}\nRecall/sensitivity is \"accuracy\" for the positive cases (in this instance, patients with heart disease).\n30 / (30 + 17)\n\nPrecision/PPV is \"accuracy\" for the positive predictions (when model predicts yes).  \n30 / (30 + 13)\n:::\n\n--------------------------------------------------------------------------------\n\n$F1$ is the harmonic mean of Recall and Precision\n\n- Harmonic means are used with rates (see [more detail](https://en.wikipedia.org/wiki/Mean) about the Pythagorean means, if interested)\n- $F1$ is an unweighted harmonic mean\n  - $F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$ \n  - or using the more general formula for harmonic means: $F1 = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}$ \n- $F_\\beta$ is a weighted version where $\\beta$ is the relative weighting of recall to precision\n  - Two commonly used values for $\\beta$ are 2, which weighs recall twice as much than precision, and 0.5, which weighs precision twice as much as recall \n  - $F_\\beta = (1 + \\beta^2) * \\frac{Precision * Recall}{(\\beta^2*Precision) + Recall}$\n  \n```{r}\ncm |> \n  summary() |> \n  filter(.metric == \"precision\" | .metric == \"recall\" | .metric == \"f_meas\") |> \n  select(-.estimator)\n```\n\n--------------------------------------------------------------------------------\n\n**Cohen's Kappa** is a bit more complicated to calculate and understand\n\n- There is a great [explanation of kappa](https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english) on stack overflow\n\n- Wikipedia also has a very detailed [definition and explanation](https://en.wikipedia.org/wiki/Cohen's_kappa) as well (though not in the context of machine learning)\n\n- Compares observed accuracy to expected accuracy (random chance)\n\n$Kappa = \\frac{observed\\:accuracy - expected\\:accuracy}{1 - expected\\:accuracy}$\n\n- When outcome is unbalanced, some agreement/accuracy (relationship between model predictions and reference/ground truth) is expected\n\n- Kappa adjusts for this\n\nKappa is essentially the proportional increase in accuracy above the accuracy expected by the base rates of the reference and classifier\n\n--------------------------------------------------------------------------------\n\nTo calculate the expected accuracy, we need to consider the probabilities of reference and classifier prediction both being positive (and both being negative) by chance given the base rates of these classes for the reference and classifier.\n\n\n|               | **Ground Truth**  |   |\n|:--------------|:--------- |:----------|\n| **Prediction**| Positive  | Negative  | \n| Positive      | TP        | FP        |\n| Negative      | FN        | TN        |\n\n\n$P_{positive} = \\frac{FN + TP}{TN + FN + FP + TP} * \\frac{FP + TP}{TN + FN + FP + TP}$\n\n$P_{negative} = \\frac{TN + FP}{TN + FN + FP + TP} * \\frac{TN + FN}{TN + FN + FP + TP}$\n\n$P_{expected} = P_{positive} + P_{negative}$\n\n--------------------------------------------------------------------------------\n\nIn our example\n\n```{r}\ncm\n```\n\n$P_{positive} = \\frac{17 + 30}{428} * \\frac{13 + 30}{428} = 0.01103262$\n\n$P_{negative} = \\frac{368 + 13}{428} * \\frac{368 + 17}{428} = 0.8007522$\n\n$P_{expected} = 0.01103262 + 0.8007522 = 0.8117848$\n\n$Actual Accuracy = 0.9299065$\n\n$Kappa = \\frac{observed\\:accuracy - expected\\:accuracy}{1 - expected\\:accuracy}$\n\n$Kappa = \\frac{0.9299065 - 0.8117848}{1 - 0.8117848} = 0.627588$\n\n```{r}\nsummary(cm) |> \n  filter(.metric == \"kap\") |> \n  pull(.estimate)\n```\n\n--------------------------------------------------------------------------------\n\nKappa Rules of Thumb w/ a big grain of salt.......\n\n- Kappa < 0: No agreement\n- Kappa between 0.00 and 0.20: Slight agreement\n- Kappa between 0.21 and 0.40: Fair agreement\n- Kappa between 0.41 and 0.60: Moderate agreement\n- Kappa between 0.61 and 0.80: Substantial agreement\n- Kappa between 0.81 and 1.00: Almost perfect agreement.\n\nSee @Landis1977 ([PDF](pdfs/landis_1977_kappa.pdf))for more details \n\n--------------------------------------------------------------------------------\n\n## The Receiver Operating Characteristic Curve\n\nLet's return now to consider sensitivity and specificity again\n\nRemember that our classifier is estimating the probability of an observation being in the positive class.\n\nWe dichotomize this probability when we formally make a class prediction\n\n- If the probability > 50%, we classify the observation as positive\n- If the probability <= 50%, we classify the observation as negative\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: How can we improve sensitivity?\n:::\n\n::: {.fragment .uwred}\nWe can use a more liberal/lower classification threshold for saying someone has heart disease.  \n\nFor example, rather than requiring a 50% probability to classify as yes for heart disease, we could lower to 20% for the classification threshold\n:::\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: What will the consequences of this change be?\n:::\n\n::: {.fragment .uwred}\nFirst, the Bayes classifier threshold of 50% produces the highest overall accuracy so accuracy will generally (though not always) drop when you shift from 50%.  \n\nIf we think about this change as it applies to the columns of our confusion matrix, we will now catch more of the yes (fewer false negatives/misses), so sensitivity \nwill go up.  This was the goal of the lower threshold.  However, we will also end up with more false positives so specificity will drop.  If you consider the rows of \nthe confusion matrix, we will have more false positives so the PPV will drop.  \n\nHowever, we will have fewer false negatives so the NPV will increase.  Whether these trade-offs are worth it are a function of the cost of different types of errors \nand how much you gain and lose with regard to each type of performance metric (ROC can inform this; more in a moment)\n:::\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n--------------------------------------------------------------------------------\n\nPreviously, we simply used `predict(fit_glmnet, feat_test)$.pred_class`.  \n\n`$.pred_class` dichotomized at 50% by default\n\n- This is the classification threshold to use with predicted probabilities that will produce the best overall accuracy (e.g., Bayes classifier)\n- However, we can use a different threshold to increase sensitivity or specificity\n- This comes at a cost to the other characteristic (its a trade-off)\n  - Lower threshold increases sensitivity but decreases specificity\n  - Higher threshold increases specificity but decreases sensitivity\n  \n--------------------------------------------------------------------------------\n\nIt is relatively easy to make a new confusion matrix and get new performance metrics with a different classification threshold\n\n- Make a tibble with truth and predicted probabilities\n```{r}\npreds <- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet, feat_test, type = \"prob\")$.pred_yes)\n\npreds\n```\n\n--------------------------------------------------------------------------------\n\n- Use this to get class estimates at any threshold we want\n- Here we threshold at 20%\n```{r}\npreds <- preds |> \n  mutate(estimate_20 = if_else(prob <= .20, \"no\", \"yes\"),\n         estimate_20 = factor(estimate_20, levels = c(\"yes\", \"no\"))) |> \n  print()\n```\n\n--------------------------------------------------------------------------------\n\nWe can now make a confusion matrix for this new set of truth and estimates using the 20% threshold\n\n```{r}\ncm_20 <- preds |> \n  conf_mat(truth = truth, estimate = estimate_20)\n\ncm_20\n```\n\nLet's compare to 50% (original)\n\n```{r}\ncm\n```\n\n--------------------------------------------------------------------------------\n\nAnd let's compare these two thresholds on a subset of our numeric metrics\n\n- 20% threshold\n```{r}\ncm_20 |> \n  summary() |> \n  filter(.metric %in% c(\"accuracy\", \"sens\", \"spec\", \"ppv\", \"npv\")) |> \n  select(-.estimator)\n```\n\n- 50% threshold\n```{r}\ncm |> \n  summary() |> \n  filter(.metric %in% c(\"accuracy\", \"sens\", \"spec\", \"ppv\", \"npv\")) |> \n  select(-.estimator)\n```\n\n\n[Do the changes on each of these metrics make sense to you? If not, please review these previous slides again!]{.uwred}\n\n--------------------------------------------------------------------------------\n\nYou can begin to visualize the classifier performance by threshold simply by plotting histograms of the predicted positive class probabilities, separately for the true positive and negative classes\n\n- Let's look at our classifier\n- Ideally, the probabilities are mostly low for the true negative class (\"no\") and mostly high for the true positive class (\"yes\")\n- You can imagine how any specific probability cut point would affect specificity (apply cut to the  left panel) or sensitivity (apply cut to the right panel)\n\n```{r}\nggplot(data = preds, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: What do you think about its performance? What insights does this plot generate?\n:::\n\n::: {.fragment .uwred}\n1. You can see that we can likely drop the threshold to somewhere about 25% without decreasing the specificity too much.  This will allow you to detect more positive cases.  \n\n2.  Our model seems to be able to predict negative cases well.  They mostly have low probabilities.   However, you can see its poor performance with positive cases.  They are spread pretty evenly across the full range of probabilities.  We likely do not have enough positive cases in our training data\n:::\n\n--------------------------------------------------------------------------------\n\nThe **Receiver Operating Characteristics (ROC)** curve for a classifier provides a more formal method to visualize the trade-offs between sensitivity and specificity across all possible thresholds for classification.\n\nLets look at this in our example\n\n- We need columns for truth and probabilities of the positive class for each observation\n- We need to specify the positive class\n- Returns tibble with data to plot an ROC curve\n\n```{r}\nroc_plot <- \n  tibble(truth = feat_test$disease,\n         prob = predict(fit_glmnet, feat_test, type = \"prob\")$.pred_yes) |> \n  roc_curve(prob, truth = truth)\n\nroc_plot\n```\n\n--------------------------------------------------------------------------------\n\nWe can `autoplot()` this\n\n```{r}\n#| out-height: 3in\n\nautoplot(roc_plot)\n```\n\n--------------------------------------------------------------------------------\n\nOr we can customize a plot passing the data into` ggplot()`\n\n- Not doing anything fancy here\n- Consider this a shell for you to build on if you want more than `autoplot()` provides\n```{r}\nroc_plot |>\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_equal() +\n  labs(x = \"1 - Specificity (FPR)\",\n       y = \"Sensitivity (TPR)\")\n```\n\n--------------------------------------------------------------------------------\n\nWhen evaluating an ROC curve:\n\n- A random classifier would have a diagonal curve from bottom-left to top-right (the dotted line)\n\n- A perfect classifier would reach up to top-left corner\n  - Sensitivity = 1 (true positive rate)\n  - 1 - Specificity = 0 (false positive rate)\n\n--------------------------------------------------------------------------------\n\nThe ROC Curve is not only a useful method to visualize classier performance across\nthresholds\n\nThe area under the ROC curve (auROC) is an attractive performance metric\n\n- Ranges from 1.0 (perfect) down to approximately 0.5 (random classifier)\n    - If the auROC was consistently less than 0.5, then the predictions could simply be inverted\n    - Values between .70 and .80 are considered fair\n    - Values between .80 and .90 are considered good\n    - Values above .90 are considered excellent\n    - These are very rough, and to my eye, the exact cuts and labels are somewhat arbitrary\n\n--------------------------------------------------------------------------------\n\n- auROC is the probability that the classifier will rank/predict a randomly selected true positive observation higher than a randomly selected true negative observation\n\n- Alternatively, it can be thought of as the average sensitivity across all decision thresholds\n\n- auROC summarizes performance (sensitivity vs. specificity trade-off) across all possible thresholds\n\n- auROC is not affected by class imbalances in contrast to many other metrics\n\n--------------------------------------------------------------------------------\n\nIt is easy to get the auROC for the ROC in tidymodels using `roc_auc()`\n\nAs with calculating the ROC curve, we need \n\n- truth \n- predicted probabilities for the positive class\n- to specify the `event_level` (default is first)\n\n```{r}\ntibble(truth = feat_test$disease,\n       prob = predict(fit_glmnet, feat_test, type = \"prob\")$.pred_yes) |> \n  roc_auc(prob, truth = truth)\n```\n\n--------------------------------------------------------------------------------\n\n## Using Alternative Performance Metrics for Model Selection\n\nYou can select the best model configuration using resampling with performance metrics other than accuracy\n\n- Aggregate measures are typically your best choice (except potentially with high imbalance - more on this later)\n\n  - For classification: \n    - Accuracy\n    - Balanced accuracy\n    - $F1$\n    - Area under ROC Curve (auROC)\n    - Kappa\n    \n  - For regression\n    - RMSE\n    - $R^2$\n    - MAE (mean absolute error)\n\n--------------------------------------------------------------------------------\n\n- You typically need to use a single metric for selection among model configurations\n  - You should generally use the performance metric that is the best aligned with your problem:\n\n- In classification\n  - Do you care about types of errors or just overall error rate\n  - Is the outcome relatively balanced or unbalanced\n  - What metric will be clearest to your audience\n\n- In regression\n  - Do you want to weight big and small errors the same\n  - What metric will be clearest to your audience (though all of these are pretty clear.  There are more complicated regression metrics)\n\n- Although you will use one metric to select the best configuration, you can evaluate/characterize the performance of your final model with as many metrics as you like\n\n--------------------------------------------------------------------------------\n\nYou should recognize the differences between the cost function for the algorithm and the performance metric:\n\n- Cost function is fundamental to the definition of the algorithm\n- Cost function is minimized to determine parameter estimates in parametric models\n- Performance metric is independent of algorithm\n- Performance metric is used to select and evaluate model configurations\n\n- Sometimes they can be the same metric (e.g., RMSE)  \n- BUT, this is not required\n\n--------------------------------------------------------------------------------\n\nWith `tidymodels`, it is easy to select hyperparameters or select among model configurations more generally using one of many different performance metrics\n\n- We will still use either `tune_grid()` or `fit_resamples()`\n- We will simply specify a different performance metric inside of `metric_set()`\n- If we only measure one performance metric, we can use defaults with `show_best()`\n\n--------------------------------------------------------------------------------\n\nHere is an example of measuring `roc_auc()` but you can use any performance function from the yardstick package\n```{r}\nfits_glmnet_auc <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(roc_auc))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_auc\")\n```\n\n--------------------------------------------------------------------------------\n\nAnd now use `show_best()`, with best defined by auROC\n```{r}\nshow_best(fits_glmnet_auc, n = 5, metric = \"roc_auc\")\n```\n\n--------------------------------------------------------------------------------\n\nYou can also measure multiple metrics during resampling but you will need to select the best configuration using only one\n\n- see `metric_set()` for the measurement of multiple metrics,\n- see `show_best()` for the use of `metric` to indicate which metric to use for selection\n\n```{r}\nfits_glmnet_many <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(roc_auc, accuracy, sens, spec, bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_many\")\n```\n\n--------------------------------------------------------------------------------\n\nBut now we need to indicate how to define **best**\n\nWe will define it based on balanced accuracy\n```{r}\nshow_best(fits_glmnet_many, metric = \"bal_accuracy\", n = 5)\n```\n\nAnd here based on auROC (same as before)\n```{r}\nshow_best(fits_glmnet_many, metric = \"roc_auc\", n = 5)\n```\n\n--------------------------------------------------------------------------------\n\nOf course, you can easily calculate any performance metric from `yardstick` [package](https://yardstick.tidymodels.org/reference/index.html) in test to evaluate your final model\n\n- Using `conf_mat()` and `summary()` as above (but not for auROC)\n- Using `*_vec()` functions; for example:\n  - `accuracy_vec()`\n  - `roc_auc_vec()`\n  - `sens_vec()`; `spec_vec()`\n- Piping a tibble that contain truth, and estimate or probability into appropriate function\n  - `accuracy()`\n  - `roc_auc()`\n  - `sens()`; `spec()`\n\n--------------------------------------------------------------------------------\n\nFit best model using auROC\n```{r}\nfit_glmnet_auc <-   \n  logistic_reg(penalty = select_best(fits_glmnet_many, metric = \"roc_auc\")$penalty, \n               mixture = select_best(fits_glmnet_many, metric = \"roc_auc\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn)\n```\n\nand get all metrics\n```{r}\ncm_auc <- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_auc, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\ncm_auc |> \n  summary()\n```\n\n--------------------------------------------------------------------------------\n\nFit best model using balanced accuracy\n```{r}\nfit_glmnet_bal <-   \n  logistic_reg(penalty = select_best(fits_glmnet_many, metric = \"roc_auc\")$penalty, \n               mixture = select_best(fits_glmnet_many, metric = \"roc_auc\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn)\n```\n\nand still get all metrics (different because best model configuration is different)\n```{r}\ncm_bal <- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_bal, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\ncm_bal |> \n  summary()\n```\n\n--------------------------------------------------------------------------------\n\n## Advanced Methods for Class Imbalances\n\nWhen there is a high degree of class imbalance:\n\n- It is often difficult to build models that predict the minority class well\n- This will yield low sensitivity if the positive class is the minority class (as in our example)\n- This will yield low specificity if the negative class is the minority class\n- Each of these issues may be a problem depending on the costs of FP and FN\n\n--------------------------------------------------------------------------------\n\nLet's see this at play again in our model\n\n- Using the 50% threshold we have low sensitivity but good specificity\n```{r}\n#| out-height: 3in\n\nautoplot(cm)\n```\n\n```{r}\ncm |> \n  summary() |> \n  filter(.metric == \"sens\" | .metric == \"spec\") |> \n  select(-.estimator)\n```\n\n--------------------------------------------------------------------------------\n\n- We do much better with probabilities for negative (majority) vs. positive (minority) class\n- We can see that we will not affect specificity much by lowering the threshold for positive classification to 20-25%\n- BUT, we really need to do do better with the distribution of probabilities for observations that are positive (yes)\n```{r}\nggplot(data = preds, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n```\n\n--------------------------------------------------------------------------------\n\nWhat can we do when we have imbalanced outcomes?\n\nWe will consider:\n\n- Changes to the classification/decision threshold that trade-off sensitivity vs. specificity for a fitted model (already discussed)\n- Changes to performance metric for selecting the best model configuration  (already demonstrated)\n- Sampling/Resampling methods that will affect the balance of the outcome in the training data to fit models that are better with the minority class (new)\n\n--------------------------------------------------------------------------------\n\n### Classification (Decision) Threshold\n\nWe have already seen an example of how the classification threshold (the probability at which we split between predicting a case as positive vs. negative) affects sensitivity vs. specificity\n\nDecreasing the threshold (probability) for classifying a case as positive will:\n\n- Increase sensitivity and decrease specificity\n- This will decrease FN but increase FP\n- This may be useful if the positive class is the minority class\n- The ROC curve is a useful display to provide this information about your classifier\n  - Curve can be colored to show the threshold\n- The separate histograms by positive vs. negative can also be useful as well\n- If you want to use your data to select the best threshold, you will need yet another set of data to make this selection\n  - Can't make the selection in training b/c those probabilities are overfit\n  - Can't make the selection of threshold in test and then also use the same test data to evaluate that model!\n\n--------------------------------------------------------------------------------\n\n### Performance Metric Considerations\n\nWhen you are choosing a performance metric for selecting your best model configuration, you should choose a performance metric that it best aligned with the nature of the performance you seek\n\n- If you want just good overall accuracy, accuracy may be a good metric\n- If the outcome is unbalanced, and you care about the types of errors, you might want\n  - Balanced accuracy (average of sensitive and specificity)\n  - Only sensitivity or specificity by itself (recommended by Kuhn)\n  - auROC\n  - An F measure (harmonic mean of sensitivity and PPV)\n  - May need to think carefully about what is most important to you\n\nEarlier, we saw that we got better sensitivity when we used balanced accuracy rather than accuracy to tune our hyperparameters\n\n--------------------------------------------------------------------------------\n\n### Sampling and Resampling to Address Class Imbalance\n\nWe can address issues of class imbalance either a priori or post-hoc with respect to data collection\n\n-  A priori method would be to over-sample to get more of the minority class into your training set\n  - Use targeted recruiting\n  - Can be very costly or impossible in many instances\n  - If possible,  this can be much better than the resampling approach below\n  \n- Post hoc, we can employ a variety of resampling procedures that are designed to make the training data more balanced\n  - We can up-sample the minority class\n  - We can down-sample the majority class\n  - We can synthesize new minority class observations e.g, SMOTE\n  \n- For both a priori sampling or post-hoc resampling strategies, it is important that your test set is not manipulated.  It should represent the expected distribution for the outcome, unaltered\n\n--------------------------------------------------------------------------------\n\n### Up-sampling\n\n- We resample minority class observations with replacement within our training set to increase the number of total observations of the minority class in the training set.\n- This simply duplicates existing minority class observations\n- Our test (or validation) set(s) should NOT be resampled.  This is handled well by `step_upsample()`\n\n--------------------------------------------------------------------------------\n\nLet's apply this in our example\n\n- Up-sampling is part of feature engineering recipe\n- Need to specify the outcome (`disease`)\n- Can set `over_ratio` to values other than 1 if desired\n- Makes sense to do this after missing data imputation and dummy coding\n- Makes sense to do this before normalizing features \n- These steps are in the `themis` package rather than `recipes` (can use namespace or load full library)\n```{r}\nrec_up <- recipe(disease ~ ., data = data_trn) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) |> \n  themis::step_upsample(disease, over_ratio = 1) |> \n  step_normalize(all_numeric_predictors())\n```\n\n--------------------------------------------------------------------------------\n\nNeed to re-tune model b/c the sample size has changed\n\nNeed to refit the final model to all of train\n```{r}\nfits_glmnet_up <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec_up, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fit_glmnet_up\")\n```\n\n--------------------------------------------------------------------------------\n\nReview hyperparameter plot\n```{r}\nplot_hyperparameters(fits_glmnet_up, hp1 = \"penalty\", hp2 = \"mixture\", metric = \"bal_accuracy\", log_hp1 = TRUE)\n```\n\n--------------------------------------------------------------------------------\n\nLet's fit this best model configuration to all of our training data and evaluate it in test\n\n- Note the use of `NULL` for `new_data` \n- `step_upsample()` is only applied to training/held-in but not held-out (truly new) data\n- `bake()` knows to use the training data provided during `prep()` if we specify `NULL`\n- Could have done this all along for baking training.  Its sometimes faster but previously same result.  Now its necessary!\n```{r}\nrec_up_prep <- rec_up |> \n  prep(data_trn)\n\nfeat_trn_up <- rec_up_prep |> \n  bake(new_data = NULL)\n```\n\n--------------------------------------------------------------------------------\n\nNotice that `disease` is now balanced in the training data!\n```{r}\nfeat_trn_up |> \n  skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\nfit_glmnet_up <-   \n  logistic_reg(penalty = select_best(fits_glmnet_up, metric = \"bal_accuracy\")$penalty, \n               mixture = select_best(fits_glmnet_up, metric = \"bal_accuracy\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn_up)\n```\n\n--------------------------------------------------------------------------------\n\nTo evaluate this model, we now need test features too\n\n- IMPORTANT: Test is NOT up-sampled\n- bake it as new data!\n```{r}\nfeat_test <- rec_up_prep |> \n  bake(data_test)  \n\nfeat_test |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\nLet's see how this model performs in test\n```{r}\ncm_up <- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_up, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\ncm_up |> \n  summary()\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\npreds_up <- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet_up, feat_test, type = \"prob\")$.pred_yes)\n\nggplot(data = preds_up, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n```\n\n--------------------------------------------------------------------------------\n\n### Down-sampling\n\nWe resample majority class observations within our training set to decrease/match the number of total observations of the minority class in the training set.\n\n- This selects a subset of the majority class\n- Our test (or validation) set(s) should NOT be resampled.  This is handled well by `step_downsample()`\n\n--------------------------------------------------------------------------------\n\nDown-sampling is part of feature engineering recipe\n\n- Need to specify the outcome (`disease`)\n- Can set `under_ratio` to values other than 1 if desired\n- Makes sense to do this after missing data imputation and dummy coding\n- Makes sense to do this before normalizing features \n\n```{r}\nrec_down <- recipe(disease ~ ., data = data_trn) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) |> \n  themis::step_downsample(disease, under_ratio = 1) |> \n  step_normalize(all_numeric_predictors())\n```\n\n--------------------------------------------------------------------------------\n\nNeed to re-tune model b/c the sample size has changed\n\nNeed to refit the final model to all of train\n```{r}\nfits_glmnet_down <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec_down, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_down\")\n```\n\n--------------------------------------------------------------------------------\n\nReview hyperparameters\n```{r}\nplot_hyperparameters(fits_glmnet_down, hp1 = \"penalty\", hp2 = \"mixture\", \n                     metric = \"bal_accuracy\", log_hp1 = TRUE)\n```\n\n--------------------------------------------------------------------------------\n\nLet's fit this best model configuration to all of our training data and evaluate it in test\n\n- Note the use of `NULL` again when getting features for training data. Very important!\n- `step_downsample()` is not applied to baked data (See its default for `skip = TRUE` argument)\n- NOTE: sample size and ratio of yes/now for disease!\n```{r}\nrec_down_prep <- rec_down |> \n  prep(data_trn)\n\nfeat_trn_down <- rec_down_prep |> \n  bake(new_data = NULL)\n\nfeat_trn_down |> skim_some()\n```\n\n--------------------------------------------------------------------------------\n\nNow fit the model to these downsampled training data\n```{r}\nfit_glmnet_down <-   \n  logistic_reg(penalty = select_best(fits_glmnet_down, metric = \"bal_accuracy\")$penalty, \n               mixture = select_best(fits_glmnet_down, metric = \"bal_accuracy\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn_down)\n```\n\n--------------------------------------------------------------------------------\n\nLet's see how this model performs in test\n\n- First we need test features\n- IMPORTANT: Test is NOT down-sampled\n\n```{r}\nfeat_test <- rec_down_prep |> \n  bake(data_test)\n\nfeat_test |> skim_some()\n```\n\n--------------------------------------------------------------------------------\n\nGet metrics in test\n```{r}\ncm_down <- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_down, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\ncm_down |> \n  summary()\n```\n\n--------------------------------------------------------------------------------\n\nAnd plot faceted probabilites\n```{r u8-bal-15}\npreds_down <- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet_down, feat_test, type = \"prob\")$.pred_yes)\n\nggplot(data = preds_down, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n```\n\n--------------------------------------------------------------------------------\n\n### SMOTE\n\nA third approach to resampling is called the synthetic minority over-sampling technique (SMOTE)\n\nTo up-sample the minority class, SMOTE synthesizes new observations.  \n\n- To do this, an observation is randomly selected from the minority class.  \n- This observation's K-nearest neighbors (KNNs) are then determined. \n- The new synthetic observation retains the outcome but a random combination of the predictors values from the randomly selected observation and its neighbors.  \n\n--------------------------------------------------------------------------------\n\nThis is easily implemented by recipe in `tidymodels` using `step_smote()`\n\n- Need to specify the outcome (`disease`)\n- Can set `over_ratio` to values other than 1 (default) if desired\n- Can set `neighbors` to values other than 5 (default) if desired\n- Makes sense to do this after missing data imputation and dummy coding\n- Other features will need to be scaled/range-corrected prior to use (for distance)\n- Makes sense to do this before normalizing features for glmnet, etc \n\n```{r u8-bal-16}\nrec_smote <- recipe(disease ~ ., data = data_trn) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) |>\n  step_range(all_predictors()) |> \n  themis::step_smote(disease, over_ratio = 1, neighbors = 5) |> \n  step_normalize(all_numeric_predictors())\n```\n\n--------------------------------------------------------------------------------\n\nNeed to re-tune model b/c the sample size has changed\n\nNeed to refit the final model to all of train\n```{r}\nfits_glmnet_smote <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec_smote, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_smote\")\n```\n\n--------------------------------------------------------------------------------\n\nReview hyperparameters\n```{r}\nplot_hyperparameters(fits_glmnet_smote, hp1 = \"penalty\", hp2 = \"mixture\", \n                     metric = \"bal_accuracy\", log_hp1 = TRUE)\n```\n\n--------------------------------------------------------------------------------\n\nLet's fit this best model configuration to all of our training data and evaluate it in test\n\n- Note the use of `NULL` (once again!) for `new_data`\n- `step_smote()` is not applied to new data\n- NOTE: sample size and outcome balance\n```{r}\nrec_smote_prep <- rec_smote |> \n  prep(data_trn)\n\nfeat_trn_smote <- rec_smote_prep |> \n  bake(NULL)\n\nfeat_trn_smote |> skim_some()\n```\n\n--------------------------------------------------------------------------------\n\nFit model to smote sampled training data\n```{r}\nfit_glmnet_smote <-   \n  logistic_reg(penalty = select_best(fits_glmnet_smote, metric = \"bal_accuracy\")$penalty, \n               mixture = select_best(fits_glmnet_smote, metric = \"bal_accuracy\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn_smote)\n```\n\n--------------------------------------------------------------------------------\n\nLet's see how this model performs in test\n\n- IMPORTANT: Test is NOT SMOTE up-sampled\n```{r}\nfeat_test <- rec_smote_prep |> \n  bake(data_test)\n\ncm_smote <- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_smote, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\ncm_smote |> \n  summary()\n```\n\n--------------------------------------------------------------------------------\n\nPlot faceted predicted probabilites\n```{r}\npreds_smote <- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet_smote, feat_test, type = \"prob\")$.pred_yes)\n\nggplot(data = preds_smote, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n```","srcMarkdownNoYaml":"\n\n::: {.content-visible unless-format=\"revealjs\"}\n# Advanced Performance Metrics\n:::\n::: {.content-visible when-format=\"revealjs\"}\n# IAML Unit 8: Advanced Performance Metrics\n:::\n\n\n## Learning Objectives\n\n- Understand costs and benefits of accuracy \n- Use of a confusion matrix\n- Understand costs and benefits of other performance metrics\n- The ROC curve and area under the curve\n- Model selection using other performance metrics\n- How to address class imbalance\n  - Selection of performance metric\n  - Selection of classification threshold\n  - Sampling and resampling approaches\n\n--------------------------------------------------------------------------------\n\n## Introduction\n\n```{r}\n#| include: false\n\n# set up environment.  Now hidden from view\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\noptions(conflicts.policy = \"depends.ok\")\n\nlibrary(xfun, include.only = \"cache_rds\")\n\n# parallel processing\nlibrary(future)\nplan(multisession, workers = parallel::detectCores(logical = FALSE))\n\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\npath_data <- \"./data\"\n\nrerun_setting <- FALSE \n``` \n\nIn this unit, we will again use the Cleveland heart disease dataset.\n\nHowever, I have modified this to make the outcome unbalanced such that Yes represents approximately 10% of the observations\n\nNow that we will calculate performance metrics beyond accuracy, the order of the levels of our outcome variable(`disease`) matters.  We will make sure that the positive class (event of interest; in this case yes for `disease`) is the **first level**.\n\n--------------------------------------------------------------------------------\n\nFirst, lets open and skim the raw data\n```{r}\ndata_all <- read_csv(here::here(path_data, \"cleveland_unbalanced.csv\"), \n                     col_names = FALSE, na = \"?\", \n                     col_types = cols()) |> \n  rename(age = X1,\n         sex = X2,\n         cp = X3,\n         rest_bp = X4,\n         chol = X5,\n         fbs = X6,\n         rest_ecg = X7,\n         exer_max_hr = X8,\n         exer_ang = X9,\n         exer_st_depress = X10,\n         exer_st_slope = X11,\n         ca = X12,\n         thal = X13,\n         disease = X14) \n\ndata_all |> skim_some()\n```\n\n--------------------------------------------------------------------------------\n\nCode categorical variables as factors with meaningful text labels (and no spaces)\n\n- NOTE the use of `disease = fct_relevel (disease, \"yes\")` to set yes as positive class (first level) for disease\n```{r}\ndata_all <- data_all |> \n  mutate(disease = factor(disease, levels = 0:4, \n                          labels = c(\"no\", \"yes\", \"yes\", \"yes\", \"yes\")),\n         disease = fct_relevel (disease, \"yes\"),\n         sex = factor(sex,  levels = c(0, 1), labels = c(\"female\", \"male\")),\n         fbs = factor(fbs, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_ang = factor(exer_ang, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_st_slope = factor(exer_st_slope, levels = 1:3, \n                                labels = c(\"upslope\", \"flat\", \"downslope\")),\n         cp = factor(cp, levels = 1:4, \n                     labels = c(\"typ_ang\", \"atyp_ang\", \"non_anginal\", \"non_anginal\")),\n         rest_ecg = factor(rest_ecg, levels = 0:2, \n                           labels = c(\"normal\", \"wave_abn\", \"ventric_hypertrophy\")),\n         thal = factor(thal, levels = c(3, 6, 7), \n                       labels = c(\"normal\", \"fixeddefect\", \"reversabledefect\")))\n\ndata_all |> skim_some()\n```\n\n--------------------------------------------------------------------------------\n\nDisease is now unbalanced\n```{r}\ndata_all |> tab(disease)\n```\n\n--------------------------------------------------------------------------------\n\nFor this example, we will evaluate our final model using a held out test set\n\n```{r}\nset.seed(20140102)\nsplits_test <- data_all |> \n  initial_split(prop = 2/3, strata = \"disease\")\n\ndata_trn <- splits_test |> \n  analysis()\n\ndata_test <- splits_test |> \n  assessment()\n```\n\n--------------------------------------------------------------------------------\n\nWe will be fitting a penalized logistic regression again (using glmnet)\n\nWe will do only basic feature engineering for this algorithm and to handle missing data\n\n```{r}\nrec <- recipe(disease ~ ., data = data_trn) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) |> \n  step_normalize(all_predictors())\n```\n\n--------------------------------------------------------------------------------\n\nWe tune/select best hyperparameter values via bootstrap resampling with the training data\n\n- get bootstrap splits\n- make grid of hyperparameter values\n```{r}\nsplits_boot <- data_trn |> \n  bootstraps(times = 100, strata = \"disease\")  \n\ngrid_glmnet <- expand_grid(penalty = exp(seq(-8, 3, length.out = 200)),\n                           mixture = seq(0, 1, length.out = 6))\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\n#| label: fits_glmnet\n\nfits_glmnet <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(accuracy))\n  },\n  dir = \"cache/008/\",\n  file = \"fits_glmnet\",\n  rerun = rerun_setting)\n```\n\n--------------------------------------------------------------------------------\n\nReview hyperparameter plot and best values for hyperparameters\n```{r}\nplot_hyperparameters(fits_glmnet, hp1 = \"penalty\", hp2 = \"mixture\", \n                     metric = \"accuracy\", log_hp1 = TRUE)\n```\n\n```{r}\nshow_best(fits_glmnet, n = 1, metric = \"accuracy\")\n```\n\n--------------------------------------------------------------------------------\n\nLet's fit this best model configuration to all of our training data and evaluate it in test\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\nfeat_trn <- rec_prep |> \n  bake(data_trn)\n\nfit_glmnet <-   \n  logistic_reg(penalty = select_best(fits_glmnet, metric = \"accuracy\")$penalty, \n               mixture = select_best(fits_glmnet, metric = \"accuracy\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\nAnd evaluate it by predicting into test\n```{r}\nfeat_test <- rec_prep |> \n  bake(data_test)\n\n(model_accuracy <- accuracy_vec(feat_test$disease, predict(fit_glmnet, feat_test)$.pred_class))\n```\n\n--------------------------------------------------------------------------------\n\nAccuracy is an attractive measure because it is:\n\n- Intuitive and widely understood\n- Naturally extends to multi-class scenarios\n- These are not trivial advantages in research or application\n\nHowever, accuracy has at least three problems in some situations\n\n- If the outcome is unbalanced, it can be misleading\n  - High performance from simply predicting the majority (vs. minority) class for all observations\n  - Need to anchor evaluation of accuracy to baseline performance based on the majority case percentage\n\n- If the outcome is unbalanced, selecting among model configurations with accuracy can be biased toward configurations that predict the majority class because that will yield high accuracy by itself even without any signal from the predictors\n\n- Regardless of outcome distribution, it considers false positives (FP) and false negatives (FN) equivalent in their costs\n  - This is often not the case\n\n--------------------------------------------------------------------------------\n\nOutcome distributions :\n\n- May start to be considered unbalanced at ratios of 1:5 (20% of cases in the infrequent class)\n- In many real life applications (e.g., fraud detection), imbalance ratios ranging from 1:1000 up to 1:5000 are not atypical\n\nWhen working with unbalanced datasets:\n\n- The class or classes with many observations are called the major or **majority class(es)**\n- The class with few observations (and there is typically just one) is called the minor or **minority class**.\n\nIn our example, the majority class is negative (no) for heart disease and the minority class is positive (yes) for heart disease\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: In our example, our model's accuracy in test seemed high but was it really performing as well as this seems?\n:::\n\n::: {.fragment .uwred}\nAlthough this test accuracy seems high, this is somewhat misleading.  A model that simply labeled everyone as negative for heart disease would achieve almost as high accuracy in our test data\n:::\n\n--------------------------------------------------------------------------------\n\n```{r}\n(model_accuracy <- accuracy_vec(feat_test$disease, \n                                predict(fit_glmnet, feat_test)$.pred_class))\n\nfeat_test |> tab(disease)\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: Perhaps more importantly, are the costs of false positives (screening someone as positive when they do not have heart disease) and false negatives (screening someone as negative when they do have heart disease) comparable for a preliminary screening method?\n:::\n\n::: {.fragment .uwred}\nProbably not.  A false positive might mean that we do more testing that is unnecessary and later find out they do not have heart disease.   This comes with some monetary cost and also likely some distress for the patient.   However, a false negative means we send the patient home thinking they are healthy and may suffer a heart attack or other bad outcome.  That seems worse if this is only a preliminary screen.\n:::\n\n--------------------------------------------------------------------------------\n\n## The Confusion Matrix and Related Performance Metrics\n\nAt a minimum, it seems important to consider these issues explicitly but accuracy is not sufficiently informative.\n\nThe first step to this more careful assessment is to construct a confusion matrix\n\n--------------------------------------------------------------------------------\n\n|               | **Ground Truth** |    |\n|:--------------|:--------- |:----------|\n| **Prediction**| Positive  | Negative  | \n| Positive      | TP        | FP        |\n| Negative      | FN        | TN        |\n\nDefinitions: \n\n- TP: True positive\n- TN: True negative\n- FP: False positive (Type 1 error/false alarm)\n- FN: False negative (Type 2 error/miss)\n\nPerfect classifier has all the observations on the diagonal from top left to bottom right\n\nThe two types of errors (on the other diagonal) may have different costs\n\nWe can now begin to consider these costs\n\n--------------------------------------------------------------------------------\n\nLets look at the confusion matrix associated with our model's performance in test\n\n- We will use `conf_mat()` to calculate the confusion matrix\n- There does NOT (yet?) seem to be a vector version (i.e., `conf_mat_vec()`)\n- Therefore, we have to build a tibble of `truth` and `estimate` to pass into `conf_mat()`\n- It is best to assign the result to an object (e.g., `cm`) because we will use it for a few different tasks\n\n```{r}\ncm <- tibble(truth = feat_test$disease,\n             estimate = predict(fit_glmnet, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n```\n\n--------------------------------------------------------------------------------\n\nLet's display the matrix\n\n- The columns are sorted based on the true value for the observation (i.e., ground truth)\n  - In this case, that is the patients' true disease status\n  - We can see it is unbalanced with most of the cases in the \"no\" column\n\n- The rows are sorted by our model's predictions\n\n- As we noted above, correct predictions fall on the top/left- bottom/right diagonal\n\n```{r}\ncm\n```\n\n--------------------------------------------------------------------------------\n\nTidy model's makes it easy to visualize this matrix in one of two types of plots\n\n- mosaic (the default)\n```{r}\n#| out-height: 3in\n\nautoplot(cm)\n```\n\n- heatmap\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n--------------------------------------------------------------------------------\n\nRegardless of the plot, you can now begin to see the issues with our model\n\n- It seems accurate for patients that do NOT have heart disease\n  - 368/381 correct\n- It is not very accurate for patients that DO have heart disease\n  - 30/47 correct\n\n- This differential performance was masked by our global accuracy measure because overall accuracy was weighted heavily toward accuracy for patients without heart disease given their much higher numbers\n\n--------------------------------------------------------------------------------\n\nWe can use this confusion matrix as the starting point for MANY other common metrics and methods for evaluating the performance of a classification model.  In many instances, the metrics come in pairs that are relevant for FP and FN errors\n\n- Sensitivity & Specificity\n- Positive and Negative Predictive Value (PPV, NPV)\n- Precision and Recall\n\n--------------------------------------------------------------------------------\n\nThere are also some single metric approaches (like accuracy) that may be preferred when the outcome is unbalanced\n\n- Balanced accuracy\n- F1 (and other F-scores)\n- Kappa\n\n--------------------------------------------------------------------------------\n\nThere are also graphical approaches are based on either sensitivity/specificity or precision/recall.  These are:\n\n  - The Receiver Operating Characteristic (ROC) curve\n  - The Precision/Recall Curve (not covered further in this unit)\n  - Each of these curves also yields a single metric that represents the area under the curve\n  \n--------------------------------------------------------------------------------\n\nThe best metric/method is a function both of your intended use and the class distributions\n\n- As noted, **accuracy** is widely understood but can be misleading when class distributions are highly unbalanced\n\n- **Sensitivity/Specificity** are common in literatures that consider diagnosis (clinical psychology/psychiatry, medicine)\n\n- **Positive/Negative predictive value** are key to consider with sensitivity/specificity when classes are unbalanced\n\n- **ROC curve and its auROC metric** provide nice summary visualization and metric when considering classification thresholds other than 50% (also common when classes are unbalanced or types of errors matter)\n\n--------------------------------------------------------------------------------\n\nHere are definitions of many of the most common metrics linked to the confusion matrix\n\n\n|               | **Ground Truth**  |   |\n|:--------------|:--------- |:----------|\n| **Prediction**| Positive  | Negative  | \n| Positive      | TP        | FP        |\n| Negative      | FN        | TN        |\n\n$Accuracy = \\frac{TN + TP}{TN + TP + FN + FP}$\n\n$Sensitivity\\:(Recall) = \\frac{TP}{TP + FN}$\n\n$Specificity = \\frac{TN}{TN + FP}$\n\n$Positive\\:Predictive\\:Value\\:(Precision) = \\frac{TP}{TP + FP}$\n\n$Negative\\:Predictive\\:Value = \\frac{TN}{TN + FN}$\n\n$Balanced\\:accuracy = \\frac{Sensitivity + Specificity}{2}$\n\n$F_\\beta$ score:\n\n- $F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$ (most common; harmonic mean of precision and recall)\n\n- $F_\\beta = (1 + \\beta^2) * \\frac{Precision * Recall}{(\\beta^2*Precision) + Recall}$\n\n- $F_\\beta$ was derived so that it measures the effectiveness of a classifier for someone who assigns $\\beta$ times as much importance to recall as precision\n\n\n--------------------------------------------------------------------------------\n\nIt is easy to get any of these performance metrics using `summary()` on the confusion matrix\n\nMany of the statistics generated are based on an understanding of which level is the positive level.  \n\n- Tidymodels (yardstick to be precise) will default to consider the first level the positive level.  \n- If this is not true, some statistics (e.g., sensitivity, specificity) will be incorrect (i.e., swapped). \n- You can override this default by setting the following parameter inside any function that is affected by the order of the classes\" `event_level = \"second\"`\n\n--------------------------------------------------------------------------------\n\nHere is summary in action!\n```{r}\ncm |> summary()\n```\n\n--------------------------------------------------------------------------------\n\nLet's consider some of what these metrics are telling us about our classifier by looking at the metrics and a confusion matrix plot\n\nLet's start with **sensitivity** and **specificity** and their arithmetic mean (**balanced accuracy**)\n\n- These are column specific accuracies\n- Focus is on truth (columns)\n- Focuses a priori on two types of patients that may walk into the clinic to use our classifier\n```{r}\ncm |> \n  summary() |> \n  filter(.metric == \"sens\" | .metric == \"spec\" | .metric == \"bal_accuracy\") |> \n  select(-.estimator)\n```\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: Can you link the numbers in the confusion matrix on the previous slide to Sensitivity and Specificity metrics\n:::\n\n::: {.fragment .uwred}\nSensivity is \"accuracy\" for the positive cases (in this instance, those with disease = yes).  \nSensitivity = 30 / (17 + 30)\n\nSpecificity is \"accuracy\" for the negative cases (disease = no).  \nSpecificity = 368 / (368 + 13)\n:::\n\n--------------------------------------------------------------------------------\n\nNow let's consider **positive predictive value** and **negative predictive value** \n\n- These are row specific accuracies\n- Focus is on model predictions (rows)\n- Focuses on the utility of the information/screening result provided from our classifier\n- Not typically reported alone but instead in combo with sensitivity/specificity and prevalence (see next pages)\n- Mosaic plot is better visualization for sensitivity/specificity (though I also like the numbers).  Not that useful for PPV/NPV\n- Use heatmap?\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n```{r}\ncm |> \n  summary() |>\n  filter(.metric == \"ppv\" | .metric == \"npv\") |> \n  select(-.estimator)\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: Can you link the numbers in the confusion matrix on the previous slide to PPV and NPV metrics\n:::\n\n::: {.fragment .uwred}\nPPV is \"accuracy\" for the positive predictions (in this instance, when the model predicts yes. \nPPV = 30 / (30 + 13)\n\nNPV is \"accuracy\" for the negative predictions (disease = no).  \nNPV = 368 / (368 + 17)\n:::\n\n--------------------------------------------------------------------------------\n\n- PPV and NPV are influenced by both sensitivity and specificity BUT ALSO prevalence.\n\n- This becomes important in unbalanced settings where prevalence of classes is not equal\n  - Your classifier's PPV will be lower, even with good sensitivity and specificity if the prevalence of the positive class is low   \n  - Conversely, your classifier's NPV will be lower, even with good sensitivity and specificity, if the prevalence of the negative class is low.\n\n- Prevalence also can vary by testing setting\n\n--------------------------------------------------------------------------------\n\nTests for many genetic disorders have very good sensitivity and specificity but their PPV (and NPV) vary widely by setting/patients tested\n\n- Test for multiple endocrine neoplasia type 2 (MEN2) based on mutations in RET\n  - Sensitivity = 98%\n  - Specificity = 99.9%\n\nMENS2 has prevalence of 1/30,000 in general population.  If using the test in the general population with 3 million people:\n\n- 100 will have the disease\n- 2,999,900 will not have the disease\n- Column accuracies (sensitivity and specificity) are high\n- PPV will be very BAD;  98/(3000 + 98) = 3.2%\n- Though NPV will be very near perfect! 2996900 / (2996900 + 2)\n\n|               | **Ground Truth**|     |\n|:--------------|:--------- |:----------|\n| **Prediction**| Positive  | Negative  | \n| Positive      | 98        |    3000   |\n| Negative      |  2        | 2996900   | \n\n--------------------------------------------------------------------------------\n\nHowever, MENS2 prevalence is high (1/5) among patients who present in a clinic with medullary thyroid carcinoma.  If we only used the test among 3 million of these patients \n\n- 600,000 will have the disease\n- 2,400,000 will NOT have the disease (still unbalanced by but much less)\n- Column accuracies (sensitivity and specificity) remain the same (98% and 99.9%)\n  - These are properties of the test/classifier\n- PPV is now much better; 588,000 / (2400 + 588,000) = 99.6%\n  \n|               | **Ground Truth**|     |\n|:--------------|:--------- |:----------|\n| **Prediction**| Positive  | Negative  | \n| Positive      | 588000    |    2400   |\n| Negative      |  12000    | 2397600   | \n\n--------------------------------------------------------------------------------\n\nNow think about \"accuracy\" of any specific COVID test\n\n- It dismayed me to see talk of accuracy\n  - The cost of the two types of errors is different!\n- Occasionally, there was talk of sensitivity and specificity\n- There was rarely/never discussion of PPV and NPV, which is what matters most when you are given your test result\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: How would the PPV and NPV change when we moved from testing only people with symptoms who presented at the hospital to testing everyone (e.g., all college students)?\n:::\n\n::: {.fragment .uwred}\nRelatively speaking, when testing someone with obvious COVID symptoms PPV would be high but NPV could be low.  Conversely, for our students PPV is likely low but NPV is likely high\n:::\n\n--------------------------------------------------------------------------------\n\nIn some instances, it may be more useful to focus on **precision** and **recall** rather than sensitivity and specificity.  The **F1 measure** is the harmonic mean of precision and recall\n\n- This is a row and column accuracy \n- Recall (sensitivity) focuses on how many true positive cases will we correctly identify\n- Precision (PPV) focuses on how accurate the prediction of \"positive\" will be (prevalence dependent)\n- This keeps the focus on positive cases\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n```{r}\ncm |> \n  summary() |> \n  filter(.metric == \"precision\" | .metric == \"recall\" | .metric == \"f_meas\") |> \n  select(-.estimator)\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: Can you link the numbers in the confusion matrix on the previous slide to Recall (Sensitivity) and Precision (PPV) metrics\n:::\n\n::: {.fragment .uwred}\nRecall/sensitivity is \"accuracy\" for the positive cases (in this instance, patients with heart disease).\n30 / (30 + 17)\n\nPrecision/PPV is \"accuracy\" for the positive predictions (when model predicts yes).  \n30 / (30 + 13)\n:::\n\n--------------------------------------------------------------------------------\n\n$F1$ is the harmonic mean of Recall and Precision\n\n- Harmonic means are used with rates (see [more detail](https://en.wikipedia.org/wiki/Mean) about the Pythagorean means, if interested)\n- $F1$ is an unweighted harmonic mean\n  - $F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$ \n  - or using the more general formula for harmonic means: $F1 = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}$ \n- $F_\\beta$ is a weighted version where $\\beta$ is the relative weighting of recall to precision\n  - Two commonly used values for $\\beta$ are 2, which weighs recall twice as much than precision, and 0.5, which weighs precision twice as much as recall \n  - $F_\\beta = (1 + \\beta^2) * \\frac{Precision * Recall}{(\\beta^2*Precision) + Recall}$\n  \n```{r}\ncm |> \n  summary() |> \n  filter(.metric == \"precision\" | .metric == \"recall\" | .metric == \"f_meas\") |> \n  select(-.estimator)\n```\n\n--------------------------------------------------------------------------------\n\n**Cohen's Kappa** is a bit more complicated to calculate and understand\n\n- There is a great [explanation of kappa](https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english) on stack overflow\n\n- Wikipedia also has a very detailed [definition and explanation](https://en.wikipedia.org/wiki/Cohen's_kappa) as well (though not in the context of machine learning)\n\n- Compares observed accuracy to expected accuracy (random chance)\n\n$Kappa = \\frac{observed\\:accuracy - expected\\:accuracy}{1 - expected\\:accuracy}$\n\n- When outcome is unbalanced, some agreement/accuracy (relationship between model predictions and reference/ground truth) is expected\n\n- Kappa adjusts for this\n\nKappa is essentially the proportional increase in accuracy above the accuracy expected by the base rates of the reference and classifier\n\n--------------------------------------------------------------------------------\n\nTo calculate the expected accuracy, we need to consider the probabilities of reference and classifier prediction both being positive (and both being negative) by chance given the base rates of these classes for the reference and classifier.\n\n\n|               | **Ground Truth**  |   |\n|:--------------|:--------- |:----------|\n| **Prediction**| Positive  | Negative  | \n| Positive      | TP        | FP        |\n| Negative      | FN        | TN        |\n\n\n$P_{positive} = \\frac{FN + TP}{TN + FN + FP + TP} * \\frac{FP + TP}{TN + FN + FP + TP}$\n\n$P_{negative} = \\frac{TN + FP}{TN + FN + FP + TP} * \\frac{TN + FN}{TN + FN + FP + TP}$\n\n$P_{expected} = P_{positive} + P_{negative}$\n\n--------------------------------------------------------------------------------\n\nIn our example\n\n```{r}\ncm\n```\n\n$P_{positive} = \\frac{17 + 30}{428} * \\frac{13 + 30}{428} = 0.01103262$\n\n$P_{negative} = \\frac{368 + 13}{428} * \\frac{368 + 17}{428} = 0.8007522$\n\n$P_{expected} = 0.01103262 + 0.8007522 = 0.8117848$\n\n$Actual Accuracy = 0.9299065$\n\n$Kappa = \\frac{observed\\:accuracy - expected\\:accuracy}{1 - expected\\:accuracy}$\n\n$Kappa = \\frac{0.9299065 - 0.8117848}{1 - 0.8117848} = 0.627588$\n\n```{r}\nsummary(cm) |> \n  filter(.metric == \"kap\") |> \n  pull(.estimate)\n```\n\n--------------------------------------------------------------------------------\n\nKappa Rules of Thumb w/ a big grain of salt.......\n\n- Kappa < 0: No agreement\n- Kappa between 0.00 and 0.20: Slight agreement\n- Kappa between 0.21 and 0.40: Fair agreement\n- Kappa between 0.41 and 0.60: Moderate agreement\n- Kappa between 0.61 and 0.80: Substantial agreement\n- Kappa between 0.81 and 1.00: Almost perfect agreement.\n\nSee @Landis1977 ([PDF](pdfs/landis_1977_kappa.pdf))for more details \n\n--------------------------------------------------------------------------------\n\n## The Receiver Operating Characteristic Curve\n\nLet's return now to consider sensitivity and specificity again\n\nRemember that our classifier is estimating the probability of an observation being in the positive class.\n\nWe dichotomize this probability when we formally make a class prediction\n\n- If the probability > 50%, we classify the observation as positive\n- If the probability <= 50%, we classify the observation as negative\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: How can we improve sensitivity?\n:::\n\n::: {.fragment .uwred}\nWe can use a more liberal/lower classification threshold for saying someone has heart disease.  \n\nFor example, rather than requiring a 50% probability to classify as yes for heart disease, we could lower to 20% for the classification threshold\n:::\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: What will the consequences of this change be?\n:::\n\n::: {.fragment .uwred}\nFirst, the Bayes classifier threshold of 50% produces the highest overall accuracy so accuracy will generally (though not always) drop when you shift from 50%.  \n\nIf we think about this change as it applies to the columns of our confusion matrix, we will now catch more of the yes (fewer false negatives/misses), so sensitivity \nwill go up.  This was the goal of the lower threshold.  However, we will also end up with more false positives so specificity will drop.  If you consider the rows of \nthe confusion matrix, we will have more false positives so the PPV will drop.  \n\nHowever, we will have fewer false negatives so the NPV will increase.  Whether these trade-offs are worth it are a function of the cost of different types of errors \nand how much you gain and lose with regard to each type of performance metric (ROC can inform this; more in a moment)\n:::\n\n```{r}\n#| out-height: 3in\n\nautoplot(cm, type = \"heatmap\")\n```\n\n--------------------------------------------------------------------------------\n\nPreviously, we simply used `predict(fit_glmnet, feat_test)$.pred_class`.  \n\n`$.pred_class` dichotomized at 50% by default\n\n- This is the classification threshold to use with predicted probabilities that will produce the best overall accuracy (e.g., Bayes classifier)\n- However, we can use a different threshold to increase sensitivity or specificity\n- This comes at a cost to the other characteristic (its a trade-off)\n  - Lower threshold increases sensitivity but decreases specificity\n  - Higher threshold increases specificity but decreases sensitivity\n  \n--------------------------------------------------------------------------------\n\nIt is relatively easy to make a new confusion matrix and get new performance metrics with a different classification threshold\n\n- Make a tibble with truth and predicted probabilities\n```{r}\npreds <- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet, feat_test, type = \"prob\")$.pred_yes)\n\npreds\n```\n\n--------------------------------------------------------------------------------\n\n- Use this to get class estimates at any threshold we want\n- Here we threshold at 20%\n```{r}\npreds <- preds |> \n  mutate(estimate_20 = if_else(prob <= .20, \"no\", \"yes\"),\n         estimate_20 = factor(estimate_20, levels = c(\"yes\", \"no\"))) |> \n  print()\n```\n\n--------------------------------------------------------------------------------\n\nWe can now make a confusion matrix for this new set of truth and estimates using the 20% threshold\n\n```{r}\ncm_20 <- preds |> \n  conf_mat(truth = truth, estimate = estimate_20)\n\ncm_20\n```\n\nLet's compare to 50% (original)\n\n```{r}\ncm\n```\n\n--------------------------------------------------------------------------------\n\nAnd let's compare these two thresholds on a subset of our numeric metrics\n\n- 20% threshold\n```{r}\ncm_20 |> \n  summary() |> \n  filter(.metric %in% c(\"accuracy\", \"sens\", \"spec\", \"ppv\", \"npv\")) |> \n  select(-.estimator)\n```\n\n- 50% threshold\n```{r}\ncm |> \n  summary() |> \n  filter(.metric %in% c(\"accuracy\", \"sens\", \"spec\", \"ppv\", \"npv\")) |> \n  select(-.estimator)\n```\n\n\n[Do the changes on each of these metrics make sense to you? If not, please review these previous slides again!]{.uwred}\n\n--------------------------------------------------------------------------------\n\nYou can begin to visualize the classifier performance by threshold simply by plotting histograms of the predicted positive class probabilities, separately for the true positive and negative classes\n\n- Let's look at our classifier\n- Ideally, the probabilities are mostly low for the true negative class (\"no\") and mostly high for the true positive class (\"yes\")\n- You can imagine how any specific probability cut point would affect specificity (apply cut to the  left panel) or sensitivity (apply cut to the right panel)\n\n```{r}\nggplot(data = preds, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question: What do you think about its performance? What insights does this plot generate?\n:::\n\n::: {.fragment .uwred}\n1. You can see that we can likely drop the threshold to somewhere about 25% without decreasing the specificity too much.  This will allow you to detect more positive cases.  \n\n2.  Our model seems to be able to predict negative cases well.  They mostly have low probabilities.   However, you can see its poor performance with positive cases.  They are spread pretty evenly across the full range of probabilities.  We likely do not have enough positive cases in our training data\n:::\n\n--------------------------------------------------------------------------------\n\nThe **Receiver Operating Characteristics (ROC)** curve for a classifier provides a more formal method to visualize the trade-offs between sensitivity and specificity across all possible thresholds for classification.\n\nLets look at this in our example\n\n- We need columns for truth and probabilities of the positive class for each observation\n- We need to specify the positive class\n- Returns tibble with data to plot an ROC curve\n\n```{r}\nroc_plot <- \n  tibble(truth = feat_test$disease,\n         prob = predict(fit_glmnet, feat_test, type = \"prob\")$.pred_yes) |> \n  roc_curve(prob, truth = truth)\n\nroc_plot\n```\n\n--------------------------------------------------------------------------------\n\nWe can `autoplot()` this\n\n```{r}\n#| out-height: 3in\n\nautoplot(roc_plot)\n```\n\n--------------------------------------------------------------------------------\n\nOr we can customize a plot passing the data into` ggplot()`\n\n- Not doing anything fancy here\n- Consider this a shell for you to build on if you want more than `autoplot()` provides\n```{r}\nroc_plot |>\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_equal() +\n  labs(x = \"1 - Specificity (FPR)\",\n       y = \"Sensitivity (TPR)\")\n```\n\n--------------------------------------------------------------------------------\n\nWhen evaluating an ROC curve:\n\n- A random classifier would have a diagonal curve from bottom-left to top-right (the dotted line)\n\n- A perfect classifier would reach up to top-left corner\n  - Sensitivity = 1 (true positive rate)\n  - 1 - Specificity = 0 (false positive rate)\n\n--------------------------------------------------------------------------------\n\nThe ROC Curve is not only a useful method to visualize classier performance across\nthresholds\n\nThe area under the ROC curve (auROC) is an attractive performance metric\n\n- Ranges from 1.0 (perfect) down to approximately 0.5 (random classifier)\n    - If the auROC was consistently less than 0.5, then the predictions could simply be inverted\n    - Values between .70 and .80 are considered fair\n    - Values between .80 and .90 are considered good\n    - Values above .90 are considered excellent\n    - These are very rough, and to my eye, the exact cuts and labels are somewhat arbitrary\n\n--------------------------------------------------------------------------------\n\n- auROC is the probability that the classifier will rank/predict a randomly selected true positive observation higher than a randomly selected true negative observation\n\n- Alternatively, it can be thought of as the average sensitivity across all decision thresholds\n\n- auROC summarizes performance (sensitivity vs. specificity trade-off) across all possible thresholds\n\n- auROC is not affected by class imbalances in contrast to many other metrics\n\n--------------------------------------------------------------------------------\n\nIt is easy to get the auROC for the ROC in tidymodels using `roc_auc()`\n\nAs with calculating the ROC curve, we need \n\n- truth \n- predicted probabilities for the positive class\n- to specify the `event_level` (default is first)\n\n```{r}\ntibble(truth = feat_test$disease,\n       prob = predict(fit_glmnet, feat_test, type = \"prob\")$.pred_yes) |> \n  roc_auc(prob, truth = truth)\n```\n\n--------------------------------------------------------------------------------\n\n## Using Alternative Performance Metrics for Model Selection\n\nYou can select the best model configuration using resampling with performance metrics other than accuracy\n\n- Aggregate measures are typically your best choice (except potentially with high imbalance - more on this later)\n\n  - For classification: \n    - Accuracy\n    - Balanced accuracy\n    - $F1$\n    - Area under ROC Curve (auROC)\n    - Kappa\n    \n  - For regression\n    - RMSE\n    - $R^2$\n    - MAE (mean absolute error)\n\n--------------------------------------------------------------------------------\n\n- You typically need to use a single metric for selection among model configurations\n  - You should generally use the performance metric that is the best aligned with your problem:\n\n- In classification\n  - Do you care about types of errors or just overall error rate\n  - Is the outcome relatively balanced or unbalanced\n  - What metric will be clearest to your audience\n\n- In regression\n  - Do you want to weight big and small errors the same\n  - What metric will be clearest to your audience (though all of these are pretty clear.  There are more complicated regression metrics)\n\n- Although you will use one metric to select the best configuration, you can evaluate/characterize the performance of your final model with as many metrics as you like\n\n--------------------------------------------------------------------------------\n\nYou should recognize the differences between the cost function for the algorithm and the performance metric:\n\n- Cost function is fundamental to the definition of the algorithm\n- Cost function is minimized to determine parameter estimates in parametric models\n- Performance metric is independent of algorithm\n- Performance metric is used to select and evaluate model configurations\n\n- Sometimes they can be the same metric (e.g., RMSE)  \n- BUT, this is not required\n\n--------------------------------------------------------------------------------\n\nWith `tidymodels`, it is easy to select hyperparameters or select among model configurations more generally using one of many different performance metrics\n\n- We will still use either `tune_grid()` or `fit_resamples()`\n- We will simply specify a different performance metric inside of `metric_set()`\n- If we only measure one performance metric, we can use defaults with `show_best()`\n\n--------------------------------------------------------------------------------\n\nHere is an example of measuring `roc_auc()` but you can use any performance function from the yardstick package\n```{r}\nfits_glmnet_auc <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(roc_auc))\n\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_auc\")\n```\n\n--------------------------------------------------------------------------------\n\nAnd now use `show_best()`, with best defined by auROC\n```{r}\nshow_best(fits_glmnet_auc, n = 5, metric = \"roc_auc\")\n```\n\n--------------------------------------------------------------------------------\n\nYou can also measure multiple metrics during resampling but you will need to select the best configuration using only one\n\n- see `metric_set()` for the measurement of multiple metrics,\n- see `show_best()` for the use of `metric` to indicate which metric to use for selection\n\n```{r}\nfits_glmnet_many <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(roc_auc, accuracy, sens, spec, bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_many\")\n```\n\n--------------------------------------------------------------------------------\n\nBut now we need to indicate how to define **best**\n\nWe will define it based on balanced accuracy\n```{r}\nshow_best(fits_glmnet_many, metric = \"bal_accuracy\", n = 5)\n```\n\nAnd here based on auROC (same as before)\n```{r}\nshow_best(fits_glmnet_many, metric = \"roc_auc\", n = 5)\n```\n\n--------------------------------------------------------------------------------\n\nOf course, you can easily calculate any performance metric from `yardstick` [package](https://yardstick.tidymodels.org/reference/index.html) in test to evaluate your final model\n\n- Using `conf_mat()` and `summary()` as above (but not for auROC)\n- Using `*_vec()` functions; for example:\n  - `accuracy_vec()`\n  - `roc_auc_vec()`\n  - `sens_vec()`; `spec_vec()`\n- Piping a tibble that contain truth, and estimate or probability into appropriate function\n  - `accuracy()`\n  - `roc_auc()`\n  - `sens()`; `spec()`\n\n--------------------------------------------------------------------------------\n\nFit best model using auROC\n```{r}\nfit_glmnet_auc <-   \n  logistic_reg(penalty = select_best(fits_glmnet_many, metric = \"roc_auc\")$penalty, \n               mixture = select_best(fits_glmnet_many, metric = \"roc_auc\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn)\n```\n\nand get all metrics\n```{r}\ncm_auc <- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_auc, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\ncm_auc |> \n  summary()\n```\n\n--------------------------------------------------------------------------------\n\nFit best model using balanced accuracy\n```{r}\nfit_glmnet_bal <-   \n  logistic_reg(penalty = select_best(fits_glmnet_many, metric = \"roc_auc\")$penalty, \n               mixture = select_best(fits_glmnet_many, metric = \"roc_auc\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn)\n```\n\nand still get all metrics (different because best model configuration is different)\n```{r}\ncm_bal <- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_bal, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\ncm_bal |> \n  summary()\n```\n\n--------------------------------------------------------------------------------\n\n## Advanced Methods for Class Imbalances\n\nWhen there is a high degree of class imbalance:\n\n- It is often difficult to build models that predict the minority class well\n- This will yield low sensitivity if the positive class is the minority class (as in our example)\n- This will yield low specificity if the negative class is the minority class\n- Each of these issues may be a problem depending on the costs of FP and FN\n\n--------------------------------------------------------------------------------\n\nLet's see this at play again in our model\n\n- Using the 50% threshold we have low sensitivity but good specificity\n```{r}\n#| out-height: 3in\n\nautoplot(cm)\n```\n\n```{r}\ncm |> \n  summary() |> \n  filter(.metric == \"sens\" | .metric == \"spec\") |> \n  select(-.estimator)\n```\n\n--------------------------------------------------------------------------------\n\n- We do much better with probabilities for negative (majority) vs. positive (minority) class\n- We can see that we will not affect specificity much by lowering the threshold for positive classification to 20-25%\n- BUT, we really need to do do better with the distribution of probabilities for observations that are positive (yes)\n```{r}\nggplot(data = preds, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n```\n\n--------------------------------------------------------------------------------\n\nWhat can we do when we have imbalanced outcomes?\n\nWe will consider:\n\n- Changes to the classification/decision threshold that trade-off sensitivity vs. specificity for a fitted model (already discussed)\n- Changes to performance metric for selecting the best model configuration  (already demonstrated)\n- Sampling/Resampling methods that will affect the balance of the outcome in the training data to fit models that are better with the minority class (new)\n\n--------------------------------------------------------------------------------\n\n### Classification (Decision) Threshold\n\nWe have already seen an example of how the classification threshold (the probability at which we split between predicting a case as positive vs. negative) affects sensitivity vs. specificity\n\nDecreasing the threshold (probability) for classifying a case as positive will:\n\n- Increase sensitivity and decrease specificity\n- This will decrease FN but increase FP\n- This may be useful if the positive class is the minority class\n- The ROC curve is a useful display to provide this information about your classifier\n  - Curve can be colored to show the threshold\n- The separate histograms by positive vs. negative can also be useful as well\n- If you want to use your data to select the best threshold, you will need yet another set of data to make this selection\n  - Can't make the selection in training b/c those probabilities are overfit\n  - Can't make the selection of threshold in test and then also use the same test data to evaluate that model!\n\n--------------------------------------------------------------------------------\n\n### Performance Metric Considerations\n\nWhen you are choosing a performance metric for selecting your best model configuration, you should choose a performance metric that it best aligned with the nature of the performance you seek\n\n- If you want just good overall accuracy, accuracy may be a good metric\n- If the outcome is unbalanced, and you care about the types of errors, you might want\n  - Balanced accuracy (average of sensitive and specificity)\n  - Only sensitivity or specificity by itself (recommended by Kuhn)\n  - auROC\n  - An F measure (harmonic mean of sensitivity and PPV)\n  - May need to think carefully about what is most important to you\n\nEarlier, we saw that we got better sensitivity when we used balanced accuracy rather than accuracy to tune our hyperparameters\n\n--------------------------------------------------------------------------------\n\n### Sampling and Resampling to Address Class Imbalance\n\nWe can address issues of class imbalance either a priori or post-hoc with respect to data collection\n\n-  A priori method would be to over-sample to get more of the minority class into your training set\n  - Use targeted recruiting\n  - Can be very costly or impossible in many instances\n  - If possible,  this can be much better than the resampling approach below\n  \n- Post hoc, we can employ a variety of resampling procedures that are designed to make the training data more balanced\n  - We can up-sample the minority class\n  - We can down-sample the majority class\n  - We can synthesize new minority class observations e.g, SMOTE\n  \n- For both a priori sampling or post-hoc resampling strategies, it is important that your test set is not manipulated.  It should represent the expected distribution for the outcome, unaltered\n\n--------------------------------------------------------------------------------\n\n### Up-sampling\n\n- We resample minority class observations with replacement within our training set to increase the number of total observations of the minority class in the training set.\n- This simply duplicates existing minority class observations\n- Our test (or validation) set(s) should NOT be resampled.  This is handled well by `step_upsample()`\n\n--------------------------------------------------------------------------------\n\nLet's apply this in our example\n\n- Up-sampling is part of feature engineering recipe\n- Need to specify the outcome (`disease`)\n- Can set `over_ratio` to values other than 1 if desired\n- Makes sense to do this after missing data imputation and dummy coding\n- Makes sense to do this before normalizing features \n- These steps are in the `themis` package rather than `recipes` (can use namespace or load full library)\n```{r}\nrec_up <- recipe(disease ~ ., data = data_trn) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) |> \n  themis::step_upsample(disease, over_ratio = 1) |> \n  step_normalize(all_numeric_predictors())\n```\n\n--------------------------------------------------------------------------------\n\nNeed to re-tune model b/c the sample size has changed\n\nNeed to refit the final model to all of train\n```{r}\nfits_glmnet_up <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec_up, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fit_glmnet_up\")\n```\n\n--------------------------------------------------------------------------------\n\nReview hyperparameter plot\n```{r}\nplot_hyperparameters(fits_glmnet_up, hp1 = \"penalty\", hp2 = \"mixture\", metric = \"bal_accuracy\", log_hp1 = TRUE)\n```\n\n--------------------------------------------------------------------------------\n\nLet's fit this best model configuration to all of our training data and evaluate it in test\n\n- Note the use of `NULL` for `new_data` \n- `step_upsample()` is only applied to training/held-in but not held-out (truly new) data\n- `bake()` knows to use the training data provided during `prep()` if we specify `NULL`\n- Could have done this all along for baking training.  Its sometimes faster but previously same result.  Now its necessary!\n```{r}\nrec_up_prep <- rec_up |> \n  prep(data_trn)\n\nfeat_trn_up <- rec_up_prep |> \n  bake(new_data = NULL)\n```\n\n--------------------------------------------------------------------------------\n\nNotice that `disease` is now balanced in the training data!\n```{r}\nfeat_trn_up |> \n  skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\nfit_glmnet_up <-   \n  logistic_reg(penalty = select_best(fits_glmnet_up, metric = \"bal_accuracy\")$penalty, \n               mixture = select_best(fits_glmnet_up, metric = \"bal_accuracy\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn_up)\n```\n\n--------------------------------------------------------------------------------\n\nTo evaluate this model, we now need test features too\n\n- IMPORTANT: Test is NOT up-sampled\n- bake it as new data!\n```{r}\nfeat_test <- rec_up_prep |> \n  bake(data_test)  \n\nfeat_test |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\nLet's see how this model performs in test\n```{r}\ncm_up <- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_up, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\ncm_up |> \n  summary()\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\npreds_up <- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet_up, feat_test, type = \"prob\")$.pred_yes)\n\nggplot(data = preds_up, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n```\n\n--------------------------------------------------------------------------------\n\n### Down-sampling\n\nWe resample majority class observations within our training set to decrease/match the number of total observations of the minority class in the training set.\n\n- This selects a subset of the majority class\n- Our test (or validation) set(s) should NOT be resampled.  This is handled well by `step_downsample()`\n\n--------------------------------------------------------------------------------\n\nDown-sampling is part of feature engineering recipe\n\n- Need to specify the outcome (`disease`)\n- Can set `under_ratio` to values other than 1 if desired\n- Makes sense to do this after missing data imputation and dummy coding\n- Makes sense to do this before normalizing features \n\n```{r}\nrec_down <- recipe(disease ~ ., data = data_trn) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) |> \n  themis::step_downsample(disease, under_ratio = 1) |> \n  step_normalize(all_numeric_predictors())\n```\n\n--------------------------------------------------------------------------------\n\nNeed to re-tune model b/c the sample size has changed\n\nNeed to refit the final model to all of train\n```{r}\nfits_glmnet_down <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec_down, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_down\")\n```\n\n--------------------------------------------------------------------------------\n\nReview hyperparameters\n```{r}\nplot_hyperparameters(fits_glmnet_down, hp1 = \"penalty\", hp2 = \"mixture\", \n                     metric = \"bal_accuracy\", log_hp1 = TRUE)\n```\n\n--------------------------------------------------------------------------------\n\nLet's fit this best model configuration to all of our training data and evaluate it in test\n\n- Note the use of `NULL` again when getting features for training data. Very important!\n- `step_downsample()` is not applied to baked data (See its default for `skip = TRUE` argument)\n- NOTE: sample size and ratio of yes/now for disease!\n```{r}\nrec_down_prep <- rec_down |> \n  prep(data_trn)\n\nfeat_trn_down <- rec_down_prep |> \n  bake(new_data = NULL)\n\nfeat_trn_down |> skim_some()\n```\n\n--------------------------------------------------------------------------------\n\nNow fit the model to these downsampled training data\n```{r}\nfit_glmnet_down <-   \n  logistic_reg(penalty = select_best(fits_glmnet_down, metric = \"bal_accuracy\")$penalty, \n               mixture = select_best(fits_glmnet_down, metric = \"bal_accuracy\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn_down)\n```\n\n--------------------------------------------------------------------------------\n\nLet's see how this model performs in test\n\n- First we need test features\n- IMPORTANT: Test is NOT down-sampled\n\n```{r}\nfeat_test <- rec_down_prep |> \n  bake(data_test)\n\nfeat_test |> skim_some()\n```\n\n--------------------------------------------------------------------------------\n\nGet metrics in test\n```{r}\ncm_down <- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_down, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\ncm_down |> \n  summary()\n```\n\n--------------------------------------------------------------------------------\n\nAnd plot faceted probabilites\n```{r u8-bal-15}\npreds_down <- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet_down, feat_test, type = \"prob\")$.pred_yes)\n\nggplot(data = preds_down, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n```\n\n--------------------------------------------------------------------------------\n\n### SMOTE\n\nA third approach to resampling is called the synthetic minority over-sampling technique (SMOTE)\n\nTo up-sample the minority class, SMOTE synthesizes new observations.  \n\n- To do this, an observation is randomly selected from the minority class.  \n- This observation's K-nearest neighbors (KNNs) are then determined. \n- The new synthetic observation retains the outcome but a random combination of the predictors values from the randomly selected observation and its neighbors.  \n\n--------------------------------------------------------------------------------\n\nThis is easily implemented by recipe in `tidymodels` using `step_smote()`\n\n- Need to specify the outcome (`disease`)\n- Can set `over_ratio` to values other than 1 (default) if desired\n- Can set `neighbors` to values other than 5 (default) if desired\n- Makes sense to do this after missing data imputation and dummy coding\n- Other features will need to be scaled/range-corrected prior to use (for distance)\n- Makes sense to do this before normalizing features for glmnet, etc \n\n```{r u8-bal-16}\nrec_smote <- recipe(disease ~ ., data = data_trn) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) |>\n  step_range(all_predictors()) |> \n  themis::step_smote(disease, over_ratio = 1, neighbors = 5) |> \n  step_normalize(all_numeric_predictors())\n```\n\n--------------------------------------------------------------------------------\n\nNeed to re-tune model b/c the sample size has changed\n\nNeed to refit the final model to all of train\n```{r}\nfits_glmnet_smote <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), \n                 mixture = tune()) |> \n      set_engine(\"glmnet\") |> \n      set_mode(\"classification\") |> \n      tune_grid(preprocessor = rec_smote, \n                resamples = splits_boot, grid = grid_glmnet, \n                metrics = metric_set(bal_accuracy))\n  },\n  rerun = rerun_setting,\n  dir = \"cache/008/\",\n  file = \"fits_glmnet_smote\")\n```\n\n--------------------------------------------------------------------------------\n\nReview hyperparameters\n```{r}\nplot_hyperparameters(fits_glmnet_smote, hp1 = \"penalty\", hp2 = \"mixture\", \n                     metric = \"bal_accuracy\", log_hp1 = TRUE)\n```\n\n--------------------------------------------------------------------------------\n\nLet's fit this best model configuration to all of our training data and evaluate it in test\n\n- Note the use of `NULL` (once again!) for `new_data`\n- `step_smote()` is not applied to new data\n- NOTE: sample size and outcome balance\n```{r}\nrec_smote_prep <- rec_smote |> \n  prep(data_trn)\n\nfeat_trn_smote <- rec_smote_prep |> \n  bake(NULL)\n\nfeat_trn_smote |> skim_some()\n```\n\n--------------------------------------------------------------------------------\n\nFit model to smote sampled training data\n```{r}\nfit_glmnet_smote <-   \n  logistic_reg(penalty = select_best(fits_glmnet_smote, metric = \"bal_accuracy\")$penalty, \n               mixture = select_best(fits_glmnet_smote, metric = \"bal_accuracy\")$mixture) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"classification\") |> \n  fit(disease ~ ., data = feat_trn_smote)\n```\n\n--------------------------------------------------------------------------------\n\nLet's see how this model performs in test\n\n- IMPORTANT: Test is NOT SMOTE up-sampled\n```{r}\nfeat_test <- rec_smote_prep |> \n  bake(data_test)\n\ncm_smote <- \n  tibble(truth = feat_test$disease,\n         estimate = predict(fit_glmnet_smote, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\ncm_smote |> \n  summary()\n```\n\n--------------------------------------------------------------------------------\n\nPlot faceted predicted probabilites\n```{r}\npreds_smote <- tibble(truth = feat_test$disease,\n                prob = predict(fit_glmnet_smote, feat_test, type = \"prob\")$.pred_yes)\n\nggplot(data = preds_smote, aes(x = prob)) + \n   geom_histogram(bins = 15) +\n   facet_wrap(vars(truth), nrow = 2) +\n   xlab(\"Pr(Disease)\")\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":"html_document","warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["book.css"],"output-file":"l08_advanced_performance_metrics.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","bibliography":["refs.bib"],"callout-icon":false,"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}