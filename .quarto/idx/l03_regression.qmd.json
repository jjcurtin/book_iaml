{"title":"Introduction to Regression Models","markdown":{"yaml":{"editor_options":{"chunk_output_type":"console"}},"headingText":"Introduction to Regression Models","containsRefs":false,"markdown":"\n\n::: {.content-visible unless-format=\"revealjs\"}\n:::\n::: {.content-visible when-format=\"revealjs\"}\n# IAML Unit 3: Introduction to Regression Models\n:::\n\n## Overview of Unit\n\n### Learning Objectives\n  \n- Use of root mean square error (RMSE) in training and validation sets for model performance evaluation\n\n- The General Linear Model as a machine learning model  \n  - Extensions to categorical variables (Dummy coding features)\n  - Extensions to interactive and non-linear effects of features\n  \n- K Nearest Neighbor (KNN)\n  - Hyperparameter $k$\n  - Scaling predictors\n  - Extensions to categorical variables\n\n--------------------------------------------------------------------------------\n\nOur goal in this unit is to build a machine learning regression model that can accurately (we hope) predict the `sale_price` for future sales of houses (in Iowa? more generally?)\n\nTo begin this project we need to:\n\n- Load the packages we will need.  I am only loading `tidymodels` and `tidyverse` because the other functions we need are only called occasionally (so we will call them by namespace.)\n- Set up conflicts policies (just in case we decide to load other packages later)\n- We will hide this in future units\n```{r}\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\n\noptions(conflicts.policy = \"depends.ok\")\n```\n\n--------------------------------------------------------------------------------\n\n- source additional class functions libraries\n- We will hide this in future units\n- You might consider copying (some) of these functions to your own function library rather than using mine\n```{r}\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n```\n\n- set display options\n- We will hide this in future units\n```{r}\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\n```\n\n- handle paths\n```{r}\npath_data <- \"./data\"\n``` \n\n--------------------------------------------------------------------------------\n\n- Set up function to class ames data (copied with one improvement from last unit)\n```{r}\nclass_ames <- function(df){\n  df |>\n    mutate(across(where(is.character), factor)) |> \n    mutate(overall_qual = factor(overall_qual, levels = 1:10), \n           garage_qual = suppressWarnings(fct_relevel(garage_qual, # <1> \n                                                      c(\"no_garage\", \"po\", \"fa\", \n                                                    \"ta\", \"gd\", \"ex\"))))\n}\n```\n1. Warnings should be considered errors until investigated.  Once investigated, they can be ignored.  To explicitly ignore, use `suppressWarnings()`\n\n--------------------------------------------------------------------------------\n\n- Open the cleaned training set\n\n```{r}\ndata_trn <- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           col_types = cols()) |>  \n  class_ames() |> \n  glimpse()\n```\n\n-----\n\n- Open the cleaned validation set\n\n```{r}\ndata_val <- read_csv(here::here(path_data, \"ames_clean_class_val.csv\"),\n                     show_col_types = FALSE) |> \n  class_ames() |> \n  glimpse()\n```\n\n**NOTE**: Remember, I have held back an additional test set that we will use only once to evaluate the final model that we each develop in this unit.\n\n-----\n\nWe will also make a dataframe to track validation error across the models we fit\n\n```{r}\nerror_val <- tibble(model = character(), rmse_val = numeric()) |> \n  glimpse()\n```\n\n-----\n\nWe will fit regression models with various model configurations.\n\nThese configurations will differ with respect to statistical algorithm:\n\n- A General Linear Model (lm) - a parametric approach\n- K Nearest Neighbor (KNN) - a non-parametric approach\n\nThese configurations will differ with respect to the features\n\n- Single feature (i.e., simple regression)\n- Various sets of multiple features that vary by:\n  - Raw predictors used\n  - Transformations applied to those predictors as part of feature engineering\n  - Inclusion (or not) of interactions among features\n\n- The KNN model configurations will also differ with respect to its hyperparameter- $k$\n\n-----\n\nTo build models that will work well in new data (e.g., the data that I have held back from you so far):\n\n- We have split the remaining data into a training and validation set for our own use during model building\n\n- We will fit models in train\n\n- We will evaluate them in validation\n\n\nRemember that we:\n\n- Used a 75/25 stratified (on `sale_price`) split of the data at the end of cleaning EDA to create training and validation sets\n- Are only using a subset of the available predictors.  The same ones I used for the EDA unit\n\nYou will work with all of my predictors and all the predictors you used for your EDA when you do the application assignment for this unit\n\n--------------------------------------------------------------------------------\n-----\n\nPause for a moment to answer this question: \n\n::: {.callout-important}\n# Question \nWhy do we need independent validation data to select the best model configuration?  In other words, why cant we just fit and evaluate all of the models in our one training set?\n:::\n\n::: {.fragment .uwred}\nThese models will all overfit the dataset within which they are fit to some degree.   \n\nIn other words, they will predict both systematic variance (the DGP) and some noise in the training set.  However, they will differ in how much they overfit the training set.   \n\nAs the models get more flexible they will have the potential to overfit to a greater degree.Models with a larger number of features (e.g., more predictors, features based on interactions as well as raw predictors) will overfit to a greater degree.  All other things equal, the non-parametric KNN will also be more flexible than the general linear model so it may overfit to a greater degree as well if the true DGP is linear on the features.  \n\nTherefore, just because a model fits the training set well does not mean it will work well in new data because the noise will be different in every new dataset. This overfitting will be removed from our performance estimate if we calculate it with new data (the validation set).\n:::\n\n\n--------------------------------------------------------------------------------\n\nLet's take a quick look at the available raw predictors in the training set\n\n```{r}\ndata_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\nRemember from our modeling EDA that we have some issues to address as part of our feature engineering:\n\n- Missing values\n- Possible transformation of `sale_price`\n- Possible transformation of other numeric predictors\n- We will need to use some feature engineering techniques to handle categorical variables\n- We may need to consider interactions among features\n\nAll of this will be accomplished with a recipe\n\nBut first, let's discuss/review our first statistical algorithm\n\n--------------------------------------------------------------------------------\n\n## The Simple (General) Linear Model (LM)\n\nWe will start with only a quick review of the use of the simple (one feature) linear model (LM) as a machine learning model because you should be very familiar with this statistical model at this point\n\n- $Y = \\beta_0 + \\beta_1*X_1 + \\epsilon$\n\nApplied to our regression problem, we might fit a model such as:\n\n- $sale\\_price = \\beta_0 + \\beta_1*gr\\_liv\\_area + \\epsilon$\n\nThe (general) linear model is a parametric model.  We need to estimate two parameters using our training set\n\n- $\\beta_0$ \n- $\\beta_1$\n\nYou already know how to do this using `lm()` in base R.  However, we will use the `tidymodels` modeling approach.\n\n--------------------------------------------------------------------------------\n\nWe use `tidymodels` because:\n\n- It provides a consistent interface to many (and growing numbers) of statistical algorithms\n- It provides very strong and easy feature engineering routines (e.g., missing data, scaling, transformations, near-zero variance, collinearity) via recipes\n- It simplifies model performance evaluation using resampling approaches (that you don't know yet!)\n- It supports numerous performance metrics\n- It is tightly integrated within the tidyverse\n- It is under active development and support\n- You can see documentation for all of the packages at the [tidymodels website](https://www.tidymodels.org/).  It is worth a quick review now to get a sense of what is available\n\n--------------------------------------------------------------------------------\n\nTo fit a model with a specific configuration, we need to: \n\n- Set up a feature engineering recipe\n- Use the recipe to make a feature matrix\n  - `prep()` it with training data\n  - `bake()` it with data you want to use to calculate feature matrix\n- Select and define the statistical algorithm\n- Fit the algorithm in the feature matrix in our training set\n\nThese steps are accomplished with functions from the [recipes](https://recipes.tidymodels.org/) and [parsnip](https://parsnip.tidymodels.org/reference/index.html) packages.\n\n--------------------------------------------------------------------------------\n\nWe will start with a simple model configuration\n\n- General linear model (lm)\n- One feature (raw `gr_liv_area`)\n- Fit in training data\n\n--------------------------------------------------------------------------------\n\nSet up a VERY SIMPLE feature engineering recipe\n\n- Include outcome on the left size of `~`\n- Include raw predictors (not yet features) on the right side of `~`.  \n- Indicate the training data\n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area, data = data_trn) \n```\n\n\n- We can see a summary of it to verify it is doing what you expect by calling\n  - `rec`\n  - `summary(rec)`\n  \n--------------------------------------------------------------------------------\n\nWe can then prep the recipe and bake the data to make our feature matrix from the training dataset\n\n- Again, remember we always prep a recipe with training data but use the prepped recipe to bake any data\n- In this instance we will prep with `data_trn` and then bake `data_trn` so that we have features from our training set to train/fit the model.\n\n```{r}\nrec_prep <- rec |> \n  prep(training = data_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- And now  we can bake the training data to make a feature matrix\n- Training features were already saved in the prepped recipe so we set `new_data = NULL`\n\n```{r}\nfeat_trn <- rec_prep |> \n  bake(new_data = NULL)\n```\n\n--------------------------------------------------------------------------------\n\nYou should always review the feature matrix to make sure it looks as you expect\n\n- includes outcome (`sale_price`)\n- includes expected feature (`gr_liv_area`)\n- Sample size is as expected\n- No missing data\n\n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\nNow let's consider the statistical algorithm\n\n- `tidymodels` breaks this apart into two pieces for clarity\n- First, you specify the broad category of algorithm\n  - e.g., `linear_reg()`, `nearest_neighbor()`, `logistic_reg()`\n- Next, you `set_mode()` to indicate if if the model is for regression or classification broadly\n  - Not needed if the engine can only be used for one mode (e.g., `'linear_reg()` is only for regression.\n- Then you select a function from a specific R package (or base R) that will implement the algorithm\n  - `tidymodels` calls this setting the engine\n  - e.g., `lm`, `kknn`, `glm`, `glmnet`\n\n--------------------------------------------------------------------------------\n\nYou can see the available engines (and modes: regression vs. classification) for the broad classes of algorithms\n\nWe will work with many of these algorithms later in the course\n```{r}\nshow_engines(\"linear_reg\")\nshow_engines(\"nearest_neighbor\")\nshow_engines(\"logistic_reg\")\nshow_engines(\"decision_tree\")\nshow_engines(\"rand_forest\")\nshow_engines(\"mlp\")\n```\n\n--------------------------------------------------------------------------------\n\nYou can load even more engines from the `discrim` package.  We will use some of these later too.  You need to load the package to use these engines.\n\n```{r}\nlibrary(discrim, exclude = \"smoothness\") # needed for these engines\n\nshow_engines(\"discrim_linear\") \nshow_engines(\"discrim_regularized\") \nshow_engines(\"naive_Bayes\")\n```\n\n--------------------------------------------------------------------------------\n\nYou can also better understand how the engine will be called using `translate()`\n\nNot useful here but will be with more complicated algorithms\n\n```{r}\nlinear_reg() |> \n  set_engine(\"lm\") |> \n  translate()\n```\n\n--------------------------------------------------------------------------------\n\nLet's combine our feature matrix with an algorithm to fit a model in our training set using only raw `gr_liv_area` as a feature\n\nNote the specification of\n\n- The category of algorithm\n- The engine (no need to set mode of engine b/c `lm` are only for the regression mode)\n- The use of the `.` to indicate all features in the matrix.\n  - not that useful here because there is only one feature: `gr_liv_area`\n  - will be useful when we have **many** features in the matrix\n- We use the the feature matrix (rather than raw data) from the training set to fit the model.\n\n```{r}\nfit_lm_1 <- \n  linear_reg() |> # <1> \n  set_engine(\"lm\") |> # <2> \n  fit(sale_price ~ ., data = feat_trn) # <3>\n```\n1. category of algorithm\n2. engine\n3. use of `.` for all features and use of feature matrix from training set\n\n--------------------------------------------------------------------------------\n\nWe can get the parameter estimates, standard errors, and statistical tests for each $\\beta$ = 0 for this model using `tidy()` from the `broom` package (loaded as part of the `tidyverse`)\n\n```{r}\nfit_lm_1 |>  tidy()\n```\n\n--------------------------------------------------------------------------------\n\nThere are a variety of ways to pull out the estimates for each feature (and intercept)\n\nOption 1: Pull all estimates from the tidy object\n```{r}\nfit_lm_1 |> \n  tidy() |> \n  pull(estimate)\n```\n\n--------------------------------------------------------------------------------\n\nOption 2: Extract a single estimate using $ and row number.  Be careful that order of features won't change!  This assumes the feature coefficient for the relevant feature is **always** the second coefficient.\n```{r}\ntidy(fit_lm_1)$estimate[[2]]\n```\n\n--------------------------------------------------------------------------------\n\nOption 3: Filter tidy df to the relevant row (using term ==) and pull the estimate. Safer!\n```{r}\nfit_lm_1 |> \n  tidy() |> \n  filter(term == \"gr_liv_area\") |> \n  pull(estimate)\n```\n\n--------------------------------------------------------------------------------\n\nOption 4: Write a function if we plan to do this a lot.  We include this function in the `fun_ml.R` script in our repo.   Better still (safe and code efficient)!\n```{r}\n#| eval: false\n\nget_estimate <- function(the_fit, the_term){\n  the_fit |> \n    tidy() |> \n    filter(term == the_term) |> \n    pull(estimate)\n}\n```\n\nand then use this function\n```{r}\nget_estimate(fit_lm_1, \"gr_liv_area\")\nget_estimate(fit_lm_1, \"(Intercept)\")\n```\n\n--------------------------------------------------------------------------------\n\nRegardless of the method, we now have a simple parametric model for `sale_price`\n```{r}\n#| include: false\n\nb0 <- round(get_estimate(fit_lm_1, \"(Intercept)\"), 1)\nb1 <- round(get_estimate(fit_lm_1, \"gr_liv_area\"), 1)\n```\n\n$\\hat{sale\\_price} = `r b0` + `r b1` * gr\\_liv\\_area$\n\n--------------------------------------------------------------------------------\n\nWe can get the predicted values for `sale_price` (i.e., $\\hat{sale\\_price}$) in our validation set using `predict()`\n\nHowever, we first need to make a feature matrix for our validation set\n\n- We use the same recipe that we previously prepped with training data (`data_trn`)\n- But now we **bake** new data that were not used to train the recipe - the validation data, `data_val`\n\n```{r}\nfeat_val <- rec_prep |> \n  bake(new_data = data_val)\n```\n\n--------------------------------------------------------------------------------\n\nAs always, we should skim these new features\n\n- Sample size matches what we expect for validation set\n- No missing data\n- Includes expected outcome and features\n\n```{r}\nfeat_val |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\nNow we can get predictions using our model with validation features\n\n`predict()` returns a dataframe with one column named `.pred` and one row for every observation in dataframe (e.g., validation feature set)\n\n```{r}\npredict(fit_lm_1, feat_val)\n```\n\n--------------------------------------------------------------------------------\n\nWe can visualize how well this model performs in the validation set by plotting predicted `sale_price` ($\\hat{sale\\_price}$) vs. `sale_price` (**ground truth** in machine learning terminology) for these data\n\nWe might do this a lot so let's write a function.  We have also included this function in `fun_ml.R`\n\n```{r}\n#| eval: false\n\nplot_truth <- function(truth, estimate) {\n  ggplot(mapping = aes(x = truth, y = estimate)) + \n    geom_abline(lty = 2, color = \"red\") + \n    geom_point(alpha = 0.5) + \n    labs(y = \"predicted outcome\", x = \"outcome\") +\n    coord_obs_pred()   # scale axes uniformly (from tune package)\n}\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_1, feat_val)$.pred)\n```\n\nPerfect performance would have all the points right on the dotted line (same value for actual and predicted outcome)\n\n- Our model doesn't do that well yet.  Not surprising\n- Pattern also has some indication of fanning of residuals AND some non-linearity with higher outcome scores that suggests need for a power transformation of outcome (e.g., log)\n- This is consistent with our earlier modeling EDA\n- Perhaps not that bad here b/c both `sale_price` and `gr_liv_area` were positively skewed\n- We will need consider this eventually\n\n--------------------------------------------------------------------------------\n\nWe can quantify model performance by selecting a performance metric\n\n- The `yardstick` package within the `tidymodels` framework supports calculation of many performance metrics for regression and classification models\n- See the [list](https://yardstick.tidymodels.org/reference/index.html) of all currently available metrics\n\nRoot mean square error (RMSE) is a common performance metric for regression models\n\n- You focused on a related metric, sum of squared error (SSE), in PSY 610/710\n- RMSE simply divides SSE by N (to get mean squared error; MSE) and then takes the square root to return the metric to the original units for the outcome variable\n- It is easy to calculate using `rmse_vec()` from the `yardstick` package\n\n```{r}\nrmse_vec(truth = feat_val$sale_price, \n         estimate = predict(fit_lm_1, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\nLet's record how well this model performed in validation so we can compare it to subsequent models\n\n\n```{r}\nerror_val <- bind_rows(error_val, \n                       tibble(model = \"simple linear model\", \n                              rmse_val = rmse_vec(truth = feat_val$sale_price, \n                                                  estimate = predict(fit_lm_1,\n                                                                     feat_val)$.pred)))\nerror_val\n```\n\nNOTE: I will continue to bind RMSE to this dataframe for newer models but plan to hide this code chuck to avoid distractions.  You can reuse this code repeatedly to track your own models if you like.  (Perhaps we should write a function??)\n\n--------------------------------------------------------------------------------\n\nFor explanatory purposes, we might want to visualize the relationship between a feature and the outcome (in addition to examining the parameter estimates and the associated statistical tests)\n\n- Here is a plot of $\\hat{sale\\_price}$ by `gr_liv_area` superimposed over a scatterplot of the raw data from the validation set\n\n```{r}\nfeat_val |> \n  ggplot(aes(x = gr_liv_area)) +\n    geom_point(aes(y = sale_price), color = \"gray\") +\n    geom_line(aes(y = predict(fit_lm_1, data_val)$.pred), \n              linewidth = 1.25, color = \"blue\") + \n    ggtitle(\"Validation Set\")\n```\n\n- As expected, there is a moderately strong positive relationship between `gr_liv_area` and `sale_price`.  \n- We can also again see the heteroscadasticity in the errors that might be corrected by a power transformation of `sale_price` (or `gr_liv_area`)  \n\n--------------------------------------------------------------------------------\n\n## Extension of LM to Multiple Predictors\n\nWe can improve model performance by moving from simple linear model to a linear model with multiple features derived from multiple predictors\n\nWe have many other numeric variables available to use, even in this pared down version of the dataset.\n\n```{r}\ndata_trn |>  names()\n```\n\nLet's expand our model to also include `lot_area`, `year_built`, and `garage_cars`\n\nAgain, we need:\n\n- A feature engineering recipe\n- Training (and eventually validation) feature matrices\n- An algorithm to fit in training feature matrix\n\n--------------------------------------------------------------------------------\n\nWith the addition of new predictors, we now have a feature engineering task\n\n- We have missing data on `garage_cars` in the training set\n- We need to decide how we will handle it\n\nA simple solution is to do median imputation - substitute the median of the non-missing scores for any missing score.\n\n- This is fast and easy to understand\n- It works OK (but there are certainly better options that we will consider later in the course)\n  - see other options in [Step Functions - Imputation](https://recipes.tidymodels.org/reference/index.html) section on `tidymodels` website\n- There is only one missing value so it likely doesn't matter much anyway\n\n--------------------------------------------------------------------------------\n\nLet's add this to our recipe. All of the defaults are appropriate but you should see `?step_impute_median()` to review them\n\n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars, # <1>\n         data = data_trn) |> \n  step_impute_median(garage_cars)\n```\n1. Notice we now list four predictors for our recipe using `+` between them\n\n--------------------------------------------------------------------------------\n\nNow we need to \n\n- First prep recipe\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)  # <1>\n```\n1. I am going to stop using `training =` at this point.  Remember, we prep recipes with training data.\n\n--------------------------------------------------------------------------------\n\n- Next, bake the training data with prepped recipe to get training features\n\n```{r}\nfeat_trn <- rec_prep |> \n  bake(NULL) # <1>\n```\n1. I am going to stop using `new_data = ` but remember, we use NULL if we want the previously saved training features.  If instead, we want features for new data, we provide that dataset.\n\n--------------------------------------------------------------------------------\n\n- And take a quick look at the features\n  - Sample size is correct\n  - 4 features and the outcome variable\n  - All features are numeric\n  - No missing data for `garage_qual`\n  \n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- And finally, bake the validation data with the same prepped recipe to get validation features\n```{r}\nfeat_val <- rec_prep |> \n  bake(data_val) # <1>\n```\n1. Notice that here we are now baking `data_val`\n\n--------------------------------------------------------------------------------\n\n- And take a quick look \n  - Correct sample size (N = 490)\n  - 4 features and outcome\n  - All numeric\n  - No missing data\n  \n```{r}\nfeat_val |> skim_all()\n```\n\n-------------------------------------------------------------------------------- \n\nNow let's combine our algorithm and training features to fit this model configuration with 4 features\n```{r}\nfit_lm_4 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~ ., data = feat_trn) # <1>\n```\n1. the `.` is a bit more useful now\n\n--------------------------------------------------------------------------------\n\nThis yields these parameter estimates (which as we know from 610/710 were selected to minimize SSE in the training set):\n\n```{r}\nfit_lm_4 |> tidy()\n```\n\n--------------------------------------------------------------------------------\n\nHere is our parametric model\n```{r}\n#| include: false\nb0 <- round(get_estimate(fit_lm_4, \"(Intercept)\"), 1)\nb1 <- round(get_estimate(fit_lm_4, \"gr_liv_area\"), 1)\nb2 <- round(get_estimate(fit_lm_4, \"lot_area\"), 1)\nb3 <- round(get_estimate(fit_lm_4, \"year_built\"), 1)\nb4 <- round(get_estimate(fit_lm_4, \"garage_cars\"), 1)\n```\n\n- $\\hat{sale\\_price} = `r b0` + `r  b1` * gr\\_liv\\_area + `r  b2` * lot\\_area + `r  b3` * year\\_built + `r  b4` * garage\\_cars$\n\nCompared with our previous simple regression model:\n```{r}\n#| include: false\nb0 <- round(get_estimate(fit_lm_1, \"(Intercept)\"), 1)\nb1 <- round(get_estimate(fit_lm_1, \"gr_liv_area\"), 1)\n```\n\n- $\\hat{sale\\_price} = `r b0` + `r b1` * gr\\_liv\\_area$\n\n--------------------------------------------------------------------------------\n\nOf course, these four features are correlated both with `sale_price` but also with each other\n\nLet's look at correlations in the training set.  \n\n```{r}\nfeat_trn |> \n  cor() |> \n  corrplot::corrplot.mixed()\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\nWhat are the implications of the correlations among many of these predictors?\n:::\n\n::: {.fragment .uwred}\nThe multiple regression model coefficients represent unique effects, controlling for all other variables in the model.  You can see how the unique effect of `gr_liv_area` is smaller than its overall effect from the simple regression.   This also means that the overall predictive strength of the model will not be a sum of the effects of each predictor considered in isolation - it will likely be less.  \n\nAlso, if the correlations are high, problems with multicollinearity will emerge.  This will yield large standard errors which means that the models will start to have more variance when fit in different training datasets!  We will soon learn about other regularized versions of the GLM that do not have these issues with correlated predictors.\n:::\n\n--------------------------------------------------------------------------------\n\nHow well does this more complex model perform in validation?   Let's compare the previous and current visualizations of $sale\\_price$ vs. $\\hat{sale\\_price}$\n\n- Looks like the errors are smaller (closer to the diagonal line that would represent prefect prediction)\n- Clear signs of non-linearity are now present as well.  Time for more Modeling EDA!!\n\n```{r}\nplot_1 <- plot_truth(truth = feat_val$sale_price, \n                     estimate = predict(fit_lm_1, feat_val)$.pred)\n\nplot_4 <- plot_truth(truth = feat_val$sale_price, \n                     estimate = predict(fit_lm_4, feat_val)$.pred)\n\ncowplot::plot_grid(plot_1, plot_4, \n          labels = list(\"1 feature\", \"4 features\"), hjust = -1.5)\n```\n\n::: {.callout-tip}\n# Coding sidebar\nNotice the use of `plot_grid()` from the `cowplot` package to make side by side plots.   This also required returning the individual plots as objects (just assign to a object name, e.g., plot_1)\n:::\n\n--------------------------------------------------------------------------------\n\nLet's compare model performance for the two models using RMSE in the validation set\n\n- The one feature simple linear model\n```{r}\nrmse_vec(feat_val$sale_price, \n         predict(fit_lm_1, feat_val)$.pred)\n```\n\n- The four feature linear model.  A clear improvement!\n```{r}\nrmse_vec(feat_val$sale_price, \n         predict(fit_lm_4, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- Let's bind the new performance metric to our results table\n```{r}\n#| echo: false\n\nerror_val <- \n  bind_rows(error_val, \n            tibble(model = \"4 feature linear model\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_lm_4, feat_val)$.pred)))\n\nerror_val\n```\n\n-------------------------------------------------------------------------------- \n\nGiven the non-linearity suggested by the truth vs. estimate plots, we might wonder if we could improve the fit if we transformed our features to be closer to normal\n\n- There are a number of recipe functions that do transformations (see [Step Functions - Individual Transformations](https://recipes.tidymodels.org/reference/index.html))\n\n- We will apply `step_YeoJohnson()`, which is similar to a Box-Cox transformation but can be more broadly applied because the scores don't need to be strictly positive\n\n--------------------------------------------------------------------------------\n\nLet's do it all again, now with transformed features!\n\n- Define the feature engineering recipe\n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars, \n         data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_YeoJohnson(lot_area, gr_liv_area, year_built, garage_cars)\n```\n\n--------------------------------------------------------------------------------\n\n- Prep the recipe with training set\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n```\n\n- Use prepped recipe to bake the training set into features\n\n- Notice the features are now less skewed (but `sale_price` is still skewed)\n```{r}\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Use same prepped recipe to bake the validation set into features\n\n- Again, features are less skewed\n```{r}\nfeat_val <- rec_prep |> \n  bake(data_val)\n\nfeat_val |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Fit model\n```{r}\nfit_lm_4yj <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- View truth vs. estimate plot\n```{r}\nplot_truth(truth = feat_val$sale_price,\n           estimate = predict(fit_lm_4yj, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- and look at the error\n```{r}\n#| echo: false\n\nerror_val <- bind_rows(error_val, \n                        tibble(model = \"4 feature linear model with YJ\", \n                               rmse_val = rmse_vec(feat_val$sale_price, \n                                                   predict(fit_lm_4yj,\n                                                           feat_val)$.pred)))\n\nerror_val\n```\n\n- That didn't help at all.   Error still high and still non-linearity in plot.\n\n--------------------------------------------------------------------------------\n\nWe may need to consider \n\n- a transformation of `sale_price` (We will leave that to you for the application assignment if you are daring!)\n- or a different algorithm that can handle non-linear relationships better\n\n--------------------------------------------------------------------------------\n\n## Extension to Categorical Predictors\n\nMany important predictors in our models may be categorical (nominal and some ordinal predictors)\n\n- Some statistical algorithms (e.g., random forest) can accept even nominal predictors as features without any further feature engineering\n- But many cannot.  Linear models cannot.\n- The type of feature engineering may differ for nominal vs. ordinal categorical predictors\n- For **nominal categorical predictors**:\n  - We need to learn a common approach to transform them to numeric features - dummy coding.  We will learn the concept in general AND how to accomplish within a feature engineering recipe.\n- For **ordinal predictors**:\n  - We can treat them like numeric predictors\n  - We can treat them like nominal categorical predictors\n- See article on [Categorical Predictors](https://recipes.tidymodels.org/articles/Dummies.html) on the `tidymodels` website for more details\n\n--------------------------------------------------------------------------------\n\n### Dummy Coding\n\nFor many algorithms, we will need to use feature engineering to convert a categorical predictor to numeric features.  One common technique is to use dummy coding.  When dummy coding a predictor, we transform the original categorical predictor with m levels into m-1 dummy coded features.\n\nTo better understand how and why we do this, lets consider a version of `ms_zoning` in the Ames dataset.   \n\n```{r}\ndata_trn |> \n  pull(ms_zoning) |> \n  table()\n```\n\nWe will recode `ms_zoning` to have only 3 levels to make our example simple (though dummy codes can be used for predictors with any number of levels)\n\n```{r}\ndata_dummy <- data_trn |> \n  select(sale_price, ms_zoning)  |> # <1>\n  mutate(ms_zoning3 = fct_collapse(ms_zoning, # <2>\n                                 \"residential\" = c(\"res_high\", \"res_med\",\n                                                   \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\")) |> # <3>\n  select(-ms_zoning) # <4>\n```\n1. Make a df (dataframe) with only `sale_price` and `ms_zoning`\n2. `fct_collapse()` from the `forcats` package is our preferred way to collapse levels of a factor.  See `fct_recode()` for more generic recoding of levels.\n3. We could have left this line out and float would have stayed as a level named float\n4. Remove original `ms_zoning` predictor\n\n--------------------------------------------------------------------------------\n\nTake a look at the new predictor\n```{r}\ndata_dummy |> \n  pull(ms_zoning3) |> \n  table() \n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\nWhy can't we simply recode each level with a different consecutive value (e.g., commercial = 1, floating =2 , residential = 3)?\n:::\n\n::: {.fragment .uwred}\nThere is no meaningful way to order the numbers that we assign to the levels of  this unordered categorical predictor. The shape and strength of the relationship between it and sale_price will completely change based on arbitrary ordering of the levels.\n:::\n\n--------------------------------------------------------------------------------\n\nImagine fitting a straight line to predict `sale_price` from `ms_zoning3` using these three different ways to arbitrarily assign numbers to levels.\n```{r}\ndata_dummy |> \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 1,\n                                ms_zoning3 == \"commercial\" ~ 2,\n                                ms_zoning3 == \"floating\" ~ 3)) |> \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n```\n\n```{r}\ndata_dummy |> \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 2,\n                                ms_zoning3 == \"commercial\" ~ 1,\n                                ms_zoning3 == \"floating\" ~ 3)) |> \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n```\n\n```{r}\ndata_dummy |> \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 3,\n                                ms_zoning3 == \"commercial\" ~ 1,\n                                ms_zoning3 == \"floating\" ~ 2)) |> \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n```\n\n--------------------------------------------------------------------------------\n\nDummy coding resolves this issue.\n\n- When using dummy codes, we transform (i.e., feature engineer) our original m-level categorical predictor to m-1 dummy features.\n- Each of these m-1 features represents a contrast between a specific level of the categorical variable and a **reference level** \n- The full set of m-1 features represents the overall effect of the categorical predictor variable.\n- We assign values of 0 or 1 to each observation on each feature in a meaningful pattern (see below)\n\n--------------------------------------------------------------------------------\n\nFor example, with our three-level predictor: `ms_zoning3`\n\n- We need 2 dummmy features (d1, d2) to represent this 3-level categorical predictor\n- Dummy feature 1 is coded 1 for residential and 0 for all other levels\n- Dummy feature 2 is coded 1 for floating and 0 for all other levels\n\n\nHere is this coding scheme displayed in a table\n\n```{r}\n#| echo: false\ntibble(ms_zoning3 = c(\"commercial\", \"residential\", \"floating\"),\n       d1 = c(0,1,0),\n       d2 = c(0,0,1)) |> \n  print()\n```\n\n-------------------------------------------------------------------------------- \n\nWith this coding:\n\n- Commercial properties are coded 0 for both d1 and d2.  \n- This means that commercial properties will become the reference level against which both residential and floating village are compared.\n- Because we are focused on prediction, the choice of reference level is mostly arbitrary.  For explanatory goals, you might consider which level is best suited to be the reference.\n- There is much deeper coverage of dummy and other contrast coding in 610/710\n\n--------------------------------------------------------------------------------\n\nWe can add these two features manually to the data frame and view a handful of observations to make this coding scheme more concrete  \n\n```{r}\n#| echo: false\n\ndata_dummy <- data_dummy |> \n  mutate(d1 = if_else(ms_zoning3 == \"residential\", 1, 0),\n         d2 = if_else(ms_zoning3 == \"floating\", 1, 0))\n\n# slice a few example observations\ndata_dummy |> \n  slice(c(1, 2, 199, 3, 377, 200, 4, 376)) |> \n  print()\n```\n\n-------------------------------------------------------------------------------- \n\nIf we now fit a model where we predict `sale_price` from these two dummy coded features, each feature would represent the contrast of the mean `sale_price` for the level coded 1 vs. the mean `sale_price` for the level that is coded 0 for all features (i.e., commercial)\n\n- d1 is the contrast of mean `sale_price` for residential vs. commercial\n- d2 is the contrast of mean `sale_price` for floating vs. commercial\n- The combined effect of these two features represents the overall effect of `ms_zoning3` on `sale_price`\n\n--------------------------------------------------------------------------------\n\nLets do this quickly in base R using `lm()` as you have done previously in 610.\n```{r}\nm <- lm(sale_price ~ d1 + d2, data = data_dummy) \n\nm |> summary()\n```\n\n- The mean sale price of residential properties is `r round(round(m$coefficients[\"d1\"]))` dollars higher than commercial properties.\n\n- The mean sale price of floating villages is `r round((m$coefficients[\"d2\"]))` dollars higher than commercial properties. \n\n--------------------------------------------------------------------------------\n\nTo understand this conceptually, it is easiest to visualize the linear model that would predict `sale_price` with these two dichotomous features.\n\n- There are only three columns of `sale_price` because the only possible values for `d1` and `d2` (which are both dichotomous) are \n  - 0,0 (commercial)\n  - 1,0 (residential)\n  - 0,1 (floating village)\n- This regression with two features yields a prediction plane (displayed)\n- The left/right tilt of the plane will be the parameter estimate for d1 and it is the contrast of residential vs. commercial\n- The front/back tilt of the plane will be the parameter estimate for d2 and it is the contrast of floating village vs. commercial\n\n```{r}\n#| warning: false\n#| echo: false\n\nd <- data_dummy |> \n  slice_sample(n = 500) |>   # select a subset for display purposes\n  select(d1, d2, sale_price)\n\nplot_3d <- d |> \n  scatterplot3d::scatterplot3d(angle=55, label.tick.marks = FALSE, \n                               color = \"red\")\nplot_3d$plane3d(lm(sale_price ~ d1 + d2, data = d))\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Statistical sidebar\n\n- Any full rank (# levels - 1) set of features regardless of coding system predicts exactly the same (e.g., dummy, helmert, contrast coding)\n- Preference among coding systems is simply to get single df contrasts of theoretical importance (i.e., for explanation rather than prediction)\n- Final (m^th^) dummy feature is not included b/c its is completely redundant (perfectly multicollinear) with other dummy features. This would also prevent a linear model from fitting ('**dummy variable trap**').\n- However, some statistical algorithms do not have problems with perfect multicollinearity (e.g., LASSO, ridge regression).  \n  - For these algorithms, you will sometimes see modified version of dummy coding called [one-hot coding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).  \n  - This approach uses one additional dummy coded feature for the final category.  \n  - We won't spend time on this but you should be familiar with the term b/c it is often confused with dummy coding.\n::: \n\n::: {.callout-tip}\n# Coding Sidebar\n\nWhen creating dummy coded features from factors that have levels with infrequent observations, you may occasionally end up with novel levels in your validation or test sets that were not present in your training set.  \n\n- This will cause you issues.  \n- These issues are mostly resolved if you make sure to explicitly list all possible levels for a factor when classing that factor in the training data, even if the level doesn't exist in the training data.   \n- We provide more detail on this issue in an [appendix](https://jjcurtin.github.io/book_iaml/app_dummy_coding.html).\n:::\n\n--------------------------------------------------------------------------------\n\n###  Nominal Predictors\n\nNow that we understand how to use dummy coding to feature engineer nominal predictors, let's consider some potentially important ones that are available to us.\n\nWe can discuss if any look promising.\n\n--------------------------------------------------------------------------------\n\nLets return first to `ms_zoning`\n\n\n```{r}\n#| out-height: 3in\n\ndata_trn |> \n  plot_categorical(\"ms_zoning\", \"sale_price\") |> \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n```\n\t\nWe might: \n\n- Represent it with 6 dummy features (because there are 7 raw levels) but many of the categories are very low n - won't account for much variance?\n- Combine all the commercial categories (agri, commer, indus), which would take care of most of the low n groups.  They also all tend to have the lower prices.\n-  Combine all the residential to get a better feature to variance accounted ratio.  They all tend to have similar prices on average and res_high is also pretty low n. \n\nData dictionary entry: Identifies the general zoning classification of the sale.\n\n- agri:\tAgriculture\n- commer:\tCommercial\n- float:\tFloating Village Residential\n- indus:\tIndustrial\n- res_high:\tResidential High Density\n- res_med:\tResidential Medium Density\n- res_low:\tResidential Low Density\n\n--------------------------------------------------------------------------------\n\n`lot_config`\n\n```{r}\n#| out-height: 3in\n\ndata_trn |> \n  plot_categorical(\"lot_config\", \"sale_price\") |> \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n```\n\n\nWe see that: \n\n- Most are inside lots, some of the lot categories are low n\n- Median `sale_price` is not very different between configurations\n- Not very promising but could help some (particularly given the large sample size)\n\nData dictionary entry: Lot configuration\n\n- inside:\tInside lot\n- corner:\tCorner lot\n- culdsac:\tCul-de-sac\n- fr2:\tFrontage on 2 sides of property\n- fr3:\tFrontage on 3 sides of property\n\n--------------------------------------------------------------------------------\n\n`bldg_type`\n\n```{r}\n#| out-height: 3in\n\ndata_trn |> \n  plot_categorical(\"bldg_type\", \"sale_price\") |> \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n```\n\nWe see that:\n\n- Most of the houses are in one category - one_fam\n- There is not much difference in median `sale_price` among categories\n- Not very promising\n\n\nData dictionary entry: Type of dwelling\n\t\t\n- one_fam:\tSingle-family Detached\t\n- two_fam:\tTwo-family Conversion; originally built as one-family dwelling\n- duplex: Duplex\n- town_end:\tTownhouse End Unit\n- town_inside:\tTownhouse Inside Unit\n\n--------------------------------------------------------------------------------\n\nLet's do some feature engineering with `ms_zoning`.  We can now do this formally in a recipe so that it can be used in our modeling workflow.\n\n- First, if you noticed earlier, there are some levels for `ms_zoning` that are pretty infrequent.  Lets make sure both `data_trn` and `data_val` have all levels set for this factor.\n\n```{r}\ndata_trn |> pull(ms_zoning) |> levels()\ndata_val |> pull(ms_zoning) |> levels()\n```\n\n- As expected, we are missing a level (`agri`) in `data_val`.  Lets fix that here \n\n```{r}\ndata_val <- data_val |> \n  mutate(ms_zoning = factor(ms_zoning, \n                            levels = c(\"agri\", \"commer\", \"float\", \"indus\", \n                                       \"res_high\", \"res_low\", \"res_med\")))\n```\n\n[Note:  Ideally, you would go back to cleaning EDA and add this level to the full dataset and then re-split into training, validation and test.  This is a sloppy shortcut!]\n\n--------------------------------------------------------------------------------\n\nWith that fixed, let's proceed:\n\n- We will collapse categories down to three levels (commercial, residential, floating village) as before but now using `step_mutate()` combined with `fct_collapse()` to do this inside of our recipe.  \n- We will convert to dummy features using `step_dummy()`.  The first level of the factor will be set to the reference level when we call `step_dummy()`. \n- `step_dummy()` is a poor choice for function name.  It actually uses whatever contrast coding we have set up in R.  However, the default is are dummy coded contrasts (R calls this treatment contrasts). See `?contrasts` and `options(\"contrasts\")` for more info.\n  \n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + ms_zoning, \n         data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\")) |>\n  step_dummy(ms_zoning)\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding Sidebar\n\nYou should also read more about some other `step_()` functions that you might use for categorical predictors:\n- `step_other()` to combine all low frequency categories into a single \"other\" category.\n- `step_unknown()` to assign missing values their own category\n- You can use selector functions.  For example, you could make dummy variables out of all of your factors in one step using `step_dummy(all_nominal_predictors())`.  \n\nSee the [Step Functions - Dummy Variables and Encoding section](https://recipes.tidymodels.org/reference/index.html) on the `tidymodels` website for additional useful functions.\n\nWe have also described these in the section on factor steps in Appendix 1\n:::\n\n--------------------------------------------------------------------------------\n\nLet's see if the addition of `ms_zoning` helped\n\n- Notice the addition of the dummy coded features to the feature matrix\n- Notice the removal of the factor `ms_zoning`\n\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- skim\n\n```{r}\nfeat_trn |> skim_all()\n\nfeat_val |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Now lets fit a model with these features\n\n```{r}\nfit_lm_6 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- plot it\n\n```{r}\n#| out-height: 3in\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_6, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- And evaluate it\n\n```{r}\n#| echo: false\n\nerror_val <- error_val |> \n  bind_rows(tibble(model = \"6 feature linear model w/ms_zoning\", \n                   rmse_val = rmse_vec(feat_val$sale_price,\n                                       predict(fit_lm_6,\n                                               feat_val)$.pred)))\n\nerror_val\n```\n\n- Removing Yeo Johnson transformation but adding dummy coded `ms_zoning` may have helped a little\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\nWill the addition of new predictors/features to a model always reduce RMSE in train?  in validation?\n:::\n\n::: {.fragment .uwred}\nAs you know, the estimation procedure in linear models is OLS.  Parameter estimates are derived to minimize the SSE in the data set in which they are derived.  For this reason, adding a predictor will never increase RMSE in the training set and it will usually lower it even when it is not part of the DGP.   \n\nHowever, this is not true in validation.  A predictor will only meaningfully lower RMSE in validation if it is part of the DGP.  Also, a bad predictor could even increase RMSE in validation due tooverfitting.\n:::\n\n--------------------------------------------------------------------------------\n\n### Ordinal Predictors\n\nWe have two paths to pursue for ordinal predictors\n\n- We can treat them like nominal predictors (e.g., dummy code)\n- We can treat them like numeric predictors (either raw or with an added transformation if needed)\n\n--------------------------------------------------------------------------------\n\nLet's consider `overall_qual`\n\n```{r}\n#| out-height: 3in\n\ndata_trn |> \n  plot_categorical(\"overall_qual\", \"sale_price\") |> \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n```\n\nObservations:\n\n- Low frequency for low and to some degree high quality response options.  If dummy coding, may want to collapse some (1-2)\n- There is a monotonic relationship (mostly linear) with `sale_price`.   Treat as numeric?\n- Not skewed so doesn't likely need to be transformed if treated as numeric\n- Numeric will take one feature vs. many (9?) features for dummy codes.  \n- Dummy codes are more flexible but we may not need this flexibility (and unnecessary flexibility increases overfitting)\n\n--------------------------------------------------------------------------------\n\nLet's add `overall_qual` to our model as numeric\n\nRemember that this predictor was ordinal so we paid special attention to the order of the levels when we classed this factor.  Lets confirm they are in order\n\n```{r}\ndata_trn |> pull(overall_qual) |> levels()\n```\n\n--------------------------------------------------------------------------------\n\nTo convert `overall_qual` to numeric (with levels in the specified order), we can use another simple mutate inside our recipe.\n\n```{r}\nrec <- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |>\n  step_dummy(ms_zoning)\n```\n\n::: {.callout-tip}\n# Coding Sidebar\nThere is a step function called `step_ordinalscore()` but it requires that the factor is classed as an ordered factor.  It is also more complicated than needed in our opinion.  Just use `as.numeric()`\n:::\n\n--------------------------------------------------------------------------------\n\nLet's evaluate this model\n\n- Making features\n- Skipping the skim to save space (we promised we checked it previously!)\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- Fitting model\n\n```{r}\nfit_lm_7 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- Plotting results\n\n```{r}\nplot_truth(truth = feat_val$sale_price, \n                                 estimate = predict(fit_lm_7, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- Quantifying held out error\n\n```{r}\n#| echo: false\n\nerror_val <- bind_rows(error_val, \n                        tibble(model = \"7 feature linear model\", \n                               rmse_val = rmse_vec(feat_val$sale_price,\n                                                   predict(fit_lm_7,\n                                                           feat_val)$.pred)))\n\nerror_val\n```\n\n- That helped!\n\n--------------------------------------------------------------------------------\n\n## Extensions to Interactive Models and Non-linear Models\n\n\n### Interactions \n\nThere may be interactive effects among our predictors\n\n- Some statistical algorithms (e.g., KNN) can naturally accommodate interactive effects without any feature engineering\n- Linear models cannot\n- Nothing to fear, tidymodels makes it easy to feature engineer interactions\n- [BUT - as we will learn, we generally think that if you expect lots of interactions, the linear model may not be the best model to use]\n\n--------------------------------------------------------------------------------\n\nFor example, it may be that the relationship between `year_built` and `sale_price` depends on `overall_qual`.\n\n- Old houses are expensive if they are in good condition \n- but old houses are very cheap if they are in poor condition\n\n--------------------------------------------------------------------------------\n\nIn the `tidymodels` framework\n\n- Coding interactions is done by feature engineering, not by formula (Note that formula does not change below in recipe)\n- This seems appropriate to us as we are making new features to represent interactions\n- We still use an R formula like interface to specify the interaction term features that will be created\n- see [more details](https://recipes.tidymodels.org/reference/step_interact.html) on the `tidymodels` website\n\n```{r}\nrec <- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |>\n  step_dummy(ms_zoning) |> \n  step_interact(~ overall_qual:year_built)\n```\n\n--------------------------------------------------------------------------------\n\nLet's prep, bake, fit, and evaluate! \n\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- Note the new interaction term (we just skim `feat_trn` here)\n- Named using \"_x_\" to specify the interaction\n\n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- fit model\n\n```{r}\nfit_lm_8 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~., \n      data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- plot\n\n```{r}\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_8, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- calculate held out error\n\n```{r}\n#| echo: false\n\nerror_val <- bind_rows(error_val, \n                      tibble(model = \"8 feature linear model w/interaction\", \n                             rmse_val = rmse_vec(feat_val$sale_price,\n                                                 predict(fit_lm_8,\n                                                         feat_val)$.pred)))\n\nerror_val\n```\n\n\n- That helped!\n\n--------------------------------------------------------------------------------\n\nYou can also feature engineer interactions with nominal (and ordinal predictors treated as nominal) predictors\n\n- The nominal predictors should first be converted to dummy code features\n- You will indicate the interactions using the variable names that will be assigned to these dummy code features\n- Use `starts_with()` or `matches()` to make it easy if there are many features associated with a categorical predictor\n- Can use \"~ .^2\" to include all two way interactions (be careful if you have dummy coded features!)\n\n--------------------------------------------------------------------------------\n\nLet's code an interaction between `ms_zoning` & `year_built`.  \n\n- Old homes are cool\n- Old commercial spaces are never cool\n- Maybe this is why the main effect of `ms_zoning` wasn't useful\n\n```{r}\nrec <- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |>\n  step_dummy(ms_zoning) |> \n  step_interact(~ overall_qual:year_built) |> \n  step_interact(~ starts_with(\"ms_zoning_\"):year_built)  \n```\n\n--------------------------------------------------------------------------------\n\n- prep, bake\n\n```{r}\n#| warning: false\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- Yup, we have two new interaction features as expected\n\n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Fit model\n\n```{r}\nfit_lm_10 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- Plot\n\n```{r}\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_10, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- Quantify held out error\n\n```{r}\nerror_val <- error_val |> \n  bind_rows(tibble(model = \"10 feature linear model w/interactions\", \n                   rmse_val = rmse_vec(feat_val$sale_price,\n                                       predict(fit_lm_10,\n                                               feat_val)$.pred)))\n\nerror_val\n```\n\n- Not really any better \n- Shouldn't just include all interactions without reason\n  - Either you have done EDA to support them or\n  - You have substantive interest in them (explanatory question)\n  - If you want all interactions, use a statistical algorithm that supports those relationships without feature engineering (e.g., KNN, random forest and other decision trees)\n  \n--------------------------------------------------------------------------------\n\n### Non-linear Models\n \nWe may also want to model non-linear effects of our predictors\n\n- Some non-parametric models can accommodate non-linear effects without feature engineering (e.g., KNN, Random Forest).\n- Non-linear effects can be accommodated in a linear model with feature engineering\n  - Transformations of Y or X.  See [Step Functions - Individual Transformations](https://recipes.tidymodels.org/reference/index.html) on `tidymodels` website\n  - Ordinal predictors can be coded with dummy variables\n  - Numeric predictors can be split at threshold\n  - Polynomial contrasts for numeric or ordinal predictors (see `step_poly()`)  \n  \n\n- We will continue to explore these options throughout the course\n\n--------------------------------------------------------------------------------\n\n## KNN Regression\n\nK Nearest Neighbor\n\n- Is a non-parametric regression and classification statistical algorithm\n  - It does not yield specific parameter estimates for features/predictors (or statistical tests for those parameter estimates)\n  - There are still ways to use it to address explanatory questions (visualizations, model comparisons, feature importance)\n\n- Very simple but also powerful (listed commonly among top 10 algorithms)\n  - By powerful, it is quite flexible and can accommodate many varied DGPs without the need for much feature engineering with its predictors\n  - May not need most transformations of X or Y\n  - May not need to model interactions\n  - Still need to handle missing data, outliers, and categorical predictors\n\n--------------------------------------------------------------------------------\n\nK Nearest Neighbor\n\n- Algorithm \"memorizes\" the training set (**lazy learning**)\n  - Lazy learning is most useful for large, continuously changing datasets with few attributes (features) that are commonly queried (e.g., online recommendation systems)\n\n- Prediction for any new observation is based on $k$ most similar observations from the dataset\n  - $k$ provides direct control  over the bias-variance trade-off for this algorithm\n  \n--------------------------------------------------------------------------------\n\nTo better understand KNN let's simulate training data for three different DGPs (linear - y, polynomial - y2, and step - y3)\n\n```{r}\n#| echo: false\n\nset.seed(55231)\ndata_trn_demo <- tibble(x = runif(150, 1, 100),\n               y = rnorm(150, x, 10),\n               y_dgp = rnorm(150, x ,0),\n               y2 = rnorm(150, x^4 / 800000, 8),\n               y2_dgp = rnorm(150, x^4 / 800000 ,0),\n               y3 = if_else(x < 40, rnorm(150, 25, 10),  rnorm(150, 75, 10)),\n               y3_dgp = if_else(x < 40, rnorm(150, 25, 0),  rnorm(150, 75, 0)))\n```\n\n--------------------------------------------------------------------------------\n\nLet's start with a simple example where the DGP for Y is linear on one predictor (X)\n\nDGP: $y = rnorm(150, x, 10)$\n\nThis figure displays:\n\n - DGP\n - Prediction line from a simple linear model\n - Red lines to represent three new observations (X = 10, 50, 90) we want to make predictions for via a standard KNN\n\n\n```{r}\n#| echo: false\n\ndata_trn_demo |> \n  ggplot(aes(x = x, y = y)) +\n    geom_line(aes(x = x, y = y_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_smooth(aes(color = \"green\"), method='lm',formula=y ~ x,  se = FALSE) +\n    geom_vline(xintercept = 10, color = \"red\", linewidth = 1.5)+\n    geom_vline(xintercept = 50, color = \"red\", linewidth = 1.5)+\n    geom_vline(xintercept = 90, color = \"red\", linewidth = 1.5) +\n    geom_point(aes(y = y), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"green\"),\n                       labels = c(\"DGP\", \"linear model\"),\n                       guide = \"legend\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\nWhat would 5-NN predictions look like for each of these three new values of X in the figure above?\n:::\n\n::: {.fragment .uwred}\nFor x = 10, find the five observations that have X values closest to 10.  Average the Y values for those 5 observations and that is your predicted Y associated with that new value of X.  \n\nRepeat to make predictions for Y for any other value of X,e.g., 50, 90, or any other value\n:::\n\n--------------------------------------------------------------------------------\n\nKNN can easily accommodate non-linear relationships between numeric predictors and outcomes without any feature engineering for predictors\n\nIn fact, it can flexibly handle **any** shape of relationship\n\nDGP: $y2 = rnorm(150, x^4 / 800000, 8)$\n\n```{r}\n#| echo: false\n\ndata_trn_demo |> \n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), size = 1.5) +\n    geom_smooth(aes(color = \"green\"), method='lm',formula=y ~ x,  se = FALSE) +\n    geom_vline(xintercept = 10, color = \"red\", size = 1.5)+\n    geom_vline(xintercept = 50, color = \"red\", size = 1.5)+\n    geom_vline(xintercept = 90, color = \"red\", size = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"green\"),\n                       labels = c(\"DGP\", \"linear model\"),\n                       guide = \"legend\")\n```\n\n--------------------------------------------------------------------------------\n\nDGP: $y3 = if\\_else(x < 40, rnorm(150, 25, 10),  rnorm(150, 75, 10))$\n\n```{r}\n#| echo: false\n\ndata_trn_demo |> \n  ggplot(aes(x = x, y = y3)) +\n    geom_line(aes(x = x, y = y3_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_smooth(aes(color = \"green\"), method='lm',formula=y ~ x,  se = FALSE) +\n    geom_vline(xintercept = 10, color = \"red\", size = 1.5)+\n    geom_vline(xintercept = 50, color = \"red\", size = 1.5)+\n    geom_vline(xintercept = 90, color = \"red\", size = 1.5) +\n    geom_point(aes(y = y3), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"green\"),\n                       labels = c(\"DGP\", \"linear model\"),\n                       guide = \"legend\")\n```\n\n--------------------------------------------------------------------------------\n\n## The Hyperparameter k in KNN\n\nKNN is our first example of a statistical algorithm that includes a **hyperparameter**, in this case $k$\n\n- Algorithm hyperparameters differ from parameters in that they cannot be estimated while fitting the algorithm to the training set\n\n- They must be set in advance\n\n- `k = 5` is the default for `kknn()`, the engine from the `kknn` package that we will use to fit a KNN within `tidymodels`.  \n\n  - [$kknn()$](https://cran.r-project.org/web/packages/kknn/kknn.pdf) weights observations (neighbors) based on distance.  \n  - An option exists for unweighted as well but not likely used much (default is optimal weighting, use it!).\n  \n--------------------------------------------------------------------------------\n\nUsing the polynomial DGP above, let's look at a 5-NN yields\n\n- Note the new category of algorithm, new engine, and the need to set a mode (because KNN can be used for regression and classification)\n- We can look in the package documentation to better understand what is being done (`?kknn::train.kknn`).\n```{r}\nnearest_neighbor() |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  translate()\n```\n\n--------------------------------------------------------------------------------\n\n- Set up simple feature engineering recipe and get training features (nothing happening but let's follow normal routine anyway)\n```{r}\nrec <- \n  recipe(y2 ~ x, data = data_trn_demo)\n\nrec_prep <- rec |> \n  prep(data_trn_demo)\n\nfeat_trn_demo <- rec_prep |> \n  bake(NULL)\n```\n\n--------------------------------------------------------------------------------\n\n- Fit 5NN\n```{r}\nfit_5nn_demo <- \n  nearest_neighbor() |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(y2 ~ ., data = feat_trn_demo)\n```\n\n--------------------------------------------------------------------------------\n\n- Get features for a validation set (a new sample using same polynomial DGP)\n```{r}\n#| echo: false\n\ndata_val_demo <- tibble(x = runif(200, 1, 100),\n               y = rnorm(200, x, 10),\n               y_dgp = rnorm(200, x ,0),\n               y2 = rnorm(200, x^4 / 800000, 8),\n               y2_dgp = rnorm(200, x^4 / 800000 ,0),\n               y3 = if_else(x < 40, rnorm(200, 25, 10),  rnorm(200, 75, 10)),\n               y3_dgp = if_else(x < 40, rnorm(200, 25, 0),  rnorm(200, 75, 0)))\n```\n\n```{r}\nfeat_val_demo <- rec_prep |> \n  bake(data_val_demo)\n```\n\n--------------------------------------------------------------------------------\n\n- Display 5NN predictions in validation \n\n  - KNN (with `k = 5`) does a pretty good job of representing the shape of the DGP (low bias)\n  - KNN displays some (but minimal) evidence of overfitting\n  - Simple linear model does not perform well (clear/high bias)\n\n```{r}\n#| echo: false\n\nfeat_val_demo |> \n  bind_cols(data_val_demo |> select(y, y2_dgp)) |> # add in other outcomes from data\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_smooth(aes(color = \"green\"), method = 'lm', formula = y ~ x,  se = FALSE) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_val_demo)$.pred, color = \"red\"), \n              size = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"green\", \"red\"),\n                       labels = c(\"DGP\", \"linear model\", \"k = 5\"),\n                       guide = \"legend\")\n```\n\n--------------------------------------------------------------------------------\n\nLet's pause and consider our conceptual understanding of the impact of $k$ on the bias-variance trade-off\n\n::: {.callout-important}\n# Question\nHow will the size of k influence model performance (e.g., bias, overfitting/variance)?\n:::\n\n::: {.fragment .uwred}\nSmaller values of k will tend to increase overfitting (and therefore variance across training samples) but decrease bias.  Larger values of k will tend to decrease overfitting but increase bias.  We need to find the Goldilocks \"sweet spot\"\n:::\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\nHow will k = 1 perform in training and validation sets?\n:::\n\n::: {.fragment .uwred}\nk = 1 will perfectly fit the training set.  Therefore it is very dependent on the training set (high variance).  It will fit both the DGP and the noise in the training set.  Clearly it will likely not do as well in validation (it will be overfit to training).  \n\nk needs to be larger if there is more noise (to average over more cases).  k needs to be smaller if the relationships are complex. (More on choosing k by resampling in unit 5.\n:::\n\n--------------------------------------------------------------------------------\n\n`k = 1`\n\n- Fit new model\n\n- Recipe and features have not changed\n\n```{r}\nfit_1nn_demo <- \n  nearest_neighbor(neighbors = 1) |>  # <1> \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(y2 ~ ., data = feat_trn_demo)\n```\n1. Set k with `neighbors = `\n\n--------------------------------------------------------------------------------\n\nVisualize prediction models in Train and Validation\n\n```{r}\n#| echo: false\n#| out-height: 5in\n\nplot_train <- feat_trn_demo |> \n  bind_cols(data_trn_demo |> select(y, y2_dgp)) |> # add in other outcomes for fig\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_trn_demo)$.pred, \n                  color = \"red\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_1nn_demo, feat_trn_demo)$.pred, \n                  color = \"green\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"red\", \"green\"),\n                       labels = c(\"DGP\", \"k = 5\", \"k = 1\"),\n                       guide = \"legend\")\n\nplot_val <- feat_val_demo |> \n  bind_cols(data_val_demo |> select(y, y2_dgp)) |> # add in other outcomes from data\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_val_demo)$.pred, \n                  color = \"red\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_1nn_demo, feat_val_demo)$.pred, \n                  color = \"green\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"red\", \"green\"),\n                       labels = c(\"DGP\", \"k = 5\", \"k = 1\"),\n                       guide = \"legend\")\n\ncowplot::plot_grid(plot_train, plot_val, \n                   labels = list(\"Training Set\", \"Validation Set\"), \n                   ncol = 2, nrow = 1, hjust = -1)\n```\n\n--------------------------------------------------------------------------------\n\nCalculate RMSE in validation for two KNN models\n\n`k = 1`\n```{r}\nrmse_vec(feat_val_demo$y2, \n         predict(fit_1nn_demo, feat_val_demo)$.pred)\n```\n\n`k = 5`\n```{r}\nrmse_vec(feat_val_demo$y2, \n         predict(fit_5nn_demo, feat_val_demo)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\nWhat if we go the other way and increase $k$ to 75\n\n```{r}\nfit_75nn_demo <- \n  nearest_neighbor(neighbors = 75) |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(y2 ~ ., data = feat_trn_demo)\n```\n\n--------------------------------------------------------------------------------\n\nVisualize prediction models in Train and Validation\n\n```{r}\n#| echo: false\n#| out-height: 5in\n\nplot_train <- feat_trn_demo |> \n  bind_cols(data_trn_demo |> select(y, y2_dgp)) |> # add in other outcomes for fig\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_trn_demo)$.pred, \n                  color = \"red\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_1nn_demo, feat_trn_demo)$.pred, \n                  color = \"green\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_75nn_demo, feat_trn_demo)$.pred, \n                  color = \"yellow\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"red\", \"green\", \"yellow\"),\n                       labels = c(\"DGP\", \"k = 5\", \"k = 1\", \"k = 75\"),\n                       guide = \"legend\")\n\nplot_val <- feat_val_demo |> \n  bind_cols(data_val_demo |> select(y, y2_dgp)) |> # add in other outcomes for fig\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_val_demo)$.pred, \n                  color = \"red\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_1nn_demo, feat_val_demo)$.pred, \n                  color = \"green\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_75nn_demo, feat_val_demo)$.pred, \n                  color = \"yellow\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n     scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"red\", \"green\", \"yellow\"),\n                       labels = c(\"DGP\", \"k = 5\", \"k = 1\", \"k = 75\"),\n                       guide = \"legend\")\n\ncowplot::plot_grid(plot_train, plot_val, \n                   labels = list(\"Training Set\", \"Validation Set\"), \n                   ncol = 2, nrow = 1, hjust = -1)\n```\n\n--------------------------------------------------------------------------------\n\nCalculate RMSE in validation for three KNN models\n\n**This is the bias-variance trade-off in action**\n\n- `k = 1`  - high variance\n```{r}\nrmse_vec(feat_val_demo$y2, \n         predict(fit_1nn_demo, feat_val_demo)$.pred)\n```\n\n- `k = 5`  - just right (well better at least)\n```{r}\nrmse_vec(feat_val_demo$y2, \n         predict(fit_5nn_demo, feat_val_demo)$.pred)\n```\n\n- `k = 75` - high bias\n```{r}\nrmse_vec(feat_val_demo$y2, \n         predict(fit_75nn_demo, feat_val_demo)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n## Distance and Scaling in KNN\n\n### Defining \"Nearest\"\n\nTo make a prediction for some new observation, we need to identify the observations from the training set that are nearest to it\n\n- Need a distance measure to define \"nearest\"\n\n- IMPORTANT: We care only about:\n\n  - Distance between a validation observation and all the training observations\n  - Need to find the $k$ observations in training that are nearest to the validation observation (i.e., its neighbors)\n  - Distance is defined based on these observations' features, not their outcomes\n  \n- There are a number of different distance measures available (e.g., Euclidean, Manhattan, Chebyshev, Cosine, Minkowski)\n\n  - Euclidean is most commonly used in KNN\n\n--------------------------------------------------------------------------------\n\nEuclidean distance between any two points is an n-dimensional extension of the Pythagorean formula (which applies explicitly with 2 features/2 dimensional space).  \n\n$C^2 = A^2 + B^2$\n\n$C = \\sqrt{A^2 + B^2}$\n\n...where C is the distance between two points\n\n--------------------------------------------------------------------------------\n\nThe Euclidean distance between 2 points (p and q) in two dimensions (2 predictors, x1 = A, x2 = B)\n\n$Distance = \\sqrt{A^2 + B^2}$\n\n$Distance = \\sqrt{(q1 - p1)^2 + (q2 - p2)^2}$\n\n$Distance = \\sqrt{(2 - 1)^2 + (5 - 2)^2}$\n\n$Distance = 3.2$\n\n```{r}\n#| echo: false\n\ndistance2 <- tibble(x1 = c(1, 2), x2 = c(2, 5), \n                   x1h = c(1, 2), x2h = c(2, 2),\n                   x1v = c(2, 2), x2v = c(2, 5))\n\ndistance2 |> \n  ggplot(aes(x = x1, y = x2)) +\n    geom_point(color = c(\"red\", \"blue\"), size = 3) +\n    geom_line(color = \"green\", linewidth = 1.5) +\n    geom_line(aes(x = x1h, y = x2h), color = \"gray\", linewidth = 1.5) +\n    geom_line(aes(x = x1v, y = x2v), color = \"gray\", linewidth = 1.5) +\n    geom_text(x = .9, y = 2, label = \"p\", size = 5) +\n    geom_text(x = 2.1, y = 5, label = \"q\", size = 5) +\n    geom_text(x = 1.5, y = 1.8, label = \"A = 1\", size = 5) +\n    geom_text(x = 2.2, y = 3.5, label = \"B = 3\", size = 5) +\n    geom_text(x = 1.2, y = 3.5, label = \"C = 3.2\", size = 5) +\n    scale_x_continuous(limits = c(0,6), breaks = 0:6) +\n    scale_y_continuous(limits = c(0,6), breaks = 0:6)\n```\n\n--------------------------------------------------------------------------------\n\nOne dimensional (one feature) is simply the subtraction of scores on that feature (x1) between p and q\n\n$Distance = \\sqrt{(q1 - p1)^2}$\n\n$Distance = \\sqrt{(2 - 1)^2}$\n\n$Distance = 1$\n\n```{r}\n#| echo: false\n\ndistance1 <- tibble(x1 = c(1, 2), x2 = c(1, 1))\n\ndistance1 |> \n  ggplot(aes(x = x1, y = x2)) +\n    geom_point(color = c(\"red\", \"blue\"), size = 3) +\n    geom_line(color = \"green\", linewidth = 1.5) +\n    geom_text(x = .9, y = 1, label = \"p\", size = 5) +\n    geom_text(x = 2.1, y = 1, label = \"q\", size = 5) +\n    geom_text(x = 1.5, y = 1.1, label = \"Distance = 1\", size = 5) +\n    scale_x_continuous(limits = c(0,6), breaks = 0:6) +\n    scale_y_continuous(limits = c(0,2), breaks = NULL, labels = NULL, name = NULL)\n```\n\nN-dimensional generalization for n features:\n\n$Distance = \\sqrt{(q1 - p1)^2 + (q2 - p2)^2 + ... + (qn - pn)^2}$\n\n--------------------------------------------------------------------------------\n\nManhattan distance is also referred to as city block distance\n\n- Travel down the \"A\" street for 1 unit\n- Travel down the \"B\" street for 3 units\n- Total distance = 4 units\n\nFor two features/dimensions\n\n$Distance = |A + B|$\n\n```{r}\n#| echo: false\n\ndistance2 |> \n  ggplot(aes(x = x1, y = x2)) +\n    geom_point(color = c(\"red\", \"blue\"), size = 3) +\n    geom_line(aes(x = x1h, y = x2h), color = \"green\", linewidth = 1.5) +\n    geom_line(aes(x = x1v, y = x2v), color = \"green\", linewidth = 1.5) +\n    geom_text(x = .9, y = 2, label = \"p\", size = 5) +\n    geom_text(x = 2.1, y = 5, label = \"q\", size = 5) +\n    geom_text(x = 1.5, y = 1.8, label = \"A = 1\", size = 5) +\n    geom_text(x = 2.2, y = 3.5, label = \"B = 3\", size = 5) +\n    scale_x_continuous(limits = c(0,6), breaks = 0:6) +\n    scale_y_continuous(limits = c(0,6), breaks = 0:6)\n```\n\n--------------------------------------------------------------------------------\n\n`kknn()` uses Minkowski distance (see [Wikipedia](https://en.wikipedia.org/wiki/Minkowski_distance) or [less mathematical description](https://www.mikulskibartosz.name/minkowski-distance-explained/))\n\n- It is a more complex parameterized distance formula\n  - This parameter is called `p`, referred to as `distance` in `kknn()`\n- Euclidean and Manhattan distances are special cases where `p` = 2 and 1, respectively\n- **The default p in kknn() = 2 (Euclidean distance)**  \n- This default (like all defaults) can be changed when you define the algorithm using `nearest_neighbor()`\n\n--------------------------------------------------------------------------------\n\n### Scaling X\n\nDistance is dependent on scales of all the features.  We need to put all features on the same scale\n\n- Scale all features to SD = 1 (using `step_scale(all_numeric_predictors())`)\n- Range correct [0, 1] all features (using `step_range(all_numeric_predictors())`)\n\n--------------------------------------------------------------------------------\n\n### Categorical Predictors\n\nKNN requires numeric features (for distance calculation).\n\n- For categorical predictors, you will need to use dummy coding or other feature engineering that results in numeric features. \n- e.g., `step_dummy(all_nominal_predictors())`\n\n--------------------------------------------------------------------------------\n\n## KNN with Ames Housing Prices\n\nLet's use KNN with Ames\n\n- Train a model using only numeric predictors and `overall_qual` as numeric\n- Use the default `k = 5` algorithm\n- Set SD = 1 for all features\n\n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + overall_qual, \n         data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(overall_qual = as.numeric(overall_qual)) |> \n  step_scale(all_numeric_predictors()) # <1>\n```\n1.  Remember to take advantage of these selectors for easier code! See `?has_role` for more details\n\n--------------------------------------------------------------------------------\n\n- prep, bake\n\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |>\n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- Skim training features.  Note all SD = 1\n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Skim validation features.  Note SD.  [Why not exactly 1?]{.red}\n```{r}\nfeat_val |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Fit 5NN\n\n```{r}\nfit_5nn_5num <- \n  nearest_neighbor() |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\nerror_val <- bind_rows(error_val, \n                        tibble(model = \"5 numeric predictor 5nn\", \n                               rmse_val = rmse_vec(feat_val$sale_price, \n                                                   predict(fit_5nn_5num, feat_val)$.pred)))\n\nerror_val\n```\n\n- Not bad!\n\n--------------------------------------------------------------------------------\n\nKNN also mostly solved the linearity problem\n\n- We might be able to improve the linear models with better transformations of X and Y\n- However, this wasn't needed for KNN!\n\n```{r}\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_5nn_5num, feat_val)$.pred)\n\n```\n\n--------------------------------------------------------------------------------\n\nBut 5NN may be overfit. `k = 5` is pretty low\n\nAgain with `k = 20`\n\n```{r}\nfit_20nn_5num <- \n  nearest_neighbor(neighbors = 20) |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\n#| echo: false\n\nerror_val <- error_val |> \n  bind_rows(tibble(model = \"5 numeric predictor 20nn\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_20nn_5num, \n                                               feat_val)$.pred)))\n\nerror_val\n```\n\n- That helped some\n\n-------------------------------------------------------------------------------- \n\nOne  more time with `k = 50` to see where we are in the bias-variance function\n\n\n```{r}\nfit_50nn_5num <- \n  nearest_neighbor(neighbors = 50) |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\n#| echo: false\n\nerror_val <- error_val |> \n  bind_rows(tibble(model = \"5 numeric predictor 50nn\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_50nn_5num, \n                                               feat_val)$.pred)))\n\nerror_val\n\n```\n\n- Too high, now we have bias......\n- We will learn a more rigorous method for selecting the optimal value for $k$ (i.e., **tuning** this hyperparameter) in unit 5\n\n--------------------------------------------------------------------------------\n\nTo better understand bias-variance trade-off, let's look at error across these three values of $k$ in train and validation for Ames\n\nTraining\n\n- Remember that training error would be 0 for `k = 1`\n- Training error is increasing as $k$ increases b/c it KNN is overfitting less (so its not fitting the noise in train as well)\n\n```{r}\nrmse_vec(feat_trn$sale_price, \n         predict(fit_5nn_5num, feat_trn)$.pred)\nrmse_vec(feat_trn$sale_price, \n         predict(fit_20nn_5num, feat_trn)$.pred)\nrmse_vec(feat_trn$sale_price, \n         predict(fit_50nn_5num, feat_trn)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\nValidation\n\n- Validation error is first going down as $k$ increases (and it would have been very high for `k = 1`)\n- Bias is likely increasing a bit\n- But this is compensated by big decreases in overfitting variance\n- The trade-off is good for `k = 20` relative to 5 and 1\n- At some point, as $k$ increases the increase in bias outweighed the decrease in variance and validation error increased too.\n```{r}\nrmse_vec(feat_val$sale_price, \n         predict(fit_5nn_5num, feat_val)$.pred)\nrmse_vec(feat_val$sale_price, \n         predict(fit_20nn_5num, feat_val)$.pred)\nrmse_vec(feat_val$sale_price, \n         predict(fit_50nn_5num, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\nLet's do one final example and add one of our nominal variables into the model: `ms_zoning`\n\n- Need to collapse levels and then dummy\n\n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + \n           overall_qual + ms_zoning, data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(overall_qual = as.numeric(overall_qual)) |> \n  step_scale(all_numeric_predictors()) |>  # done before dummy coding avoid scaling binary features \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\")) |>\n  step_dummy(ms_zoning) \n```\n\n--------------------------------------------------------------------------------\n\n- prep, bake\n\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- Fit \n```{r}\nfit_20nn_5num_mszone <- \n  nearest_neighbor(neighbors = 20) |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- evaluate\n\n```{r}\n#| echo: false\n\nerror_val <- error_val |> \n  bind_rows(tibble(model = \"5 numeric predictor 20nn with ms_zoning\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_20nn_5num_mszone, \n                                               feat_val)$.pred)))\n\nerror_val\n```\n\n- Now it helps.  \n- Might have to do with interactions with other predictors that we didn't model in the linear model\n- KNN automatically accommodates interactions.  Why?\n- This model is a bit more complex and might benefit further from higher $k$\n\n--------------------------------------------------------------------------------\n\nAs a teaser, here is another performance metric for this model - $R^2$.   Not too shabby!  Remember, there is certainly some irreducible error in `sale_price` that will put a ceiling on $R^2$ and a floor on RMSE\n```{r}\nrsq_vec(feat_val$sale_price, \n        predict(fit_20nn_5num_mszone, feat_val)$.pred)\n```\n\nOverall, we now have a model that predicts housing prices with about 30K of RMSE and accounting for 86% of the variance.  I am sure you can improve on this!","srcMarkdownNoYaml":"\n\n::: {.content-visible unless-format=\"revealjs\"}\n# Introduction to Regression Models\n:::\n::: {.content-visible when-format=\"revealjs\"}\n# IAML Unit 3: Introduction to Regression Models\n:::\n\n## Overview of Unit\n\n### Learning Objectives\n  \n- Use of root mean square error (RMSE) in training and validation sets for model performance evaluation\n\n- The General Linear Model as a machine learning model  \n  - Extensions to categorical variables (Dummy coding features)\n  - Extensions to interactive and non-linear effects of features\n  \n- K Nearest Neighbor (KNN)\n  - Hyperparameter $k$\n  - Scaling predictors\n  - Extensions to categorical variables\n\n--------------------------------------------------------------------------------\n\nOur goal in this unit is to build a machine learning regression model that can accurately (we hope) predict the `sale_price` for future sales of houses (in Iowa? more generally?)\n\nTo begin this project we need to:\n\n- Load the packages we will need.  I am only loading `tidymodels` and `tidyverse` because the other functions we need are only called occasionally (so we will call them by namespace.)\n- Set up conflicts policies (just in case we decide to load other packages later)\n- We will hide this in future units\n```{r}\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\n\noptions(conflicts.policy = \"depends.ok\")\n```\n\n--------------------------------------------------------------------------------\n\n- source additional class functions libraries\n- We will hide this in future units\n- You might consider copying (some) of these functions to your own function library rather than using mine\n```{r}\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n```\n\n- set display options\n- We will hide this in future units\n```{r}\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\n```\n\n- handle paths\n```{r}\npath_data <- \"./data\"\n``` \n\n--------------------------------------------------------------------------------\n\n- Set up function to class ames data (copied with one improvement from last unit)\n```{r}\nclass_ames <- function(df){\n  df |>\n    mutate(across(where(is.character), factor)) |> \n    mutate(overall_qual = factor(overall_qual, levels = 1:10), \n           garage_qual = suppressWarnings(fct_relevel(garage_qual, # <1> \n                                                      c(\"no_garage\", \"po\", \"fa\", \n                                                    \"ta\", \"gd\", \"ex\"))))\n}\n```\n1. Warnings should be considered errors until investigated.  Once investigated, they can be ignored.  To explicitly ignore, use `suppressWarnings()`\n\n--------------------------------------------------------------------------------\n\n- Open the cleaned training set\n\n```{r}\ndata_trn <- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           col_types = cols()) |>  \n  class_ames() |> \n  glimpse()\n```\n\n-----\n\n- Open the cleaned validation set\n\n```{r}\ndata_val <- read_csv(here::here(path_data, \"ames_clean_class_val.csv\"),\n                     show_col_types = FALSE) |> \n  class_ames() |> \n  glimpse()\n```\n\n**NOTE**: Remember, I have held back an additional test set that we will use only once to evaluate the final model that we each develop in this unit.\n\n-----\n\nWe will also make a dataframe to track validation error across the models we fit\n\n```{r}\nerror_val <- tibble(model = character(), rmse_val = numeric()) |> \n  glimpse()\n```\n\n-----\n\nWe will fit regression models with various model configurations.\n\nThese configurations will differ with respect to statistical algorithm:\n\n- A General Linear Model (lm) - a parametric approach\n- K Nearest Neighbor (KNN) - a non-parametric approach\n\nThese configurations will differ with respect to the features\n\n- Single feature (i.e., simple regression)\n- Various sets of multiple features that vary by:\n  - Raw predictors used\n  - Transformations applied to those predictors as part of feature engineering\n  - Inclusion (or not) of interactions among features\n\n- The KNN model configurations will also differ with respect to its hyperparameter- $k$\n\n-----\n\nTo build models that will work well in new data (e.g., the data that I have held back from you so far):\n\n- We have split the remaining data into a training and validation set for our own use during model building\n\n- We will fit models in train\n\n- We will evaluate them in validation\n\n\nRemember that we:\n\n- Used a 75/25 stratified (on `sale_price`) split of the data at the end of cleaning EDA to create training and validation sets\n- Are only using a subset of the available predictors.  The same ones I used for the EDA unit\n\nYou will work with all of my predictors and all the predictors you used for your EDA when you do the application assignment for this unit\n\n--------------------------------------------------------------------------------\n-----\n\nPause for a moment to answer this question: \n\n::: {.callout-important}\n# Question \nWhy do we need independent validation data to select the best model configuration?  In other words, why cant we just fit and evaluate all of the models in our one training set?\n:::\n\n::: {.fragment .uwred}\nThese models will all overfit the dataset within which they are fit to some degree.   \n\nIn other words, they will predict both systematic variance (the DGP) and some noise in the training set.  However, they will differ in how much they overfit the training set.   \n\nAs the models get more flexible they will have the potential to overfit to a greater degree.Models with a larger number of features (e.g., more predictors, features based on interactions as well as raw predictors) will overfit to a greater degree.  All other things equal, the non-parametric KNN will also be more flexible than the general linear model so it may overfit to a greater degree as well if the true DGP is linear on the features.  \n\nTherefore, just because a model fits the training set well does not mean it will work well in new data because the noise will be different in every new dataset. This overfitting will be removed from our performance estimate if we calculate it with new data (the validation set).\n:::\n\n\n--------------------------------------------------------------------------------\n\nLet's take a quick look at the available raw predictors in the training set\n\n```{r}\ndata_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\nRemember from our modeling EDA that we have some issues to address as part of our feature engineering:\n\n- Missing values\n- Possible transformation of `sale_price`\n- Possible transformation of other numeric predictors\n- We will need to use some feature engineering techniques to handle categorical variables\n- We may need to consider interactions among features\n\nAll of this will be accomplished with a recipe\n\nBut first, let's discuss/review our first statistical algorithm\n\n--------------------------------------------------------------------------------\n\n## The Simple (General) Linear Model (LM)\n\nWe will start with only a quick review of the use of the simple (one feature) linear model (LM) as a machine learning model because you should be very familiar with this statistical model at this point\n\n- $Y = \\beta_0 + \\beta_1*X_1 + \\epsilon$\n\nApplied to our regression problem, we might fit a model such as:\n\n- $sale\\_price = \\beta_0 + \\beta_1*gr\\_liv\\_area + \\epsilon$\n\nThe (general) linear model is a parametric model.  We need to estimate two parameters using our training set\n\n- $\\beta_0$ \n- $\\beta_1$\n\nYou already know how to do this using `lm()` in base R.  However, we will use the `tidymodels` modeling approach.\n\n--------------------------------------------------------------------------------\n\nWe use `tidymodels` because:\n\n- It provides a consistent interface to many (and growing numbers) of statistical algorithms\n- It provides very strong and easy feature engineering routines (e.g., missing data, scaling, transformations, near-zero variance, collinearity) via recipes\n- It simplifies model performance evaluation using resampling approaches (that you don't know yet!)\n- It supports numerous performance metrics\n- It is tightly integrated within the tidyverse\n- It is under active development and support\n- You can see documentation for all of the packages at the [tidymodels website](https://www.tidymodels.org/).  It is worth a quick review now to get a sense of what is available\n\n--------------------------------------------------------------------------------\n\nTo fit a model with a specific configuration, we need to: \n\n- Set up a feature engineering recipe\n- Use the recipe to make a feature matrix\n  - `prep()` it with training data\n  - `bake()` it with data you want to use to calculate feature matrix\n- Select and define the statistical algorithm\n- Fit the algorithm in the feature matrix in our training set\n\nThese steps are accomplished with functions from the [recipes](https://recipes.tidymodels.org/) and [parsnip](https://parsnip.tidymodels.org/reference/index.html) packages.\n\n--------------------------------------------------------------------------------\n\nWe will start with a simple model configuration\n\n- General linear model (lm)\n- One feature (raw `gr_liv_area`)\n- Fit in training data\n\n--------------------------------------------------------------------------------\n\nSet up a VERY SIMPLE feature engineering recipe\n\n- Include outcome on the left size of `~`\n- Include raw predictors (not yet features) on the right side of `~`.  \n- Indicate the training data\n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area, data = data_trn) \n```\n\n\n- We can see a summary of it to verify it is doing what you expect by calling\n  - `rec`\n  - `summary(rec)`\n  \n--------------------------------------------------------------------------------\n\nWe can then prep the recipe and bake the data to make our feature matrix from the training dataset\n\n- Again, remember we always prep a recipe with training data but use the prepped recipe to bake any data\n- In this instance we will prep with `data_trn` and then bake `data_trn` so that we have features from our training set to train/fit the model.\n\n```{r}\nrec_prep <- rec |> \n  prep(training = data_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- And now  we can bake the training data to make a feature matrix\n- Training features were already saved in the prepped recipe so we set `new_data = NULL`\n\n```{r}\nfeat_trn <- rec_prep |> \n  bake(new_data = NULL)\n```\n\n--------------------------------------------------------------------------------\n\nYou should always review the feature matrix to make sure it looks as you expect\n\n- includes outcome (`sale_price`)\n- includes expected feature (`gr_liv_area`)\n- Sample size is as expected\n- No missing data\n\n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\nNow let's consider the statistical algorithm\n\n- `tidymodels` breaks this apart into two pieces for clarity\n- First, you specify the broad category of algorithm\n  - e.g., `linear_reg()`, `nearest_neighbor()`, `logistic_reg()`\n- Next, you `set_mode()` to indicate if if the model is for regression or classification broadly\n  - Not needed if the engine can only be used for one mode (e.g., `'linear_reg()` is only for regression.\n- Then you select a function from a specific R package (or base R) that will implement the algorithm\n  - `tidymodels` calls this setting the engine\n  - e.g., `lm`, `kknn`, `glm`, `glmnet`\n\n--------------------------------------------------------------------------------\n\nYou can see the available engines (and modes: regression vs. classification) for the broad classes of algorithms\n\nWe will work with many of these algorithms later in the course\n```{r}\nshow_engines(\"linear_reg\")\nshow_engines(\"nearest_neighbor\")\nshow_engines(\"logistic_reg\")\nshow_engines(\"decision_tree\")\nshow_engines(\"rand_forest\")\nshow_engines(\"mlp\")\n```\n\n--------------------------------------------------------------------------------\n\nYou can load even more engines from the `discrim` package.  We will use some of these later too.  You need to load the package to use these engines.\n\n```{r}\nlibrary(discrim, exclude = \"smoothness\") # needed for these engines\n\nshow_engines(\"discrim_linear\") \nshow_engines(\"discrim_regularized\") \nshow_engines(\"naive_Bayes\")\n```\n\n--------------------------------------------------------------------------------\n\nYou can also better understand how the engine will be called using `translate()`\n\nNot useful here but will be with more complicated algorithms\n\n```{r}\nlinear_reg() |> \n  set_engine(\"lm\") |> \n  translate()\n```\n\n--------------------------------------------------------------------------------\n\nLet's combine our feature matrix with an algorithm to fit a model in our training set using only raw `gr_liv_area` as a feature\n\nNote the specification of\n\n- The category of algorithm\n- The engine (no need to set mode of engine b/c `lm` are only for the regression mode)\n- The use of the `.` to indicate all features in the matrix.\n  - not that useful here because there is only one feature: `gr_liv_area`\n  - will be useful when we have **many** features in the matrix\n- We use the the feature matrix (rather than raw data) from the training set to fit the model.\n\n```{r}\nfit_lm_1 <- \n  linear_reg() |> # <1> \n  set_engine(\"lm\") |> # <2> \n  fit(sale_price ~ ., data = feat_trn) # <3>\n```\n1. category of algorithm\n2. engine\n3. use of `.` for all features and use of feature matrix from training set\n\n--------------------------------------------------------------------------------\n\nWe can get the parameter estimates, standard errors, and statistical tests for each $\\beta$ = 0 for this model using `tidy()` from the `broom` package (loaded as part of the `tidyverse`)\n\n```{r}\nfit_lm_1 |>  tidy()\n```\n\n--------------------------------------------------------------------------------\n\nThere are a variety of ways to pull out the estimates for each feature (and intercept)\n\nOption 1: Pull all estimates from the tidy object\n```{r}\nfit_lm_1 |> \n  tidy() |> \n  pull(estimate)\n```\n\n--------------------------------------------------------------------------------\n\nOption 2: Extract a single estimate using $ and row number.  Be careful that order of features won't change!  This assumes the feature coefficient for the relevant feature is **always** the second coefficient.\n```{r}\ntidy(fit_lm_1)$estimate[[2]]\n```\n\n--------------------------------------------------------------------------------\n\nOption 3: Filter tidy df to the relevant row (using term ==) and pull the estimate. Safer!\n```{r}\nfit_lm_1 |> \n  tidy() |> \n  filter(term == \"gr_liv_area\") |> \n  pull(estimate)\n```\n\n--------------------------------------------------------------------------------\n\nOption 4: Write a function if we plan to do this a lot.  We include this function in the `fun_ml.R` script in our repo.   Better still (safe and code efficient)!\n```{r}\n#| eval: false\n\nget_estimate <- function(the_fit, the_term){\n  the_fit |> \n    tidy() |> \n    filter(term == the_term) |> \n    pull(estimate)\n}\n```\n\nand then use this function\n```{r}\nget_estimate(fit_lm_1, \"gr_liv_area\")\nget_estimate(fit_lm_1, \"(Intercept)\")\n```\n\n--------------------------------------------------------------------------------\n\nRegardless of the method, we now have a simple parametric model for `sale_price`\n```{r}\n#| include: false\n\nb0 <- round(get_estimate(fit_lm_1, \"(Intercept)\"), 1)\nb1 <- round(get_estimate(fit_lm_1, \"gr_liv_area\"), 1)\n```\n\n$\\hat{sale\\_price} = `r b0` + `r b1` * gr\\_liv\\_area$\n\n--------------------------------------------------------------------------------\n\nWe can get the predicted values for `sale_price` (i.e., $\\hat{sale\\_price}$) in our validation set using `predict()`\n\nHowever, we first need to make a feature matrix for our validation set\n\n- We use the same recipe that we previously prepped with training data (`data_trn`)\n- But now we **bake** new data that were not used to train the recipe - the validation data, `data_val`\n\n```{r}\nfeat_val <- rec_prep |> \n  bake(new_data = data_val)\n```\n\n--------------------------------------------------------------------------------\n\nAs always, we should skim these new features\n\n- Sample size matches what we expect for validation set\n- No missing data\n- Includes expected outcome and features\n\n```{r}\nfeat_val |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\nNow we can get predictions using our model with validation features\n\n`predict()` returns a dataframe with one column named `.pred` and one row for every observation in dataframe (e.g., validation feature set)\n\n```{r}\npredict(fit_lm_1, feat_val)\n```\n\n--------------------------------------------------------------------------------\n\nWe can visualize how well this model performs in the validation set by plotting predicted `sale_price` ($\\hat{sale\\_price}$) vs. `sale_price` (**ground truth** in machine learning terminology) for these data\n\nWe might do this a lot so let's write a function.  We have also included this function in `fun_ml.R`\n\n```{r}\n#| eval: false\n\nplot_truth <- function(truth, estimate) {\n  ggplot(mapping = aes(x = truth, y = estimate)) + \n    geom_abline(lty = 2, color = \"red\") + \n    geom_point(alpha = 0.5) + \n    labs(y = \"predicted outcome\", x = \"outcome\") +\n    coord_obs_pred()   # scale axes uniformly (from tune package)\n}\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_1, feat_val)$.pred)\n```\n\nPerfect performance would have all the points right on the dotted line (same value for actual and predicted outcome)\n\n- Our model doesn't do that well yet.  Not surprising\n- Pattern also has some indication of fanning of residuals AND some non-linearity with higher outcome scores that suggests need for a power transformation of outcome (e.g., log)\n- This is consistent with our earlier modeling EDA\n- Perhaps not that bad here b/c both `sale_price` and `gr_liv_area` were positively skewed\n- We will need consider this eventually\n\n--------------------------------------------------------------------------------\n\nWe can quantify model performance by selecting a performance metric\n\n- The `yardstick` package within the `tidymodels` framework supports calculation of many performance metrics for regression and classification models\n- See the [list](https://yardstick.tidymodels.org/reference/index.html) of all currently available metrics\n\nRoot mean square error (RMSE) is a common performance metric for regression models\n\n- You focused on a related metric, sum of squared error (SSE), in PSY 610/710\n- RMSE simply divides SSE by N (to get mean squared error; MSE) and then takes the square root to return the metric to the original units for the outcome variable\n- It is easy to calculate using `rmse_vec()` from the `yardstick` package\n\n```{r}\nrmse_vec(truth = feat_val$sale_price, \n         estimate = predict(fit_lm_1, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\nLet's record how well this model performed in validation so we can compare it to subsequent models\n\n\n```{r}\nerror_val <- bind_rows(error_val, \n                       tibble(model = \"simple linear model\", \n                              rmse_val = rmse_vec(truth = feat_val$sale_price, \n                                                  estimate = predict(fit_lm_1,\n                                                                     feat_val)$.pred)))\nerror_val\n```\n\nNOTE: I will continue to bind RMSE to this dataframe for newer models but plan to hide this code chuck to avoid distractions.  You can reuse this code repeatedly to track your own models if you like.  (Perhaps we should write a function??)\n\n--------------------------------------------------------------------------------\n\nFor explanatory purposes, we might want to visualize the relationship between a feature and the outcome (in addition to examining the parameter estimates and the associated statistical tests)\n\n- Here is a plot of $\\hat{sale\\_price}$ by `gr_liv_area` superimposed over a scatterplot of the raw data from the validation set\n\n```{r}\nfeat_val |> \n  ggplot(aes(x = gr_liv_area)) +\n    geom_point(aes(y = sale_price), color = \"gray\") +\n    geom_line(aes(y = predict(fit_lm_1, data_val)$.pred), \n              linewidth = 1.25, color = \"blue\") + \n    ggtitle(\"Validation Set\")\n```\n\n- As expected, there is a moderately strong positive relationship between `gr_liv_area` and `sale_price`.  \n- We can also again see the heteroscadasticity in the errors that might be corrected by a power transformation of `sale_price` (or `gr_liv_area`)  \n\n--------------------------------------------------------------------------------\n\n## Extension of LM to Multiple Predictors\n\nWe can improve model performance by moving from simple linear model to a linear model with multiple features derived from multiple predictors\n\nWe have many other numeric variables available to use, even in this pared down version of the dataset.\n\n```{r}\ndata_trn |>  names()\n```\n\nLet's expand our model to also include `lot_area`, `year_built`, and `garage_cars`\n\nAgain, we need:\n\n- A feature engineering recipe\n- Training (and eventually validation) feature matrices\n- An algorithm to fit in training feature matrix\n\n--------------------------------------------------------------------------------\n\nWith the addition of new predictors, we now have a feature engineering task\n\n- We have missing data on `garage_cars` in the training set\n- We need to decide how we will handle it\n\nA simple solution is to do median imputation - substitute the median of the non-missing scores for any missing score.\n\n- This is fast and easy to understand\n- It works OK (but there are certainly better options that we will consider later in the course)\n  - see other options in [Step Functions - Imputation](https://recipes.tidymodels.org/reference/index.html) section on `tidymodels` website\n- There is only one missing value so it likely doesn't matter much anyway\n\n--------------------------------------------------------------------------------\n\nLet's add this to our recipe. All of the defaults are appropriate but you should see `?step_impute_median()` to review them\n\n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars, # <1>\n         data = data_trn) |> \n  step_impute_median(garage_cars)\n```\n1. Notice we now list four predictors for our recipe using `+` between them\n\n--------------------------------------------------------------------------------\n\nNow we need to \n\n- First prep recipe\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)  # <1>\n```\n1. I am going to stop using `training =` at this point.  Remember, we prep recipes with training data.\n\n--------------------------------------------------------------------------------\n\n- Next, bake the training data with prepped recipe to get training features\n\n```{r}\nfeat_trn <- rec_prep |> \n  bake(NULL) # <1>\n```\n1. I am going to stop using `new_data = ` but remember, we use NULL if we want the previously saved training features.  If instead, we want features for new data, we provide that dataset.\n\n--------------------------------------------------------------------------------\n\n- And take a quick look at the features\n  - Sample size is correct\n  - 4 features and the outcome variable\n  - All features are numeric\n  - No missing data for `garage_qual`\n  \n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- And finally, bake the validation data with the same prepped recipe to get validation features\n```{r}\nfeat_val <- rec_prep |> \n  bake(data_val) # <1>\n```\n1. Notice that here we are now baking `data_val`\n\n--------------------------------------------------------------------------------\n\n- And take a quick look \n  - Correct sample size (N = 490)\n  - 4 features and outcome\n  - All numeric\n  - No missing data\n  \n```{r}\nfeat_val |> skim_all()\n```\n\n-------------------------------------------------------------------------------- \n\nNow let's combine our algorithm and training features to fit this model configuration with 4 features\n```{r}\nfit_lm_4 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~ ., data = feat_trn) # <1>\n```\n1. the `.` is a bit more useful now\n\n--------------------------------------------------------------------------------\n\nThis yields these parameter estimates (which as we know from 610/710 were selected to minimize SSE in the training set):\n\n```{r}\nfit_lm_4 |> tidy()\n```\n\n--------------------------------------------------------------------------------\n\nHere is our parametric model\n```{r}\n#| include: false\nb0 <- round(get_estimate(fit_lm_4, \"(Intercept)\"), 1)\nb1 <- round(get_estimate(fit_lm_4, \"gr_liv_area\"), 1)\nb2 <- round(get_estimate(fit_lm_4, \"lot_area\"), 1)\nb3 <- round(get_estimate(fit_lm_4, \"year_built\"), 1)\nb4 <- round(get_estimate(fit_lm_4, \"garage_cars\"), 1)\n```\n\n- $\\hat{sale\\_price} = `r b0` + `r  b1` * gr\\_liv\\_area + `r  b2` * lot\\_area + `r  b3` * year\\_built + `r  b4` * garage\\_cars$\n\nCompared with our previous simple regression model:\n```{r}\n#| include: false\nb0 <- round(get_estimate(fit_lm_1, \"(Intercept)\"), 1)\nb1 <- round(get_estimate(fit_lm_1, \"gr_liv_area\"), 1)\n```\n\n- $\\hat{sale\\_price} = `r b0` + `r b1` * gr\\_liv\\_area$\n\n--------------------------------------------------------------------------------\n\nOf course, these four features are correlated both with `sale_price` but also with each other\n\nLet's look at correlations in the training set.  \n\n```{r}\nfeat_trn |> \n  cor() |> \n  corrplot::corrplot.mixed()\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\nWhat are the implications of the correlations among many of these predictors?\n:::\n\n::: {.fragment .uwred}\nThe multiple regression model coefficients represent unique effects, controlling for all other variables in the model.  You can see how the unique effect of `gr_liv_area` is smaller than its overall effect from the simple regression.   This also means that the overall predictive strength of the model will not be a sum of the effects of each predictor considered in isolation - it will likely be less.  \n\nAlso, if the correlations are high, problems with multicollinearity will emerge.  This will yield large standard errors which means that the models will start to have more variance when fit in different training datasets!  We will soon learn about other regularized versions of the GLM that do not have these issues with correlated predictors.\n:::\n\n--------------------------------------------------------------------------------\n\nHow well does this more complex model perform in validation?   Let's compare the previous and current visualizations of $sale\\_price$ vs. $\\hat{sale\\_price}$\n\n- Looks like the errors are smaller (closer to the diagonal line that would represent prefect prediction)\n- Clear signs of non-linearity are now present as well.  Time for more Modeling EDA!!\n\n```{r}\nplot_1 <- plot_truth(truth = feat_val$sale_price, \n                     estimate = predict(fit_lm_1, feat_val)$.pred)\n\nplot_4 <- plot_truth(truth = feat_val$sale_price, \n                     estimate = predict(fit_lm_4, feat_val)$.pred)\n\ncowplot::plot_grid(plot_1, plot_4, \n          labels = list(\"1 feature\", \"4 features\"), hjust = -1.5)\n```\n\n::: {.callout-tip}\n# Coding sidebar\nNotice the use of `plot_grid()` from the `cowplot` package to make side by side plots.   This also required returning the individual plots as objects (just assign to a object name, e.g., plot_1)\n:::\n\n--------------------------------------------------------------------------------\n\nLet's compare model performance for the two models using RMSE in the validation set\n\n- The one feature simple linear model\n```{r}\nrmse_vec(feat_val$sale_price, \n         predict(fit_lm_1, feat_val)$.pred)\n```\n\n- The four feature linear model.  A clear improvement!\n```{r}\nrmse_vec(feat_val$sale_price, \n         predict(fit_lm_4, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- Let's bind the new performance metric to our results table\n```{r}\n#| echo: false\n\nerror_val <- \n  bind_rows(error_val, \n            tibble(model = \"4 feature linear model\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_lm_4, feat_val)$.pred)))\n\nerror_val\n```\n\n-------------------------------------------------------------------------------- \n\nGiven the non-linearity suggested by the truth vs. estimate plots, we might wonder if we could improve the fit if we transformed our features to be closer to normal\n\n- There are a number of recipe functions that do transformations (see [Step Functions - Individual Transformations](https://recipes.tidymodels.org/reference/index.html))\n\n- We will apply `step_YeoJohnson()`, which is similar to a Box-Cox transformation but can be more broadly applied because the scores don't need to be strictly positive\n\n--------------------------------------------------------------------------------\n\nLet's do it all again, now with transformed features!\n\n- Define the feature engineering recipe\n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars, \n         data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_YeoJohnson(lot_area, gr_liv_area, year_built, garage_cars)\n```\n\n--------------------------------------------------------------------------------\n\n- Prep the recipe with training set\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n```\n\n- Use prepped recipe to bake the training set into features\n\n- Notice the features are now less skewed (but `sale_price` is still skewed)\n```{r}\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Use same prepped recipe to bake the validation set into features\n\n- Again, features are less skewed\n```{r}\nfeat_val <- rec_prep |> \n  bake(data_val)\n\nfeat_val |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Fit model\n```{r}\nfit_lm_4yj <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- View truth vs. estimate plot\n```{r}\nplot_truth(truth = feat_val$sale_price,\n           estimate = predict(fit_lm_4yj, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- and look at the error\n```{r}\n#| echo: false\n\nerror_val <- bind_rows(error_val, \n                        tibble(model = \"4 feature linear model with YJ\", \n                               rmse_val = rmse_vec(feat_val$sale_price, \n                                                   predict(fit_lm_4yj,\n                                                           feat_val)$.pred)))\n\nerror_val\n```\n\n- That didn't help at all.   Error still high and still non-linearity in plot.\n\n--------------------------------------------------------------------------------\n\nWe may need to consider \n\n- a transformation of `sale_price` (We will leave that to you for the application assignment if you are daring!)\n- or a different algorithm that can handle non-linear relationships better\n\n--------------------------------------------------------------------------------\n\n## Extension to Categorical Predictors\n\nMany important predictors in our models may be categorical (nominal and some ordinal predictors)\n\n- Some statistical algorithms (e.g., random forest) can accept even nominal predictors as features without any further feature engineering\n- But many cannot.  Linear models cannot.\n- The type of feature engineering may differ for nominal vs. ordinal categorical predictors\n- For **nominal categorical predictors**:\n  - We need to learn a common approach to transform them to numeric features - dummy coding.  We will learn the concept in general AND how to accomplish within a feature engineering recipe.\n- For **ordinal predictors**:\n  - We can treat them like numeric predictors\n  - We can treat them like nominal categorical predictors\n- See article on [Categorical Predictors](https://recipes.tidymodels.org/articles/Dummies.html) on the `tidymodels` website for more details\n\n--------------------------------------------------------------------------------\n\n### Dummy Coding\n\nFor many algorithms, we will need to use feature engineering to convert a categorical predictor to numeric features.  One common technique is to use dummy coding.  When dummy coding a predictor, we transform the original categorical predictor with m levels into m-1 dummy coded features.\n\nTo better understand how and why we do this, lets consider a version of `ms_zoning` in the Ames dataset.   \n\n```{r}\ndata_trn |> \n  pull(ms_zoning) |> \n  table()\n```\n\nWe will recode `ms_zoning` to have only 3 levels to make our example simple (though dummy codes can be used for predictors with any number of levels)\n\n```{r}\ndata_dummy <- data_trn |> \n  select(sale_price, ms_zoning)  |> # <1>\n  mutate(ms_zoning3 = fct_collapse(ms_zoning, # <2>\n                                 \"residential\" = c(\"res_high\", \"res_med\",\n                                                   \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\")) |> # <3>\n  select(-ms_zoning) # <4>\n```\n1. Make a df (dataframe) with only `sale_price` and `ms_zoning`\n2. `fct_collapse()` from the `forcats` package is our preferred way to collapse levels of a factor.  See `fct_recode()` for more generic recoding of levels.\n3. We could have left this line out and float would have stayed as a level named float\n4. Remove original `ms_zoning` predictor\n\n--------------------------------------------------------------------------------\n\nTake a look at the new predictor\n```{r}\ndata_dummy |> \n  pull(ms_zoning3) |> \n  table() \n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\nWhy can't we simply recode each level with a different consecutive value (e.g., commercial = 1, floating =2 , residential = 3)?\n:::\n\n::: {.fragment .uwred}\nThere is no meaningful way to order the numbers that we assign to the levels of  this unordered categorical predictor. The shape and strength of the relationship between it and sale_price will completely change based on arbitrary ordering of the levels.\n:::\n\n--------------------------------------------------------------------------------\n\nImagine fitting a straight line to predict `sale_price` from `ms_zoning3` using these three different ways to arbitrarily assign numbers to levels.\n```{r}\ndata_dummy |> \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 1,\n                                ms_zoning3 == \"commercial\" ~ 2,\n                                ms_zoning3 == \"floating\" ~ 3)) |> \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n```\n\n```{r}\ndata_dummy |> \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 2,\n                                ms_zoning3 == \"commercial\" ~ 1,\n                                ms_zoning3 == \"floating\" ~ 3)) |> \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n```\n\n```{r}\ndata_dummy |> \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 3,\n                                ms_zoning3 == \"commercial\" ~ 1,\n                                ms_zoning3 == \"floating\" ~ 2)) |> \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n```\n\n--------------------------------------------------------------------------------\n\nDummy coding resolves this issue.\n\n- When using dummy codes, we transform (i.e., feature engineer) our original m-level categorical predictor to m-1 dummy features.\n- Each of these m-1 features represents a contrast between a specific level of the categorical variable and a **reference level** \n- The full set of m-1 features represents the overall effect of the categorical predictor variable.\n- We assign values of 0 or 1 to each observation on each feature in a meaningful pattern (see below)\n\n--------------------------------------------------------------------------------\n\nFor example, with our three-level predictor: `ms_zoning3`\n\n- We need 2 dummmy features (d1, d2) to represent this 3-level categorical predictor\n- Dummy feature 1 is coded 1 for residential and 0 for all other levels\n- Dummy feature 2 is coded 1 for floating and 0 for all other levels\n\n\nHere is this coding scheme displayed in a table\n\n```{r}\n#| echo: false\ntibble(ms_zoning3 = c(\"commercial\", \"residential\", \"floating\"),\n       d1 = c(0,1,0),\n       d2 = c(0,0,1)) |> \n  print()\n```\n\n-------------------------------------------------------------------------------- \n\nWith this coding:\n\n- Commercial properties are coded 0 for both d1 and d2.  \n- This means that commercial properties will become the reference level against which both residential and floating village are compared.\n- Because we are focused on prediction, the choice of reference level is mostly arbitrary.  For explanatory goals, you might consider which level is best suited to be the reference.\n- There is much deeper coverage of dummy and other contrast coding in 610/710\n\n--------------------------------------------------------------------------------\n\nWe can add these two features manually to the data frame and view a handful of observations to make this coding scheme more concrete  \n\n```{r}\n#| echo: false\n\ndata_dummy <- data_dummy |> \n  mutate(d1 = if_else(ms_zoning3 == \"residential\", 1, 0),\n         d2 = if_else(ms_zoning3 == \"floating\", 1, 0))\n\n# slice a few example observations\ndata_dummy |> \n  slice(c(1, 2, 199, 3, 377, 200, 4, 376)) |> \n  print()\n```\n\n-------------------------------------------------------------------------------- \n\nIf we now fit a model where we predict `sale_price` from these two dummy coded features, each feature would represent the contrast of the mean `sale_price` for the level coded 1 vs. the mean `sale_price` for the level that is coded 0 for all features (i.e., commercial)\n\n- d1 is the contrast of mean `sale_price` for residential vs. commercial\n- d2 is the contrast of mean `sale_price` for floating vs. commercial\n- The combined effect of these two features represents the overall effect of `ms_zoning3` on `sale_price`\n\n--------------------------------------------------------------------------------\n\nLets do this quickly in base R using `lm()` as you have done previously in 610.\n```{r}\nm <- lm(sale_price ~ d1 + d2, data = data_dummy) \n\nm |> summary()\n```\n\n- The mean sale price of residential properties is `r round(round(m$coefficients[\"d1\"]))` dollars higher than commercial properties.\n\n- The mean sale price of floating villages is `r round((m$coefficients[\"d2\"]))` dollars higher than commercial properties. \n\n--------------------------------------------------------------------------------\n\nTo understand this conceptually, it is easiest to visualize the linear model that would predict `sale_price` with these two dichotomous features.\n\n- There are only three columns of `sale_price` because the only possible values for `d1` and `d2` (which are both dichotomous) are \n  - 0,0 (commercial)\n  - 1,0 (residential)\n  - 0,1 (floating village)\n- This regression with two features yields a prediction plane (displayed)\n- The left/right tilt of the plane will be the parameter estimate for d1 and it is the contrast of residential vs. commercial\n- The front/back tilt of the plane will be the parameter estimate for d2 and it is the contrast of floating village vs. commercial\n\n```{r}\n#| warning: false\n#| echo: false\n\nd <- data_dummy |> \n  slice_sample(n = 500) |>   # select a subset for display purposes\n  select(d1, d2, sale_price)\n\nplot_3d <- d |> \n  scatterplot3d::scatterplot3d(angle=55, label.tick.marks = FALSE, \n                               color = \"red\")\nplot_3d$plane3d(lm(sale_price ~ d1 + d2, data = d))\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Statistical sidebar\n\n- Any full rank (# levels - 1) set of features regardless of coding system predicts exactly the same (e.g., dummy, helmert, contrast coding)\n- Preference among coding systems is simply to get single df contrasts of theoretical importance (i.e., for explanation rather than prediction)\n- Final (m^th^) dummy feature is not included b/c its is completely redundant (perfectly multicollinear) with other dummy features. This would also prevent a linear model from fitting ('**dummy variable trap**').\n- However, some statistical algorithms do not have problems with perfect multicollinearity (e.g., LASSO, ridge regression).  \n  - For these algorithms, you will sometimes see modified version of dummy coding called [one-hot coding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).  \n  - This approach uses one additional dummy coded feature for the final category.  \n  - We won't spend time on this but you should be familiar with the term b/c it is often confused with dummy coding.\n::: \n\n::: {.callout-tip}\n# Coding Sidebar\n\nWhen creating dummy coded features from factors that have levels with infrequent observations, you may occasionally end up with novel levels in your validation or test sets that were not present in your training set.  \n\n- This will cause you issues.  \n- These issues are mostly resolved if you make sure to explicitly list all possible levels for a factor when classing that factor in the training data, even if the level doesn't exist in the training data.   \n- We provide more detail on this issue in an [appendix](https://jjcurtin.github.io/book_iaml/app_dummy_coding.html).\n:::\n\n--------------------------------------------------------------------------------\n\n###  Nominal Predictors\n\nNow that we understand how to use dummy coding to feature engineer nominal predictors, let's consider some potentially important ones that are available to us.\n\nWe can discuss if any look promising.\n\n--------------------------------------------------------------------------------\n\nLets return first to `ms_zoning`\n\n\n```{r}\n#| out-height: 3in\n\ndata_trn |> \n  plot_categorical(\"ms_zoning\", \"sale_price\") |> \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n```\n\t\nWe might: \n\n- Represent it with 6 dummy features (because there are 7 raw levels) but many of the categories are very low n - won't account for much variance?\n- Combine all the commercial categories (agri, commer, indus), which would take care of most of the low n groups.  They also all tend to have the lower prices.\n-  Combine all the residential to get a better feature to variance accounted ratio.  They all tend to have similar prices on average and res_high is also pretty low n. \n\nData dictionary entry: Identifies the general zoning classification of the sale.\n\n- agri:\tAgriculture\n- commer:\tCommercial\n- float:\tFloating Village Residential\n- indus:\tIndustrial\n- res_high:\tResidential High Density\n- res_med:\tResidential Medium Density\n- res_low:\tResidential Low Density\n\n--------------------------------------------------------------------------------\n\n`lot_config`\n\n```{r}\n#| out-height: 3in\n\ndata_trn |> \n  plot_categorical(\"lot_config\", \"sale_price\") |> \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n```\n\n\nWe see that: \n\n- Most are inside lots, some of the lot categories are low n\n- Median `sale_price` is not very different between configurations\n- Not very promising but could help some (particularly given the large sample size)\n\nData dictionary entry: Lot configuration\n\n- inside:\tInside lot\n- corner:\tCorner lot\n- culdsac:\tCul-de-sac\n- fr2:\tFrontage on 2 sides of property\n- fr3:\tFrontage on 3 sides of property\n\n--------------------------------------------------------------------------------\n\n`bldg_type`\n\n```{r}\n#| out-height: 3in\n\ndata_trn |> \n  plot_categorical(\"bldg_type\", \"sale_price\") |> \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n```\n\nWe see that:\n\n- Most of the houses are in one category - one_fam\n- There is not much difference in median `sale_price` among categories\n- Not very promising\n\n\nData dictionary entry: Type of dwelling\n\t\t\n- one_fam:\tSingle-family Detached\t\n- two_fam:\tTwo-family Conversion; originally built as one-family dwelling\n- duplex: Duplex\n- town_end:\tTownhouse End Unit\n- town_inside:\tTownhouse Inside Unit\n\n--------------------------------------------------------------------------------\n\nLet's do some feature engineering with `ms_zoning`.  We can now do this formally in a recipe so that it can be used in our modeling workflow.\n\n- First, if you noticed earlier, there are some levels for `ms_zoning` that are pretty infrequent.  Lets make sure both `data_trn` and `data_val` have all levels set for this factor.\n\n```{r}\ndata_trn |> pull(ms_zoning) |> levels()\ndata_val |> pull(ms_zoning) |> levels()\n```\n\n- As expected, we are missing a level (`agri`) in `data_val`.  Lets fix that here \n\n```{r}\ndata_val <- data_val |> \n  mutate(ms_zoning = factor(ms_zoning, \n                            levels = c(\"agri\", \"commer\", \"float\", \"indus\", \n                                       \"res_high\", \"res_low\", \"res_med\")))\n```\n\n[Note:  Ideally, you would go back to cleaning EDA and add this level to the full dataset and then re-split into training, validation and test.  This is a sloppy shortcut!]\n\n--------------------------------------------------------------------------------\n\nWith that fixed, let's proceed:\n\n- We will collapse categories down to three levels (commercial, residential, floating village) as before but now using `step_mutate()` combined with `fct_collapse()` to do this inside of our recipe.  \n- We will convert to dummy features using `step_dummy()`.  The first level of the factor will be set to the reference level when we call `step_dummy()`. \n- `step_dummy()` is a poor choice for function name.  It actually uses whatever contrast coding we have set up in R.  However, the default is are dummy coded contrasts (R calls this treatment contrasts). See `?contrasts` and `options(\"contrasts\")` for more info.\n  \n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + ms_zoning, \n         data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\")) |>\n  step_dummy(ms_zoning)\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding Sidebar\n\nYou should also read more about some other `step_()` functions that you might use for categorical predictors:\n- `step_other()` to combine all low frequency categories into a single \"other\" category.\n- `step_unknown()` to assign missing values their own category\n- You can use selector functions.  For example, you could make dummy variables out of all of your factors in one step using `step_dummy(all_nominal_predictors())`.  \n\nSee the [Step Functions - Dummy Variables and Encoding section](https://recipes.tidymodels.org/reference/index.html) on the `tidymodels` website for additional useful functions.\n\nWe have also described these in the section on factor steps in Appendix 1\n:::\n\n--------------------------------------------------------------------------------\n\nLet's see if the addition of `ms_zoning` helped\n\n- Notice the addition of the dummy coded features to the feature matrix\n- Notice the removal of the factor `ms_zoning`\n\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- skim\n\n```{r}\nfeat_trn |> skim_all()\n\nfeat_val |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Now lets fit a model with these features\n\n```{r}\nfit_lm_6 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- plot it\n\n```{r}\n#| out-height: 3in\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_6, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- And evaluate it\n\n```{r}\n#| echo: false\n\nerror_val <- error_val |> \n  bind_rows(tibble(model = \"6 feature linear model w/ms_zoning\", \n                   rmse_val = rmse_vec(feat_val$sale_price,\n                                       predict(fit_lm_6,\n                                               feat_val)$.pred)))\n\nerror_val\n```\n\n- Removing Yeo Johnson transformation but adding dummy coded `ms_zoning` may have helped a little\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\nWill the addition of new predictors/features to a model always reduce RMSE in train?  in validation?\n:::\n\n::: {.fragment .uwred}\nAs you know, the estimation procedure in linear models is OLS.  Parameter estimates are derived to minimize the SSE in the data set in which they are derived.  For this reason, adding a predictor will never increase RMSE in the training set and it will usually lower it even when it is not part of the DGP.   \n\nHowever, this is not true in validation.  A predictor will only meaningfully lower RMSE in validation if it is part of the DGP.  Also, a bad predictor could even increase RMSE in validation due tooverfitting.\n:::\n\n--------------------------------------------------------------------------------\n\n### Ordinal Predictors\n\nWe have two paths to pursue for ordinal predictors\n\n- We can treat them like nominal predictors (e.g., dummy code)\n- We can treat them like numeric predictors (either raw or with an added transformation if needed)\n\n--------------------------------------------------------------------------------\n\nLet's consider `overall_qual`\n\n```{r}\n#| out-height: 3in\n\ndata_trn |> \n  plot_categorical(\"overall_qual\", \"sale_price\") |> \n  cowplot::plot_grid(plotlist = _, ncol = 2)\n```\n\nObservations:\n\n- Low frequency for low and to some degree high quality response options.  If dummy coding, may want to collapse some (1-2)\n- There is a monotonic relationship (mostly linear) with `sale_price`.   Treat as numeric?\n- Not skewed so doesn't likely need to be transformed if treated as numeric\n- Numeric will take one feature vs. many (9?) features for dummy codes.  \n- Dummy codes are more flexible but we may not need this flexibility (and unnecessary flexibility increases overfitting)\n\n--------------------------------------------------------------------------------\n\nLet's add `overall_qual` to our model as numeric\n\nRemember that this predictor was ordinal so we paid special attention to the order of the levels when we classed this factor.  Lets confirm they are in order\n\n```{r}\ndata_trn |> pull(overall_qual) |> levels()\n```\n\n--------------------------------------------------------------------------------\n\nTo convert `overall_qual` to numeric (with levels in the specified order), we can use another simple mutate inside our recipe.\n\n```{r}\nrec <- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |>\n  step_dummy(ms_zoning)\n```\n\n::: {.callout-tip}\n# Coding Sidebar\nThere is a step function called `step_ordinalscore()` but it requires that the factor is classed as an ordered factor.  It is also more complicated than needed in our opinion.  Just use `as.numeric()`\n:::\n\n--------------------------------------------------------------------------------\n\nLet's evaluate this model\n\n- Making features\n- Skipping the skim to save space (we promised we checked it previously!)\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- Fitting model\n\n```{r}\nfit_lm_7 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- Plotting results\n\n```{r}\nplot_truth(truth = feat_val$sale_price, \n                                 estimate = predict(fit_lm_7, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- Quantifying held out error\n\n```{r}\n#| echo: false\n\nerror_val <- bind_rows(error_val, \n                        tibble(model = \"7 feature linear model\", \n                               rmse_val = rmse_vec(feat_val$sale_price,\n                                                   predict(fit_lm_7,\n                                                           feat_val)$.pred)))\n\nerror_val\n```\n\n- That helped!\n\n--------------------------------------------------------------------------------\n\n## Extensions to Interactive Models and Non-linear Models\n\n\n### Interactions \n\nThere may be interactive effects among our predictors\n\n- Some statistical algorithms (e.g., KNN) can naturally accommodate interactive effects without any feature engineering\n- Linear models cannot\n- Nothing to fear, tidymodels makes it easy to feature engineer interactions\n- [BUT - as we will learn, we generally think that if you expect lots of interactions, the linear model may not be the best model to use]\n\n--------------------------------------------------------------------------------\n\nFor example, it may be that the relationship between `year_built` and `sale_price` depends on `overall_qual`.\n\n- Old houses are expensive if they are in good condition \n- but old houses are very cheap if they are in poor condition\n\n--------------------------------------------------------------------------------\n\nIn the `tidymodels` framework\n\n- Coding interactions is done by feature engineering, not by formula (Note that formula does not change below in recipe)\n- This seems appropriate to us as we are making new features to represent interactions\n- We still use an R formula like interface to specify the interaction term features that will be created\n- see [more details](https://recipes.tidymodels.org/reference/step_interact.html) on the `tidymodels` website\n\n```{r}\nrec <- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |>\n  step_dummy(ms_zoning) |> \n  step_interact(~ overall_qual:year_built)\n```\n\n--------------------------------------------------------------------------------\n\nLet's prep, bake, fit, and evaluate! \n\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- Note the new interaction term (we just skim `feat_trn` here)\n- Named using \"_x_\" to specify the interaction\n\n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- fit model\n\n```{r}\nfit_lm_8 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~., \n      data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- plot\n\n```{r}\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_8, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- calculate held out error\n\n```{r}\n#| echo: false\n\nerror_val <- bind_rows(error_val, \n                      tibble(model = \"8 feature linear model w/interaction\", \n                             rmse_val = rmse_vec(feat_val$sale_price,\n                                                 predict(fit_lm_8,\n                                                         feat_val)$.pred)))\n\nerror_val\n```\n\n\n- That helped!\n\n--------------------------------------------------------------------------------\n\nYou can also feature engineer interactions with nominal (and ordinal predictors treated as nominal) predictors\n\n- The nominal predictors should first be converted to dummy code features\n- You will indicate the interactions using the variable names that will be assigned to these dummy code features\n- Use `starts_with()` or `matches()` to make it easy if there are many features associated with a categorical predictor\n- Can use \"~ .^2\" to include all two way interactions (be careful if you have dummy coded features!)\n\n--------------------------------------------------------------------------------\n\nLet's code an interaction between `ms_zoning` & `year_built`.  \n\n- Old homes are cool\n- Old commercial spaces are never cool\n- Maybe this is why the main effect of `ms_zoning` wasn't useful\n\n```{r}\nrec <- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |>\n  step_dummy(ms_zoning) |> \n  step_interact(~ overall_qual:year_built) |> \n  step_interact(~ starts_with(\"ms_zoning_\"):year_built)  \n```\n\n--------------------------------------------------------------------------------\n\n- prep, bake\n\n```{r}\n#| warning: false\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- Yup, we have two new interaction features as expected\n\n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Fit model\n\n```{r}\nfit_lm_10 <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- Plot\n\n```{r}\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_10, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n- Quantify held out error\n\n```{r}\nerror_val <- error_val |> \n  bind_rows(tibble(model = \"10 feature linear model w/interactions\", \n                   rmse_val = rmse_vec(feat_val$sale_price,\n                                       predict(fit_lm_10,\n                                               feat_val)$.pred)))\n\nerror_val\n```\n\n- Not really any better \n- Shouldn't just include all interactions without reason\n  - Either you have done EDA to support them or\n  - You have substantive interest in them (explanatory question)\n  - If you want all interactions, use a statistical algorithm that supports those relationships without feature engineering (e.g., KNN, random forest and other decision trees)\n  \n--------------------------------------------------------------------------------\n\n### Non-linear Models\n \nWe may also want to model non-linear effects of our predictors\n\n- Some non-parametric models can accommodate non-linear effects without feature engineering (e.g., KNN, Random Forest).\n- Non-linear effects can be accommodated in a linear model with feature engineering\n  - Transformations of Y or X.  See [Step Functions - Individual Transformations](https://recipes.tidymodels.org/reference/index.html) on `tidymodels` website\n  - Ordinal predictors can be coded with dummy variables\n  - Numeric predictors can be split at threshold\n  - Polynomial contrasts for numeric or ordinal predictors (see `step_poly()`)  \n  \n\n- We will continue to explore these options throughout the course\n\n--------------------------------------------------------------------------------\n\n## KNN Regression\n\nK Nearest Neighbor\n\n- Is a non-parametric regression and classification statistical algorithm\n  - It does not yield specific parameter estimates for features/predictors (or statistical tests for those parameter estimates)\n  - There are still ways to use it to address explanatory questions (visualizations, model comparisons, feature importance)\n\n- Very simple but also powerful (listed commonly among top 10 algorithms)\n  - By powerful, it is quite flexible and can accommodate many varied DGPs without the need for much feature engineering with its predictors\n  - May not need most transformations of X or Y\n  - May not need to model interactions\n  - Still need to handle missing data, outliers, and categorical predictors\n\n--------------------------------------------------------------------------------\n\nK Nearest Neighbor\n\n- Algorithm \"memorizes\" the training set (**lazy learning**)\n  - Lazy learning is most useful for large, continuously changing datasets with few attributes (features) that are commonly queried (e.g., online recommendation systems)\n\n- Prediction for any new observation is based on $k$ most similar observations from the dataset\n  - $k$ provides direct control  over the bias-variance trade-off for this algorithm\n  \n--------------------------------------------------------------------------------\n\nTo better understand KNN let's simulate training data for three different DGPs (linear - y, polynomial - y2, and step - y3)\n\n```{r}\n#| echo: false\n\nset.seed(55231)\ndata_trn_demo <- tibble(x = runif(150, 1, 100),\n               y = rnorm(150, x, 10),\n               y_dgp = rnorm(150, x ,0),\n               y2 = rnorm(150, x^4 / 800000, 8),\n               y2_dgp = rnorm(150, x^4 / 800000 ,0),\n               y3 = if_else(x < 40, rnorm(150, 25, 10),  rnorm(150, 75, 10)),\n               y3_dgp = if_else(x < 40, rnorm(150, 25, 0),  rnorm(150, 75, 0)))\n```\n\n--------------------------------------------------------------------------------\n\nLet's start with a simple example where the DGP for Y is linear on one predictor (X)\n\nDGP: $y = rnorm(150, x, 10)$\n\nThis figure displays:\n\n - DGP\n - Prediction line from a simple linear model\n - Red lines to represent three new observations (X = 10, 50, 90) we want to make predictions for via a standard KNN\n\n\n```{r}\n#| echo: false\n\ndata_trn_demo |> \n  ggplot(aes(x = x, y = y)) +\n    geom_line(aes(x = x, y = y_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_smooth(aes(color = \"green\"), method='lm',formula=y ~ x,  se = FALSE) +\n    geom_vline(xintercept = 10, color = \"red\", linewidth = 1.5)+\n    geom_vline(xintercept = 50, color = \"red\", linewidth = 1.5)+\n    geom_vline(xintercept = 90, color = \"red\", linewidth = 1.5) +\n    geom_point(aes(y = y), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"green\"),\n                       labels = c(\"DGP\", \"linear model\"),\n                       guide = \"legend\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\nWhat would 5-NN predictions look like for each of these three new values of X in the figure above?\n:::\n\n::: {.fragment .uwred}\nFor x = 10, find the five observations that have X values closest to 10.  Average the Y values for those 5 observations and that is your predicted Y associated with that new value of X.  \n\nRepeat to make predictions for Y for any other value of X,e.g., 50, 90, or any other value\n:::\n\n--------------------------------------------------------------------------------\n\nKNN can easily accommodate non-linear relationships between numeric predictors and outcomes without any feature engineering for predictors\n\nIn fact, it can flexibly handle **any** shape of relationship\n\nDGP: $y2 = rnorm(150, x^4 / 800000, 8)$\n\n```{r}\n#| echo: false\n\ndata_trn_demo |> \n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), size = 1.5) +\n    geom_smooth(aes(color = \"green\"), method='lm',formula=y ~ x,  se = FALSE) +\n    geom_vline(xintercept = 10, color = \"red\", size = 1.5)+\n    geom_vline(xintercept = 50, color = \"red\", size = 1.5)+\n    geom_vline(xintercept = 90, color = \"red\", size = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"green\"),\n                       labels = c(\"DGP\", \"linear model\"),\n                       guide = \"legend\")\n```\n\n--------------------------------------------------------------------------------\n\nDGP: $y3 = if\\_else(x < 40, rnorm(150, 25, 10),  rnorm(150, 75, 10))$\n\n```{r}\n#| echo: false\n\ndata_trn_demo |> \n  ggplot(aes(x = x, y = y3)) +\n    geom_line(aes(x = x, y = y3_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_smooth(aes(color = \"green\"), method='lm',formula=y ~ x,  se = FALSE) +\n    geom_vline(xintercept = 10, color = \"red\", size = 1.5)+\n    geom_vline(xintercept = 50, color = \"red\", size = 1.5)+\n    geom_vline(xintercept = 90, color = \"red\", size = 1.5) +\n    geom_point(aes(y = y3), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"green\"),\n                       labels = c(\"DGP\", \"linear model\"),\n                       guide = \"legend\")\n```\n\n--------------------------------------------------------------------------------\n\n## The Hyperparameter k in KNN\n\nKNN is our first example of a statistical algorithm that includes a **hyperparameter**, in this case $k$\n\n- Algorithm hyperparameters differ from parameters in that they cannot be estimated while fitting the algorithm to the training set\n\n- They must be set in advance\n\n- `k = 5` is the default for `kknn()`, the engine from the `kknn` package that we will use to fit a KNN within `tidymodels`.  \n\n  - [$kknn()$](https://cran.r-project.org/web/packages/kknn/kknn.pdf) weights observations (neighbors) based on distance.  \n  - An option exists for unweighted as well but not likely used much (default is optimal weighting, use it!).\n  \n--------------------------------------------------------------------------------\n\nUsing the polynomial DGP above, let's look at a 5-NN yields\n\n- Note the new category of algorithm, new engine, and the need to set a mode (because KNN can be used for regression and classification)\n- We can look in the package documentation to better understand what is being done (`?kknn::train.kknn`).\n```{r}\nnearest_neighbor() |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  translate()\n```\n\n--------------------------------------------------------------------------------\n\n- Set up simple feature engineering recipe and get training features (nothing happening but let's follow normal routine anyway)\n```{r}\nrec <- \n  recipe(y2 ~ x, data = data_trn_demo)\n\nrec_prep <- rec |> \n  prep(data_trn_demo)\n\nfeat_trn_demo <- rec_prep |> \n  bake(NULL)\n```\n\n--------------------------------------------------------------------------------\n\n- Fit 5NN\n```{r}\nfit_5nn_demo <- \n  nearest_neighbor() |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(y2 ~ ., data = feat_trn_demo)\n```\n\n--------------------------------------------------------------------------------\n\n- Get features for a validation set (a new sample using same polynomial DGP)\n```{r}\n#| echo: false\n\ndata_val_demo <- tibble(x = runif(200, 1, 100),\n               y = rnorm(200, x, 10),\n               y_dgp = rnorm(200, x ,0),\n               y2 = rnorm(200, x^4 / 800000, 8),\n               y2_dgp = rnorm(200, x^4 / 800000 ,0),\n               y3 = if_else(x < 40, rnorm(200, 25, 10),  rnorm(200, 75, 10)),\n               y3_dgp = if_else(x < 40, rnorm(200, 25, 0),  rnorm(200, 75, 0)))\n```\n\n```{r}\nfeat_val_demo <- rec_prep |> \n  bake(data_val_demo)\n```\n\n--------------------------------------------------------------------------------\n\n- Display 5NN predictions in validation \n\n  - KNN (with `k = 5`) does a pretty good job of representing the shape of the DGP (low bias)\n  - KNN displays some (but minimal) evidence of overfitting\n  - Simple linear model does not perform well (clear/high bias)\n\n```{r}\n#| echo: false\n\nfeat_val_demo |> \n  bind_cols(data_val_demo |> select(y, y2_dgp)) |> # add in other outcomes from data\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_smooth(aes(color = \"green\"), method = 'lm', formula = y ~ x,  se = FALSE) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_val_demo)$.pred, color = \"red\"), \n              size = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"green\", \"red\"),\n                       labels = c(\"DGP\", \"linear model\", \"k = 5\"),\n                       guide = \"legend\")\n```\n\n--------------------------------------------------------------------------------\n\nLet's pause and consider our conceptual understanding of the impact of $k$ on the bias-variance trade-off\n\n::: {.callout-important}\n# Question\nHow will the size of k influence model performance (e.g., bias, overfitting/variance)?\n:::\n\n::: {.fragment .uwred}\nSmaller values of k will tend to increase overfitting (and therefore variance across training samples) but decrease bias.  Larger values of k will tend to decrease overfitting but increase bias.  We need to find the Goldilocks \"sweet spot\"\n:::\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\nHow will k = 1 perform in training and validation sets?\n:::\n\n::: {.fragment .uwred}\nk = 1 will perfectly fit the training set.  Therefore it is very dependent on the training set (high variance).  It will fit both the DGP and the noise in the training set.  Clearly it will likely not do as well in validation (it will be overfit to training).  \n\nk needs to be larger if there is more noise (to average over more cases).  k needs to be smaller if the relationships are complex. (More on choosing k by resampling in unit 5.\n:::\n\n--------------------------------------------------------------------------------\n\n`k = 1`\n\n- Fit new model\n\n- Recipe and features have not changed\n\n```{r}\nfit_1nn_demo <- \n  nearest_neighbor(neighbors = 1) |>  # <1> \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(y2 ~ ., data = feat_trn_demo)\n```\n1. Set k with `neighbors = `\n\n--------------------------------------------------------------------------------\n\nVisualize prediction models in Train and Validation\n\n```{r}\n#| echo: false\n#| out-height: 5in\n\nplot_train <- feat_trn_demo |> \n  bind_cols(data_trn_demo |> select(y, y2_dgp)) |> # add in other outcomes for fig\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_trn_demo)$.pred, \n                  color = \"red\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_1nn_demo, feat_trn_demo)$.pred, \n                  color = \"green\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"red\", \"green\"),\n                       labels = c(\"DGP\", \"k = 5\", \"k = 1\"),\n                       guide = \"legend\")\n\nplot_val <- feat_val_demo |> \n  bind_cols(data_val_demo |> select(y, y2_dgp)) |> # add in other outcomes from data\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_val_demo)$.pred, \n                  color = \"red\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_1nn_demo, feat_val_demo)$.pred, \n                  color = \"green\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"red\", \"green\"),\n                       labels = c(\"DGP\", \"k = 5\", \"k = 1\"),\n                       guide = \"legend\")\n\ncowplot::plot_grid(plot_train, plot_val, \n                   labels = list(\"Training Set\", \"Validation Set\"), \n                   ncol = 2, nrow = 1, hjust = -1)\n```\n\n--------------------------------------------------------------------------------\n\nCalculate RMSE in validation for two KNN models\n\n`k = 1`\n```{r}\nrmse_vec(feat_val_demo$y2, \n         predict(fit_1nn_demo, feat_val_demo)$.pred)\n```\n\n`k = 5`\n```{r}\nrmse_vec(feat_val_demo$y2, \n         predict(fit_5nn_demo, feat_val_demo)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\nWhat if we go the other way and increase $k$ to 75\n\n```{r}\nfit_75nn_demo <- \n  nearest_neighbor(neighbors = 75) |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(y2 ~ ., data = feat_trn_demo)\n```\n\n--------------------------------------------------------------------------------\n\nVisualize prediction models in Train and Validation\n\n```{r}\n#| echo: false\n#| out-height: 5in\n\nplot_train <- feat_trn_demo |> \n  bind_cols(data_trn_demo |> select(y, y2_dgp)) |> # add in other outcomes for fig\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_trn_demo)$.pred, \n                  color = \"red\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_1nn_demo, feat_trn_demo)$.pred, \n                  color = \"green\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_75nn_demo, feat_trn_demo)$.pred, \n                  color = \"yellow\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"red\", \"green\", \"yellow\"),\n                       labels = c(\"DGP\", \"k = 5\", \"k = 1\", \"k = 75\"),\n                       guide = \"legend\")\n\nplot_val <- feat_val_demo |> \n  bind_cols(data_val_demo |> select(y, y2_dgp)) |> # add in other outcomes for fig\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_val_demo)$.pred, \n                  color = \"red\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_1nn_demo, feat_val_demo)$.pred, \n                  color = \"green\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_75nn_demo, feat_val_demo)$.pred, \n                  color = \"yellow\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n     scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"red\", \"green\", \"yellow\"),\n                       labels = c(\"DGP\", \"k = 5\", \"k = 1\", \"k = 75\"),\n                       guide = \"legend\")\n\ncowplot::plot_grid(plot_train, plot_val, \n                   labels = list(\"Training Set\", \"Validation Set\"), \n                   ncol = 2, nrow = 1, hjust = -1)\n```\n\n--------------------------------------------------------------------------------\n\nCalculate RMSE in validation for three KNN models\n\n**This is the bias-variance trade-off in action**\n\n- `k = 1`  - high variance\n```{r}\nrmse_vec(feat_val_demo$y2, \n         predict(fit_1nn_demo, feat_val_demo)$.pred)\n```\n\n- `k = 5`  - just right (well better at least)\n```{r}\nrmse_vec(feat_val_demo$y2, \n         predict(fit_5nn_demo, feat_val_demo)$.pred)\n```\n\n- `k = 75` - high bias\n```{r}\nrmse_vec(feat_val_demo$y2, \n         predict(fit_75nn_demo, feat_val_demo)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\n## Distance and Scaling in KNN\n\n### Defining \"Nearest\"\n\nTo make a prediction for some new observation, we need to identify the observations from the training set that are nearest to it\n\n- Need a distance measure to define \"nearest\"\n\n- IMPORTANT: We care only about:\n\n  - Distance between a validation observation and all the training observations\n  - Need to find the $k$ observations in training that are nearest to the validation observation (i.e., its neighbors)\n  - Distance is defined based on these observations' features, not their outcomes\n  \n- There are a number of different distance measures available (e.g., Euclidean, Manhattan, Chebyshev, Cosine, Minkowski)\n\n  - Euclidean is most commonly used in KNN\n\n--------------------------------------------------------------------------------\n\nEuclidean distance between any two points is an n-dimensional extension of the Pythagorean formula (which applies explicitly with 2 features/2 dimensional space).  \n\n$C^2 = A^2 + B^2$\n\n$C = \\sqrt{A^2 + B^2}$\n\n...where C is the distance between two points\n\n--------------------------------------------------------------------------------\n\nThe Euclidean distance between 2 points (p and q) in two dimensions (2 predictors, x1 = A, x2 = B)\n\n$Distance = \\sqrt{A^2 + B^2}$\n\n$Distance = \\sqrt{(q1 - p1)^2 + (q2 - p2)^2}$\n\n$Distance = \\sqrt{(2 - 1)^2 + (5 - 2)^2}$\n\n$Distance = 3.2$\n\n```{r}\n#| echo: false\n\ndistance2 <- tibble(x1 = c(1, 2), x2 = c(2, 5), \n                   x1h = c(1, 2), x2h = c(2, 2),\n                   x1v = c(2, 2), x2v = c(2, 5))\n\ndistance2 |> \n  ggplot(aes(x = x1, y = x2)) +\n    geom_point(color = c(\"red\", \"blue\"), size = 3) +\n    geom_line(color = \"green\", linewidth = 1.5) +\n    geom_line(aes(x = x1h, y = x2h), color = \"gray\", linewidth = 1.5) +\n    geom_line(aes(x = x1v, y = x2v), color = \"gray\", linewidth = 1.5) +\n    geom_text(x = .9, y = 2, label = \"p\", size = 5) +\n    geom_text(x = 2.1, y = 5, label = \"q\", size = 5) +\n    geom_text(x = 1.5, y = 1.8, label = \"A = 1\", size = 5) +\n    geom_text(x = 2.2, y = 3.5, label = \"B = 3\", size = 5) +\n    geom_text(x = 1.2, y = 3.5, label = \"C = 3.2\", size = 5) +\n    scale_x_continuous(limits = c(0,6), breaks = 0:6) +\n    scale_y_continuous(limits = c(0,6), breaks = 0:6)\n```\n\n--------------------------------------------------------------------------------\n\nOne dimensional (one feature) is simply the subtraction of scores on that feature (x1) between p and q\n\n$Distance = \\sqrt{(q1 - p1)^2}$\n\n$Distance = \\sqrt{(2 - 1)^2}$\n\n$Distance = 1$\n\n```{r}\n#| echo: false\n\ndistance1 <- tibble(x1 = c(1, 2), x2 = c(1, 1))\n\ndistance1 |> \n  ggplot(aes(x = x1, y = x2)) +\n    geom_point(color = c(\"red\", \"blue\"), size = 3) +\n    geom_line(color = \"green\", linewidth = 1.5) +\n    geom_text(x = .9, y = 1, label = \"p\", size = 5) +\n    geom_text(x = 2.1, y = 1, label = \"q\", size = 5) +\n    geom_text(x = 1.5, y = 1.1, label = \"Distance = 1\", size = 5) +\n    scale_x_continuous(limits = c(0,6), breaks = 0:6) +\n    scale_y_continuous(limits = c(0,2), breaks = NULL, labels = NULL, name = NULL)\n```\n\nN-dimensional generalization for n features:\n\n$Distance = \\sqrt{(q1 - p1)^2 + (q2 - p2)^2 + ... + (qn - pn)^2}$\n\n--------------------------------------------------------------------------------\n\nManhattan distance is also referred to as city block distance\n\n- Travel down the \"A\" street for 1 unit\n- Travel down the \"B\" street for 3 units\n- Total distance = 4 units\n\nFor two features/dimensions\n\n$Distance = |A + B|$\n\n```{r}\n#| echo: false\n\ndistance2 |> \n  ggplot(aes(x = x1, y = x2)) +\n    geom_point(color = c(\"red\", \"blue\"), size = 3) +\n    geom_line(aes(x = x1h, y = x2h), color = \"green\", linewidth = 1.5) +\n    geom_line(aes(x = x1v, y = x2v), color = \"green\", linewidth = 1.5) +\n    geom_text(x = .9, y = 2, label = \"p\", size = 5) +\n    geom_text(x = 2.1, y = 5, label = \"q\", size = 5) +\n    geom_text(x = 1.5, y = 1.8, label = \"A = 1\", size = 5) +\n    geom_text(x = 2.2, y = 3.5, label = \"B = 3\", size = 5) +\n    scale_x_continuous(limits = c(0,6), breaks = 0:6) +\n    scale_y_continuous(limits = c(0,6), breaks = 0:6)\n```\n\n--------------------------------------------------------------------------------\n\n`kknn()` uses Minkowski distance (see [Wikipedia](https://en.wikipedia.org/wiki/Minkowski_distance) or [less mathematical description](https://www.mikulskibartosz.name/minkowski-distance-explained/))\n\n- It is a more complex parameterized distance formula\n  - This parameter is called `p`, referred to as `distance` in `kknn()`\n- Euclidean and Manhattan distances are special cases where `p` = 2 and 1, respectively\n- **The default p in kknn() = 2 (Euclidean distance)**  \n- This default (like all defaults) can be changed when you define the algorithm using `nearest_neighbor()`\n\n--------------------------------------------------------------------------------\n\n### Scaling X\n\nDistance is dependent on scales of all the features.  We need to put all features on the same scale\n\n- Scale all features to SD = 1 (using `step_scale(all_numeric_predictors())`)\n- Range correct [0, 1] all features (using `step_range(all_numeric_predictors())`)\n\n--------------------------------------------------------------------------------\n\n### Categorical Predictors\n\nKNN requires numeric features (for distance calculation).\n\n- For categorical predictors, you will need to use dummy coding or other feature engineering that results in numeric features. \n- e.g., `step_dummy(all_nominal_predictors())`\n\n--------------------------------------------------------------------------------\n\n## KNN with Ames Housing Prices\n\nLet's use KNN with Ames\n\n- Train a model using only numeric predictors and `overall_qual` as numeric\n- Use the default `k = 5` algorithm\n- Set SD = 1 for all features\n\n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + overall_qual, \n         data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(overall_qual = as.numeric(overall_qual)) |> \n  step_scale(all_numeric_predictors()) # <1>\n```\n1.  Remember to take advantage of these selectors for easier code! See `?has_role` for more details\n\n--------------------------------------------------------------------------------\n\n- prep, bake\n\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |>\n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- Skim training features.  Note all SD = 1\n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Skim validation features.  Note SD.  [Why not exactly 1?]{.red}\n```{r}\nfeat_val |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n- Fit 5NN\n\n```{r}\nfit_5nn_5num <- \n  nearest_neighbor() |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\nerror_val <- bind_rows(error_val, \n                        tibble(model = \"5 numeric predictor 5nn\", \n                               rmse_val = rmse_vec(feat_val$sale_price, \n                                                   predict(fit_5nn_5num, feat_val)$.pred)))\n\nerror_val\n```\n\n- Not bad!\n\n--------------------------------------------------------------------------------\n\nKNN also mostly solved the linearity problem\n\n- We might be able to improve the linear models with better transformations of X and Y\n- However, this wasn't needed for KNN!\n\n```{r}\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_5nn_5num, feat_val)$.pred)\n\n```\n\n--------------------------------------------------------------------------------\n\nBut 5NN may be overfit. `k = 5` is pretty low\n\nAgain with `k = 20`\n\n```{r}\nfit_20nn_5num <- \n  nearest_neighbor(neighbors = 20) |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\n#| echo: false\n\nerror_val <- error_val |> \n  bind_rows(tibble(model = \"5 numeric predictor 20nn\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_20nn_5num, \n                                               feat_val)$.pred)))\n\nerror_val\n```\n\n- That helped some\n\n-------------------------------------------------------------------------------- \n\nOne  more time with `k = 50` to see where we are in the bias-variance function\n\n\n```{r}\nfit_50nn_5num <- \n  nearest_neighbor(neighbors = 50) |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\n#| echo: false\n\nerror_val <- error_val |> \n  bind_rows(tibble(model = \"5 numeric predictor 50nn\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_50nn_5num, \n                                               feat_val)$.pred)))\n\nerror_val\n\n```\n\n- Too high, now we have bias......\n- We will learn a more rigorous method for selecting the optimal value for $k$ (i.e., **tuning** this hyperparameter) in unit 5\n\n--------------------------------------------------------------------------------\n\nTo better understand bias-variance trade-off, let's look at error across these three values of $k$ in train and validation for Ames\n\nTraining\n\n- Remember that training error would be 0 for `k = 1`\n- Training error is increasing as $k$ increases b/c it KNN is overfitting less (so its not fitting the noise in train as well)\n\n```{r}\nrmse_vec(feat_trn$sale_price, \n         predict(fit_5nn_5num, feat_trn)$.pred)\nrmse_vec(feat_trn$sale_price, \n         predict(fit_20nn_5num, feat_trn)$.pred)\nrmse_vec(feat_trn$sale_price, \n         predict(fit_50nn_5num, feat_trn)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\nValidation\n\n- Validation error is first going down as $k$ increases (and it would have been very high for `k = 1`)\n- Bias is likely increasing a bit\n- But this is compensated by big decreases in overfitting variance\n- The trade-off is good for `k = 20` relative to 5 and 1\n- At some point, as $k$ increases the increase in bias outweighed the decrease in variance and validation error increased too.\n```{r}\nrmse_vec(feat_val$sale_price, \n         predict(fit_5nn_5num, feat_val)$.pred)\nrmse_vec(feat_val$sale_price, \n         predict(fit_20nn_5num, feat_val)$.pred)\nrmse_vec(feat_val$sale_price, \n         predict(fit_50nn_5num, feat_val)$.pred)\n```\n\n--------------------------------------------------------------------------------\n\nLet's do one final example and add one of our nominal variables into the model: `ms_zoning`\n\n- Need to collapse levels and then dummy\n\n```{r}\nrec <- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + \n           overall_qual + ms_zoning, data = data_trn) |> \n  step_impute_median(garage_cars) |> \n  step_mutate(overall_qual = as.numeric(overall_qual)) |> \n  step_scale(all_numeric_predictors()) |>  # done before dummy coding avoid scaling binary features \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\")) |>\n  step_dummy(ms_zoning) \n```\n\n--------------------------------------------------------------------------------\n\n- prep, bake\n\n```{r}\nrec_prep <- rec |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_val <- rec_prep |> \n  bake(data_val)\n```\n\n--------------------------------------------------------------------------------\n\n- Fit \n```{r}\nfit_20nn_5num_mszone <- \n  nearest_neighbor(neighbors = 20) |>   \n  set_engine(\"kknn\") |>   \n  set_mode(\"regression\") |> \n  fit(sale_price ~ ., data = feat_trn)\n```\n\n--------------------------------------------------------------------------------\n\n- evaluate\n\n```{r}\n#| echo: false\n\nerror_val <- error_val |> \n  bind_rows(tibble(model = \"5 numeric predictor 20nn with ms_zoning\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_20nn_5num_mszone, \n                                               feat_val)$.pred)))\n\nerror_val\n```\n\n- Now it helps.  \n- Might have to do with interactions with other predictors that we didn't model in the linear model\n- KNN automatically accommodates interactions.  Why?\n- This model is a bit more complex and might benefit further from higher $k$\n\n--------------------------------------------------------------------------------\n\nAs a teaser, here is another performance metric for this model - $R^2$.   Not too shabby!  Remember, there is certainly some irreducible error in `sale_price` that will put a ceiling on $R^2$ and a floor on RMSE\n```{r}\nrsq_vec(feat_val$sale_price, \n        predict(fit_20nn_5num_mszone, feat_val)$.pred)\n```\n\nOverall, we now have a model that predicts housing prices with about 30K of RMSE and accounting for 86% of the variance.  I am sure you can improve on this!"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["book.css"],"output-file":"l03_regression.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","bibliography":["refs.bib"],"callout-icon":false,"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}