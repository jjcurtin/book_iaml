{"title":"Principal Components Analysis","markdown":{"yaml":{"editor_options":{"chunk_output_type":"console"}},"headingText":"Principal Components Analysis ","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n##  Overview\n\n### General Information\n\nTo start, the introductory sections about PCA from [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis) provide a nice orientation and summary of the technique.\n\nThere is also a useful demonstration of PCA in R on [CRAN](https://cran.r-project.org/web/packages/LearnPCA/vignettes/Vig_03_Step_By_Step_PCA.pdf)\n\nIn short, PCA linearly transforms our data (our `x`s to be precise) onto a new coordinate system such that the directions/axes (principal components) capturing the largest variation in the data can be easily identified.\n\nA set of up to `p` principal components can be derived from `p` raw variables (e.g. predictors) such that the first principal component is a linear combination of the `p` raw variables that accounts for the most variance across those variables.   The 2nd principal component is a similar linear combination of the raw variables that is orthogonal (uncorrelated) with the first component and accounts for the largest proportion of remaining variance among the raw variables.  PCA continues to derive additional components that are each orthogonal to the previous components and account for successively less variance.\n\nThese new components represent a new coordinate system within which to represent scores for our observations.   This new system is a rotation of the original coordinate system that consists of orthogonal axes (e.g., x, y, z) defined by our raw variables.\n\nWe can use PCA to\n\n- Understand the structure of our data\n- Transform our raw (correlated) variables into a set of uncorrelated features\n- Most importantly, retain a large portion of the the variance from the `p` original variables with << `p` principal components.  This last benefit may allow us to fit lower variance prediction models (due to less overfitting) without increasing model bias by much.\n\n### Applications in machine learning\n\nWhen we discuss PCA in the machine learning world, we consider it an example of an unsupervised machine learning approach.  \n\n- It is applied to the raw predictors (the `x`s, ignoring `y`).  \n- We use it to reduce the dimensionality of our features to minimize overfitting that can contribute to model variance.\n\n## A Two Dimensional Example\n\nThis example is based loosely on a tutorial and demonstration data developed by [Lindsay Smith](https://www.iro.umontreal.ca/~pift6080/H09/documents/papers/pca_tutorial.pdf)\n\nFor a second example that extends to 3D space, see this [website](https://setosa.io/ev/principal-component-analysis/)\n\nAnd to be clear, PCA can be applied to any number of dimensions and is most useful for high dimensional data (e.g., p >> 2).  \n\n--------------------------------------------------------------------------------\n\nLet's start with a toy dataset for two variables (e.g., predictors), `x1` and `x2` and a sample size of 10.  We will work in two dimensions to make the example easier to visualize but the generalization to p dimensions (where p = number of variables) is not difficult.\n\n```{r}\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nd <- tibble(x1 = c(2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1),\n            x2 = c(2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9))\nd |> print()\n```\n\n--------------------------------------------------------------------------------\n\nHere is a scatterplot of the dataset.  I've also added a green dot (that is not part of the dataset) at the mean of `x1` and `x2`.  This will be the point of rotation for the dataset as we attempt to find a new coordinate system (not defined by `x1` and `x2`), where the dataset's variance is maximized across the new axes (the principal components) and the observations are uncorrelated in this new coordinate system.\n\n```{r}\ntheme_set(theme_classic())\nd |> ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  geom_point(data = tibble(x1=mean(d$x1), x2 = mean(d$x2)), \n             size = 2, color = \"green\") +\n  xlim(-1,4) +\n  ylim(-1,4) +\n  coord_fixed()\n```\n\n--------------------------------------------------------------------------------\n\nThe first step in the process is to center both `x1` and `x2` such that their means are zero.  This moves the green point to the origin.  This will make it easier to rotate the data around that point (the definitions of the new principal components will be defined as a linear combination of the original variables but there is not offset/intercept in those transformation formulas).  \n\nI've left the green point at the mean of `x1c` and `x2c` for this plot to reinforce the impact of centering.  I will remove it from later figures.  I have also drawn the true axes in blue to make the original coordinate system defined by `x1` and `x2` salient.  PCA will rotate this coordinate system to achieve its goals.\n\n```{r}\nd <- d |> \n  mutate(x1c = x1 - mean(x1), x2c = x2 - mean(x2))\n\nplot <- d |> \n  ggplot(aes(x = x1c, y = x2c)) +\n    geom_point() +\n    geom_hline(yintercept = 0, color = \"blue\") +\n    geom_vline(xintercept = 0, color = \"blue\") +\n    xlim(-2.5, 2.5) +\n    ylim(-2.5, 2.5) +\n    coord_fixed()\n\nplot + \n  geom_point(data = tibble(x1c=mean(d$x1c), x2c = mean(d$x2c)), \n               size = 2, color = \"green\")\n```\n\nNOTE: It is sometimes useful to also scale the original variables (i.e.,, set their standard deviations = 1).  \n\n- This may be important if the variables have very difference variances.\n- If you dont scale the variables, the variables with the large variances will have more influence on the rotation than those with smaller variances.  \n- If this is not desirable, scale the variables as well as center them.  \n- However, do know that sometimes variances are larger because of noise and if you scale, you will magnify that noise. \n\n--------------------------------------------------------------------------------\n\nOur goal now is to find the axes of the new coordinate system.   The first axis (the first principal component) will be situated such it maximizes the variance in the data across its span.  Imagine fittinng all possible lines through the green dot and choosing the line that moves along the widest spread of the data.  That line (displayed in green below) will be the first axis of the new coordinate system and projections of the points onto that axis will represent the scores for each observation on our first principal component defined by this axis.\n\n```{r}\n#| echo: false\n\nei <- d |> \n  select(x1c, x2c) |>\n  cov() |> \n  eigen(symmetric = TRUE)\n\nplot +\n  geom_abline(slope = ei$vectors[2,1]/ei$vectors[1,1], \n              intercept = 0, \n              color = \"green\")\n```\n\n--------------------------------------------------------------------------------\n\nThis axis associated with the first principal component is similar to a regression line but not identical.\n\n- The red regression line (below) was fit to minimize the sum of the squared errors when predicting the outcome (in this instance when regressing `x2c` on `x1c`) from a predictor (in this instance, `x1c`.  These errors are the vertical distances from the red line to the points.\n\n- In contrast, the green line maximized variance across that PC1 dimension.  As a result, it also minimized deviations around the line.  However, thsee deviations are the squared perpendicular distances between the green line and the points.  These distances go up and to the left and down and to the right from the green line to the points rather than vertical.   They are not the same line!\n\n```{r}\n#| echo: false\n\nm <- lm(x2c ~ x1c, data = d)\n\nplot +\n  geom_abline(slope = ei$vectors[2,1]/ei$vectors[1,1], \n              intercept = 0, \n              color = \"green\") +\n  geom_abline(slope = m$coefficients[2],\n              intercept = m$coefficients[1],\n              color = \"red\")\n```\n\n--------------------------------------------------------------------------------\n\nTo find the next principal component, we need to find a new axis that is perpendicular to the first component and in the direction that accounts for the largest proportion of the remaining variance in the dataset. \n\nIn our example with only two variables, there is only one direction remaining that is perpendicular/orthogonal with `PC1` because we are in two dimensional space given only two original variables.\n\nHowever, if we were in a higher dimensional space with p > 2 variables, this next component could follow the direction of maximal remaining variance in a direction orthogonal to `PC1`.  Subsequent components up to the pth component given p variables would each be orthogonal to all previous components and in the direction of maximal variance.\n\nWe have added this second component (orange) to our 2 dimensional example below.\n\n```{r}\n#| echo: false\n\nplot +\n  geom_abline(slope = ei$vectors[2,1]/ei$vectors[1,1], \n              intercept = 0, color = \"green\") +\n  geom_abline(slope = ei$vectors[2,2]/ei$vectors[1,2], \n              intercept = 0, color = \"orange\") + \n  coord_fixed()\n```\n\n--------------------------------------------------------------------------------\n\nThese two components define a new coordinate system within which to measure/score our observations.  The new axes of this system are the PCs.  This new coordinate system is a rotation of our original system that was previously defined in axes based on `x1` and `x2`.  \n\nHere is a figure displaying the data in this new coordinate system. Notice the group of four points to the left (three vertical and one further to the left).  Those same points were previously in the top right quadrant of the coordinate system defined by `x1c` and `x2c`.\n\n- Scores on each of the two components are obtained by projecting the observations onto the associated axis for that component.\n- The data show the highest variance over `PC1` (the x axis) and the next highest variance over `PC2` (the y axis) \n- When the observations are scored/defined on these PCs, `PC1` and `PC2` are now new, uncorrelated features we can use to describe the data.\n\n```{r}\n#| echo: false\n\npca <- prcomp(d)\n\npca$x |> ggplot(aes(x = PC1, y = PC2)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"green\") +\n  geom_vline(xintercept = 0, color = \"orange\") +\n  xlim(-2.5, 2.5) +\n  ylim(-2.5, 2.5) +\n  coord_fixed() \n```\n\n## PCA using Eigenvectors and Eigenvalues\n\nWe can derive the new coordinate system that maximizes the variance on `PC1`, and then `PC2`, etc by doing an eigen decomposition of the covariance matrix (or correlation matrix if the x's are to be scaled).  When applied to a p x p covariance matrix, this yields `p` pairs of [eigen vectors and eigen values](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).  Complete explanation of this process is beyond the scope of this tutorial but the interested reader can consult [Linear Algebra and its Applications by Gilbert Strang](https://rksmvv.ac.in/wp-content/uploads/2021/04/Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf).\n\nIn short, the eigen vectors represent the new axes for the principal components and the eigen values indicate the variances of the principal components.  \n\n-------------------------------------------------------------------------------\n\nHere is an eigen decomposition of the covariance for our toy data\n```{r}\nei <- d |> \n  select(x1c, x2c) |>\n  cov() |> \n  eigen(symmetric = TRUE)\n```\n\n- **Eigenvectors**.  These are unit vectors that point in the direction of the axes of the new coordinate system.  \n```{r}\nei$vectors\n```\n\nThis figure plots these two vectors on our original coordinate system defined by `x1c` and `x2c`.  Note that these vectors map on the new axes we demonstrated earlier.\n\n```{r}\nplot +\n  annotate(\"segment\", x = 0, y = 0, xend = ei$vectors[1,1], yend = ei$vectors[2,1], \n              color = \"green\") + \n  annotate(\"segment\", x = 0, y = 0, xend = ei$vectors[1,2], yend = ei$vectors[2,2], \n              color = \"green\")\n```\n\n--------------------------------------------------------------------------------\n\n- **Eigenvalues**. These are the variances associated with the principal components\n```{r}\nei$values\n```\n\nThe eigen decomposition parses the complete variance in the original variables such that `PC1` has the most variance, `PC2`, the second-most, etc.  The full set of the PCs will contain all the variance of the original variables.\n\nIn our example:\n\n- Variance was originally split across `x1` and `x2`.  \n- `PC1` now contains most of the variance in the dataset (see eigenvalues above)\n```{r}\nvar(d$x1)\nvar(d$x2)\n```\n\nAll variance accounted for both in original variables and new PCs\n```{r}\nei$values[1] + ei$values[2]\nvar(d$x1) + var(d$x2)\n```\n\n## Stable Compuation of Principal Components\n\nIt is more numerically stable to get the principal components using singular vector decomposition (`svd()` in R) than eigen decomposition.  \n\nIn base R, `prcomp()` uses svd and also directly calculates the PCs.\n\n- See [help](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp)\n- We pass in our raw variables.\n- By default, the raw variables are centered by not scaled (We included both arguements below with their defaults so you can see their names)\n\n```{r}\npca <- prcomp(d |> select(x1, x2), center = TRUE, scale = FALSE)\n```\n\n--------------------------------------------------------------------------------\n\nWe can get the vectors associated with the new coordinate system from `$rotation`  Note that direction of the PCs is arbitrary (e.g., PCs are opposite direction from the solution using `eigen()` with these data)\n\n```{r}\nei$vectors\n\npca$rotation\n```\n\n--------------------------------------------------------------------------------\n\n`$sdev` returns the square root of the eigenvalues.  This represents the standard deviations of the PCs\n\n```{r}\npca$sdev\n\n# square for variances\npca$sdev ^2\n\n# compare to eigenvalues\nei$values\n```\n\n--------------------------------------------------------------------------------\n\n`$x` contains the new scores on the PCs for the dataset\n\n```{r}\npca$x\n```\n\nAs expected, they are uncorrelated\n```{r}\nround(cor(pca$x[,1], pca$x[,2]), 5)\n```\n\n## Using PCA for Dimensionality Reduction\n\nIn our example, `PC1` and `PC2`\n\n- Contain all the variance from `x1` and `x2`\n- Are orthogonal \n\nBut when using PCA for dimensionality reduction, we wanted to use the variance of our variables in fewer dimensions (with fewer features) for prediction to reduce overfitting.\n\n- Most of variance from the full dataset is now in `PC1`\n- We can use `PC1` as a feature rather than both `x1` and `x2`.\n\nThese were the variances of the orginal variables `x1` and `x2`.  \n```{r}\nvar(d$x1) # variance of x1\nvar(d$x2) # variance of x2\n```\n\nAnd now most of the variance is in `PC1`.  \n\n- We can see this in the eigenvalues \n\n```{r}\npca$sdev ^2\n```\n\n- and also by just looking at the variances of the PCs directly\n\n```{r}\nvar(pca$x[,1]) # variance of PC1\nvar(pca$x[,2]) # variance of PC2\n```\n\n\n## Reconstructing the Original Data\n\nIf we use all of the principal components, we can reconstruct the original data exactly.  However, if our goal is dimensionality reduction, we plan to use fewer than our full set of PCs in our subsequent analyses.  Therefore, it can be instructive to see how well we can reproduce the raw data using fewer PCs.\n\nIf we used `prcomp()` to get the PCs, we can reconstruct the original data using the following code\n\n- Lets use this code with our toy dataset\n- We will recreate the raw data using the first principal component only from our example.\n\nThis first step reproduces the original Xs, but in their centered/scaled format\n```{r}\nn_pc <- 1 # use only the first PC\nx_estimated <- pca$x[, 1:n_pc] %*% t(pca$rotation[, 1:n_pc])\n```\n\nIf the raw data were scaled and centered, we need to add back the means and scale the data back to the original scale.  We need to do this in two steps (in this order)\n\nFirst, we unscale the data (we did not scale, so we will show the code but not execute it)\n\n```{r}\n#| eval: false\n\norig_x_sd <- c(0.7852105, 0.846496) # original sd of x1 and x2\nx_estimated <- scale(x_estimated, center = FALSE, scale = 1/orig_x_sd)\n```\n\nAnd then we can add back the means of our raw Xs that we previously subtracted out.\n\n```{r}\norig_x_means <- c(1.81, 1.91) # original means of x1 and x2\nx_estimated <- scale(x_estimated, center = -orig_x_means, scale = FALSE)\n```\n\nLets see how well we were able to recreate the original data using only the first principal component.  Below are plots of the original and reconstructed data.  \n\n- As expected, we do pretty well.  \n- We retain all the variance over the direction of the first PC.  \n- And of course, we lose all the variance over the direction of the second PC because we didnnt use it.  \n- However, in this dataset, most of the variance was in the first PC.  This one PC may likely work well for us. \n```{r}\nd |> \n  ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  xlim(-1,4) +\n  ylim(-1,4) +\n  coord_fixed() +\n  ggtitle(\"Original Data\")\n\nx_estimated |> \n  as_tibble() |> \n  ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  xlim(-1,4) +\n  ylim(-1,4) +\n  coord_fixed() +\n  ggtitle(\"Reconstructed Data\")\n```\n\n## Deciding on the Number of Components to Retain\n\nWhen using PCA for dimensionality reduction, we need to decide how many components to retain.  \n\nLets make another toy dataset with 8 variables that represent a self-report measure that has two subscales.  In this instance, if the scale works well, we might expect that we could reduce the 8 items to just 2 components (one for each subscale).  This would be a nice reduction from 8 variables to 2 variables. (of course, we don't need PCA for this because we could just use two subscales to reduce the dimensionality from 8 to 2, but this is just for demonstration purposes).\n\nWe will use `mvrnorm()` to generate a dataset with 8 variables that are correlated.  \n\n- The first four variables will load primarily on the first component and the last four will load on the second component.  \n- This will create a situation where we can reduce from 8 variables to 2 components.\n```{r}\nset.seed(12345)\ncov_matrix <- matrix(0, nrow = 8, ncol = 8)\ncov_matrix[1:4, 1:4] <- 0.5 \ncov_matrix[5:8, 5:8] <- 0.5\ncov_matrix[1:4, 5:8] <- 0.2 # some cross0loadings to make it more realistic\ncov_matrix[5:8, 1:4] <- 0.2\ndiag(cov_matrix) <- 1 # set variances of all items to 1\n\nd2 <- MASS::mvrnorm(n = 200, mu = rep(0, 8), Sigma = cov_matrix) |> \n  magrittr::set_colnames(str_c(\"i\", 1:8)) |>\n  as_tibble()\n```\n\nGet principal components for this dataset\n\n```{r}\npca2 <- prcomp(d2)\n```\n\nThere are several common approaches for deciding on the number of components to retain:\n  \n1. **Variance Explained**  \n\n    - Look at the proportion of variance explained by each component.  \n    - You can plot this using a scree plot and look for an \"elbow\" in the plot where the addition of more components explains less variance.  \n    - In this case, we would retain the two components to the left of the elbow.\n```{r}\npca2 |> screeplot(type = \"lines\")\n```\n\n2. **PCs that account for more variance than individual variables**\n\n    - The eigenvalues (`$sdev` from `prcomp()`) quantify the variance explained by each component.  \n    - In our example, the individual items had raw variances of 1.  Therefore PCs with eigenvalues > 1 account for more variance than any single item.  \n    - Using this approach, we would retain 2 components\n```{r}\npca2$sdev^2\n```\n\n3. **Cumulative Variance Explained**\n\n    - You can also look at the cumulative proportion of variance explained by the components.  \n    - You may choose to retain enough components such that you explain a certain threshold of variance (e.g., 70% or 90%).\n    - Here is the proportion of total variance accounted for by each PC\n```{r}\nvar_explained <- pca2$sdev^2 / sum(pca2$sdev^2)\n```\n\n- And here is the cumulative variance as more PCs are retained. Of course, the threshold for variance can feel somewhat arbitrary in most instances so we see less use for this method except in special circumstances. \n```{r}\nvar_explained |> cumsum()\n```\n\n\n## Limitations of PCA\n\nPCA is a linear transformation of the data.  As a result, it only works well if the structure in the data is linear.  In our preceding example, there was a clear linear relationship between the two variables `x1` and `x2`.  PCA was able to capture this linear relationship well with the first principal component. \n\nIf instead, there was a more complicated, non-linear relationship between the variables, PCA may not be able to capture the structure of the data well with just a few components.  In this case, we may need to use more components to capture the variance in the data or consider other non-linear dimensionality reduction techniques (e.g., using auto-encoders as in our neural networks unit)\n\nLets see this problem in a final toy example where this is a quadratic relationship between `x1` and `x2`.  In this case, PCA will not be able to capture the variance in the data well with just one component.\n\n```{r}\nset.seed(12345)\nd3 <- tibble(x1 = rnorm(200, 0, 1),\n             x2 = rnorm(200, 0, .5) + (x1)^2) |>  # quadratic relationship\n             mutate(x1 = x1 / sd(x1),\n                    x2 = x2 / sd(x2)) # keeping scale of both Xs comparable for simplicity\n\nd3 |> ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  xlim(-4, 4) +\n  ylim(-1, 7) +\n  coord_fixed()\n```\n\nDo the PCA\n```{r}\npca3 <- prcomp(d3)\n```\n\nLook at eigenvalues for the two PCs.  PCA wasn't successful at capturing the variance in the data with just one component.  The first component only accounts for only slightly more variance that the original Xs (raw Xs has variance of 1.0 for both `x1` and `x2`). \n\n```{r}\npca3$sdev ^2\n```\n\nAnd if we were to try to reconstruct the original data using only the first principal component, we would not do a good job.  PCA is not a good choice for dimensionality reduction in this case!!\n\n```{r}\nn_pc <- 1 # use only the first PC\nx_estimated3 <- pca3$x[, 1:n_pc] %*% t(pca3$rotation[, 1:n_pc])\norig_x_means3 <- c(mean(d3$x1), mean(d3$x2)) # original means of x1 and x2\nx_estimated3 <- scale(x_estimated3, center = -orig_x_means3, scale = FALSE)\n\nx_estimated3 |> \n  as_tibble() |> \n  ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  xlim(-1,4) +\n  ylim(-1,4) +\n  coord_fixed() +\n  ggtitle(\"Reconstructed Data\")\n```\n\n## PCA in Tidymodels\n\nWe can use PCA for dimensionality reduction as part of our feature engineering.\n\n- `step_pca()` uses `prcomp()`\n- See [help](https://recipes.tidymodels.org/reference/step_pca.html)\n- Default is center = false and scale = false.  You definitely want to center and maybe scale predictors in a previous recipe step before using `step_pca()`\n- You can choose number of components to retain by specifying the exact number (`num_comp = `) or by indicating the minimum variance retained across PCs (`threshold = `)","srcMarkdownNoYaml":"\n\n# Principal Components Analysis  { .unnumbered}\n\n##  Overview\n\n### General Information\n\nTo start, the introductory sections about PCA from [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis) provide a nice orientation and summary of the technique.\n\nThere is also a useful demonstration of PCA in R on [CRAN](https://cran.r-project.org/web/packages/LearnPCA/vignettes/Vig_03_Step_By_Step_PCA.pdf)\n\nIn short, PCA linearly transforms our data (our `x`s to be precise) onto a new coordinate system such that the directions/axes (principal components) capturing the largest variation in the data can be easily identified.\n\nA set of up to `p` principal components can be derived from `p` raw variables (e.g. predictors) such that the first principal component is a linear combination of the `p` raw variables that accounts for the most variance across those variables.   The 2nd principal component is a similar linear combination of the raw variables that is orthogonal (uncorrelated) with the first component and accounts for the largest proportion of remaining variance among the raw variables.  PCA continues to derive additional components that are each orthogonal to the previous components and account for successively less variance.\n\nThese new components represent a new coordinate system within which to represent scores for our observations.   This new system is a rotation of the original coordinate system that consists of orthogonal axes (e.g., x, y, z) defined by our raw variables.\n\nWe can use PCA to\n\n- Understand the structure of our data\n- Transform our raw (correlated) variables into a set of uncorrelated features\n- Most importantly, retain a large portion of the the variance from the `p` original variables with << `p` principal components.  This last benefit may allow us to fit lower variance prediction models (due to less overfitting) without increasing model bias by much.\n\n### Applications in machine learning\n\nWhen we discuss PCA in the machine learning world, we consider it an example of an unsupervised machine learning approach.  \n\n- It is applied to the raw predictors (the `x`s, ignoring `y`).  \n- We use it to reduce the dimensionality of our features to minimize overfitting that can contribute to model variance.\n\n## A Two Dimensional Example\n\nThis example is based loosely on a tutorial and demonstration data developed by [Lindsay Smith](https://www.iro.umontreal.ca/~pift6080/H09/documents/papers/pca_tutorial.pdf)\n\nFor a second example that extends to 3D space, see this [website](https://setosa.io/ev/principal-component-analysis/)\n\nAnd to be clear, PCA can be applied to any number of dimensions and is most useful for high dimensional data (e.g., p >> 2).  \n\n--------------------------------------------------------------------------------\n\nLet's start with a toy dataset for two variables (e.g., predictors), `x1` and `x2` and a sample size of 10.  We will work in two dimensions to make the example easier to visualize but the generalization to p dimensions (where p = number of variables) is not difficult.\n\n```{r}\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nd <- tibble(x1 = c(2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1),\n            x2 = c(2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9))\nd |> print()\n```\n\n--------------------------------------------------------------------------------\n\nHere is a scatterplot of the dataset.  I've also added a green dot (that is not part of the dataset) at the mean of `x1` and `x2`.  This will be the point of rotation for the dataset as we attempt to find a new coordinate system (not defined by `x1` and `x2`), where the dataset's variance is maximized across the new axes (the principal components) and the observations are uncorrelated in this new coordinate system.\n\n```{r}\ntheme_set(theme_classic())\nd |> ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  geom_point(data = tibble(x1=mean(d$x1), x2 = mean(d$x2)), \n             size = 2, color = \"green\") +\n  xlim(-1,4) +\n  ylim(-1,4) +\n  coord_fixed()\n```\n\n--------------------------------------------------------------------------------\n\nThe first step in the process is to center both `x1` and `x2` such that their means are zero.  This moves the green point to the origin.  This will make it easier to rotate the data around that point (the definitions of the new principal components will be defined as a linear combination of the original variables but there is not offset/intercept in those transformation formulas).  \n\nI've left the green point at the mean of `x1c` and `x2c` for this plot to reinforce the impact of centering.  I will remove it from later figures.  I have also drawn the true axes in blue to make the original coordinate system defined by `x1` and `x2` salient.  PCA will rotate this coordinate system to achieve its goals.\n\n```{r}\nd <- d |> \n  mutate(x1c = x1 - mean(x1), x2c = x2 - mean(x2))\n\nplot <- d |> \n  ggplot(aes(x = x1c, y = x2c)) +\n    geom_point() +\n    geom_hline(yintercept = 0, color = \"blue\") +\n    geom_vline(xintercept = 0, color = \"blue\") +\n    xlim(-2.5, 2.5) +\n    ylim(-2.5, 2.5) +\n    coord_fixed()\n\nplot + \n  geom_point(data = tibble(x1c=mean(d$x1c), x2c = mean(d$x2c)), \n               size = 2, color = \"green\")\n```\n\nNOTE: It is sometimes useful to also scale the original variables (i.e.,, set their standard deviations = 1).  \n\n- This may be important if the variables have very difference variances.\n- If you dont scale the variables, the variables with the large variances will have more influence on the rotation than those with smaller variances.  \n- If this is not desirable, scale the variables as well as center them.  \n- However, do know that sometimes variances are larger because of noise and if you scale, you will magnify that noise. \n\n--------------------------------------------------------------------------------\n\nOur goal now is to find the axes of the new coordinate system.   The first axis (the first principal component) will be situated such it maximizes the variance in the data across its span.  Imagine fittinng all possible lines through the green dot and choosing the line that moves along the widest spread of the data.  That line (displayed in green below) will be the first axis of the new coordinate system and projections of the points onto that axis will represent the scores for each observation on our first principal component defined by this axis.\n\n```{r}\n#| echo: false\n\nei <- d |> \n  select(x1c, x2c) |>\n  cov() |> \n  eigen(symmetric = TRUE)\n\nplot +\n  geom_abline(slope = ei$vectors[2,1]/ei$vectors[1,1], \n              intercept = 0, \n              color = \"green\")\n```\n\n--------------------------------------------------------------------------------\n\nThis axis associated with the first principal component is similar to a regression line but not identical.\n\n- The red regression line (below) was fit to minimize the sum of the squared errors when predicting the outcome (in this instance when regressing `x2c` on `x1c`) from a predictor (in this instance, `x1c`.  These errors are the vertical distances from the red line to the points.\n\n- In contrast, the green line maximized variance across that PC1 dimension.  As a result, it also minimized deviations around the line.  However, thsee deviations are the squared perpendicular distances between the green line and the points.  These distances go up and to the left and down and to the right from the green line to the points rather than vertical.   They are not the same line!\n\n```{r}\n#| echo: false\n\nm <- lm(x2c ~ x1c, data = d)\n\nplot +\n  geom_abline(slope = ei$vectors[2,1]/ei$vectors[1,1], \n              intercept = 0, \n              color = \"green\") +\n  geom_abline(slope = m$coefficients[2],\n              intercept = m$coefficients[1],\n              color = \"red\")\n```\n\n--------------------------------------------------------------------------------\n\nTo find the next principal component, we need to find a new axis that is perpendicular to the first component and in the direction that accounts for the largest proportion of the remaining variance in the dataset. \n\nIn our example with only two variables, there is only one direction remaining that is perpendicular/orthogonal with `PC1` because we are in two dimensional space given only two original variables.\n\nHowever, if we were in a higher dimensional space with p > 2 variables, this next component could follow the direction of maximal remaining variance in a direction orthogonal to `PC1`.  Subsequent components up to the pth component given p variables would each be orthogonal to all previous components and in the direction of maximal variance.\n\nWe have added this second component (orange) to our 2 dimensional example below.\n\n```{r}\n#| echo: false\n\nplot +\n  geom_abline(slope = ei$vectors[2,1]/ei$vectors[1,1], \n              intercept = 0, color = \"green\") +\n  geom_abline(slope = ei$vectors[2,2]/ei$vectors[1,2], \n              intercept = 0, color = \"orange\") + \n  coord_fixed()\n```\n\n--------------------------------------------------------------------------------\n\nThese two components define a new coordinate system within which to measure/score our observations.  The new axes of this system are the PCs.  This new coordinate system is a rotation of our original system that was previously defined in axes based on `x1` and `x2`.  \n\nHere is a figure displaying the data in this new coordinate system. Notice the group of four points to the left (three vertical and one further to the left).  Those same points were previously in the top right quadrant of the coordinate system defined by `x1c` and `x2c`.\n\n- Scores on each of the two components are obtained by projecting the observations onto the associated axis for that component.\n- The data show the highest variance over `PC1` (the x axis) and the next highest variance over `PC2` (the y axis) \n- When the observations are scored/defined on these PCs, `PC1` and `PC2` are now new, uncorrelated features we can use to describe the data.\n\n```{r}\n#| echo: false\n\npca <- prcomp(d)\n\npca$x |> ggplot(aes(x = PC1, y = PC2)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"green\") +\n  geom_vline(xintercept = 0, color = \"orange\") +\n  xlim(-2.5, 2.5) +\n  ylim(-2.5, 2.5) +\n  coord_fixed() \n```\n\n## PCA using Eigenvectors and Eigenvalues\n\nWe can derive the new coordinate system that maximizes the variance on `PC1`, and then `PC2`, etc by doing an eigen decomposition of the covariance matrix (or correlation matrix if the x's are to be scaled).  When applied to a p x p covariance matrix, this yields `p` pairs of [eigen vectors and eigen values](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).  Complete explanation of this process is beyond the scope of this tutorial but the interested reader can consult [Linear Algebra and its Applications by Gilbert Strang](https://rksmvv.ac.in/wp-content/uploads/2021/04/Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf).\n\nIn short, the eigen vectors represent the new axes for the principal components and the eigen values indicate the variances of the principal components.  \n\n-------------------------------------------------------------------------------\n\nHere is an eigen decomposition of the covariance for our toy data\n```{r}\nei <- d |> \n  select(x1c, x2c) |>\n  cov() |> \n  eigen(symmetric = TRUE)\n```\n\n- **Eigenvectors**.  These are unit vectors that point in the direction of the axes of the new coordinate system.  \n```{r}\nei$vectors\n```\n\nThis figure plots these two vectors on our original coordinate system defined by `x1c` and `x2c`.  Note that these vectors map on the new axes we demonstrated earlier.\n\n```{r}\nplot +\n  annotate(\"segment\", x = 0, y = 0, xend = ei$vectors[1,1], yend = ei$vectors[2,1], \n              color = \"green\") + \n  annotate(\"segment\", x = 0, y = 0, xend = ei$vectors[1,2], yend = ei$vectors[2,2], \n              color = \"green\")\n```\n\n--------------------------------------------------------------------------------\n\n- **Eigenvalues**. These are the variances associated with the principal components\n```{r}\nei$values\n```\n\nThe eigen decomposition parses the complete variance in the original variables such that `PC1` has the most variance, `PC2`, the second-most, etc.  The full set of the PCs will contain all the variance of the original variables.\n\nIn our example:\n\n- Variance was originally split across `x1` and `x2`.  \n- `PC1` now contains most of the variance in the dataset (see eigenvalues above)\n```{r}\nvar(d$x1)\nvar(d$x2)\n```\n\nAll variance accounted for both in original variables and new PCs\n```{r}\nei$values[1] + ei$values[2]\nvar(d$x1) + var(d$x2)\n```\n\n## Stable Compuation of Principal Components\n\nIt is more numerically stable to get the principal components using singular vector decomposition (`svd()` in R) than eigen decomposition.  \n\nIn base R, `prcomp()` uses svd and also directly calculates the PCs.\n\n- See [help](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp)\n- We pass in our raw variables.\n- By default, the raw variables are centered by not scaled (We included both arguements below with their defaults so you can see their names)\n\n```{r}\npca <- prcomp(d |> select(x1, x2), center = TRUE, scale = FALSE)\n```\n\n--------------------------------------------------------------------------------\n\nWe can get the vectors associated with the new coordinate system from `$rotation`  Note that direction of the PCs is arbitrary (e.g., PCs are opposite direction from the solution using `eigen()` with these data)\n\n```{r}\nei$vectors\n\npca$rotation\n```\n\n--------------------------------------------------------------------------------\n\n`$sdev` returns the square root of the eigenvalues.  This represents the standard deviations of the PCs\n\n```{r}\npca$sdev\n\n# square for variances\npca$sdev ^2\n\n# compare to eigenvalues\nei$values\n```\n\n--------------------------------------------------------------------------------\n\n`$x` contains the new scores on the PCs for the dataset\n\n```{r}\npca$x\n```\n\nAs expected, they are uncorrelated\n```{r}\nround(cor(pca$x[,1], pca$x[,2]), 5)\n```\n\n## Using PCA for Dimensionality Reduction\n\nIn our example, `PC1` and `PC2`\n\n- Contain all the variance from `x1` and `x2`\n- Are orthogonal \n\nBut when using PCA for dimensionality reduction, we wanted to use the variance of our variables in fewer dimensions (with fewer features) for prediction to reduce overfitting.\n\n- Most of variance from the full dataset is now in `PC1`\n- We can use `PC1` as a feature rather than both `x1` and `x2`.\n\nThese were the variances of the orginal variables `x1` and `x2`.  \n```{r}\nvar(d$x1) # variance of x1\nvar(d$x2) # variance of x2\n```\n\nAnd now most of the variance is in `PC1`.  \n\n- We can see this in the eigenvalues \n\n```{r}\npca$sdev ^2\n```\n\n- and also by just looking at the variances of the PCs directly\n\n```{r}\nvar(pca$x[,1]) # variance of PC1\nvar(pca$x[,2]) # variance of PC2\n```\n\n\n## Reconstructing the Original Data\n\nIf we use all of the principal components, we can reconstruct the original data exactly.  However, if our goal is dimensionality reduction, we plan to use fewer than our full set of PCs in our subsequent analyses.  Therefore, it can be instructive to see how well we can reproduce the raw data using fewer PCs.\n\nIf we used `prcomp()` to get the PCs, we can reconstruct the original data using the following code\n\n- Lets use this code with our toy dataset\n- We will recreate the raw data using the first principal component only from our example.\n\nThis first step reproduces the original Xs, but in their centered/scaled format\n```{r}\nn_pc <- 1 # use only the first PC\nx_estimated <- pca$x[, 1:n_pc] %*% t(pca$rotation[, 1:n_pc])\n```\n\nIf the raw data were scaled and centered, we need to add back the means and scale the data back to the original scale.  We need to do this in two steps (in this order)\n\nFirst, we unscale the data (we did not scale, so we will show the code but not execute it)\n\n```{r}\n#| eval: false\n\norig_x_sd <- c(0.7852105, 0.846496) # original sd of x1 and x2\nx_estimated <- scale(x_estimated, center = FALSE, scale = 1/orig_x_sd)\n```\n\nAnd then we can add back the means of our raw Xs that we previously subtracted out.\n\n```{r}\norig_x_means <- c(1.81, 1.91) # original means of x1 and x2\nx_estimated <- scale(x_estimated, center = -orig_x_means, scale = FALSE)\n```\n\nLets see how well we were able to recreate the original data using only the first principal component.  Below are plots of the original and reconstructed data.  \n\n- As expected, we do pretty well.  \n- We retain all the variance over the direction of the first PC.  \n- And of course, we lose all the variance over the direction of the second PC because we didnnt use it.  \n- However, in this dataset, most of the variance was in the first PC.  This one PC may likely work well for us. \n```{r}\nd |> \n  ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  xlim(-1,4) +\n  ylim(-1,4) +\n  coord_fixed() +\n  ggtitle(\"Original Data\")\n\nx_estimated |> \n  as_tibble() |> \n  ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  xlim(-1,4) +\n  ylim(-1,4) +\n  coord_fixed() +\n  ggtitle(\"Reconstructed Data\")\n```\n\n## Deciding on the Number of Components to Retain\n\nWhen using PCA for dimensionality reduction, we need to decide how many components to retain.  \n\nLets make another toy dataset with 8 variables that represent a self-report measure that has two subscales.  In this instance, if the scale works well, we might expect that we could reduce the 8 items to just 2 components (one for each subscale).  This would be a nice reduction from 8 variables to 2 variables. (of course, we don't need PCA for this because we could just use two subscales to reduce the dimensionality from 8 to 2, but this is just for demonstration purposes).\n\nWe will use `mvrnorm()` to generate a dataset with 8 variables that are correlated.  \n\n- The first four variables will load primarily on the first component and the last four will load on the second component.  \n- This will create a situation where we can reduce from 8 variables to 2 components.\n```{r}\nset.seed(12345)\ncov_matrix <- matrix(0, nrow = 8, ncol = 8)\ncov_matrix[1:4, 1:4] <- 0.5 \ncov_matrix[5:8, 5:8] <- 0.5\ncov_matrix[1:4, 5:8] <- 0.2 # some cross0loadings to make it more realistic\ncov_matrix[5:8, 1:4] <- 0.2\ndiag(cov_matrix) <- 1 # set variances of all items to 1\n\nd2 <- MASS::mvrnorm(n = 200, mu = rep(0, 8), Sigma = cov_matrix) |> \n  magrittr::set_colnames(str_c(\"i\", 1:8)) |>\n  as_tibble()\n```\n\nGet principal components for this dataset\n\n```{r}\npca2 <- prcomp(d2)\n```\n\nThere are several common approaches for deciding on the number of components to retain:\n  \n1. **Variance Explained**  \n\n    - Look at the proportion of variance explained by each component.  \n    - You can plot this using a scree plot and look for an \"elbow\" in the plot where the addition of more components explains less variance.  \n    - In this case, we would retain the two components to the left of the elbow.\n```{r}\npca2 |> screeplot(type = \"lines\")\n```\n\n2. **PCs that account for more variance than individual variables**\n\n    - The eigenvalues (`$sdev` from `prcomp()`) quantify the variance explained by each component.  \n    - In our example, the individual items had raw variances of 1.  Therefore PCs with eigenvalues > 1 account for more variance than any single item.  \n    - Using this approach, we would retain 2 components\n```{r}\npca2$sdev^2\n```\n\n3. **Cumulative Variance Explained**\n\n    - You can also look at the cumulative proportion of variance explained by the components.  \n    - You may choose to retain enough components such that you explain a certain threshold of variance (e.g., 70% or 90%).\n    - Here is the proportion of total variance accounted for by each PC\n```{r}\nvar_explained <- pca2$sdev^2 / sum(pca2$sdev^2)\n```\n\n- And here is the cumulative variance as more PCs are retained. Of course, the threshold for variance can feel somewhat arbitrary in most instances so we see less use for this method except in special circumstances. \n```{r}\nvar_explained |> cumsum()\n```\n\n\n## Limitations of PCA\n\nPCA is a linear transformation of the data.  As a result, it only works well if the structure in the data is linear.  In our preceding example, there was a clear linear relationship between the two variables `x1` and `x2`.  PCA was able to capture this linear relationship well with the first principal component. \n\nIf instead, there was a more complicated, non-linear relationship between the variables, PCA may not be able to capture the structure of the data well with just a few components.  In this case, we may need to use more components to capture the variance in the data or consider other non-linear dimensionality reduction techniques (e.g., using auto-encoders as in our neural networks unit)\n\nLets see this problem in a final toy example where this is a quadratic relationship between `x1` and `x2`.  In this case, PCA will not be able to capture the variance in the data well with just one component.\n\n```{r}\nset.seed(12345)\nd3 <- tibble(x1 = rnorm(200, 0, 1),\n             x2 = rnorm(200, 0, .5) + (x1)^2) |>  # quadratic relationship\n             mutate(x1 = x1 / sd(x1),\n                    x2 = x2 / sd(x2)) # keeping scale of both Xs comparable for simplicity\n\nd3 |> ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  xlim(-4, 4) +\n  ylim(-1, 7) +\n  coord_fixed()\n```\n\nDo the PCA\n```{r}\npca3 <- prcomp(d3)\n```\n\nLook at eigenvalues for the two PCs.  PCA wasn't successful at capturing the variance in the data with just one component.  The first component only accounts for only slightly more variance that the original Xs (raw Xs has variance of 1.0 for both `x1` and `x2`). \n\n```{r}\npca3$sdev ^2\n```\n\nAnd if we were to try to reconstruct the original data using only the first principal component, we would not do a good job.  PCA is not a good choice for dimensionality reduction in this case!!\n\n```{r}\nn_pc <- 1 # use only the first PC\nx_estimated3 <- pca3$x[, 1:n_pc] %*% t(pca3$rotation[, 1:n_pc])\norig_x_means3 <- c(mean(d3$x1), mean(d3$x2)) # original means of x1 and x2\nx_estimated3 <- scale(x_estimated3, center = -orig_x_means3, scale = FALSE)\n\nx_estimated3 |> \n  as_tibble() |> \n  ggplot(aes(x = x1, y = x2)) +\n  geom_point() +\n  xlim(-1,4) +\n  ylim(-1,4) +\n  coord_fixed() +\n  ggtitle(\"Reconstructed Data\")\n```\n\n## PCA in Tidymodels\n\nWe can use PCA for dimensionality reduction as part of our feature engineering.\n\n- `step_pca()` uses `prcomp()`\n- See [help](https://recipes.tidymodels.org/reference/step_pca.html)\n- Default is center = false and scale = false.  You definitely want to center and maybe scale predictors in a previous recipe step before using `step_pca()`\n- You can choose number of components to retain by specifying the exact number (`num_comp = `) or by indicating the minimum variance retained across PCs (`threshold = `)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["book.css"],"output-file":"app_pca.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","bibliography":["refs.bib"],"callout-icon":false,"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}