{"title":"Resampling Methods for Model Selection and Evaluation","markdown":{"yaml":{"output":"html_document","editor_options":{"chunk_output_type":"console"}},"headingText":"Resampling Methods for Model Selection and Evaluation","containsRefs":false,"markdown":"\n\n\n## Overview of Unit\n\n### Learning Objectives\n\n- Bias vs. variance wrt model performance estimates\n  - How is this different from bias vs. variable of model itself\n- Methods for computationally intense calculations\n  - Parallel processing\n  - Cache\n- Types of resampling\n  - Validation set approach\n  - Leave One Out CV\n  - K-Fold and Repeated K-Fold\n  - Grouped K-Fold\n  - Bootstrap resampling\n- Use of resampling for tuning hyperparameters  \n- Combining these resampling approaches with a Test set\n  - Used for simultaneous model selection and evaluation\n  - Single independent test set\n  - Advanced topic: Nested resampling\n\n-----\n\n### Readings\n\n- @APM [Chapter 4, pp 61 - 80](https://vuquangnguyen2016.files.wordpress.com/2018/03/applied-predictive-modeling-max-kuhn-kjell-johnson_1518.pdf)\n\n- **Supplemental:** @ISL [Chapter 5, pp 197 - 208 186](pdfs/isl_2.pdf)\n\nPost questions to the readings channel in Slack\n\n### Lecture Videos\n\n- [Lecture 1: Overview & Parallel Processing](https://mediaspace.wisc.edu/media/iaml+unit+5-1/1_sv29qmvz) ~ 16 mins\n- [Lecture 2: Introduction to Resampling](https://mediaspace.wisc.edu/media/iaml+unit+5-2/1_pyr647gu) ~ 11 mins\n- [Lecture 3: Single Validation/Test Set Approach](https://mediaspace.wisc.edu/media/iaml+unit+5-3/1_21l94nvt) ~ 26 mins\n- [Lecture 4: Leave One Out Cross Validation](https://mediaspace.wisc.edu/media/iaml+unit+5-4/1_uuaa5nj1) ~ 9 mins \n- [Lecture 5: K-fold Cross Validation Approaches](https://mediaspace.wisc.edu/media/iaml+unit+5-5/1_f60kfk66) ~ 21 mins\n- [Lecture 6: Repeated and Grouped K-fold Approaches](https://mediaspace.wisc.edu/media/iaml+unit+5-6/1_a6yo0euq) ~ 11 mins\n- [Lecture 7: Bootstrap Resampling](https://mediaspace.wisc.edu/media/iaml+unit+5-7/1_zijywf6b) ~ 11 mins \n- [Lecture 8: Using Resampling to Select Best Model Configurations](https://mediaspace.wisc.edu/media/iaml+unit+5-8/1_0sxcqnj4) ~ 17 mins\n- [Lecture 9: Resampling for Both Model Selection and Evaluation](https://mediaspace.wisc.edu/media/iaml+unit+5-9/1_b8abgszd) ~ 11 mins\n- [Lecture 10: Nested Resampling](https://mediaspace.wisc.edu/media/iaml+unit+5-10/1_g9jh3461) ~ 14 mins\n\nPost questions to the video-lectures channel in Slack\n\n-----\n\n### Application Assignment and Quiz\n  \n- [data](application_assignments/unit_05/smoking_ema.csv)\n- [data dictionary](application_assignments/unit_05/data_dictionary.csv)\n- [qmd shell](https://raw.githubusercontent.com/jjcurtin/book_iaml/main/application_assignments/unit_05/hw_unit_05_resampling.qmd)\n- [solution](https://dionysus.psych.wisc.edu/iaml/key_unit_05_resampling.html) \n\n\nPost questions to application-assignments Slack channel\n\nSubmit the application assignment [here](https://canvas.wisc.edu/courses/395546/assignments/2187690) and complete the [unit quiz](https://canvas.wisc.edu/courses/395546/quizzes/514059) by 8 pm on Wednesday, February 21st\n\n-----\n\n## Some Technical Details for Costly Computations\n\nBefore we dive into resampling, we need to introduce two coding techniques that can save us a lot of time when implementing resampling methods\n\n- Parallel processing\n- Caching time-consuming computations\n\n-----\n\n### Parallel Processing\n\nWhen using resampling, we often end up fitting many, many model configurations\n\n- This can be the same model configuration in many different training sets\n- Or many different model configurations in many different training sets (even more computationally demanding)\n\\\n\\\n\nCritically\n\n- The fitting process for each of these configurations is independent for the others\n- The order that the configurations are fit doesn't matter either\n- When these two criteria are met, the processes can be run in parallel with an often big time savings\n\n-----\n\nTo do this in R, we need to set up a parallel processing backend\n\n- Lots of [options and details](https://tune.tidymodels.org/articles/extras/optimizations.html) depending on the code you intend to run in parallel to do it really well\n- We can discuss some of these issues/details and other solutions (i.e., High Throughput Computing at CHTC)\n- Some options are OS specific\n- Provide more details [elsewhere](https://jjcurtin.github.io/book_dwvt/parallel_processing.html)\n\n-----\n\nTLDR - copy the following code chunk into your scripts after you load your other libraries (e.g., tidyverse and tidymodels)\n```{r}\n#| eval: false\ncl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))\ndoParallel::registerDoParallel(cl)\n```\n\n-----\n\n### Using Cache\n\nEven with parallel processing, resampling procedures can STILL take a lot of time, particularly on notebook computers that don't have a lot of cores available\n\\\n\\\nIn these instances, you may also want to consider caching the result\n\n- When you cache some set of calculations, you are essentially saving the results of the calculations\n- If you need to run the script again, you simply load the saved calculations again from disk, rather than re-calculating them (its much quicker to just read them from a file)\n\\\n\\\n\nBut...\n\n- You need to redo the calculations if you change anything in your script that could affect them\n- This is called \"invalidating the cache\"\n- You need to be very careful to reuse the cache when you can but also to invalidate it when the calculations have changed\n\n-----\n\nIn other notes, we describe [three options](https://jjcurtin.github.io/book_dwvt/cache.html) to cache calculations that are available in R.  \n\n- You should read more about those options if you plan to use one\n- Our preferred solution is to use `xfun::cache_rds()` \n- Read the help for this function (`?xfun::cache_rds`) if you plan to use it \n- Cache is complicated and can lead to errors.   \n- But cache can also save you a lot of time during development!\n\n-----\n\nStart by loading only that function for the `xfun` package.  You can add this line of code after your other libraries (e.g., tidyverse, tidymodels)\n\n```{r}\n#| eval: false\nlibrary(xfun, include.only = \"cache_rds\")\n```\n\n-----\n\nTo use the function\n\n- You will pass the code for the calculations you want to cache as the first argument (`expr`) to the function inside a set of curly brackets `{}`\n- You need to list the path (`dir =`) and filename (`file =`) for the rds file that will save the cached calculations.  \n  - The `/` at the end of the path is needed.  \n  - You should use a meaningful (and distinct) filename.\n- Provide `rerun = FALSE` as a third argument.  \n  - You can set this to true temporarily if you need to invalidate the cache to redo the calculations\n  - We like to set it up as an environment variable (see `rerun_setting` below)\n  - Keep it as FALSE during development\n  - Set it to TRUE at the end of our development so that we make sure we didn't make any cache invalidation errors\n- You may also provide a list of globals to `hash =`.  See more details at previous link\n```{r}\n#| eval: false\n\ncache_rds(\n  expr = {\n },\n dir = \"cache/\",\n file = \"filename\", \n rerun = rerun_setting \n)\n```\n\\\n\\\nWe will demonstrate the use of this function throughout the book.  BUT you do not need to use it if you find it confusing.\n\n-----\n\n## Introduction to Resampling\n\nWe will use resampling for two goals:\n\n- To **select** among model configurations based on relative performance estimates of these configurations in new data\n- To **evaluate** the performance of our best/final model configuration in new data\n\nFor both of these goals we are using new data to **estimate performance** of model configuration(s)\n\n-----\n\nThere are two kinds of problems that can emerge from using a sub-optimal resampling approach\n\n- We can get a **biased estimate** of model performance (i.e., we can systematically under or over-estimate its performance)\n- We can get an **imprecise estimate** of model performance (i.e., high variance in our model performance metric if it was repeatedly calculated in different samples of held-out data)\n\n-----\n\nEssentially, this is the bias and variance problem again, but now not with respect to the model's actual performance but instead with **our estimate of how the model will perform**\n\nThis is a very important distinction to keep in mind or you will be confused as we discuss bias and variance into the future.  We have:\n\n- bias and variance of model performance (i.e., the predictions the model makes)\n- bias and variance of our estimate of how well the model will perform in new data\n- different factors affect each\n\n-----\n\n```{r}\n#| include: false\n\n# set up environment.  Now hidden from view\n\noptions(conflicts.policy = \"depends.ok\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\ntidymodels_conflictRules()\n\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\nlibrary(xfun, include.only = \"cache_rds\")\n\ncl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))\ndoParallel::registerDoParallel(cl)\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\npath_data <- \"./data\"\n\nrerun_setting <- FALSE \n``` \n\nLet's get a dataset for this unit.  We will use the [heart disease dataset](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) from the UCI Machine Learning Repository.  We will focus on the Cleveland data subset, whose variable are defined in this [data dictionary](data/cleveland_variables.pdf)\n\nThese data are less well prepared\n\n- No variable/column names exist\n- NA is coded with `?`\n- Use `rename()` to add tidy variable names\n\n```{r}\ndata_all <- read_csv(here::here(path_data, \"cleveland.csv\"), \n                     col_names = FALSE, # <1> \n                     na = \"?\") |> # <2> \n  rename(age = X1,\n         sex = X2,\n         cp = X3,\n         rest_bp = X4,\n         chol = X5,\n         fbs = X6,\n         rest_ecg = X7,\n         max_hr = X8,\n         exer_ang = X9,\n         exer_st_depress = X10,\n         exer_st_slope = X11,\n         ca = X12,\n         thal = X13,\n         disease = X14)\n```\n1. Indicating that column names are NOT on the first row.  First row begins with data\n2. Specifying a non-standard value for NA\n\n-----\n\nCode categorical variables as factors with meaningful text labels (and no spaces)\n\n```{r}\ndata_all <- data_all |> \n  mutate(disease = factor(disease, levels = 0:4, \n                          labels = c(\"no\", \"yes\", \"yes\", \"yes\", \"yes\")),\n         sex = factor(sex,  levels = c(0, 1), labels = c(\"female\", \"male\")),\n         fbs = factor(fbs, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_ang = factor(exer_ang, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_st_slope = factor(exer_st_slope, levels = 1:3, \n                                labels = c(\"upslope\", \"flat\", \"downslope\")),\n         cp = factor(cp, levels = 1:4, \n                     labels = c(\"typ_ang\", \"atyp_ang\", \"non_anginal\", \"non_anginal\")),\n         rest_ecg = factor(rest_ecg, levels = 0:2, \n                           labels = c(\"normal\", \"abnormal1\", \"abnormal2\")),\n         rest_ecg = fct_collapse(rest_ecg, \n                                abnormal = c(\"abnormal1\", \"abnormal2\")),\n         thal = factor(thal, levels = c(3, 6, 7), \n                       labels = c(\"normal\", \"fixeddefect\", \"reversabledefect\"))) |> \n  glimpse()\n```\n\n-----\n\nWe won't do EDA in this unit but lets at least do a quick skim to inform ourselves\n\n- 303 cases\n- a dichotomous outcome, disease (yes or no for heart disease)\n- 7 other categorical predictors\n- 6 numeric predictors\n- 2 missing values for `thal`, which is categorical\n- 4 missing values for ca, which is numeric\n\n```{r}\ndata_all |> skim_all()\n```\n\n-----\n\nWe will be fitting a logistic regression with all of the predictors for the first half of this unit\n\\\n\\\nLets set up a recipe for feature engineering with this statistical algorithm\n\n- Impute missing data for all numeric predictors using median imputation \n- Impute missing data for all nominal predictors using the modal value\n- Dummy code all nominal predictors\n```{r}\nrec_lr <- recipe(disease ~ ., data = data_all) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) \n```\n\n-----\n\nThe order of steps in a recipe matter\n\\\n\\\nWhile your projectâ€™s needs may vary, here is a suggested order of potential steps that should work for most problems [according to tidy models folks](https://recipes.tidymodels.org/articles/Ordering.html):\n\n- [Convert character to factor]  (we do this outside our recipe as part of cleaning)\n- Impute\n- Individual transformations for skewness and other issues\n- Discretize (if needed and if you have no other choice)\n- Create dummy variables\n- Create interactions\n- Normalization steps (center, scale, range, etc)\n- Multivariate transformation (e.g. PCA, spatial sign, etc)\n\n-----\n\n## The single validation (test) set approach\n\nTo date, you have essentially learned how to do the single validation set approach (although we haven't called it that)\n\\\n\\\nWith this approach, we would take our full n = 303 and:\n\n- Split into one training set and one held-out set\n- Fit a model in our training set\n- Use this trained model to predict scores in held-out set\n- Calculate a performance metric (e.g., accuracy, rmse) based on predicted and observed scores in the held-out set\n\n-----\n\nIf our goal was to evaluate the expected performance of a single model configuration in new data\n\n  - We called this held-out set a test set\n  - We would report this performance metric from the held-out test set as our estimate of the performance of our model in new data\n\n-----\n\nIf our goal was to select the best model configuration among many candidate configurations\n\n- We called this held-out set a validation set\n- We would use this performance metric from the held-out validation set to select the best model configuration\n\n-----\n\nWe call this the single validation set approach but that single held-out set can be either a validation or test set depending on our goals\n\\\n\\\nIf you need to BOTH select a best model configuration AND evaluate that best model configuration, you would need both a validation and a test set.\n\n-----\n\nWe have been doing the single validation set approach all along but we will provide one more example now (with a 50/50 split) to transition the code we are using to a more general workflow that will accommodate our more complicated resampling approaches\n\\\n\\\nIn the first half of this unit, we will focus on assessing the performance of a single model configuration\n\n  - Logistic regression algorithm\n  - No hyperparameters\n  - Features based on all available predictors\n  \nWe will call the held-out set a test set and use it to evaluate the expected future performance of this single configuration\n\n-----\n\nPreviously:\n\n- We would fit the model configuration in training and then made predictions for observations in the held-out test set in separate steps\n- We did this in separate steps so you could better understand the process\n- I will show you that first again as a baseline\n  \nThen:\n\n- We will now do these tasks in one step using $validation\\_split()$\n- I will show you this combined approach second\n- This latter approach will be an example for how we code this for our more complicated resampling approaches\n\n-----\n\n- Let's do a 50/50 split, stratified on our outcome, disease\n\n```{r}\nset.seed(19690127)\n\nsplits <- data_all |> \n  initial_split(prop = 0.5, strata = \"disease\")\n\ndata_trn <- analysis(splits)\ndata_trn |>  nrow()\n\ndata_test <- assessment(splits)\ndata_test |> nrow()\n```\n\n-----\n\n- Make features for train and test (skim them on your own time!)\n```{r}\nrec_prep <- rec_lr |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_test <- rec_prep |> \n  bake(data_test)\n```\n\n-----\n\n- Fit model in train\n\n```{r}\nfit_lr <-\n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(disease ~ ., data = feat_trn)\n```\n\n-----\n\n- Evaluate model in test \n\n```{r}\naccuracy_vec(feat_test$disease, predict(fit_lr, feat_test, type = \"class\")$.pred_class)\n```\n\n-----\n\nNow lets do this in a new and more efficient workflow\n\n- We still start by settting up a splits object\n- Note use of `splits_validate()` rather than `initial_split()`\n- We will use a variety of functions at this step depending on how we decide to handle resampling\n```{r}\nset.seed(19690127)\nsplits_validate <- data_all |> \n  validation_split(prop = 0.5, strata = \"disease\")\n```\n\n-----\n\nNow we can fit our model configuration in our training set(s) and calculate performance metric(s) in the held-out sets using `fit_resamples()`\n\n-  You can and should [read more](https://tune.tidymodels.org/reference/fit_resamples.html) about this function\n- Takes algorithm (broad category, engine, and mode if needed), recipe, and splits as inputs\n- Specify the (set of) metrics we want to use to estimate for our model configuration\n- Don't need to explicitly create feature matrices for held-in and held-out sets.\n  - But also don't see these feature matrices\n  - May still want to create and skim them as a check?\n\n```{r}\nfits_lr <-\n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit_resamples(preprocessor = rec_lr, resamples = splits_validate, \n                 metrics = metric_set(accuracy))\n```\n\n-----\n\nThe object (we will call it `fits_`) that is returned in NOT a model using our model figuration (what we got using `fit()`, which we called `fit_`)\n\\\n\\\nInstead, it contains the performance metrics for the configuration, estimated by \n\n- Fitting the model configuration in the held in set(s) and then\n- Predicting into the held-out sets  \n\n-----\n\nWe pull these performance estimates out of the fits object using `collect_metrics()`\n\n- There is one performance estimate\n- It is for our model configurations performance in test set\n- It is an estimate of how well our model configuration will work with new data\n- It matches what we got previously when doing this manually\n\n```{r}\nfits_lr |> \n  collect_metrics(summarize = FALSE)\n```\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: How many participants were used to fit the model that we used to estimate the performance of our model configuration? \n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nThe model configuration was fit in the training set. The training set had N = 151 participants.\n```\n\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: If we planned to implement this model (i.e., really use it in practice to predict heart disease in new patients) is this the best model we can develop or can we improve it?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nThis model was trained with N = 151 but we have 303 participants.  If we trained\nthe same model configuration with all of our data, that model would be expected\nto performance better than the N-151 model.  \n```\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: Why will the N = 303 model always be better (or at worst equivalent) to the model trained with N = 151.\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nIncreasing the sample size used to fit our model configuration will decrease\nmodel variance but not change model bias. This will produce overall lower \nerror.\n\nThis might not be true if the additional data were not similar quality to our\ntraining data but we know our validation/test set is similar because we did a\nrandom resample.\n```\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: So why did we not just fit this model configuration using N = 303 to start?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nBecause then we would not have had any new data left to get an estimate of its performance in new data!\n```\n:::\n\n-----\n\nIf you plan to actually use your model in the real world for prediction, you should always re-fit the best configuration using all available data!\n\n\n::: {.callout-important collapse=\"false\"}\n### Question: But what does this mean about our estimate of the performance of this final model (fit with all available data) data when we get that estimate using a model that as fit with a smaller sample size (the sample size in our training set)?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nOur estimate will likely be biased.  It will underestimate the true expected \npeformance of our final model.  We can think of it as a lower bound on that expected\nperformance.  The amount of bias will be a function of the difference between the \nsample size of the training set and the size the of full dataset.   If we want less \nbiased estimates, we want to allocate as much data as possible to the training set \nwhen estimating the performance of our final model configuration (but this will come \nwith other costs!)\n```\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: Contrast the costs/benefits of a 50/50 vs. 80/20 split for train and test\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nUsing a training set with 80% of the sample will yield a less biased (under)\nestimate of the final (using all data) model performance than a training set with\n50% of the sample.  \n\nHowever, using a test set of 20% of the data will produce a \nmore variable (less precise) estimate of performance than the 50% test set.  \n\nThis  is another bias-variance trade off but now instead of talking about model\nperformance, we are seeing that we have to trade off bias and variance in our\nestimate of the model performance too!\n```\n:::\n\n\nThis recognition of a bias-variance trade-off in our performance estimates is what motivates the more complicated resampling approaches we will now consider.\n\n-----\n\nIn our example, we plan to use this model for future predictions, so now lets fit it a final time using the full dataset\n\n- We do this manually\n- NOTE: tidymodels has routines to do all of this including fitting final models (read more about \"workflows\")\n- We do not use them in the course because they hide steps that are important for conceptual understanding\n- We do not use them in our lab because we break apart all of these steps to train models using high throughput computing\n\n-----\n\nMake a feature matrix for the full dataset\n\\\n\\\nWe are now using the full data set as our new training set so we prep and bake with the full dataset\n```{r}\nrec_prep <- rec_lr |> \n  prep(data_all)\n\nfeat_all <- rec_prep |> \n  bake(data_all)\n```\n\n-----\n\nAnd then fit your model configuration\n\n```{r}\nfit_lr <-\n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(disease ~ ., data = feat_all)\n```\n\n-----\n\n```{r}\nfit_lr |> tidy()\n```\n\nIf we need to predict disease in the future, this is the model we would use (with these parameter estimates)\n\\\n\\\nOur estimate of its future accuracy is based on our previous assessment using the **held-in** training set to fit the model configuration and the **held-out** test set to estimate its performance\n\\\n\\\nThis estimate should be considered a lower bound on its expected performance\n\n-----\n\n## Leave One Out Cross Validation\n\nLet's turn to a new resampling technique and start with some questions to motivate it\n\n\n::: {.callout-important collapse=\"false\"}\n### Question: How could you use this single validation set approach to get the least biased estimate of model performance with your n = 303 dataset that would still allow you to estimate its performance in a held out test set?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"show answer\"\nPut all but one case into the training set (i.e., leave only one case out in the\ntest set).  In our example, you would fit a model with n = 302  this model will \nhave essentially equivalent overfitting as n = 303 so it will not yield much bias \nwhen we use it to estimate the performance of the n = 303 model.\n```\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: What will be the biggest problem with this approach?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nYou will estimate performance with only n = 1 in the test set.  This means there \nwill be high variance in your performance estimate.\n```\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: How might you reduce this problem?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nRepeat this split between training and test n times so that there are n different\nsets of n = 1 test sets.  Then average the performance across all n of these test \nsets to get a more stable estimate of performance. Averaging is a good way to reduce \nthe variance of any estimate.  \n\nThis is leave one out cross-validation!\n```\n:::\n\n-----\n\nComparisons across LOOCV and single validation set approaches\n\n- The performance estimate from LOOCV has less bias than the single validation set method (because the models that are used to estimate performance were fit with close to the full n of the final model that will be fit to all the data)\n\n- LOOCV uses all observations in the held-out set at some point.   This may yield less variance than single 20% or 50% validation set?\n\nbut...\n\n- LOOCV can be computationally expensive (need to fit and evaluate the same model configuration n times).  \n- This is a real problem when you are also working with a high number of model configurations (i.e., number fits = n * number of model configurations).\n\n-----\n\nLOOCV eventually uses all the data in the held-out set across the 'n' held-out sets.  \n\n- Averaging also helps reduce variance in the performance metric.\n- However, averaging reduces variance to a greater degree when the performance measures being averaged are less related/more independent.\n- The n fitted models are very similar in LOOCV b/c they are each fit on almost the same data (each with n-1 observations)\n\\\n\\\n\nK-fold cross validation (next method) improves the variance of the average performance metric by averaging across more independent (less overlapping) training sets\n\n- For this reason, it is superior and (always?) preferred over LOOCV \n- We are not demonstrating LOOCV b/c we strongly prefer other methods (k-fold)\n- Still important to understand it conceptually and its strengths/weaknesses\n- If you wanted to use this resampling approach, simply substitute `loo_cv()` for `vfold_cv()` in the next example\n\n-----\n\n## K-fold Cross Validation\n\nK-fold cross validation\n\n1.  Divide the observations into K equal size independent \"folds\" (each observation appears in only one fold)\n2.  Hold out 1 of these folds (1/Kth of the dataset) to use as a held-out set\n3.  Fit a model in the remaining K-1 folds\n4.  Repeat until each of the folds has been held out once\n5.  Performance estimate is the average performance across the K held-out folds\n\\\n\\\n\nCommon values of K are 5 and 10\n\\\n\\\nNote that K is sometimes referred to as V in some fields/literatures (Don't blame me!)\n\n-----\n\nVisualization of K-fold\n\n![](figs/unit5_kfold.png){height=5in}\n\n-----\n\nLet's demonstrate the code for K-fold Cross-validation\n\n- First, we split into 10 folds (default)\n  - repeats = 1 (default; more on this in a bit)\n  - stratify on `disease`\n\n```{r}\nsplits_kfold <- data_all |> \n  vfold_cv(v = 10, repeats = 1, strata = \"disease\")\n\nsplits_kfold\n```\n\n-----\n\n- Fit model configuration in first 9 folds, evaluate in 10th fold.  Repeat 9 more times for each additional held-out fold\n  - Use `fit_resamples()` as before \n  - Still no need to continue to remake features for held-in and held-out sets.  Just provide `splits` and `rec`\n  - Set performance metrics with `metric_set()`\n\n```{r}\nfits_lr_kfold <- \n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit_resamples(preprocessor = rec_lr, \n                resamples = splits_kfold, \n                metrics = metric_set(accuracy))\n```\n\n-----\n\n- Then, we review performance estimates in held out folds using `collect_metrics()`\n\n  - Can see performance in all folds using `summarize = FALSE`\n  - The performance estimates are not all the same.  This is what we mean when we talk about the variance in our performance estimates.  You couldn't see it before with only one held-out set but now that we have 10, it becomes more concrete.  Ideally this variance is as low as possible. \n```{r}\nmetrics_kfold <- collect_metrics(fits_lr_kfold, summarize = FALSE)\n\nmetrics_kfold |> print_kbl()\n```\n\n-----\n\n- Could plot this as a histogram to visualize this variance (i.e., the sampling distribution of performance estimates)\n- Would be better if we had more folds (see repeats in a bit)\n```{r}\nmetrics_kfold |> plot_hist(\".estimate\")\n``` \n\n-----\n\n- Can see the average performance over folds along with its standard error using `summarize = TRUE`\n- This average will have lower variance (which is estimated by the standard error.  Still not zero!)\n```{r}\ncollect_metrics(fits_lr_kfold, summarize = TRUE)\n```\n\n-----\n\n  As a last step, we still fit the final model as before using the full dataset\n\n  - We already have the feature matrix for the full dataset (`feat_all`) from earlier\n  - Otherwise, remake it\n```{r}\nfit_lr <-\n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(disease ~ ., data = feat_all)\n```\n\n-----\n\n```{r}\nfit_lr |> tidy()\n```\n\\\n\\\nIf we need to predict disease in the future, this is the fitted model we would use (with these parameter estimates)\n\\\n\\\nOur estimate of its future accuracy is `r collect_metrics(fits_lr_kfold, summarize = TRUE)$mean` with a standard error of `r collect_metrics(fits_lr_kfold, summarize = TRUE)$std_err`\n\n-----\n\nComparisons between K-fold vs. LOOCV and Single Validation set\n\\\n\\\nFor Bias:\n\n- K-fold typically has less bias than the single validation set method\n  - E.g. 10-fold fits models with 9/10th of the data vs. 50% or 80%, etc\n\n- K Fold has somewhat more bias than LOOCV because LOOCV uses n - 1 observations for fitting models\n\n-----\n\nFor Variance:\n\n- K-fold has less variance than LOOCV\n  - Like LOOCV, it uses all observations in test at some point\n  - The averaged models are more independent b/c models are fitted on less overlapping training sets\n  \n- K-fold has less variance than single validation set b/c it uses all data as test at some point (vs. a subset of held-out test data)\n\n- K-fold is less computationally expensive than LOOCV (though more expensive than single validation set)\n\n- K-fold is generally preferred over both of these other approaches\n\n-----\n\nK-fold is less computationally intensive BUT still can be costly.  Particularly when you are getting performance estimates for multiple model configurations (more on that when we learn how to tune hyperparameters)\n\\\n\\\nSo you may want to start caching the `fits_` object so you don't have to recalculate it.  \n\n-----\n\nHere is a demonstration\n\n- First we set up an environment variable that we can flip between true/false to invalidate all our cached calculations\n- Put this near the top of your code with other environment settings so its easy to find\n```{r}\n#| eval: false\n\nrerun_setting <- TRUE \n```\n\n-----\n\n- Now use the `cache_rds()` function\n- Put the resampling code inside of `{}`\n- Cached file will be saved in `cached/` folder with filename `fits_lr_kfold_HASH`\n- If you change any code in the function, it will invalidate the cache and re-run the code\n- However, it will not invalidate the cache if you change objects or data that affect this function but are outside it, e.g., \n  - Fix errors in data\n  - Update `rec_lr`\n  - Update `splits_kfold`\n  - Be VERY careful!\n- You can manually invalidate just the cache for just this code chunk by setting `rerun = TRUE` temporarily or  you can change `rerun_setting <- TRUE` at the top of your script to do fresh calculations for all of your cached code chunks\n- You can set `rerun_settings <- TRUE` when you are done with your development to make sure everything is accurate (review output carefully for any changes!)\n\n```{r fits_lr_kfold}\nfits_lr_kfold <- cache_rds(\n  expr = {\n    logistic_reg() |> \n    set_engine(\"glm\") |> \n    fit_resamples(preprocessor = rec_lr, \n                  resamples = splits_kfold, \n                  metrics = metric_set(accuracy))\n  }, \n  dir = \"cache/005/\",\n  file = \"fits_lr_kfold\",\n  rerun = rerun_setting)\n```\n\n-----\n\n## Repeated K-fold Cross Validation\n\nYou can repeat the K-fold procedure multiple times with new splits for a different mix of K folds each time\n\\\n\\\nTwo benefits:\n\n- More stable performance estimate (because averaged over more folds: repeats * K)\n- Many more estimates of performance to characterize (SE; plot) of your performance estimate\n\\\n\\\n\nBut it is computationally expensive (depending on number of repeats)\n\n-----\n\nAn example of Repeated K-fold Cross-validation\n\n- Splits with repeats = 10 (will do 10 different splits of 10-fold)\n```{r}\nset.seed(19690127)\nsplits_kfold10x <- data_all |> \n  vfold_cv(v = 10, repeats = 10, strata = \"disease\")\n\nsplits_kfold10x\n```\n\n-----\n\n- Everything else is the same!\n```{r fits_lr_kfold10x}\nfits_lr_kfold10x <- cache_rds(\n  expr = {\n    logistic_reg() |> \n      set_engine(\"glm\") |> \n      fit_resamples(preprocessor = rec_lr, \n                    resamples = splits_kfold10x, \n                    metrics = metric_set(accuracy))\n  }, \n  dir = \"cache/005/\",\n  file = \"fits_lr_kfold10x\",\n  rerun = rerun_setting)\n```\n\n-----\n\n- Individual estimates across 100 held-out folds\n```{r}\nmetrics_kfold10x <- collect_metrics(fits_lr_kfold10x, summarize = FALSE)\n\nmetrics_kfold10x |> print_kbl()\n```\n\n-----\n\n- Histogram of those 100 individual estimates\n```{r}\nmetrics_kfold10x |> plot_hist(\".estimate\", bins = 10)\n```\n\n-----\n\nAverage performance estimated (and its SE) across the 100 held-out folds\n```{r}\ncollect_metrics(fits_lr_kfold10x, summarize = TRUE)\n```\n\n-----\n\n- You should also refit a final model in the full data at the end as before\n- We wont demonstrate that here\n\n-----\n\nComparisons between repeated K-fold and K-fold\n\\\n\\\nRepeated K-fold: \n\n- Has same bias as K-fold (still fitting models with K-1 folds)\n- Has all the benefits of single K-fold\n- Has even more stable estimate of performance (mean over more folds/repeats)\n- Provides more info about distribution for the performance estimate\n- **But** is more computationally expensive\n\\\n\\\n\nRepeated K-fold is preferred over K-fold to the degree possible based on computational limitations (parallel, N, p, statistical algorithm, # of model configurations)\n\n-----\n\n## Grouped K-fold\n\nWe have to be particularly careful with resampling methods when we have repeated observations for the same participant (or unit of analysis more generally)\n\n- We can often predict an individual's own data better using some of their own data.  \n- If our model will not ever encounter that individual again, this will optimistically bias our estimate of our models performance with new/future observations.  \n- We can remove that bias by making sure that all observations from an individual are grouped together so that they always either held-in or held-out but never split across both.  \n- Easy to do a grouped K-fold by making splits using [`group_vfold_cv()`](https://rsample.tidymodels.org/reference/group_vfold_cv.html) and then proceeding as before with all other analyses/code\n  - set the `group` argument to the name of the variable that codes for subid or unit of analysis that is repeated.\n\n-----\n\n## Bootstrap Resampling\n\nA bootstrap sample is a random sample taken with replacement (i.e., same observations can be sampled multiple times within one bootstrap sample)\n\\\n\\\nIf you bootstrap a new sample of size n from a dataset with sample size n, approximately 63.2% of the original observations end up in the bootstrap sample\n\\\n\\\nThe remaining 36.8% of the observations are often called the \"out of bag\" (OOB) samples\n\n-----\n\nBootstrap Resampling\n\n- Creates B bootstrap samples of size n = n from the original dataset\n- For any specific bootstrap (b)\n  - Model(s) are fit to the bootstrap sample\n  - Model performance is evaluated in the associated out of bag (held-out) samples\n- This is repeated B times such that you have B assessments of model performance\n\n-----\n\nAn example of Bootstrap resampling\n\n- Again, all that changes is how you form the splits/resamples\n- You will use [$bootstraps()$](https://rsample.tidymodels.org/reference/bootstraps.html) to form the splits\n- Here are 100 bootstraps stratified on `disease`\n```{r}\nset.seed(19690127)\nsplits_boot <- data_all |> \n  bootstraps(times = 100, strata = \"disease\") \n\nsplits_boot\n```\n\n-----\n\n- Everything else is the same!\n```{r fits_lr_boot}\nfits_lr_boot <- cache_rds(\n  expr = {\n    logistic_reg() |> \n      set_engine(\"glm\") |> \n      fit_resamples(preprocessor = rec_lr, \n                    resamples = splits_boot, \n                    metrics = metric_set(accuracy))\n\n  },\n  dir = \"cache/005/\",\n  file = \"fits_lr_boot\", \n  rerun = rerun_setting)\n```\n\n-----\n\n- 100 individual performance estimates from the 100 OOB sets\n```{r}\nmetrics_boot <- collect_metrics(fits_lr_boot, summarize = FALSE)\n\nmetrics_boot |> print_kbl()\n```\n\n-----\n\n- Histogram of those 100 performance estimates\n```{r}\nmetrics_boot |> plot_hist(\".estimate\", bins = 10)\n```\n\n-----\n\n- Average performance over those 100 estimates\n```{r}\ncollect_metrics(fits_lr_boot, summarize = TRUE)\n```\n\n- You should also refit a final model in the full data at the end as before to get final single fitted model for later use\n\n-----\n\nRelevant comparisons, strengths/weaknesses for bootstrap for resampling\n\n-  The bootstrap resampling performance estimate will have higher bias than K-fold using typical K values (bias equivalent to about K = 2)\n- Although training sets have full n, they only include about 63% unique observations.  These models under perform training sets with 80 - 90% unique observations\n- With smaller training set sizes, this bias is considered too high by some (Kuhn)\n \n-----\n\n- The bootstrap resampling performance estimate will have less variance than K-fold\n- Compare SE of accuracy for 100 resamples using k-fold with repeats: `r collect_metrics(fits_lr_kfold10x, summarize = TRUE)$std_err` vs. bootstrap: `r collect_metrics(fits_lr_boot, summarize = TRUE)$std_err`\n- With 1000 bootstraps (and test sets with ~ 37% of n) can get a very precise estimate of held-out error\n\n-----\n\n- Can also represent the variance of our held-out error (like repeated K-fold)\n\n- Used primarily for selecting among model configurations when you don't care about bias and just want a precise selection metric\n- Useful in explanation scenarios where you just need the \"best\" model\n- \"Inner loop\" of nested cross validation (more on this later)\n\n-----\n\n## Using Resampling to Select Best Model Configurations \n\nIn all of the previous examples, we have used various resampling methods only to evaluate the performance of a single model configuration in new data.  In these instances, we were treating the held-out sets as test sets.\n\\\n\\\nResampling is also used to get held out performance estimates to select best model configurations.  \n\\\n\\\n**Best** means the model configuration that performs the best in new data and therefore is closest to the true DGP for the data\n\n-----\n\nFor example, we might want to select among model configurations in an explanatory scenario to have a principled approach to determine the model configuration that best matches the true DGP (and would be best to test your hypotheses).  e.g., \n\n- Selecting covariates to include\n- Deciding on X transformations\n- Outlier identification approach\n- Statistical algorithm\n\n-----\n\nWe can simply get performance estimates for each configuration using one of the previously described resampling methods\n\n- We would call the held-out data (the single set, the folds, the OOB samples) a validation set\n- We select the model configuration with the best mean (or median?) across our resampled validation sets on the relevant performance metric.\n\n-----\n\nOne additional common scenario where you will do model selection across many model configurations is when \"tuning\" (i.e., selecting) the best values for hyperparameters for a statistical algorithm (e.g., k in KNN).  \n\n-----\n\n`tidymodels` makes this easy and it follows a very similar workflow as earlier with a few changes\n\n- We will need to indicate which hyperparameters we plan to tune in the statistical algorithm\n- We need (or can) select values to consider for that hyperparameter (or we can let the `tune` package functions decide in some cases)\n- We will now use [`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html) rather `fit_resamples()` to fit and evaluate the models configurations that differ with respect to their hyperparameters\n- This IS computationally costly.  Now fitting and estimating performance for multiple configurations across multipe held-in/held-out sets\n\n-----\n\nLets use bootstrap resampling to select the best K for KNN applied to our heart disease dataset\n\\\n\\\nWe can use the same splits are established as before (`splits_boot`)\n\\\n\\\nWe need a slightly different recipe for KNN vs. logistic regression\n\n- We have to scale (lets range correct) the features \n- No need to do this to the dummy features.  They are already range corrected\n\n```{r}\nrec_knn <- recipe(disease ~ ., data = data_all) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_range(all_numeric()) |> \n  step_dummy(all_nominal_predictors())\n```\n\n-----\n\nThe fitting process is what is different\n\n- We set up a tibble with values of the hyperparameters to consider\n- We indicate which hyperparameters need to be tuned\n  \n```{r}\nhyper_grid <- expand.grid(neighbors = seq(1, 150, by = 3))\nhyper_grid\n```\n  \n-----  \n\n- When model configurations differ by features or statistical algorithms, we have to use `fit_resamples()` multiple times to estimate performance of those different configurations\n- Held out performance estimates for model configurations that differ by hyperparameter values are obtained in one step using `tune_grid()` \n- We need to set `grid =` \n```{r fits_knn_boot}\nfits_knn_boot <- cache_rds(\n  expr = {\n    nearest_neighbor(neighbors = tune()) |> \n      set_engine(\"kknn\") |> \n      set_mode(\"classification\") |>\n      tune_grid(preprocessor = rec_knn, \n                resamples = splits_boot, \n                grid = hyper_grid,\n                metrics = metric_set(accuracy))\n\n  },\n  dir = \"cache/005/\",\n  file = \"fits_knn_boot\",\n  rerun = rerun_setting)\n```\n\n-----\n\nReviewing performance of model configurations is similar to before but now with multiple configurations\n\n- We can see the average performance over folds along with its standard error using `summarize = TRUE`\n- We *could* see the performance of each configuration in EACH fold too, but there are lots of them (use `summarize = FALSE`)\n```{r}\ncollect_metrics(fits_knn_boot, summarize = TRUE)\n```\n\n-----\n\n-  We can plot average performance by the values of the hyperparameter\n```{r}\ncollect_metrics(fits_knn_boot, summarize = TRUE) |> \n  ggplot(aes(x = neighbors, y = mean)) +\n    geom_line()\n```\n\nK (neighbors) is affecting the bias-variance trade-off.  As K increases, model bias increases but model variance decreases.  In most instances, model variance decreases faster than model bias increases.  Therefore performance should increase and then peak at a good point along the bias-variance trade-off.  Beyond this optimal value, performance should decrease again.  You want to select a hyperparameter value that is associated with peak (or near peak) performance.\n\n-----\n\nThe simplest way to select among model configurations (e.g., hyperparameters) is to choose the model configuration with the best performance\n\n```{r}\nshow_best(fits_knn_boot, n = 10)\n\nselect_best(fits_knn_boot)\n```\n\n-----\n\nThe next most common is to choose the simplest (least flexible) model that has performance within one SE of the best performing configuration.\n\n- Use `select_by_one_std_err()`\n- Sort performance results from least to most flexible (e.g., `desc(neighbors)`)\n```{r}\nselect_by_one_std_err(fits_knn_boot, \n                      desc(neighbors))\n```\n\n-----\n\n- We should also refit a final model with the \"best\" hyperparameter using the full data as before\n\n\n```{r}\nrec_prep <- rec_knn |>   \n  prep(data_all)\n\nfeat_all <- rec_prep |> \n  bake(data_all)\n```\n\n-----\n\n- We can use the `select_*()` from above to use this best hyperparameter in our specification of the algorithm\n\n- Note that we now fit using all the data and switch to `fit()` rather than `tune_grid()`\n\n```{r}\nfit_knn_best <-\n  nearest_neighbor(neighbors = select_best(fits_knn_boot)$neighbors) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |>\n  fit(disease ~ ., data = feat_all)\n```\n\n-----\n\n- However, we can't use the previous bootstrap resampling to evaluate this final/best model because we already used it to select this the best configuration\n- We need new, held-out data to evaluate it (a new test set!)\n\n-----\n\n## Resampling for Both Model Selection and Evaluation\n\nResampling methods can be used to get model performance estimates to **select** the best model configuration and/or **evaluate** that best model\n\n- So far we have done EITHER selection OR evaluation but not both together\n\n- The concepts to both select the best configuration and evaluation it are similar but it requires different (slightly more complicated) resampling than what we have done so far\n\n-----\n\n- If you use your held-out resamples to select the best model among a number of model configurations then the same held-out resamples cannot also be used to evaluate the performance of that same best model\n\n- If it is, the performance metric will have **optimization bias**.  To the degree that there is any noise (i.e., variance) in the measurement of performance, selecting the best model configuration will capitalize on this noise.\n\n- You need to use one set of held out resamples (validation sets) to select the best model.  Then you need a DIFFERENT set of held out resamples (test sets) to evaluate that best model.   \n\n-----\n\nThere are two strategies for this: \n\n- Strategy 1: \n    - First, hold out a test set for final/best model evaluation (using `initial_split()`).  \n    - Then use one of the above resampling methods (single validation set approach, k-fold, or bootstrap) to select the best model configuration.  \n    - Bootstrap is likely best option b/c it is typically more precise (though biased)\n  \n- Strategy 2: Nested resampling.  More on this in a moment\n\n-----\n\nOther Observations about Common Practices: \n\n- Simple resampling methods with the full sample (and no held-out test set) to both select AND evaluate are still common\n- Failure by some (even Kuhn) to appreciate the degree of optimization bias\n- Particular problem in Psychology because of small n (high variance in our performance metric)?\n- Can be fine if you just want to find the best model configuration but don't need to evaluate its performance rigorously (i.e., you don't really care that much about validity of your performance estimate of your final  model and are fine with it being a bit optimistic)\n\n-----\n\n### Bootstrap with Test Set\n\n- First we divide our data into training and test using `inital_split()`\n\n- Next, we use bootstrap resampling with the training set to split training into many held-in and held-out sets.  We use these held-out (OOB) sets as validation sets to select the best model configuration based on mean/median performance across those sets.  \n  \n- After we select the best model configuration using bootstrap resampling of the training set\n  - We refit that model configuration in the FULL training set\n  - And we use that model to predict into the test set to evaluate it\n\n- Of course, in the end, if you plan to use the model, you will refit this final model configuration to the FULL dataset but the performance estimate for that model will come from test set on th previous step (there are no more data to estimate new performance)\n\n-----\n  \n- Use `initial_split()` for first train/test split\n\n```{r}\nset.seed(123456)\nsplits_test <- data_all |> \n  initial_split(prop = 2/3, strata = \"disease\")\n\ndata_trn <- splits_test |> \n  analysis()\n\ndata_test <- splits_test |> \n  assessment()\n```\n\n-----\n\n- Use training set for model selection via resampling (in this case bootstrap)\n- No need for another seed\n\n```{r}\nsplits_boot_trn <- data_trn |> \n  bootstraps(times = 100, strata = \"disease\") \n```\n\n- Use the same grid of hyperparameters we set up earlier (`hyper_grid`)\n\n-----\n\n- Fit model configurations that vary by K in all 100 bootstrap samples\n- Make predictions and calculate accuracy for these fitted models in 100 OOB (validation) sets\n\n```{r fits_knn_boot_trn}\nfits_knn_boot_trn <- cache_rds(\n  expr = {\n    nearest_neighbor(neighbors = tune()) |> \n      set_engine(\"kknn\") |> \n      set_mode(\"classification\") |>\n      tune_grid(preprocessor = rec_knn, \n                resamples = splits_boot_trn, \n                grid = hyper_grid, \n                metrics = metric_set(accuracy))\n  },\n  dir = \"cache/005/\",\n  file = \"fits_knn_boot_trn\",\n  rerun = rerun_setting)\n```\n\n-----\n\n- Select the best model configuration (best k)\n  - K = 97 is the best model configuration determined by bootstrap resampling \n\n  - BUT this is NOT the correct estimate of its performance in new data\n\n  - We compared 50 model configurations (values of k).  This performance estimate may have some optimization bias (though 50 model configurations is really not THAT many)\n\n\n```{r}\nshow_best(fits_knn_boot_trn, n = 10)\n```\n\n-----\n\n- show exact means\n```{r}\nshow_best(fits_knn_boot_trn, n = 10)$mean\n```\n\n- select best\n```{r}\nselect_best(fits_knn_boot_trn)\n```\n\n-----\n\n- Fit the k = 97 model configuration in the full training set\n\n```{r}\nrec_prep <- rec_knn |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_test <- rec_prep |> \n  bake(data_test)\n\nfit_knn_best <-\n  nearest_neighbor(neighbors = select_best(fits_knn_boot_trn)$neighbors) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |>\n  fit(disease ~ ., data = feat_trn)\n```\n\n-----\n\n- Use that fitted model to predict into test set\n\n```{r}\naccuracy_vec(feat_test$disease, predict(fit_knn_best, feat_test)$.pred_class)\n```\n\n-----\n\nOur best estimate of how accurate a model with k = `r select_best(fits_knn_boot_trn)$neighbors` would be in new data is `r accuracy_vec(feat_test$disease, predict(fit_knn_best, feat_test)$.pred_class)`.   \n\n- However, it was only fit with n = 203 (the training set).  \n- If we truly want the best model, we should now train once again with all of our data.  \n- This model would likely perform even better b/c it has > n.  \n- However, we have no more new data to evaluate it!\n- There is no perfect performance estimate!\n\n## Nested Resampling\n\nAnd now the final, mind-blowing extension!!!!!\n\\\n\\\nThe bootstrap resampling + test set approach to simultaneously select and evaluate models is commonly used\n\\\n\\\nHowever, it suffers from the same problems as the single train, valdition, test set approach when it comes to evaluating the performance of the final best model\n  \n- It only uses a single small held out test set.  In this case, 1/3 of the total sample size.\n  - This will yield a high variance/imprecise estimate of model performance\n \n- It also yields a biased estimate of model performance\n  - The model we evaluated in test was fit to only the training data which was only 2/3 of total sample size\n  - Yet our true final model is trained with the full dataset\n  - We are likely underestimating its true performance\n\n-----\n\nNested resampling offers an improvement with respect to these two issues\n\\\n\\\nNested resampling involves two loops\n\n  - The inner loop is used for model selection\n  - The outer loop is used for model evaluation\n\\\n\\\n\nNested resampling is VERY CONFUSING at first (like the first year you use it!)\n\\\n\\\nNested resampling isn't fully supported by tidymodels as of yet.  You  have to do some coding to iterate over the outer loop\n\\\n\\\nApplication of nested resampling is outside the scope of this course but you should understand it conceptually.  For further reading on the implementation of this method, see [an example](https://www.tidymodels.org/learn/work/nested-resampling/) provided by the tidymodels folks.\n\n-----\n\nA 'simple' example using bootstrap for inner loop and 10-fold CV for outer loop\n\n- Divide the full sample into 10 folds\n- Iterate through those 10 folds as follows (this is the outer loop)\n  - Hold out fold 1\n  - Use folds 2-10 to do the inner loop bootstrap\n      - Bootstrap these folds B times\n      - Fit models in B bootstrap samples\n      - Calculate *selection performance metrics* in B out-of-bag samples\n      - Average the B bootstrapped *selection performance metrics* for each model configuration\n      - Select the best model configuration using this average bootstrapped *selection performance metric*\n  - Use best model configuration from folds 2-10 to make predictions for the held-out fold 1 to get the first (of ten) *evaluation performance metrics*\n  - Repeat for held out fold 2 - 10\n- Average the 10 *evaluation performance metrics.*   This is the expected performance of a best model configuration (selected by B bootstraps) in new data.  [You still don't know what configuration you should use because you have 10 'best' model configurations]\n- Do B bootstraps with the full sample to select the best model configuration\n- Fit this best model configuration to the full data\n\n-----\n\nNested resampling evaluates a fitting and selection process not a specific model configuration!\n\n- You therefore need to select a final model configuration using same resampling with full data\n\n- You then need to fit that new model configuration to the full data\n\n- That was the last two steps on the previous page\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: Why bootstrap on inner loop and k-fold on outer loop?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nThe inner loop is used for selecting models.  Bootstrap yields low variance performance\nestimates (but they are biased).  We want low variance to select best model\nconfiguration.  K-fold is a good method for less biased performance estimates.  \nWe want less bias in our final evaluation of our best model.  You can do repeated\nK-fold in the outer loop to both reduce its variance and give you a sense of the\nperformance sampling distribution. BUT VERY COMPUTATIONALLY INTENSIVE\n```\n:::\n\n-----\n\n## Data Exploration with Advanced Resampling Methods \nFinal words on resampling:\n\n- Iterative methods (K-fold, bootstrap) are superior to single validation set approach wrt bias-variance trade-off in performance measurement\n\n- K-Fold resampling should be used if you looking for a performance estimate of a single model configuration\n\n- Bootstrap resampling should be used if you are looking only to choose among model configurations but don't need an independent assessment of that final model\n\n- Bootstrap resampling + Test set or Nested Resampling should be used when you plan to both select among model configurations AND evaluate the best model\n\n\nIn scenarios where you will not have one test set but will eventually use all the data as test ast some point (i.e., k-fold for evaluation of a single model configuration or Nested CV)\n\n- Think carefully about how you do EDA\n- Resampling reduces overfitting in the model selection process \n- Can use eyeball sample (10-20% of full data) with little impact on final performance measure \n\n\n## Discussion\n\n### Announcements\n\n- Starting Unit 6 (Regularization/Penalized models) at end of class today; All set!\n- Unit 7 is Mid-term Exam unit\n  - No readings, lecture or quiz\n  - Application assignment as exam (due at normal application assignment day/time; Weds, March 6th at 8pm)\n  - Conceptual exam during discussion meeting (Thursday March 7th at 11 am) \n- Room assignments; 121 next week but staying in 338;  Thanks for the feedback!\n- EDA with new datasets (and the last application assignment)\n\nFeedback\n\n- concepts in color?, code visually distinct!!\n- tables for contrasts vs. text. - will try for this year but can't promise (still working on terms)\n- reading papers using the methods - have one selected will consider more once we know enough!\n- other ways to submit questions linked to **lecture slides**??\n- direct links to slides format?\n- captions? Help me!\n\n-----\n\n### Bias and Variance\n\n- General definitions - need to think about repeated estimation of something ($\\hat{f}$, $\\hat{DGP}$; $\\hat{accuracy}$, $\\hat{rmse}$)\n- Examples for our models ($\\hat{f}$, $\\hat{DGP}$)\n- Examples for our performance estimates ($\\hat{accuracy}$, $\\hat{rmse}$)\n\n-----\n\n### Describe process to use resampling to get a performance estimate to evaluate of a single model configuration\n\n- Which method? \n- Use held-in/held-out terminology in addition to train/test    \n- Discuss the bias and variance of the performance estimate of this configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n\n-----\n\n### Describe process to use resampling to get performance estimates to select a best model configuration among many (explanatory setting?)\n\n- When done (what are you selecting among)?\n- Which method? \n- Use held-in/held-out terminology in addition to train/val\n- Discuss the bias and variance of the performance estimate of this configuration\n- Discuss the bias and variance of the performance estimate used to select the best configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n- What are the implications if you use that same performance estimate to evaluate that best model configuration (i.e., estimate its performance in new data)?\n\n-----\n\n### Describe process to use resampling (other than nested) to get performance estimates to both select best configuration and evaluate it in new data\n\n- Which method?\n- Describe how to do it using held-in/held-out terminology in addition to train/val/test\n- Discuss the bias and variance of the performance estimate used to select the best configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n- Discuss the bias and variance of the performance estimate used to evaluate that best configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n\n-----\n\n### Describe process to use Nested CV to get performance estimates to both select best configuration and evaluate it in new data\n\n- When used?\n- Describe how to do it using held-in/held-out terminology in addition to train/val/test\n- Discuss the bias and variance of the performance estimate used to select the best configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n- Discuss the bias and variance of the performance estimate used to evaluate that best configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n\n-----\n\n### Methods to EITHER select best model configuration (among many) OR evaluate a single best model configuration\n\nWhat are pros, cons, and when to use?\n\n  - Single validation (or test) set\n  - LOOCV\n  - K-fold\n  - Repeated k-fold\n  - Bootstrap\n\n  - ALSO: grouped k-fold\n\n-----\n\n### Methods to BOTH select best model configuration and evaluate that best configuration\n\n- What is optimization bias (its just a form of bias but given a fancy name due to its source)?\n- Combine above method with held out test set\n- Nested CV (see [tutorial](https://www.tidymodels.org/learn/work/nested-resampling/))\n\n-----\n\n### Discuss data leakage with resampling\n\n- Why is data leakage a problem?\n- How does noise factor into the problem?\n- Why does resampling substantially reduce data leakage?\n- How to use eyeball sample with single held out test set\n\n-----\n\n### A final interesting question.....\n\nBiased estimate of model performance has troubled me in the last assignment. I did repeated k-fold cross validation with a KNN model where k = 5. The effective sample size was n ~ 800 during model selection. Then I refit the model with the full training data where n ~ 1000 and submitted my predictions for the test set. After some testing with one of the TAs, I figured that my submitted model with a larger N performed worse than the model evaluated with resamples, if I used the same K determined via resampling (number of nearest neighbors). My hunch was that the optimal K (number of nearest neighbors) was different in a model trained with n ~ 800 than a model trained with n ~ 1000. Is this a possible explanation? If so, would LOOCV better serve the purpose of selecting a K for the final KNN model (so that the training set for selecting K is very much the same as the training set used for the final model)?\n\n- On average, what do you expect about the performance estimate of your final model configuration in test vs. the performance estimates used to select the best model configuration?\n- What if you used bootstrap to select best configuration?\n- ...but, what about choosing k from train and then using that k in a model fit to train and val?\n  - best k, \n  - best k within +- 1 SE (lower limit or higher limit)\n","srcMarkdownNoYaml":"\n\n# Resampling Methods for Model Selection and Evaluation\n\n## Overview of Unit\n\n### Learning Objectives\n\n- Bias vs. variance wrt model performance estimates\n  - How is this different from bias vs. variable of model itself\n- Methods for computationally intense calculations\n  - Parallel processing\n  - Cache\n- Types of resampling\n  - Validation set approach\n  - Leave One Out CV\n  - K-Fold and Repeated K-Fold\n  - Grouped K-Fold\n  - Bootstrap resampling\n- Use of resampling for tuning hyperparameters  \n- Combining these resampling approaches with a Test set\n  - Used for simultaneous model selection and evaluation\n  - Single independent test set\n  - Advanced topic: Nested resampling\n\n-----\n\n### Readings\n\n- @APM [Chapter 4, pp 61 - 80](https://vuquangnguyen2016.files.wordpress.com/2018/03/applied-predictive-modeling-max-kuhn-kjell-johnson_1518.pdf)\n\n- **Supplemental:** @ISL [Chapter 5, pp 197 - 208 186](pdfs/isl_2.pdf)\n\nPost questions to the readings channel in Slack\n\n### Lecture Videos\n\n- [Lecture 1: Overview & Parallel Processing](https://mediaspace.wisc.edu/media/iaml+unit+5-1/1_sv29qmvz) ~ 16 mins\n- [Lecture 2: Introduction to Resampling](https://mediaspace.wisc.edu/media/iaml+unit+5-2/1_pyr647gu) ~ 11 mins\n- [Lecture 3: Single Validation/Test Set Approach](https://mediaspace.wisc.edu/media/iaml+unit+5-3/1_21l94nvt) ~ 26 mins\n- [Lecture 4: Leave One Out Cross Validation](https://mediaspace.wisc.edu/media/iaml+unit+5-4/1_uuaa5nj1) ~ 9 mins \n- [Lecture 5: K-fold Cross Validation Approaches](https://mediaspace.wisc.edu/media/iaml+unit+5-5/1_f60kfk66) ~ 21 mins\n- [Lecture 6: Repeated and Grouped K-fold Approaches](https://mediaspace.wisc.edu/media/iaml+unit+5-6/1_a6yo0euq) ~ 11 mins\n- [Lecture 7: Bootstrap Resampling](https://mediaspace.wisc.edu/media/iaml+unit+5-7/1_zijywf6b) ~ 11 mins \n- [Lecture 8: Using Resampling to Select Best Model Configurations](https://mediaspace.wisc.edu/media/iaml+unit+5-8/1_0sxcqnj4) ~ 17 mins\n- [Lecture 9: Resampling for Both Model Selection and Evaluation](https://mediaspace.wisc.edu/media/iaml+unit+5-9/1_b8abgszd) ~ 11 mins\n- [Lecture 10: Nested Resampling](https://mediaspace.wisc.edu/media/iaml+unit+5-10/1_g9jh3461) ~ 14 mins\n\nPost questions to the video-lectures channel in Slack\n\n-----\n\n### Application Assignment and Quiz\n  \n- [data](application_assignments/unit_05/smoking_ema.csv)\n- [data dictionary](application_assignments/unit_05/data_dictionary.csv)\n- [qmd shell](https://raw.githubusercontent.com/jjcurtin/book_iaml/main/application_assignments/unit_05/hw_unit_05_resampling.qmd)\n- [solution](https://dionysus.psych.wisc.edu/iaml/key_unit_05_resampling.html) \n\n\nPost questions to application-assignments Slack channel\n\nSubmit the application assignment [here](https://canvas.wisc.edu/courses/395546/assignments/2187690) and complete the [unit quiz](https://canvas.wisc.edu/courses/395546/quizzes/514059) by 8 pm on Wednesday, February 21st\n\n-----\n\n## Some Technical Details for Costly Computations\n\nBefore we dive into resampling, we need to introduce two coding techniques that can save us a lot of time when implementing resampling methods\n\n- Parallel processing\n- Caching time-consuming computations\n\n-----\n\n### Parallel Processing\n\nWhen using resampling, we often end up fitting many, many model configurations\n\n- This can be the same model configuration in many different training sets\n- Or many different model configurations in many different training sets (even more computationally demanding)\n\\\n\\\n\nCritically\n\n- The fitting process for each of these configurations is independent for the others\n- The order that the configurations are fit doesn't matter either\n- When these two criteria are met, the processes can be run in parallel with an often big time savings\n\n-----\n\nTo do this in R, we need to set up a parallel processing backend\n\n- Lots of [options and details](https://tune.tidymodels.org/articles/extras/optimizations.html) depending on the code you intend to run in parallel to do it really well\n- We can discuss some of these issues/details and other solutions (i.e., High Throughput Computing at CHTC)\n- Some options are OS specific\n- Provide more details [elsewhere](https://jjcurtin.github.io/book_dwvt/parallel_processing.html)\n\n-----\n\nTLDR - copy the following code chunk into your scripts after you load your other libraries (e.g., tidyverse and tidymodels)\n```{r}\n#| eval: false\ncl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))\ndoParallel::registerDoParallel(cl)\n```\n\n-----\n\n### Using Cache\n\nEven with parallel processing, resampling procedures can STILL take a lot of time, particularly on notebook computers that don't have a lot of cores available\n\\\n\\\nIn these instances, you may also want to consider caching the result\n\n- When you cache some set of calculations, you are essentially saving the results of the calculations\n- If you need to run the script again, you simply load the saved calculations again from disk, rather than re-calculating them (its much quicker to just read them from a file)\n\\\n\\\n\nBut...\n\n- You need to redo the calculations if you change anything in your script that could affect them\n- This is called \"invalidating the cache\"\n- You need to be very careful to reuse the cache when you can but also to invalidate it when the calculations have changed\n\n-----\n\nIn other notes, we describe [three options](https://jjcurtin.github.io/book_dwvt/cache.html) to cache calculations that are available in R.  \n\n- You should read more about those options if you plan to use one\n- Our preferred solution is to use `xfun::cache_rds()` \n- Read the help for this function (`?xfun::cache_rds`) if you plan to use it \n- Cache is complicated and can lead to errors.   \n- But cache can also save you a lot of time during development!\n\n-----\n\nStart by loading only that function for the `xfun` package.  You can add this line of code after your other libraries (e.g., tidyverse, tidymodels)\n\n```{r}\n#| eval: false\nlibrary(xfun, include.only = \"cache_rds\")\n```\n\n-----\n\nTo use the function\n\n- You will pass the code for the calculations you want to cache as the first argument (`expr`) to the function inside a set of curly brackets `{}`\n- You need to list the path (`dir =`) and filename (`file =`) for the rds file that will save the cached calculations.  \n  - The `/` at the end of the path is needed.  \n  - You should use a meaningful (and distinct) filename.\n- Provide `rerun = FALSE` as a third argument.  \n  - You can set this to true temporarily if you need to invalidate the cache to redo the calculations\n  - We like to set it up as an environment variable (see `rerun_setting` below)\n  - Keep it as FALSE during development\n  - Set it to TRUE at the end of our development so that we make sure we didn't make any cache invalidation errors\n- You may also provide a list of globals to `hash =`.  See more details at previous link\n```{r}\n#| eval: false\n\ncache_rds(\n  expr = {\n },\n dir = \"cache/\",\n file = \"filename\", \n rerun = rerun_setting \n)\n```\n\\\n\\\nWe will demonstrate the use of this function throughout the book.  BUT you do not need to use it if you find it confusing.\n\n-----\n\n## Introduction to Resampling\n\nWe will use resampling for two goals:\n\n- To **select** among model configurations based on relative performance estimates of these configurations in new data\n- To **evaluate** the performance of our best/final model configuration in new data\n\nFor both of these goals we are using new data to **estimate performance** of model configuration(s)\n\n-----\n\nThere are two kinds of problems that can emerge from using a sub-optimal resampling approach\n\n- We can get a **biased estimate** of model performance (i.e., we can systematically under or over-estimate its performance)\n- We can get an **imprecise estimate** of model performance (i.e., high variance in our model performance metric if it was repeatedly calculated in different samples of held-out data)\n\n-----\n\nEssentially, this is the bias and variance problem again, but now not with respect to the model's actual performance but instead with **our estimate of how the model will perform**\n\nThis is a very important distinction to keep in mind or you will be confused as we discuss bias and variance into the future.  We have:\n\n- bias and variance of model performance (i.e., the predictions the model makes)\n- bias and variance of our estimate of how well the model will perform in new data\n- different factors affect each\n\n-----\n\n```{r}\n#| include: false\n\n# set up environment.  Now hidden from view\n\noptions(conflicts.policy = \"depends.ok\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\ntidymodels_conflictRules()\n\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\nlibrary(xfun, include.only = \"cache_rds\")\n\ncl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))\ndoParallel::registerDoParallel(cl)\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\npath_data <- \"./data\"\n\nrerun_setting <- FALSE \n``` \n\nLet's get a dataset for this unit.  We will use the [heart disease dataset](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) from the UCI Machine Learning Repository.  We will focus on the Cleveland data subset, whose variable are defined in this [data dictionary](data/cleveland_variables.pdf)\n\nThese data are less well prepared\n\n- No variable/column names exist\n- NA is coded with `?`\n- Use `rename()` to add tidy variable names\n\n```{r}\ndata_all <- read_csv(here::here(path_data, \"cleveland.csv\"), \n                     col_names = FALSE, # <1> \n                     na = \"?\") |> # <2> \n  rename(age = X1,\n         sex = X2,\n         cp = X3,\n         rest_bp = X4,\n         chol = X5,\n         fbs = X6,\n         rest_ecg = X7,\n         max_hr = X8,\n         exer_ang = X9,\n         exer_st_depress = X10,\n         exer_st_slope = X11,\n         ca = X12,\n         thal = X13,\n         disease = X14)\n```\n1. Indicating that column names are NOT on the first row.  First row begins with data\n2. Specifying a non-standard value for NA\n\n-----\n\nCode categorical variables as factors with meaningful text labels (and no spaces)\n\n```{r}\ndata_all <- data_all |> \n  mutate(disease = factor(disease, levels = 0:4, \n                          labels = c(\"no\", \"yes\", \"yes\", \"yes\", \"yes\")),\n         sex = factor(sex,  levels = c(0, 1), labels = c(\"female\", \"male\")),\n         fbs = factor(fbs, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_ang = factor(exer_ang, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         exer_st_slope = factor(exer_st_slope, levels = 1:3, \n                                labels = c(\"upslope\", \"flat\", \"downslope\")),\n         cp = factor(cp, levels = 1:4, \n                     labels = c(\"typ_ang\", \"atyp_ang\", \"non_anginal\", \"non_anginal\")),\n         rest_ecg = factor(rest_ecg, levels = 0:2, \n                           labels = c(\"normal\", \"abnormal1\", \"abnormal2\")),\n         rest_ecg = fct_collapse(rest_ecg, \n                                abnormal = c(\"abnormal1\", \"abnormal2\")),\n         thal = factor(thal, levels = c(3, 6, 7), \n                       labels = c(\"normal\", \"fixeddefect\", \"reversabledefect\"))) |> \n  glimpse()\n```\n\n-----\n\nWe won't do EDA in this unit but lets at least do a quick skim to inform ourselves\n\n- 303 cases\n- a dichotomous outcome, disease (yes or no for heart disease)\n- 7 other categorical predictors\n- 6 numeric predictors\n- 2 missing values for `thal`, which is categorical\n- 4 missing values for ca, which is numeric\n\n```{r}\ndata_all |> skim_all()\n```\n\n-----\n\nWe will be fitting a logistic regression with all of the predictors for the first half of this unit\n\\\n\\\nLets set up a recipe for feature engineering with this statistical algorithm\n\n- Impute missing data for all numeric predictors using median imputation \n- Impute missing data for all nominal predictors using the modal value\n- Dummy code all nominal predictors\n```{r}\nrec_lr <- recipe(disease ~ ., data = data_all) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_dummy(all_nominal_predictors()) \n```\n\n-----\n\nThe order of steps in a recipe matter\n\\\n\\\nWhile your projectâ€™s needs may vary, here is a suggested order of potential steps that should work for most problems [according to tidy models folks](https://recipes.tidymodels.org/articles/Ordering.html):\n\n- [Convert character to factor]  (we do this outside our recipe as part of cleaning)\n- Impute\n- Individual transformations for skewness and other issues\n- Discretize (if needed and if you have no other choice)\n- Create dummy variables\n- Create interactions\n- Normalization steps (center, scale, range, etc)\n- Multivariate transformation (e.g. PCA, spatial sign, etc)\n\n-----\n\n## The single validation (test) set approach\n\nTo date, you have essentially learned how to do the single validation set approach (although we haven't called it that)\n\\\n\\\nWith this approach, we would take our full n = 303 and:\n\n- Split into one training set and one held-out set\n- Fit a model in our training set\n- Use this trained model to predict scores in held-out set\n- Calculate a performance metric (e.g., accuracy, rmse) based on predicted and observed scores in the held-out set\n\n-----\n\nIf our goal was to evaluate the expected performance of a single model configuration in new data\n\n  - We called this held-out set a test set\n  - We would report this performance metric from the held-out test set as our estimate of the performance of our model in new data\n\n-----\n\nIf our goal was to select the best model configuration among many candidate configurations\n\n- We called this held-out set a validation set\n- We would use this performance metric from the held-out validation set to select the best model configuration\n\n-----\n\nWe call this the single validation set approach but that single held-out set can be either a validation or test set depending on our goals\n\\\n\\\nIf you need to BOTH select a best model configuration AND evaluate that best model configuration, you would need both a validation and a test set.\n\n-----\n\nWe have been doing the single validation set approach all along but we will provide one more example now (with a 50/50 split) to transition the code we are using to a more general workflow that will accommodate our more complicated resampling approaches\n\\\n\\\nIn the first half of this unit, we will focus on assessing the performance of a single model configuration\n\n  - Logistic regression algorithm\n  - No hyperparameters\n  - Features based on all available predictors\n  \nWe will call the held-out set a test set and use it to evaluate the expected future performance of this single configuration\n\n-----\n\nPreviously:\n\n- We would fit the model configuration in training and then made predictions for observations in the held-out test set in separate steps\n- We did this in separate steps so you could better understand the process\n- I will show you that first again as a baseline\n  \nThen:\n\n- We will now do these tasks in one step using $validation\\_split()$\n- I will show you this combined approach second\n- This latter approach will be an example for how we code this for our more complicated resampling approaches\n\n-----\n\n- Let's do a 50/50 split, stratified on our outcome, disease\n\n```{r}\nset.seed(19690127)\n\nsplits <- data_all |> \n  initial_split(prop = 0.5, strata = \"disease\")\n\ndata_trn <- analysis(splits)\ndata_trn |>  nrow()\n\ndata_test <- assessment(splits)\ndata_test |> nrow()\n```\n\n-----\n\n- Make features for train and test (skim them on your own time!)\n```{r}\nrec_prep <- rec_lr |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_test <- rec_prep |> \n  bake(data_test)\n```\n\n-----\n\n- Fit model in train\n\n```{r}\nfit_lr <-\n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(disease ~ ., data = feat_trn)\n```\n\n-----\n\n- Evaluate model in test \n\n```{r}\naccuracy_vec(feat_test$disease, predict(fit_lr, feat_test, type = \"class\")$.pred_class)\n```\n\n-----\n\nNow lets do this in a new and more efficient workflow\n\n- We still start by settting up a splits object\n- Note use of `splits_validate()` rather than `initial_split()`\n- We will use a variety of functions at this step depending on how we decide to handle resampling\n```{r}\nset.seed(19690127)\nsplits_validate <- data_all |> \n  validation_split(prop = 0.5, strata = \"disease\")\n```\n\n-----\n\nNow we can fit our model configuration in our training set(s) and calculate performance metric(s) in the held-out sets using `fit_resamples()`\n\n-  You can and should [read more](https://tune.tidymodels.org/reference/fit_resamples.html) about this function\n- Takes algorithm (broad category, engine, and mode if needed), recipe, and splits as inputs\n- Specify the (set of) metrics we want to use to estimate for our model configuration\n- Don't need to explicitly create feature matrices for held-in and held-out sets.\n  - But also don't see these feature matrices\n  - May still want to create and skim them as a check?\n\n```{r}\nfits_lr <-\n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit_resamples(preprocessor = rec_lr, resamples = splits_validate, \n                 metrics = metric_set(accuracy))\n```\n\n-----\n\nThe object (we will call it `fits_`) that is returned in NOT a model using our model figuration (what we got using `fit()`, which we called `fit_`)\n\\\n\\\nInstead, it contains the performance metrics for the configuration, estimated by \n\n- Fitting the model configuration in the held in set(s) and then\n- Predicting into the held-out sets  \n\n-----\n\nWe pull these performance estimates out of the fits object using `collect_metrics()`\n\n- There is one performance estimate\n- It is for our model configurations performance in test set\n- It is an estimate of how well our model configuration will work with new data\n- It matches what we got previously when doing this manually\n\n```{r}\nfits_lr |> \n  collect_metrics(summarize = FALSE)\n```\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: How many participants were used to fit the model that we used to estimate the performance of our model configuration? \n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nThe model configuration was fit in the training set. The training set had N = 151 participants.\n```\n\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: If we planned to implement this model (i.e., really use it in practice to predict heart disease in new patients) is this the best model we can develop or can we improve it?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nThis model was trained with N = 151 but we have 303 participants.  If we trained\nthe same model configuration with all of our data, that model would be expected\nto performance better than the N-151 model.  \n```\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: Why will the N = 303 model always be better (or at worst equivalent) to the model trained with N = 151.\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nIncreasing the sample size used to fit our model configuration will decrease\nmodel variance but not change model bias. This will produce overall lower \nerror.\n\nThis might not be true if the additional data were not similar quality to our\ntraining data but we know our validation/test set is similar because we did a\nrandom resample.\n```\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: So why did we not just fit this model configuration using N = 303 to start?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nBecause then we would not have had any new data left to get an estimate of its performance in new data!\n```\n:::\n\n-----\n\nIf you plan to actually use your model in the real world for prediction, you should always re-fit the best configuration using all available data!\n\n\n::: {.callout-important collapse=\"false\"}\n### Question: But what does this mean about our estimate of the performance of this final model (fit with all available data) data when we get that estimate using a model that as fit with a smaller sample size (the sample size in our training set)?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nOur estimate will likely be biased.  It will underestimate the true expected \npeformance of our final model.  We can think of it as a lower bound on that expected\nperformance.  The amount of bias will be a function of the difference between the \nsample size of the training set and the size the of full dataset.   If we want less \nbiased estimates, we want to allocate as much data as possible to the training set \nwhen estimating the performance of our final model configuration (but this will come \nwith other costs!)\n```\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: Contrast the costs/benefits of a 50/50 vs. 80/20 split for train and test\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nUsing a training set with 80% of the sample will yield a less biased (under)\nestimate of the final (using all data) model performance than a training set with\n50% of the sample.  \n\nHowever, using a test set of 20% of the data will produce a \nmore variable (less precise) estimate of performance than the 50% test set.  \n\nThis  is another bias-variance trade off but now instead of talking about model\nperformance, we are seeing that we have to trade off bias and variance in our\nestimate of the model performance too!\n```\n:::\n\n\nThis recognition of a bias-variance trade-off in our performance estimates is what motivates the more complicated resampling approaches we will now consider.\n\n-----\n\nIn our example, we plan to use this model for future predictions, so now lets fit it a final time using the full dataset\n\n- We do this manually\n- NOTE: tidymodels has routines to do all of this including fitting final models (read more about \"workflows\")\n- We do not use them in the course because they hide steps that are important for conceptual understanding\n- We do not use them in our lab because we break apart all of these steps to train models using high throughput computing\n\n-----\n\nMake a feature matrix for the full dataset\n\\\n\\\nWe are now using the full data set as our new training set so we prep and bake with the full dataset\n```{r}\nrec_prep <- rec_lr |> \n  prep(data_all)\n\nfeat_all <- rec_prep |> \n  bake(data_all)\n```\n\n-----\n\nAnd then fit your model configuration\n\n```{r}\nfit_lr <-\n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(disease ~ ., data = feat_all)\n```\n\n-----\n\n```{r}\nfit_lr |> tidy()\n```\n\nIf we need to predict disease in the future, this is the model we would use (with these parameter estimates)\n\\\n\\\nOur estimate of its future accuracy is based on our previous assessment using the **held-in** training set to fit the model configuration and the **held-out** test set to estimate its performance\n\\\n\\\nThis estimate should be considered a lower bound on its expected performance\n\n-----\n\n## Leave One Out Cross Validation\n\nLet's turn to a new resampling technique and start with some questions to motivate it\n\n\n::: {.callout-important collapse=\"false\"}\n### Question: How could you use this single validation set approach to get the least biased estimate of model performance with your n = 303 dataset that would still allow you to estimate its performance in a held out test set?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"show answer\"\nPut all but one case into the training set (i.e., leave only one case out in the\ntest set).  In our example, you would fit a model with n = 302  this model will \nhave essentially equivalent overfitting as n = 303 so it will not yield much bias \nwhen we use it to estimate the performance of the n = 303 model.\n```\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: What will be the biggest problem with this approach?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nYou will estimate performance with only n = 1 in the test set.  This means there \nwill be high variance in your performance estimate.\n```\n:::\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: How might you reduce this problem?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nRepeat this split between training and test n times so that there are n different\nsets of n = 1 test sets.  Then average the performance across all n of these test \nsets to get a more stable estimate of performance. Averaging is a good way to reduce \nthe variance of any estimate.  \n\nThis is leave one out cross-validation!\n```\n:::\n\n-----\n\nComparisons across LOOCV and single validation set approaches\n\n- The performance estimate from LOOCV has less bias than the single validation set method (because the models that are used to estimate performance were fit with close to the full n of the final model that will be fit to all the data)\n\n- LOOCV uses all observations in the held-out set at some point.   This may yield less variance than single 20% or 50% validation set?\n\nbut...\n\n- LOOCV can be computationally expensive (need to fit and evaluate the same model configuration n times).  \n- This is a real problem when you are also working with a high number of model configurations (i.e., number fits = n * number of model configurations).\n\n-----\n\nLOOCV eventually uses all the data in the held-out set across the 'n' held-out sets.  \n\n- Averaging also helps reduce variance in the performance metric.\n- However, averaging reduces variance to a greater degree when the performance measures being averaged are less related/more independent.\n- The n fitted models are very similar in LOOCV b/c they are each fit on almost the same data (each with n-1 observations)\n\\\n\\\n\nK-fold cross validation (next method) improves the variance of the average performance metric by averaging across more independent (less overlapping) training sets\n\n- For this reason, it is superior and (always?) preferred over LOOCV \n- We are not demonstrating LOOCV b/c we strongly prefer other methods (k-fold)\n- Still important to understand it conceptually and its strengths/weaknesses\n- If you wanted to use this resampling approach, simply substitute `loo_cv()` for `vfold_cv()` in the next example\n\n-----\n\n## K-fold Cross Validation\n\nK-fold cross validation\n\n1.  Divide the observations into K equal size independent \"folds\" (each observation appears in only one fold)\n2.  Hold out 1 of these folds (1/Kth of the dataset) to use as a held-out set\n3.  Fit a model in the remaining K-1 folds\n4.  Repeat until each of the folds has been held out once\n5.  Performance estimate is the average performance across the K held-out folds\n\\\n\\\n\nCommon values of K are 5 and 10\n\\\n\\\nNote that K is sometimes referred to as V in some fields/literatures (Don't blame me!)\n\n-----\n\nVisualization of K-fold\n\n![](figs/unit5_kfold.png){height=5in}\n\n-----\n\nLet's demonstrate the code for K-fold Cross-validation\n\n- First, we split into 10 folds (default)\n  - repeats = 1 (default; more on this in a bit)\n  - stratify on `disease`\n\n```{r}\nsplits_kfold <- data_all |> \n  vfold_cv(v = 10, repeats = 1, strata = \"disease\")\n\nsplits_kfold\n```\n\n-----\n\n- Fit model configuration in first 9 folds, evaluate in 10th fold.  Repeat 9 more times for each additional held-out fold\n  - Use `fit_resamples()` as before \n  - Still no need to continue to remake features for held-in and held-out sets.  Just provide `splits` and `rec`\n  - Set performance metrics with `metric_set()`\n\n```{r}\nfits_lr_kfold <- \n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit_resamples(preprocessor = rec_lr, \n                resamples = splits_kfold, \n                metrics = metric_set(accuracy))\n```\n\n-----\n\n- Then, we review performance estimates in held out folds using `collect_metrics()`\n\n  - Can see performance in all folds using `summarize = FALSE`\n  - The performance estimates are not all the same.  This is what we mean when we talk about the variance in our performance estimates.  You couldn't see it before with only one held-out set but now that we have 10, it becomes more concrete.  Ideally this variance is as low as possible. \n```{r}\nmetrics_kfold <- collect_metrics(fits_lr_kfold, summarize = FALSE)\n\nmetrics_kfold |> print_kbl()\n```\n\n-----\n\n- Could plot this as a histogram to visualize this variance (i.e., the sampling distribution of performance estimates)\n- Would be better if we had more folds (see repeats in a bit)\n```{r}\nmetrics_kfold |> plot_hist(\".estimate\")\n``` \n\n-----\n\n- Can see the average performance over folds along with its standard error using `summarize = TRUE`\n- This average will have lower variance (which is estimated by the standard error.  Still not zero!)\n```{r}\ncollect_metrics(fits_lr_kfold, summarize = TRUE)\n```\n\n-----\n\n  As a last step, we still fit the final model as before using the full dataset\n\n  - We already have the feature matrix for the full dataset (`feat_all`) from earlier\n  - Otherwise, remake it\n```{r}\nfit_lr <-\n  logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(disease ~ ., data = feat_all)\n```\n\n-----\n\n```{r}\nfit_lr |> tidy()\n```\n\\\n\\\nIf we need to predict disease in the future, this is the fitted model we would use (with these parameter estimates)\n\\\n\\\nOur estimate of its future accuracy is `r collect_metrics(fits_lr_kfold, summarize = TRUE)$mean` with a standard error of `r collect_metrics(fits_lr_kfold, summarize = TRUE)$std_err`\n\n-----\n\nComparisons between K-fold vs. LOOCV and Single Validation set\n\\\n\\\nFor Bias:\n\n- K-fold typically has less bias than the single validation set method\n  - E.g. 10-fold fits models with 9/10th of the data vs. 50% or 80%, etc\n\n- K Fold has somewhat more bias than LOOCV because LOOCV uses n - 1 observations for fitting models\n\n-----\n\nFor Variance:\n\n- K-fold has less variance than LOOCV\n  - Like LOOCV, it uses all observations in test at some point\n  - The averaged models are more independent b/c models are fitted on less overlapping training sets\n  \n- K-fold has less variance than single validation set b/c it uses all data as test at some point (vs. a subset of held-out test data)\n\n- K-fold is less computationally expensive than LOOCV (though more expensive than single validation set)\n\n- K-fold is generally preferred over both of these other approaches\n\n-----\n\nK-fold is less computationally intensive BUT still can be costly.  Particularly when you are getting performance estimates for multiple model configurations (more on that when we learn how to tune hyperparameters)\n\\\n\\\nSo you may want to start caching the `fits_` object so you don't have to recalculate it.  \n\n-----\n\nHere is a demonstration\n\n- First we set up an environment variable that we can flip between true/false to invalidate all our cached calculations\n- Put this near the top of your code with other environment settings so its easy to find\n```{r}\n#| eval: false\n\nrerun_setting <- TRUE \n```\n\n-----\n\n- Now use the `cache_rds()` function\n- Put the resampling code inside of `{}`\n- Cached file will be saved in `cached/` folder with filename `fits_lr_kfold_HASH`\n- If you change any code in the function, it will invalidate the cache and re-run the code\n- However, it will not invalidate the cache if you change objects or data that affect this function but are outside it, e.g., \n  - Fix errors in data\n  - Update `rec_lr`\n  - Update `splits_kfold`\n  - Be VERY careful!\n- You can manually invalidate just the cache for just this code chunk by setting `rerun = TRUE` temporarily or  you can change `rerun_setting <- TRUE` at the top of your script to do fresh calculations for all of your cached code chunks\n- You can set `rerun_settings <- TRUE` when you are done with your development to make sure everything is accurate (review output carefully for any changes!)\n\n```{r fits_lr_kfold}\nfits_lr_kfold <- cache_rds(\n  expr = {\n    logistic_reg() |> \n    set_engine(\"glm\") |> \n    fit_resamples(preprocessor = rec_lr, \n                  resamples = splits_kfold, \n                  metrics = metric_set(accuracy))\n  }, \n  dir = \"cache/005/\",\n  file = \"fits_lr_kfold\",\n  rerun = rerun_setting)\n```\n\n-----\n\n## Repeated K-fold Cross Validation\n\nYou can repeat the K-fold procedure multiple times with new splits for a different mix of K folds each time\n\\\n\\\nTwo benefits:\n\n- More stable performance estimate (because averaged over more folds: repeats * K)\n- Many more estimates of performance to characterize (SE; plot) of your performance estimate\n\\\n\\\n\nBut it is computationally expensive (depending on number of repeats)\n\n-----\n\nAn example of Repeated K-fold Cross-validation\n\n- Splits with repeats = 10 (will do 10 different splits of 10-fold)\n```{r}\nset.seed(19690127)\nsplits_kfold10x <- data_all |> \n  vfold_cv(v = 10, repeats = 10, strata = \"disease\")\n\nsplits_kfold10x\n```\n\n-----\n\n- Everything else is the same!\n```{r fits_lr_kfold10x}\nfits_lr_kfold10x <- cache_rds(\n  expr = {\n    logistic_reg() |> \n      set_engine(\"glm\") |> \n      fit_resamples(preprocessor = rec_lr, \n                    resamples = splits_kfold10x, \n                    metrics = metric_set(accuracy))\n  }, \n  dir = \"cache/005/\",\n  file = \"fits_lr_kfold10x\",\n  rerun = rerun_setting)\n```\n\n-----\n\n- Individual estimates across 100 held-out folds\n```{r}\nmetrics_kfold10x <- collect_metrics(fits_lr_kfold10x, summarize = FALSE)\n\nmetrics_kfold10x |> print_kbl()\n```\n\n-----\n\n- Histogram of those 100 individual estimates\n```{r}\nmetrics_kfold10x |> plot_hist(\".estimate\", bins = 10)\n```\n\n-----\n\nAverage performance estimated (and its SE) across the 100 held-out folds\n```{r}\ncollect_metrics(fits_lr_kfold10x, summarize = TRUE)\n```\n\n-----\n\n- You should also refit a final model in the full data at the end as before\n- We wont demonstrate that here\n\n-----\n\nComparisons between repeated K-fold and K-fold\n\\\n\\\nRepeated K-fold: \n\n- Has same bias as K-fold (still fitting models with K-1 folds)\n- Has all the benefits of single K-fold\n- Has even more stable estimate of performance (mean over more folds/repeats)\n- Provides more info about distribution for the performance estimate\n- **But** is more computationally expensive\n\\\n\\\n\nRepeated K-fold is preferred over K-fold to the degree possible based on computational limitations (parallel, N, p, statistical algorithm, # of model configurations)\n\n-----\n\n## Grouped K-fold\n\nWe have to be particularly careful with resampling methods when we have repeated observations for the same participant (or unit of analysis more generally)\n\n- We can often predict an individual's own data better using some of their own data.  \n- If our model will not ever encounter that individual again, this will optimistically bias our estimate of our models performance with new/future observations.  \n- We can remove that bias by making sure that all observations from an individual are grouped together so that they always either held-in or held-out but never split across both.  \n- Easy to do a grouped K-fold by making splits using [`group_vfold_cv()`](https://rsample.tidymodels.org/reference/group_vfold_cv.html) and then proceeding as before with all other analyses/code\n  - set the `group` argument to the name of the variable that codes for subid or unit of analysis that is repeated.\n\n-----\n\n## Bootstrap Resampling\n\nA bootstrap sample is a random sample taken with replacement (i.e., same observations can be sampled multiple times within one bootstrap sample)\n\\\n\\\nIf you bootstrap a new sample of size n from a dataset with sample size n, approximately 63.2% of the original observations end up in the bootstrap sample\n\\\n\\\nThe remaining 36.8% of the observations are often called the \"out of bag\" (OOB) samples\n\n-----\n\nBootstrap Resampling\n\n- Creates B bootstrap samples of size n = n from the original dataset\n- For any specific bootstrap (b)\n  - Model(s) are fit to the bootstrap sample\n  - Model performance is evaluated in the associated out of bag (held-out) samples\n- This is repeated B times such that you have B assessments of model performance\n\n-----\n\nAn example of Bootstrap resampling\n\n- Again, all that changes is how you form the splits/resamples\n- You will use [$bootstraps()$](https://rsample.tidymodels.org/reference/bootstraps.html) to form the splits\n- Here are 100 bootstraps stratified on `disease`\n```{r}\nset.seed(19690127)\nsplits_boot <- data_all |> \n  bootstraps(times = 100, strata = \"disease\") \n\nsplits_boot\n```\n\n-----\n\n- Everything else is the same!\n```{r fits_lr_boot}\nfits_lr_boot <- cache_rds(\n  expr = {\n    logistic_reg() |> \n      set_engine(\"glm\") |> \n      fit_resamples(preprocessor = rec_lr, \n                    resamples = splits_boot, \n                    metrics = metric_set(accuracy))\n\n  },\n  dir = \"cache/005/\",\n  file = \"fits_lr_boot\", \n  rerun = rerun_setting)\n```\n\n-----\n\n- 100 individual performance estimates from the 100 OOB sets\n```{r}\nmetrics_boot <- collect_metrics(fits_lr_boot, summarize = FALSE)\n\nmetrics_boot |> print_kbl()\n```\n\n-----\n\n- Histogram of those 100 performance estimates\n```{r}\nmetrics_boot |> plot_hist(\".estimate\", bins = 10)\n```\n\n-----\n\n- Average performance over those 100 estimates\n```{r}\ncollect_metrics(fits_lr_boot, summarize = TRUE)\n```\n\n- You should also refit a final model in the full data at the end as before to get final single fitted model for later use\n\n-----\n\nRelevant comparisons, strengths/weaknesses for bootstrap for resampling\n\n-  The bootstrap resampling performance estimate will have higher bias than K-fold using typical K values (bias equivalent to about K = 2)\n- Although training sets have full n, they only include about 63% unique observations.  These models under perform training sets with 80 - 90% unique observations\n- With smaller training set sizes, this bias is considered too high by some (Kuhn)\n \n-----\n\n- The bootstrap resampling performance estimate will have less variance than K-fold\n- Compare SE of accuracy for 100 resamples using k-fold with repeats: `r collect_metrics(fits_lr_kfold10x, summarize = TRUE)$std_err` vs. bootstrap: `r collect_metrics(fits_lr_boot, summarize = TRUE)$std_err`\n- With 1000 bootstraps (and test sets with ~ 37% of n) can get a very precise estimate of held-out error\n\n-----\n\n- Can also represent the variance of our held-out error (like repeated K-fold)\n\n- Used primarily for selecting among model configurations when you don't care about bias and just want a precise selection metric\n- Useful in explanation scenarios where you just need the \"best\" model\n- \"Inner loop\" of nested cross validation (more on this later)\n\n-----\n\n## Using Resampling to Select Best Model Configurations \n\nIn all of the previous examples, we have used various resampling methods only to evaluate the performance of a single model configuration in new data.  In these instances, we were treating the held-out sets as test sets.\n\\\n\\\nResampling is also used to get held out performance estimates to select best model configurations.  \n\\\n\\\n**Best** means the model configuration that performs the best in new data and therefore is closest to the true DGP for the data\n\n-----\n\nFor example, we might want to select among model configurations in an explanatory scenario to have a principled approach to determine the model configuration that best matches the true DGP (and would be best to test your hypotheses).  e.g., \n\n- Selecting covariates to include\n- Deciding on X transformations\n- Outlier identification approach\n- Statistical algorithm\n\n-----\n\nWe can simply get performance estimates for each configuration using one of the previously described resampling methods\n\n- We would call the held-out data (the single set, the folds, the OOB samples) a validation set\n- We select the model configuration with the best mean (or median?) across our resampled validation sets on the relevant performance metric.\n\n-----\n\nOne additional common scenario where you will do model selection across many model configurations is when \"tuning\" (i.e., selecting) the best values for hyperparameters for a statistical algorithm (e.g., k in KNN).  \n\n-----\n\n`tidymodels` makes this easy and it follows a very similar workflow as earlier with a few changes\n\n- We will need to indicate which hyperparameters we plan to tune in the statistical algorithm\n- We need (or can) select values to consider for that hyperparameter (or we can let the `tune` package functions decide in some cases)\n- We will now use [`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html) rather `fit_resamples()` to fit and evaluate the models configurations that differ with respect to their hyperparameters\n- This IS computationally costly.  Now fitting and estimating performance for multiple configurations across multipe held-in/held-out sets\n\n-----\n\nLets use bootstrap resampling to select the best K for KNN applied to our heart disease dataset\n\\\n\\\nWe can use the same splits are established as before (`splits_boot`)\n\\\n\\\nWe need a slightly different recipe for KNN vs. logistic regression\n\n- We have to scale (lets range correct) the features \n- No need to do this to the dummy features.  They are already range corrected\n\n```{r}\nrec_knn <- recipe(disease ~ ., data = data_all) |> \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |>   \n  step_range(all_numeric()) |> \n  step_dummy(all_nominal_predictors())\n```\n\n-----\n\nThe fitting process is what is different\n\n- We set up a tibble with values of the hyperparameters to consider\n- We indicate which hyperparameters need to be tuned\n  \n```{r}\nhyper_grid <- expand.grid(neighbors = seq(1, 150, by = 3))\nhyper_grid\n```\n  \n-----  \n\n- When model configurations differ by features or statistical algorithms, we have to use `fit_resamples()` multiple times to estimate performance of those different configurations\n- Held out performance estimates for model configurations that differ by hyperparameter values are obtained in one step using `tune_grid()` \n- We need to set `grid =` \n```{r fits_knn_boot}\nfits_knn_boot <- cache_rds(\n  expr = {\n    nearest_neighbor(neighbors = tune()) |> \n      set_engine(\"kknn\") |> \n      set_mode(\"classification\") |>\n      tune_grid(preprocessor = rec_knn, \n                resamples = splits_boot, \n                grid = hyper_grid,\n                metrics = metric_set(accuracy))\n\n  },\n  dir = \"cache/005/\",\n  file = \"fits_knn_boot\",\n  rerun = rerun_setting)\n```\n\n-----\n\nReviewing performance of model configurations is similar to before but now with multiple configurations\n\n- We can see the average performance over folds along with its standard error using `summarize = TRUE`\n- We *could* see the performance of each configuration in EACH fold too, but there are lots of them (use `summarize = FALSE`)\n```{r}\ncollect_metrics(fits_knn_boot, summarize = TRUE)\n```\n\n-----\n\n-  We can plot average performance by the values of the hyperparameter\n```{r}\ncollect_metrics(fits_knn_boot, summarize = TRUE) |> \n  ggplot(aes(x = neighbors, y = mean)) +\n    geom_line()\n```\n\nK (neighbors) is affecting the bias-variance trade-off.  As K increases, model bias increases but model variance decreases.  In most instances, model variance decreases faster than model bias increases.  Therefore performance should increase and then peak at a good point along the bias-variance trade-off.  Beyond this optimal value, performance should decrease again.  You want to select a hyperparameter value that is associated with peak (or near peak) performance.\n\n-----\n\nThe simplest way to select among model configurations (e.g., hyperparameters) is to choose the model configuration with the best performance\n\n```{r}\nshow_best(fits_knn_boot, n = 10)\n\nselect_best(fits_knn_boot)\n```\n\n-----\n\nThe next most common is to choose the simplest (least flexible) model that has performance within one SE of the best performing configuration.\n\n- Use `select_by_one_std_err()`\n- Sort performance results from least to most flexible (e.g., `desc(neighbors)`)\n```{r}\nselect_by_one_std_err(fits_knn_boot, \n                      desc(neighbors))\n```\n\n-----\n\n- We should also refit a final model with the \"best\" hyperparameter using the full data as before\n\n\n```{r}\nrec_prep <- rec_knn |>   \n  prep(data_all)\n\nfeat_all <- rec_prep |> \n  bake(data_all)\n```\n\n-----\n\n- We can use the `select_*()` from above to use this best hyperparameter in our specification of the algorithm\n\n- Note that we now fit using all the data and switch to `fit()` rather than `tune_grid()`\n\n```{r}\nfit_knn_best <-\n  nearest_neighbor(neighbors = select_best(fits_knn_boot)$neighbors) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |>\n  fit(disease ~ ., data = feat_all)\n```\n\n-----\n\n- However, we can't use the previous bootstrap resampling to evaluate this final/best model because we already used it to select this the best configuration\n- We need new, held-out data to evaluate it (a new test set!)\n\n-----\n\n## Resampling for Both Model Selection and Evaluation\n\nResampling methods can be used to get model performance estimates to **select** the best model configuration and/or **evaluate** that best model\n\n- So far we have done EITHER selection OR evaluation but not both together\n\n- The concepts to both select the best configuration and evaluation it are similar but it requires different (slightly more complicated) resampling than what we have done so far\n\n-----\n\n- If you use your held-out resamples to select the best model among a number of model configurations then the same held-out resamples cannot also be used to evaluate the performance of that same best model\n\n- If it is, the performance metric will have **optimization bias**.  To the degree that there is any noise (i.e., variance) in the measurement of performance, selecting the best model configuration will capitalize on this noise.\n\n- You need to use one set of held out resamples (validation sets) to select the best model.  Then you need a DIFFERENT set of held out resamples (test sets) to evaluate that best model.   \n\n-----\n\nThere are two strategies for this: \n\n- Strategy 1: \n    - First, hold out a test set for final/best model evaluation (using `initial_split()`).  \n    - Then use one of the above resampling methods (single validation set approach, k-fold, or bootstrap) to select the best model configuration.  \n    - Bootstrap is likely best option b/c it is typically more precise (though biased)\n  \n- Strategy 2: Nested resampling.  More on this in a moment\n\n-----\n\nOther Observations about Common Practices: \n\n- Simple resampling methods with the full sample (and no held-out test set) to both select AND evaluate are still common\n- Failure by some (even Kuhn) to appreciate the degree of optimization bias\n- Particular problem in Psychology because of small n (high variance in our performance metric)?\n- Can be fine if you just want to find the best model configuration but don't need to evaluate its performance rigorously (i.e., you don't really care that much about validity of your performance estimate of your final  model and are fine with it being a bit optimistic)\n\n-----\n\n### Bootstrap with Test Set\n\n- First we divide our data into training and test using `inital_split()`\n\n- Next, we use bootstrap resampling with the training set to split training into many held-in and held-out sets.  We use these held-out (OOB) sets as validation sets to select the best model configuration based on mean/median performance across those sets.  \n  \n- After we select the best model configuration using bootstrap resampling of the training set\n  - We refit that model configuration in the FULL training set\n  - And we use that model to predict into the test set to evaluate it\n\n- Of course, in the end, if you plan to use the model, you will refit this final model configuration to the FULL dataset but the performance estimate for that model will come from test set on th previous step (there are no more data to estimate new performance)\n\n-----\n  \n- Use `initial_split()` for first train/test split\n\n```{r}\nset.seed(123456)\nsplits_test <- data_all |> \n  initial_split(prop = 2/3, strata = \"disease\")\n\ndata_trn <- splits_test |> \n  analysis()\n\ndata_test <- splits_test |> \n  assessment()\n```\n\n-----\n\n- Use training set for model selection via resampling (in this case bootstrap)\n- No need for another seed\n\n```{r}\nsplits_boot_trn <- data_trn |> \n  bootstraps(times = 100, strata = \"disease\") \n```\n\n- Use the same grid of hyperparameters we set up earlier (`hyper_grid`)\n\n-----\n\n- Fit model configurations that vary by K in all 100 bootstrap samples\n- Make predictions and calculate accuracy for these fitted models in 100 OOB (validation) sets\n\n```{r fits_knn_boot_trn}\nfits_knn_boot_trn <- cache_rds(\n  expr = {\n    nearest_neighbor(neighbors = tune()) |> \n      set_engine(\"kknn\") |> \n      set_mode(\"classification\") |>\n      tune_grid(preprocessor = rec_knn, \n                resamples = splits_boot_trn, \n                grid = hyper_grid, \n                metrics = metric_set(accuracy))\n  },\n  dir = \"cache/005/\",\n  file = \"fits_knn_boot_trn\",\n  rerun = rerun_setting)\n```\n\n-----\n\n- Select the best model configuration (best k)\n  - K = 97 is the best model configuration determined by bootstrap resampling \n\n  - BUT this is NOT the correct estimate of its performance in new data\n\n  - We compared 50 model configurations (values of k).  This performance estimate may have some optimization bias (though 50 model configurations is really not THAT many)\n\n\n```{r}\nshow_best(fits_knn_boot_trn, n = 10)\n```\n\n-----\n\n- show exact means\n```{r}\nshow_best(fits_knn_boot_trn, n = 10)$mean\n```\n\n- select best\n```{r}\nselect_best(fits_knn_boot_trn)\n```\n\n-----\n\n- Fit the k = 97 model configuration in the full training set\n\n```{r}\nrec_prep <- rec_knn |> \n  prep(data_trn)\n\nfeat_trn <- rec_prep |> \n  bake(NULL)\n\nfeat_test <- rec_prep |> \n  bake(data_test)\n\nfit_knn_best <-\n  nearest_neighbor(neighbors = select_best(fits_knn_boot_trn)$neighbors) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |>\n  fit(disease ~ ., data = feat_trn)\n```\n\n-----\n\n- Use that fitted model to predict into test set\n\n```{r}\naccuracy_vec(feat_test$disease, predict(fit_knn_best, feat_test)$.pred_class)\n```\n\n-----\n\nOur best estimate of how accurate a model with k = `r select_best(fits_knn_boot_trn)$neighbors` would be in new data is `r accuracy_vec(feat_test$disease, predict(fit_knn_best, feat_test)$.pred_class)`.   \n\n- However, it was only fit with n = 203 (the training set).  \n- If we truly want the best model, we should now train once again with all of our data.  \n- This model would likely perform even better b/c it has > n.  \n- However, we have no more new data to evaluate it!\n- There is no perfect performance estimate!\n\n## Nested Resampling\n\nAnd now the final, mind-blowing extension!!!!!\n\\\n\\\nThe bootstrap resampling + test set approach to simultaneously select and evaluate models is commonly used\n\\\n\\\nHowever, it suffers from the same problems as the single train, valdition, test set approach when it comes to evaluating the performance of the final best model\n  \n- It only uses a single small held out test set.  In this case, 1/3 of the total sample size.\n  - This will yield a high variance/imprecise estimate of model performance\n \n- It also yields a biased estimate of model performance\n  - The model we evaluated in test was fit to only the training data which was only 2/3 of total sample size\n  - Yet our true final model is trained with the full dataset\n  - We are likely underestimating its true performance\n\n-----\n\nNested resampling offers an improvement with respect to these two issues\n\\\n\\\nNested resampling involves two loops\n\n  - The inner loop is used for model selection\n  - The outer loop is used for model evaluation\n\\\n\\\n\nNested resampling is VERY CONFUSING at first (like the first year you use it!)\n\\\n\\\nNested resampling isn't fully supported by tidymodels as of yet.  You  have to do some coding to iterate over the outer loop\n\\\n\\\nApplication of nested resampling is outside the scope of this course but you should understand it conceptually.  For further reading on the implementation of this method, see [an example](https://www.tidymodels.org/learn/work/nested-resampling/) provided by the tidymodels folks.\n\n-----\n\nA 'simple' example using bootstrap for inner loop and 10-fold CV for outer loop\n\n- Divide the full sample into 10 folds\n- Iterate through those 10 folds as follows (this is the outer loop)\n  - Hold out fold 1\n  - Use folds 2-10 to do the inner loop bootstrap\n      - Bootstrap these folds B times\n      - Fit models in B bootstrap samples\n      - Calculate *selection performance metrics* in B out-of-bag samples\n      - Average the B bootstrapped *selection performance metrics* for each model configuration\n      - Select the best model configuration using this average bootstrapped *selection performance metric*\n  - Use best model configuration from folds 2-10 to make predictions for the held-out fold 1 to get the first (of ten) *evaluation performance metrics*\n  - Repeat for held out fold 2 - 10\n- Average the 10 *evaluation performance metrics.*   This is the expected performance of a best model configuration (selected by B bootstraps) in new data.  [You still don't know what configuration you should use because you have 10 'best' model configurations]\n- Do B bootstraps with the full sample to select the best model configuration\n- Fit this best model configuration to the full data\n\n-----\n\nNested resampling evaluates a fitting and selection process not a specific model configuration!\n\n- You therefore need to select a final model configuration using same resampling with full data\n\n- You then need to fit that new model configuration to the full data\n\n- That was the last two steps on the previous page\n\n-----\n\n::: {.callout-important collapse=\"false\"}\n### Question: Why bootstrap on inner loop and k-fold on outer loop?\n```{html}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show Answer\"\nThe inner loop is used for selecting models.  Bootstrap yields low variance performance\nestimates (but they are biased).  We want low variance to select best model\nconfiguration.  K-fold is a good method for less biased performance estimates.  \nWe want less bias in our final evaluation of our best model.  You can do repeated\nK-fold in the outer loop to both reduce its variance and give you a sense of the\nperformance sampling distribution. BUT VERY COMPUTATIONALLY INTENSIVE\n```\n:::\n\n-----\n\n## Data Exploration with Advanced Resampling Methods \nFinal words on resampling:\n\n- Iterative methods (K-fold, bootstrap) are superior to single validation set approach wrt bias-variance trade-off in performance measurement\n\n- K-Fold resampling should be used if you looking for a performance estimate of a single model configuration\n\n- Bootstrap resampling should be used if you are looking only to choose among model configurations but don't need an independent assessment of that final model\n\n- Bootstrap resampling + Test set or Nested Resampling should be used when you plan to both select among model configurations AND evaluate the best model\n\n\nIn scenarios where you will not have one test set but will eventually use all the data as test ast some point (i.e., k-fold for evaluation of a single model configuration or Nested CV)\n\n- Think carefully about how you do EDA\n- Resampling reduces overfitting in the model selection process \n- Can use eyeball sample (10-20% of full data) with little impact on final performance measure \n\n\n## Discussion\n\n### Announcements\n\n- Starting Unit 6 (Regularization/Penalized models) at end of class today; All set!\n- Unit 7 is Mid-term Exam unit\n  - No readings, lecture or quiz\n  - Application assignment as exam (due at normal application assignment day/time; Weds, March 6th at 8pm)\n  - Conceptual exam during discussion meeting (Thursday March 7th at 11 am) \n- Room assignments; 121 next week but staying in 338;  Thanks for the feedback!\n- EDA with new datasets (and the last application assignment)\n\nFeedback\n\n- concepts in color?, code visually distinct!!\n- tables for contrasts vs. text. - will try for this year but can't promise (still working on terms)\n- reading papers using the methods - have one selected will consider more once we know enough!\n- other ways to submit questions linked to **lecture slides**??\n- direct links to slides format?\n- captions? Help me!\n\n-----\n\n### Bias and Variance\n\n- General definitions - need to think about repeated estimation of something ($\\hat{f}$, $\\hat{DGP}$; $\\hat{accuracy}$, $\\hat{rmse}$)\n- Examples for our models ($\\hat{f}$, $\\hat{DGP}$)\n- Examples for our performance estimates ($\\hat{accuracy}$, $\\hat{rmse}$)\n\n-----\n\n### Describe process to use resampling to get a performance estimate to evaluate of a single model configuration\n\n- Which method? \n- Use held-in/held-out terminology in addition to train/test    \n- Discuss the bias and variance of the performance estimate of this configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n\n-----\n\n### Describe process to use resampling to get performance estimates to select a best model configuration among many (explanatory setting?)\n\n- When done (what are you selecting among)?\n- Which method? \n- Use held-in/held-out terminology in addition to train/val\n- Discuss the bias and variance of the performance estimate of this configuration\n- Discuss the bias and variance of the performance estimate used to select the best configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n- What are the implications if you use that same performance estimate to evaluate that best model configuration (i.e., estimate its performance in new data)?\n\n-----\n\n### Describe process to use resampling (other than nested) to get performance estimates to both select best configuration and evaluate it in new data\n\n- Which method?\n- Describe how to do it using held-in/held-out terminology in addition to train/val/test\n- Discuss the bias and variance of the performance estimate used to select the best configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n- Discuss the bias and variance of the performance estimate used to evaluate that best configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n\n-----\n\n### Describe process to use Nested CV to get performance estimates to both select best configuration and evaluate it in new data\n\n- When used?\n- Describe how to do it using held-in/held-out terminology in addition to train/val/test\n- Discuss the bias and variance of the performance estimate used to select the best configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n- Discuss the bias and variance of the performance estimate used to evaluate that best configuration.  Consider implications of:\n  - The method\n  - size of the held-in data\n  - size of held out data\n\n-----\n\n### Methods to EITHER select best model configuration (among many) OR evaluate a single best model configuration\n\nWhat are pros, cons, and when to use?\n\n  - Single validation (or test) set\n  - LOOCV\n  - K-fold\n  - Repeated k-fold\n  - Bootstrap\n\n  - ALSO: grouped k-fold\n\n-----\n\n### Methods to BOTH select best model configuration and evaluate that best configuration\n\n- What is optimization bias (its just a form of bias but given a fancy name due to its source)?\n- Combine above method with held out test set\n- Nested CV (see [tutorial](https://www.tidymodels.org/learn/work/nested-resampling/))\n\n-----\n\n### Discuss data leakage with resampling\n\n- Why is data leakage a problem?\n- How does noise factor into the problem?\n- Why does resampling substantially reduce data leakage?\n- How to use eyeball sample with single held out test set\n\n-----\n\n### A final interesting question.....\n\nBiased estimate of model performance has troubled me in the last assignment. I did repeated k-fold cross validation with a KNN model where k = 5. The effective sample size was n ~ 800 during model selection. Then I refit the model with the full training data where n ~ 1000 and submitted my predictions for the test set. After some testing with one of the TAs, I figured that my submitted model with a larger N performed worse than the model evaluated with resamples, if I used the same K determined via resampling (number of nearest neighbors). My hunch was that the optimal K (number of nearest neighbors) was different in a model trained with n ~ 800 than a model trained with n ~ 1000. Is this a possible explanation? If so, would LOOCV better serve the purpose of selecting a K for the final KNN model (so that the training set for selecting K is very much the same as the training set used for the final model)?\n\n- On average, what do you expect about the performance estimate of your final model configuration in test vs. the performance estimates used to select the best model configuration?\n- What if you used bootstrap to select best configuration?\n- ...but, what about choosing k from train and then using that k in a model fit to train and val?\n  - best k, \n  - best k within +- 1 SE (lower limit or higher limit)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":"html_document","warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["book.css"],"output-file":"005_resampling.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","bibliography":["refs.bib"],"callout-icon":false,"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}