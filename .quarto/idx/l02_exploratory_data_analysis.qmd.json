{"title":"Exploratory Data Analysis","markdown":{"yaml":{"editor_options":{"chunk_output_type":"console"}},"headingText":"Exploratory Data Analysis","containsRefs":false,"markdown":"\n\n::: {.content-visible unless-format=\"revealjs\"}\n:::\n::: {.content-visible when-format=\"revealjs\"}\n# IAML Unit 2: Exploratory Data Analysis\n:::\n\n## Learning Objectives\n  \n- Stages of Analysis  \n- Best practices for data storage, variable classing, data dictionaries\n- Problems and solutions regarding data leakage\n- Key goals and techniques cleaning EDA\n  - Tidying names and response labels\n  - Appropriate visualizations based on variable class\n  - Summary statistics based on variable class\n- Proper splitting for training/validation and test sets\n- Key goals and techniques modeling EDA\n  - Appropriate visualizations based on variable class\n  - Summary statistics based on variable class\n- Introductory use of recipes for feature engineering\n\n--------------------------------------------------------------------------------\n\n## Overview of Exploratory Data Analysis\n\n### Stages of Data Analysis and Model Development\n\nThese are the main stages of data analysis for machine learning and the data that are used\n\n1. EDA: Cleaning (full dataset)\n2. EDA: Split data into training, validation and test set(s)\n3. EDA: Modeling (training sets)\n\n4. Model Development: Feature engineering (training sets)\n5. Model Development: Fit many models configurations (training set)\n6. Model Development: Calculate performance metric for many models configurations (validation sets)\n\n7. Model Development: Select final/best model configuration based on performance metric (validation sets)\n8. Final Model Evaluation: Fit best model configuration (use both training and validation sets)\n9. Final Model Evaluation: Evaluate final model configuration using performance metric (test sets)\n10. Implementation: Fit best model configuration to ALL data (training, validation, and test sets) if you plan to use it for applications.\n\n--------------------------------------------------------------------------------\n\nThe earlier stages are highly iterative:\n\n- You may iterate some through EDA stages 1-3 if you find further errors to clean in stage 3 [**But make sure you resplit into the same sets**]\n- You will iterate many times though stages 3-6 as you learn more about your data both through EDA for modeling and evaluating actual models in validation\n\nYou will NOT iterate back to earlier stages after you select a final model configuration in Stage 7\n\n- Stages 7 - 10 are performed ONLY ONCE\n- Only one model configuration is selected and re-fit and only that model is brought into test for evaluation\n- Any more than this is essentially equivalent to p-hacking in traditional analyses\n- Step 10 only happens if you plan to implement the model in some application\n\n--------------------------------------------------------------------------------\n\n## Best Practices and Other Recommendations\n\n### Data file formats\n\nWe generally store data as CSV [comma-separated value] files\n\n- Easy to **view directly** in a text editor\n- Easy to **share** because others can use/import into any data analysis platform\n- Works with **version control** (e.g. git, svn)\n- use `read_csv()` and `write_csv()`\n\nExceptions include: \n\n- We **may** consider binary (.rds) format for very big files because read/write can be slow for csv files.  \n- Binary file format provides a very modest additional protection for sensitive data (which we also don't share)\n- use `read_rds()` and `write_rds()`\n\n\nSee [chapter 7 - Data Import](https://r4ds.hadley.nz/data-import) in @RDS for more details and advanced techniques for importing data using `read_csv()`\n\n--------------------------------------------------------------------------------\n\n### Classing Variables\nWe store and class variables in R based on their data type (level of measurement).\n\n- See Wikipedia definitions for [levels of measurement](https://en.wikipedia.org/wiki/Level_of_measurement) for a bit more precision that we will provide here.\n\nCoarsely, there are four levels:\n\n- nominal:  qualitative categories, no inherent order (e.g., marital status, sex, car color)\n- ordinal: qualitative categories (sometimes uses number), inherent order but not equidistant spacing (e.g., Likert scale; education level)\n- interval and ratio (generally treated the same in social sciences): quantitative scores, ordered, equidistant spacing.  Ratio has true 0.  (e.g., temperature in Celsius vs. Kelvin scales)\n\nWe generally refer to nominal and ordinal variables as categorical and interval/ratio as quantitative or numeric \n\n--------------------------------------------------------------------------------\n\nFor **nominal** variables\n\n- We store (in csv files) these variables as character class with descriptive text labels for the levels\n  - Easier to share/document\n  - Reduces errors\n- We class these variables in R as factors when we load them (using `read_csv()`)\n- In some cases, we should pay attention to the order of the levels of the variable. e.g., \n  - For a dichotomous outcome variable, the positive/event level of dichotomous factor outcome should be first level of the factor\n  - The order of levels may also matter for factor predictors (e.g., `step_dummy()` uses first level as reference).\n\n--------------------------------------------------------------------------------\n\nFor **ordinal** variables: \n\n- We store (in csv files) these variables as character class with descriptive text labels for the levels\n\t- Easier to share/document\n\t- Reduces errors\n- We class these variables in R as factors (just like nominal variables)\n  - It is easier to do EDA with these variables classes as factors\n  - We use standard factors (not ordered)\n- Confirm that the order of the levels is set up correctly.  This is very important for ordinal variables.\n- During feature engineering stage, we can then either\n  - Treat as a nominal variable and create features using `step_dummy()`\n  - Treat as an interval variable using `step_ordinalscore()`\n\n\nSimilar EDA approaches are used with both nominal and ordinal variable\n\nOrdinal variables may show non-linear relations b/c they may not be evenly spaced.  In these instances, we can use feature engineering approaches that are also used for nominal variables\n\n--------------------------------------------------------------------------------\n\nFor **interval and ratio** variables:\n\n- We store these variables as numeric\n- We class these variables as numeric (either integer or double - let R decide) during the read and clean stage (They are typically already in this class when read in)\n\nSimilar EDA approaches are used with both interval and ratio variables\n\nSimilar feature engineering approaches are used with both\n\n--------------------------------------------------------------------------------\n\n### Data Dictionaries\n\nYou should always make a data dictionary for use with your data files.  \n\n- Ideally, these are created during the planning phase of your study, prior to the start of data collection\n- Still useful if created at the start of data analysis\n\nData dictionaries:\n\n- Help you keep track of your variables and their characteristics (e.g., valid ranges, valid responses)\n- Can be used by you to check your data during EDA\n- Can be provided to others when you share your data (data are not generally useful to others without a data dictionary)\n\nWe will see a variety of data dictionaries throughout the course.   Many are not great as you will learn.\n\n--------------------------------------------------------------------------------\n\n### The Ames Housing Prices Dataset\n\nWe will use the Ames Housing Prices dataset as a running example this unit (and some future units and application assignments as well)\n\n- You can [read more](http://jse.amstat.org/v19n3/decock.pdf) about the original dataset created by Dean DeCock \n\n- The data set contains data from home sales of individual residential property in Ames, Iowa\nfrom 2006 to 2010\n\n- The original data set includes 2930 observations of sales price and a large number of explanatory\nvariables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous)\n\n- This is the [original data dictionary](data/ames_data_dictionary.pdf)\n\n- The challenge with this dataset is to build the best possible prediction model for the\nsale price of the homes.\n\n--------------------------------------------------------------------------------\n\n### Packages and Conflicts\nFirst, lets set up our environment with functions from important packages.  I strongly recommend reviewing our recommendations for best practices regarding [managing function conflicts](https://jjcurtin.github.io/book_dwvt/conflicts.html) now. It will save you a lot of headaches in the future.\n\nWe first load our two main packages (which will conflict with each other but intentionally by the package creators)\n\nNext we load packages for other functions that we will use regularly.  There are five things to note RE best practices\n\n1. If we will use a lot of functions from a package (e.g., `tidyverse`, `tidymodels`), we attach the full package\n2. If we will use only several functions from a package (but plan to use them repeatedly), we use the `include.only` parameter to just attach those functions.\n3.  At times, if we plan to use a single function from a package only 1-2x times, we may not even attach that function at all.  Instead, we just call it using its namespace (i.e.  `packagename::functionname`)\n4. If a package has a function that conflicts with our primary packages and we don't plan to use that function, we load the package but exclude the function.  If we really needed it, we can call it with its namespace as per option 3 above.\n5. Pay attention to conflicts that were allowed to make sure you understand and accept them. (I left the package messages and warnings in the book this time to see them. I will hide them to avoid cluttering book in later units but you should always review them.)\n\n```{r}\n#| message: false\n#| warning: false\n\nlibrary(tidyverse) \nlibrary(tidymodels) \n\noptions(conflicts.policy = \"depends.ok\")\n\nlibrary(janitor, include.only = \"clean_names\") # <1>\nlibrary(cowplot, include.only = \"plot_grid\") # <2> \nlibrary(kableExtra, exclude = \"group_rows\") # <3> \n```\n1. As an alternative, we could have skipped loading the package and instead called the function as `janitor::clean_names()`\n2. Same is true for `cowplot` package\n3. When loading `kableExtra` (which we use often), you will always need to exclude `groups_rows()` to prevent a conflict with `dplyr` package in the tidyverse\n\n--------------------------------------------------------------------------------\n\n### Source and Other Environment Settings\nWe will also source (from github) libraries of functions that we use commonly for exploratory data analyses.  You should review these function scripts: \n\n- [fun_ml.R](https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\n- [fun_eda.R](https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true)\n- [fun_plots.R](https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true)\n```{r}\n#| message: false\n#| warning: false\n\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n```\n\n--------------------------------------------------------------------------------\n\nFinally, we tune our environment a bit more by setting plot themes and print options that we prefer\n```{r}\ntheme_set(theme_classic())\noptions(tibble.width = Inf, tibble.print_max = Inf)\n```\n\nAnd we set a relative path to our data.  This assumes you are using an RStudio project with the path to the data relative to that project file.  I've provided more detail [elsewhere](https://jjcurtin.github.io/book_dwvt/file_and_path_management.html) on best practices for managing files and paths.\n```{r}\npath_data <- \"data\"\n```\n\n--------------------------------------------------------------------------------\n\n### Read and Glimpse Dataframe\nLets read in the data and glimpse the subset of observations we will work with in Units 2-3 and the first two application assignments.  \n\n```{r}\ndata_all <- read_csv(here::here(path_data, \"ames_raw_class.csv\"),   # <1>\n                     show_col_types = FALSE) |> # <2> \n  glimpse() # <3>\n```\n1. First we read data using a relative path and the `here::here()` function.  This is a replacement for `file.path()` that works better for both interactive use and rendering in Quarto when using projects.\n2.  We use `show_col_types = FALSE` to suppresse messages that aren't important at this point prior to EDA.\n3. It is good practice to always `glimpse()` data after you read it.\n\n--------------------------------------------------------------------------------\n\nDataset Notes: \n\n- Dataset has N = 1955 rather than 2930.\n  - I have held out remaining observations to serve as a test set for a friendly competition in Unit 3\n  - I will judge your models' performance with this test set at that time!\n  - More on the importance of held out test sets as we progress through the course\n\n- This full dataset has 81 variables.  For the lecture examples in units 2-3 we will only use a subset of the predictors\n\n- You will use different predictors in the next two application assignments\n\n--------------------------------------------------------------------------------\n\nHere we select the variables we will use for lecture\n\n```{r}\ndata_all <- data_all |> \n  select(SalePrice,\n         `Gr Liv Area`, \n         `Lot Area`, \n         `Year Built`, \n         `Overall Qual`, \n         `Garage Cars`,\n         `Garage Qual`,\n         `MS Zoning`,\n         `Lot Config` ,\n         `Bldg Type`) |> # <1> \n  glimpse()\n```\n1. Notice that the dataset used non-standard variable names that include spaces. We need to use back-ticks around the variable names to allow us reference those variables. We will fix this during the cleaning process and you should never use spaces in variable names when setting up your own data!!! \n\n--------------------------------------------------------------------------------\n Exploratory Data Analysis for Data Cleaning\n\nEDA *could* be done using either `tidyverse` packages and functions or `tidymodels` (mostly using the `recipes` package.)\n\n- We prefer to use the richer set of functions available in the tidyverse (and `dplyr` and `purrr` packages in particular).\n\n- We will reserve the use of recipes for feature engineering only when we are building features for models that we will fit in our training sets and evaluation in our validation and test sets.  \n\n--------------------------------------------------------------------------------\n\n### Data Leakage Issues\nData leakage refers to a mistake made by the developer of a machine learning model in which they accidentally share information between their training set and held-out validation or test sets\n\n- Training sets are used to fit models with different configurations\n- Validation sets are used to select the best model among those with different configurations (not needed if you only have one configuration)\n- Test sets are used to evaluate a best model\n\n- When splitting data-sets into training, validation and test sets, the goal is to ensure that no data (or information more broadly) are shared between the three sets\n  - No data or information from test should influence either fitting or selecting models \n  - Test should only be used once to evaluate a best/final model\n  - Train and validation set also must be segregated (although validation sets may be used to evaluate many model configurations)\n  - Information necessary for transformations and other feature engineering (e.g., means/sds for centering/scaling, procedures for missing data imputation) must all be based only on training data.\n  - Data leakage is common if you are not careful.\n\nIn particular, if we begin to use test data or information about test during model fitting\n\n- We risk overfitting\n- This is essentially the equivalent of p-hacking in traditional analyses\n- Our estimate of model performance will be too optimistic, which could have harmful real-world consequences.\n\n--------------------------------------------------------------------------------\n\n### Tidy variable names\n\nUse snake case for variable names\n\n- `clean_names()` from `janitor` package is useful for this.\n- May need to do further correction of variable names using `rename()`\n- See more details about tidy names for objects (e.g., variables, dfs, functions) per [Tidy Style Guide](https://style.tidyverse.org/syntax.html#object-names)\n\n\n```{r}\ndata_all <- data_all |> \n  clean_names(\"snake\")\n\ndata_all |> names()\n```\n\n--------------------------------------------------------------------------------\n\n### Explore variable classes\n\nAt this point, we should class all of our variables as either numeric or factor\n\n- Interval and ratio variables use numeric classes (dbl or int)\n- Nominal and ordinal variable use factor class\n- Useful for variable selection later (e.g., `where(is.numeric)`, `where(is.factor)`)\n\nSubsequent cleaning steps are clearer if we have this established/confirmed now\n\n--------------------------------------------------------------------------------\n\nWe have a number of nominal or ordinal variables that are classed as character. \n\nWe have one ordinal variable (`overall_qual`) that is classed as numeric (because the levels were coded with numbers rather than text)\n\n- `read_csv()` thought was numeric by the levels are coded using numbers\n- The data dictionary indicates that valid values range from 1 - 10.\n\n```{r}\ndata_all |> glimpse()\n```\n\n--------------------------------------------------------------------------------\n\nWe can the recode `overall_qual` first and set its levels\n\nWe can recode all the character variables to factor in one step.  Most are nominal.  We will handle the order for `garage_qual` later.\n\n\n```{r}\noq_levels <- 1:10 # <1>\n\ndata_all <-  data_all |> \n  mutate(overall_qual = factor(overall_qual, \n                               levels = oq_levels)) |> # <2>\n  mutate(across(where(is.character), factor)) |>  # <3>\n  glimpse()\n```\n1. It is always best to explicitly set the levels of an ordinal factor in the order you prefer.  It is not necessary here because `overall_qual` was numeric and therefore sorts in the expected order.  However, if it had been numbers stored as characters, it could sort incorrectly (e.g., 1, 10, 2, 3, ...).  And obviously if the orders levels were names, the order would have to be specified.\n2. We indicate the levels here.\n3. We use a mutate to re-class all character data to factors.  I prefer `factor()` to `forcats::fct()` because factor orders the levels alphabetically.  Be aware that this *could* change if your code is used in a region of the world where this sorting is different.  I still prefer this to the alternative (in `fct()`) that orders by the order the levels are found in your data.\n\n--------------------------------------------------------------------------------\n\n### Skimming the data\n\n`skim()` from the `skimr` package is a wonderful and customizable function for summary statistics\n\n- It is highly customizable so we can write our own versions for our own needs\n- We use different versions for cleaning and modeling EDA\n- For cleaning EDA, we just remove some stats that we don't want to see at this time\n- We can get many of the summary stats for cleaning in one call \n- We have a custom skim defined in the `fun_eda.R` function library that we use regularly.  Here is the code but you can use the function directly if you sourced `fun_eda.R` (as we did above)\n\n```{r}\n#| eval: false\n\nskim_some <- skim_with(numeric = sfl(mean = NULL, sd = NULL, p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL))\n```\n\n--------------------------------------------------------------------------------\n\nHere is what we get with our new `skim_some()` function\n\n- We will refer to this again for each characteristic we want to review for instructional purposes \n- We can already see that we can use `skim_some()` to confirm that we only have numeric and factor classes\n```{r}\ndata_all |> \n  skim_some()\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar 1\n\n- Write functions whenever you will repeat code often. You can now reuse `skim_some()`\n- `skim_with()` is an example of a function factory - a function that is used to create a new function\n  - `partial()` and `compose()` are two other function factories we will use at times\n  - More details on function factories is available in [Advanced R](https://adv-r.hadley.nz/function-factories.html)\n  \n:::\n\n::: {.callout-tip}\n# Coding sidebar 2\n  \n- Gather useful functions together in a script that you can reuse.\n- All of the reusable functions in this and later units are available to you in one of my [public github repositories](https://github.com/jjcurtin/lab_support). \n- You can load these functions into your workspace directly from github using `source()`. For example: `source(\"https://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\")`\n- You should start to gather your favorite custom functions together in your own script(s).  \n- You can save your own scripts in a local file and load them into your workspace using `source()` or you can make your own github repo so you can begin to share your code with others!\n:::\n\n--------------------------------------------------------------------------------\n\n### Missing Data - All variables\n\n`skim_some()` provides us with missing data counts and complete data proportions for each variable\n\n\n```{r}\ndata_all |> \n  skim_some() |> \n  select(skim_variable, n_missing, complete_rate) # <1>\n```\n1. `skim_some()` returns a dataframe so you can select only the subset of columns to focus its output on what you want.   Or just print it all!\n\n--------------------------------------------------------------------------------\n\nYou likely should view the full observation for missing values\n\nWe will show you a few methods to do this in your rendered output\n\n- `print()` will print only 20 rows and the number of columns that will display for width of page\n  - Set `options()` if you will do a lot of printing and want full dataframe printed\n- Use `kbl()` from `kableExtra` package for formatted tables (two methods below)\n\n\nDon't forget that you can also use `view()` interactively in R Studio\n\n--------------------------------------------------------------------------------\n\n**Option 1 (Simple)**:  Use print() with `options()`\n\n```{r}\noptions(tibble.width = Inf, tibble.print_max = Inf) # <1>\n\ndata_all |> filter(is.na(garage_cars)) |> \n  print()\n```\n1. This sets print to print all rows and columns.  Note that we set these options at the start of the unit b.c. we like to see our full tibbles.  If we want only a subset of the first (or last) rows, we use `head()` or `tail()`\n\n--------------------------------------------------------------------------------\n\nHere are some more advanced options using `kbl()` for the df with many rows\n\n- `kable()` tables from `knitr` package and `kableExtra` extensions (including `kbl()`) are very useful during EDA and also final publication quality tables\n- use `library(kableExtra)`\n- see [vignettes for kableExtra](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html)\n\n**Option 2 (more advanced)**: Use a function for kables that we created.  Code is displayed here but the function is available to you if you source fun_eda.R from Github\n\n```{r}\n#| eval: false\n\n# Might want to use height = \"100%\" if only printing a few rows\nprint_kbl <- function(data, height = \"500px\") { # <1>\n  data |> \n    kbl(align = \"r\") |> \n    kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) |> \n    scroll_box(height = height, width = \"100%\") # <2>\n}\n```\n1. Defaults to a output box of height = \"500px\".  Can set to other values if preferred.\n2. Might want to use `height = \"100%\"` if only printing a few rows. \n\n--------------------------------------------------------------------------------\n\nLet's use this function to see its output\n```{r}\ndata_all |> filter(is.na(garage_qual)) |> \n  print_kbl()\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\n\n- In the above example, we created a function (`print_kbl()`) from scratch (rather than using a function factory)\n- See [functions chapter](https://r4ds.hadley.nz/functions.html) in @RDS for help.\n- See [functionals chapter](https://adv-r.hadley.nz/functionals.html) in @AR. \n\n:::\n\n--------------------------------------------------------------------------------\n\n**Option 3 (Most advanced)**: Line by line kable table. You can make this as complicated and customized as you like. We use kable (and kableExtra) for publication quality tables.  This is a simple example of options\n\n```{r}\ndata_all |> filter(is.na(garage_qual)) |> \n  kbl(align = \"r\") |> \n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) |> \n  scroll_box(height = \"500px\", width = \"100%\")\n```\n\n--------------------------------------------------------------------------------\n\nIn this instance, if we consult our [data dictionary](data/ames_data_dictionary.pdf), we see that `NA` for `garage_qual` should be coded as \"no garage\".  We will correct this in our data set.  \n\nThis is a pretty poor choice on the part of the researchers who created the dataset because it becomes impossible to distinguish between `NA` that means no garage vs. true NA for the variable.  In fact, if you later do really careful EDA on the full data set with all variables, you will see this problem likely exists in this dataset\n\nAnyway, let's correct all the `NA` for `garage_qual` to \"no_garage\" using `mutate()` \n\n```{r}\ndata_all <- data_all |> \n  mutate(garage_qual = fct_expand(garage_qual, \"no_garage\"), # <1>\n         garage_qual = replace_na(garage_qual, \"no_garage\")) # <2>\n\n```\n1. First add a new level to the factor\n2. Then recode NA to that new level\n\nWe will leave the `NA` for `garage_cars` as `NA` because its not clear if that is truly missing or not, based on further EDA not shown here.\n\n--------------------------------------------------------------------------------\n\nWe have one other issue with `garage_qual`.  It is an ordinal variable but we never reviewed the order of its levels.  The data dictionary indicates the levels are ordered (best to worst) as:\n\n- Ex (excellent)\n- Gd (good)\n- TA (typical/average)\n- Fa (fair)\n- Po (poor)\n\nAnd we might assume that no garage is even worse than a poor garage.  Lets see what they are.\n\n```{r}\ndata_all$garage_qual |> levels()\n```\n\nTo fix this, we can use `forcats::fct_relevel()`.\n```{r}\ngq_levels <- c(\"no_garage\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\") # <1>\ndata_all <- data_all |> \n  mutate(garage_qual = fct_relevel(garage_qual, gq_levels)) # <2>\n\ndata_all$garage_qual |> levels() # <3>\n```\n1. Make a vector that indicates the valid levels in order\n2. Pass that into `fct_relevel()`.  See `?fct_relevel` for other ways to adjust the levels of a factor.\n3. Confirm that the levels are now correct\n\n--------------------------------------------------------------------------------\n\n### Explore Min/Max Response for Numeric Variables\n\nWe should explore mins and maxes for all numeric variables to detect out of valid range numeric responses\n\n- Could also do this for ordinal variables that are coded with numbers\n  - e.g., `overall_qual` (1-10) vs. `garage_qual` (no_garage, Po, Fa, TA, Gd, Ex)\n- This is only a temporary mutation of `overall_qual` for this check.  We don't assign to new df to an object\n- We can use `skim_some()` again\n  - p0 = min\n  - p100 = max\n\n```{r}\ndata_all |>\n  mutate(overall_qual = as.numeric(overall_qual)) |> \n  skim_some() |> \n  filter(skim_type == \"numeric\") |>  # <1>\n  select(skim_variable, numeric.p0, numeric.p100) # <2>\n```\n1. Select only numeric variables since min/max only apply to them\n2. Select relevant stats (min/max)\n\n--------------------------------------------------------------------------------\n\n### Explore All Responses for Categorical Variables\n\nWe should explore all unique responses for nominal variables\n\nMight also do this for ordinal variables that are coded with labels vs. numbers.  \n\n```{r}\ndata_all |> \n  select(where(is.factor)) |>\n  walk(\\(column) print(levels(column)))\n```\n\n\n::: {.callout-tip}\n# Coding sidebar\n\n- On the previous page, we demonstrated the use of an anonymous function (`\\(column) print(levels(column))`), which is a function we use once that we don't bother to assign a name (since we won't reuse it).  We often use anonymous functions when using the functions from the `purrr` package (e.g., `map()`, `walk()`) \n- We use `walk()` from the `purrr` package to apply our anonymous function to all columns of the data frame at once\n- Just copy this code for now\n- We will see simpler uses later that will help you understand iteration with `purrr` functions\n- See the chapter on [iteration](https://r4ds.hadley.nz/iteration) in *R for Data Science (2e)* for more info on `map()` and `walk()`\n\n:::\n\n--------------------------------------------------------------------------------\n\n### Tidy Responses for Categorical Variables\n\nFeature engineering with nominal and ordinal variables typically involves \n\n- Converting to factors (already did this!)\n- Often creating dummy features from these factors\n\nThis feature engineering will use response labels for naming new features\n\n- Therefore, it is a good idea to have the responses snake-cased and cleaned up a bit so that these new feature names are clean/clear.\n\nHere is an easy way to convert responses for character variables to snake case using a function (`tidy_responses()`) we share in `fun_eda.R` (reproduced here). \n\n- This uses regular expressions (regex), which will will learn about in a later unit on text processing.\n- You could expand this cleaning function if you encounter other issues that need to be cleaned in the factor levels.\n```{r}\n#| eval: false\n\ntidy_responses <- function(column){\n  # replace all non-alphanumeric with _\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\W\", \"_\"))\n  # replace whitespace with _\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\s+\", \"_\"))\n  # replace multiple _ with single _\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\_+\", \"_\"))\n  #remove _ at end of string\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\_$\", \"\"))\n  # remove _ at start of string\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\^_\", \"\"))\n  # convert to lowercase\n  column <- fct_relabel(column, tolower)\n  factor(column)\n}\n```\n\n--------------------------------------------------------------------------------\n\nLet's use the function\n\n```{r}\ndata_all <- data_all |> \n  mutate(across(where(is.factor), tidy_responses)) # <1>\n```\n1. We use the tidy selection helper function to limit our mutate to only factors.  See [more details](https://tidyselect.r-lib.org/reference/language.html) on the tidy selection helpers like `all_of()` and `where()`\n\n--------------------------------------------------------------------------------\n\nAlas, these response labels were pretty poorly chosen so some didn't convert well.  And some are really hard to understand too.\n\n- Avoid this problem and choose good response labels from the start for your own data\n- Here, we show you what we got from using `tidy_responses()`\n```{r}\ndata_all |> \n  select(where(is.factor)) |>\n  walk(\\(column) print(levels(column)))\n```\n\n--------------------------------------------------------------------------------\n\nLets clean them up a bit more manually\n\n```{r}\ndata_all <- data_all |> \n  mutate(ms_zoning = fct_recode(ms_zoning,\n                                res_low = \"rl\",\n                                res_med = \"rm\",\n                                res_high = \"rh\",\n                                float = \"fv\",\n                                agri = \"a_agr\",\n                                indus = \"i_all\",\n                                commer = \"c_all\"),\n         bldg_type = fct_recode(bldg_type,   # <1>\n                                one_fam = \"1fam\",\n                                two_fam = \"2fmcon\",\n                                town_end = \"twnhse\",\n                                town_inside = \"twnhs\"))\n                                \n```\n1. Note that I did not need to list all levels in the recode.  Only the levels I wanted to change.\n\nThe full dataset is now clean!\n\n--------------------------------------------------------------------------------\n\n### Train/Validate/Test Splits\n\nThe final task we typically do as part of the data preparation process is to split the full dataset into training, validation and test sets.\n\n- Test sets are \"typically\" between 20-30% of your full dataset\n  - There are costs and benefits to larger test sets\n  - We will learn about these costs/benefits in the unit on resampling\n  - I have already held out the test set\n  \n- There are many approaches to validation sets\n  - For now (until unit 5) we will use a single validation set approach\n  - We will use 25% of the remaining data (after holding out the test set) as a validation set for this example\n\n- It is typical to split data on the outcome within strata\n  - For a categorical outcome, this makes the proportions of the response categories more balanced across the train, validation, and test sets\n  - For a numeric outcome, we first break up the distribution into temporary bins (see `breaks = 4` below) and then we split within these bins\n- **IMPORTANT**: Set a seed so that you can reproduce these splits if you later do more cleaning\n\n\n```{r}\nset.seed(20110522)\nsplits <- data_all |> \n  initial_split(prop = 3/4, strata = \"sale_price\", breaks = 4)\n```\n\n::: {.callout-tip}\n# Coding Note\n`initial_split()` is used to create two resamples of your data.  This was used here because we already had a test set held out (outside of the code presented).  We use `analysis()` and `assessment()` to extract these two resamples.   \n\nWe will later use `initial_validation_split()` to create 3 resamples: training, validation, and test.  We will extract those three resamples using `training()`, `validation()`, and `testing()`\n:::\n\n--------------------------------------------------------------------------------\n\nWe then extract the training set from the splits and save it\n\n- Training sets are used for \"analysis\"- hence the name of the function\n```{r}\nsplits |> \n  analysis() |> # <1> \n  glimpse() |> \n  write_csv(here::here(path_data, \"ames_clean_class_trn.csv\"))\n```\n1. `analysis()` pulls out the training set from our splits of `data_all`\n\n--------------------------------------------------------------------------------\n\nWe will not need the validation set for modeling EDA\n\n- It should NOT be used for anything other than calculating performance metrics for our many model configurations to select the best model configuration\n- We do NOT do Modeling EDA or Model Fitting with the validation set\n- Save it in this clean form for easy use when you need it\n- We use the validation set to \"assess\" (calculate performance metrics for selection) models that we have fit in training sets - hence the name of the function\n\n```{r}\nsplits |> \n  assessment() |> # <1> \n  glimpse() |> \n  write_csv(here::here(path_data, \"ames_clean_class_val.csv\"))\n```\n1. `assessment()` pulls out the validation set from our splits of `data_all`\n\n--------------------------------------------------------------------------------\n\n## Exploratory Data Analysis for Modeling\n\nNow let's begin our Modeling EDA\n\nWe prefer to write separate scripts for Cleaning vs. Modeling EDA (but not displayed here)\n\n- This keeps these two processes separate in our minds\n- Cleaning EDA is done with full dataset but Modeling EDA is only done with a training set - NEVER use validation or test set\n- You will use two separate scripts for the application assignment for this unit\n\n--------------------------------------------------------------------------------\n\nLets re-load (and glimpse) our training set to pretend we are at the start of a new script.\n\n\n```{r}\n data_trn <- read_csv(here::here(path_data, \"ames_clean_class_trn.csv\")) |> \n  glimpse()\n```\n\n--------------------------------------------------------------------------------\n\nWe have some work to do (again)\n\n- Notice that `overall_qual` is back to being classed as numeric (dbl).  \n- Notice that your factors are back to character\n  - This is because csv files don't save anything other than the values (labels for factors).  They are the cleaned labels though!\n- You should class all variables using the same approach as before (often just a copy/paste). \n\n```{r}\n data_trn <- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           show_col_types = FALSE) |>  # <1>\n  mutate(across(where(is.character), factor)) |> # <2>\n  mutate(overall_qual = factor(overall_qual, levels = 1:10),  # <3>\n         garage_qual = fct_relevel(garage_qual, c(\"no_garage\", \"po\", \"fa\", \n                                                  \"ta\", \"gd\", \"ex\"))) |>  # <4> \n  glimpse()\n```\n1. use col_types = cols() to suppress messages about default class assignments\n2. use `mutate()` with `across()` to change all character variables to factors\n3. use `mutate()` with `factor()` to change numeric variable to factor.\n4. use `mutate()` with `fct_relevel()` to explicitly set levels of an ordinal factor.  Also notice the warning about the unknown level.  Always explore warnings!  In this instance, its fine.  There were only two observations with ex and neither ended up in the training split.   Still best to include this level to note it exists!\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\nWe will likely re-class the Ames dataset many times (for training, validation, test).  We could copy/paste these mutates each time but whenever you do something more than twice, I recommend writing a function.  We might write this one to re-class the ames variables\n:::\n\n```{r}\nclass_ames <- function(df){\n  \n  df |>\n    mutate(across(where(is.character), factor)) |> \n    mutate(overall_qual = factor(overall_qual, levels = 1:10), \n           garage_qual = fct_relevel(garage_qual, c(\"no_garage\", \"po\", \"fa\", \n                                                    \"ta\", \"gd\", \"ex\")))\n}\n```\n\n--------------------------------------------------------------------------------\n\nNow we can use this function every time we read in one of the Ames datasets\n\n```{r}\n#| warning: false\n\ndata_trn <- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           show_col_types = FALSE) |> \n  class_ames() |> # <1> \n  glimpse()\n```\n1. Using our new function!\n\n--------------------------------------------------------------------------------\n\nThere are 3 basic types of Modeling EDA you should always do\n\n1. Explore missingness for predictors\n2. Explore univariate distributions for outcome and predictors\n3. Explore bivariate relationships between predictors and outcome\n\nAs a result of this exploration, we will:\n\n- Identify promising predictors\n- Determine appropriate feature engineering for those predictors (e.g., transformations)\n- Identify outliers and consider how to handle when model building\n- Consider how to handle imputation for missing data (if any)\n\n--------------------------------------------------------------------------------\n\n### Overall Summary of Feature Matrix\n\nBefore we dig into individual variables and their distributions and relationships with the outcome, it's nice to start with a big picture of the dataset\n\n- We use another customized version of `skim()` from the `skimr` package to provide this\n- Just needed to augment it with skewness and kurtosis statistics for numeric variables\n- and remove histogram b/c we don't find that small histogram useful\n- included in `fun_eda.R` on github\n```{r}\n#| eval: false\n\nskew_na <- partial(e1071::skewness, na.rm = TRUE)\nkurt_na <- partial(e1071::kurtosis, na.rm = TRUE)\n\nskim_all <- skimr::skim_with(numeric = skimr::sfl(skew = skew_na, \n                                                  kurtosis = kurt_na, \n                                                  hist = NULL))\n```\n\n--------------------------------------------------------------------------------\n\nCareful review of this output provides a great orientation to our data\n\n```{r}\ndata_trn |> \n  skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n### Univariate Distributions\n\nExploration of univariate distributions are useful to \n\n- Understand variation and distributional shape\n- May suggest need to consider transformations as part of feature engineering\n- Can identify univariate outliers (valid but disconnected from distribution so not detected in cleaning)\n\n\nWe generally select different visualizations and summary statistics for categorical vs. numeric variables\n\n--------------------------------------------------------------------------------\n\n### Barplots for Categorical Variables (Univariate)\n\nThe primary visualization for categorical variables is the bar plot\n\n- We use it for both nominal and ordinal variables\n- Define and customize it within a function for repeated use.  \n- We share this and all the remaining plots used in this unit in `fun_plot.R`. Source it to use them without having to re-code each time\n```{r}\n#| eval: false\n\nplot_bar <- function(df, x){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, \n                                     size = x_label_size, vjust = 0.5, \n                                     hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\nWhen defining functions, generally put data as first argument so you can pipe in data using tidy pipelines\n\nThere are pros and cons to writing functions that accept variable names that are quoted vs. unquoted\n\n- It depends a bit on how you will use them.\n- .data[[argument]] is used in functions with quoted arguments\n- embracing {{}} is used for unquoted arguments\n- For these plot functions, I use quoted variable names and then pipe those into `map()` to make multiple plots (see below)\n- see `?vignette(\"programming\")` or info on [tidy evaluation](https://r4ds.hadley.nz/functions.html#indirection-and-tidy-evaluation) in @RDS for more details\n\n:::\n\n--------------------------------------------------------------------------------\n\nBar plots reveal low frequency responses for nominal and ordinal variables\n\n- See `bldg_type`\n\n```{r}\ndata_trn |> plot_bar(\"bldg_type\")\n```\n\n--------------------------------------------------------------------------------\n\nBar plots can display distributional shape for ordinal variables.  May suggest the \nneed for transformations if we later treat the ordinal variable as numeric\n\n- See `overall_qual`.  Though it is not very skewed.\n\n```{r}\ndata_trn |> plot_bar(\"overall_qual\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\n\nWe can make all of our plots iteratively using `map()` from the `purrr` package.\n\n```{r}\ndata_trn |> \n  select(where(is.factor)) |> # <1> \n  names() |> # <2> \n  map(\\(name) plot_bar(df = data_trn, x = name)) |> # <3> \n  plot_grid(plotlist = _, ncol = 2) # <4>\n```\n1. Select only the factor columns\n2. Get their names as strings (that is why we use quoted variables in these plot functions\n3. Use `map()` to iterative `plot_bar()` over every column.  (see [iteration](https://r4ds.hadley.nz/iteration.html) in @RDS)\n4. Use `plot_grid()` from `cowplot` package to display the list of plots in a grid\n\n:::\n\n--------------------------------------------------------------------------------\n\n### Tables for Categorical Variables (Univariate)\n\nWe tend to prefer visualizations vs. summary statistics for EDA.   However, tables can be useful.\n\nHere is a function that was described in @RDS that we like because\n\n- It includes counts and proportions\n- It includes NA as a category\n\nWe have included it in `fun_eda.R` for your use.\n```{r}\n#| eval: false\n\ntab <- function(df, var, sort = FALSE) {\n  df |>  dplyr::count({{ var }}, sort = sort) |> \n    dplyr::mutate(prop = n / sum(n))\n} \n```\n\n--------------------------------------------------------------------------------\n\nTables can be used to identify responses that have very low frequency and to think about the need to handle missing values\n\n- See `ms_zoning`\n- May want to collapse low frequency (or low percentage) categories to reduce the number of features needed to represent the predictor\n\n```{r}\ndata_trn |> tab(ms_zoning)\n```\n\n--------------------------------------------------------------------------------\n\n- We could also view the table sorted if we prefer\n```{r}\ndata_trn |> tab(ms_zoning, sort = TRUE)\n```\n\n--------------------------------------------------------------------------------\n\nbut could see all this detail with plot as well\n\n```{r}\ndata_trn |> plot_bar(\"ms_zoning\")\n```\n\n--------------------------------------------------------------------------------\n\n### Histograms for Numeric Variables (Univariate)\n\nHistograms are a useful/common visualization for numeric variables\n\nLet's define a histogram function (included in `fun_plots.r`)\n\n- Bin size should be explored a bit to find best representation\n- Somewhat dependent on n (my default here is based on this training set)\n- This is one of the limitations of histograms\n```{r}\n#| eval: false\n\nplot_hist <- function(df, x, bins = 100){\n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_histogram(bins = bins) +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\nLet's look at `sale_price`\n\n- It is positively skewed\n- May suggest units (dollars) are not interval in nature (makes sense)\n- Could cause problems for some algorithms (e.g., lm) when features are normal\n```{r}\ndata_trn |> plot_hist(\"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\n### Smoothed Frequency Polygons for Numeric Variables (Univariate) \n\nFrequency polygons are also commonly used\n\n- Define a frequency polygon function and use it (included in `fun_plots.r`)\n\n```{r}\n#| eval: false\n\nplot_freqpoly <- function(df, x, bins = 50){\n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_freqpoly(bins = bins) +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\n- Bins may matter again\n\n```{r}\ndata_trn |> plot_freqpoly(\"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\n### Simple Boxplots for Numeric Variables (Univariate) \n\nBoxplots display \n\n- Median as line\n- 25%ile and 75%ile as hinges\n- Highest and lowest points within 1.5 * IQR (interquartile-range: difference between scores at 25% and 75%iles)\n- Outliers outside of 1.5 * IQR\n\nDefine a boxplot function and use it (included in `fun_plots.r`)\n```{r}\n#| eval: false\n\nplot_boxplot <- function(df, x){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1))\n}\n```\n\n--------------------------------------------------------------------------------\n\nHere is the plot for `sale_price`\n```{r}\ndata_trn |> plot_boxplot(\"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\n### Combined Boxplot and Violin Plots for Numeric Variables (Univariate)\n\nThe combination of a boxplot and violin plot is particularly useful\n\n- This is our favorite\n- Get all the benefits of the boxplot\n- Can clearly see shape of distribution given the violin plot overlay\n- Can also clearly see the tails\n\n\nDefine a combined plot (included in `fun_plots.r`)\n\n```{r}\n#| eval: false\n\nplot_box_violin <- function(df, x){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_violin(aes(y = 0), fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1))\n}\n```\n\n--------------------------------------------------------------------------------\n\nHere is the plot for `sale_price`\n\n- In this instance, the skew is NOT due to only a few outliers\n\n```{r}\ndata_trn |> plot_box_violin(\"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\n\n- You can make figures for all numeric variables at once using `select()` and `map()` as before\n\n```{r}\ndata_trn |> \n  select(where(is.numeric)) |> # <1> \n  names() |> \n  map(\\(name) plot_box_violin(df = data_trn, x = name)) |> \n  plot_grid(plotlist = _, ncol = 2)\n```\n1. Now select numeric rather than factor but otherwise same as previous example\n\n:::\n\n--------------------------------------------------------------------------------\n\n### Summary Statistics for Numeric Variables (Univariate)\n\n`skim_all()` provided all the summary statistics you likely needed for numeric variables\n\n- mean & median (p50)\n- sd & IQR (see difference between p25 and p75)\n- skew & kurtosis\n\nYou can get skim of only numeric variables if you like\n```{r}\ndata_trn |> \n  skim_all() |> \n  filter(skim_type == \"numeric\")\n```\n\n--------------------------------------------------------------------------------\n\n### Bivariate Relationships with Outcome\n\nBivariate relationships with the outcome are useful to detect\n\n- Which predictors display some relationship with the outcome\n- What feature engineering (transformations) might maximize that relationship\n- Are there any bivariate (model) outliers\n\nAgain, we prefer visualizations but summary statistics are also available\n\n--------------------------------------------------------------------------------\n\n### Scatterplots for Numeric Variables (Bivariate)\n\nScatterplots are the preferred visualization when both variables are numeric\n\n\nDefine a scatterplot function (included in `fun_plots.r`)\n\n- add a simple line\n- add a LOWESS line (Locally Weighted Scatterplot Smoothing)\n- These lines are useful for considering shape of relationship\n```{r}\n#| eval: false\n\nplot_scatter <- function(df, x, y){\n  df |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"green\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\nLet's consider relationship between `gr_liv_area` and `sale_price`\n\n- Care most about influential points (both model outlier and leverage)\n- Can be *typically* spotted in bivariate plots (but could do more sophisticated assessments)\n- We might:\n  - retain as is\n  - drop\n  - bring to fence\n\nIf bivariate outliers are detected, you should return to cleaning mode to verify that they aren't result of scoring/coding errors.  If they are:\n\n  - Fix in full dataset\n  - Use same train/test split after fixing\n  \n```{r}\ndata_trn |> plot_scatter(\"gr_liv_area\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\nHere is another example where the relationship might be non-linear\n\n```{r}\ndata_trn |> plot_scatter(\"year_built\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\nA transformation of `sale_price` might help the relationship with $year\\_built$ but might hurt `gr_liv_area`\n\nMaybe need to transform both `sale_price` and `gr_liv_area` as both were skewed\n\nThis might require some more EDA but here is a start\n\n- Quick and temporary Log (base e) of `sale_price`\n- This doesn't seem promising by itself\n\n```{r}\n  \ndata_trn |> \n  mutate(sale_price = log(sale_price)) |> \n  plot_scatter(\"gr_liv_area\", \"sale_price\")\n\ndata_trn |> \n  mutate(sale_price = log(sale_price)) |>\n  plot_scatter(\"year_built\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\nCan make scatterplots for ordinal variables as well\n\n- But other (perhaps better) options also exist for this combination of variable classes.\n- Use `as.numeric()` to allow for lm and LOWESS lines on otherwise categorical variable\n```{r}\ndata_trn |> \n  mutate(overall_qual = as.numeric(overall_qual)) |> \n  plot_scatter(\"overall_qual\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\nUse `jitter()` with x to help with overplotting\n\n```{r}\ndata_trn |> \n  mutate(overall_qual = jitter(as.numeric(overall_qual))) |> \n  plot_scatter(\"overall_qual\", \"sale_price\")\n```\n\n:::\n\n--------------------------------------------------------------------------------\n\n### Correlations & Correlation Plots for Numeric Variables (Bivariate)\n\nCorrelations are useful summary statistics for numeric variables\n\nSome statistical algorithms are sensitive to high correlations among features (multi-collinearity)\n\nAt best, highly correlated features add unnecessary flexibility and can lead to overfitting\n\n--------------------------------------------------------------------------------\n\nWe can visualize correlations among predictors/features using `corrplot.mixed()` from `corrplot` package\n\n- Best for numeric variables\n- Can include ordinal or two level nominal variables if transformed to numeric\n- Can include nominal variables with > 2 levels if first transformed appropriately (e.g., dummy features, not demonstrated yet)\n- Works best with relatively small set of variables\n\n--------------------------------------------------------------------------------\n\n```{r}\ndata_trn |> \n  mutate(overall_qual = as.numeric(overall_qual),\n         garage_qual = as.numeric(garage_qual)) |> \n  select(where(is.numeric)) |> \n  cor(use = \"pairwise.complete.obs\") |> \n  corrplot::corrplot.mixed() # <1>\n```\n1. Note use of namespace (`corrplot::corrplot.mixed()`) to call this function from `corrplot` package\n\n--------------------------------------------------------------------------------\n\n### Grouped Box + Violin Plots for Categorical and Numeric (Bivariate)\n\nA grouped version of the combined box and violin plot is our preferred visualization for relationship between categorical and numeric variables (included in `fun_plots.r`)\n\n- Often best when feature is categorical and outcome is numeric but can reverse\n- Can use with both nominal and ordinal categorical variable\n- @RDS also describes use of grouped frequency polygons for this combination of variable classes\n\n\n```{r}\n#| eval: false\n\nplot_grouped_box_violin <- function(df, x, y){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\nHere is the relationship between `overall_qual` and `sale_price`\n\n- Tend to prefer this over the scatterplot (with `as.numeric()`) for ordinal variables\n- Increasing spread of `sale_price` at higher levels of `overall_qual` is clearer in this plot\n\n```{r}\ndata_trn |> plot_grouped_box_violin(\"overall_qual\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\nHere is a grouped box + violin with a nominal variable\n\n- More variation and skew in `sale_price` for one family homes (additional features, moderators?)\n- Position of townhouse (interior vs. exterior) seems to matter (don't collapse?)\n\n```{r}\ndata_trn |> plot_grouped_box_violin(\"bldg_type\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\nWhen we have a categorical predictor and a numeric outcome, we often want to see both the relationship between the variables AND the variability on the categorical variable alone.   \n\nWe like this combined plot enough when doing EDA to provide a specific function (included in `fun_plots.r`)!  \n\nIt is our go to for understanding the potential effect of a categorical predictor\n\n```{r}\n#| eval: false\n\nplot_categorical <- function(df, x, y, ordered = FALSE){\n  if (ordered) {\n    df <- df |>\n      mutate(!!x := fct_reorder(.data[[x]], .data[[y]]))\n  }\n  \n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  p_bar <- df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_bar()  +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n  \n  p_box <- df |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n  \n  return(list(p_bar, p_box))\n}\n```\n\n--------------------------------------------------------------------------------\n\n`sale_price` by `bldg_type`\n\n```{r}\ndata_trn |> plot_categorical(\"bldg_type\", \"sale_price\") |> \n  plot_grid(plotlist = _, ncol = 1)\n```\n\n--------------------------------------------------------------------------------\n\n### Stacked Barplots for Categorical (Bivariate)\n\nStacked Barplots:\n\n- Can be useful with both nominal and ordinal variables\n- Can create with either raw counts or percentages.  \n  - Displays different perspective (particularly with uneven distributions across levels)\n  - Depends on your question\n- Often, you will place the outcome on the x-axis and the feature is coded by fill\n\n```{r}\n#| eval: false\n\nplot_grouped_barplot_count <- function(df, x, y){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[y]], fill = .data[[x]])) +\n    geom_bar(position = \"stack\") +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\nplot_grouped_barplot_percent <- function(df, x, y){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[y]], fill = .data[[x]])) +\n    geom_bar(position = \"fill\") +\n    labs(y = \"Proportion\") +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\nFor example, if we wanted to learn about how `bldg_type` varies by `lot_config`, see these plots\n\n```{r}\ndata_trn |> plot_grouped_barplot_count(\"lot_config\", \"bldg_type\")\n```\n\n```{r}\ndata_trn |> plot_grouped_barplot_percent(\"lot_config\", \"bldg_type\")\n```\n\n--------------------------------------------------------------------------------\n\nMay want to plot both ways\n\n```{r}\ndata_trn |> plot_grouped_barplot_percent(\"lot_config\", \"bldg_type\")\n```\n\n```{r}\ndata_trn |> plot_grouped_barplot_percent(\"bldg_type\", \"lot_config\")\n```\n\n--------------------------------------------------------------------------------\n\n### Tile Plot for Ordinal Categorical (Bivariate)\n\nTile plots may be useful if both categorical variables are ordinal \n\n```{r}\n#| eval: false\n  \nplot_tile <- function(df, x, y){\n  df |>\n    count(.data[[x]], .data[[y]]) |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_tile(mapping = aes(fill = n))\n}\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\ndata_trn |> plot_tile(\"overall_qual\", \"garage_qual\")\n```\n\n--------------------------------------------------------------------------------\n\nYou might also consider a scatterplot with jitter in this instance\n```{r}\ndata_trn |> \n  mutate(overall_qual = jitter(as.numeric(overall_qual)),\n         garage_qual = as.numeric(garage_qual)) |> \n  plot_scatter(\"overall_qual\", \"garage_qual\")\n```\n\n--------------------------------------------------------------------------------\n\n### Two-way Tables for Categorical Variables (Bivariate)\n\nTwo-way tables are sometimes a useful summary statistic for two categorical variables.  We can use a 2-variable form of the tab function, `tab2()`, from my function scripts for this\n\nFor example, the relationship between `bldg_type` and `lot_config`\n```{r}\ndata_trn |> tab2(bldg_type, lot_config)\n```\n\n--------------------------------------------------------------------------------\n\n## Working with Recipes\n\nRecipes are used for feature engineering in `tidymodels` using the [`recipes`](https://recipes.tidymodels.org/) package\n\n- Used for transforming raw predictors into features used in our models\n- Describes all steps to make feature matrix.  For example:\n  - Transforming factors into \"dummy\" features if needed\n  - Linear and non-linear transformations (e.g., log, box-cox)\n  - Polynomials and interactions (i.e., `x1 * x1` or `x1 * x2`)\n  - Missing value imputations\n- Proper use of recipes is an important tool to prevent data leakage between train and either validation or test.\n- Recipes use only information from the training set in all feature engineering\n- Consider example of standardizing `x1` for a feature in train vs. validation and test.  Must use mean and sd from TRAIN to standardize `x1` in train, validate, and test.  VERY IMPORTANT.\n\n--------------------------------------------------------------------------------\n\nWe use recipes in a two step process - `prep()` and `bake()`\n  \n- \"Prepping\" a recipe involves calculating any statistics needed for the transformations that will be applied to engineer features (e.g., mean and standard deviation to normalize a numeric variable).\n  - Prepping is done with the `prep()` function.\n  - Prepping is **always** done only with training data.  A \"prepped\" recipe does not derive any statistics from validation or test sets.\n\n- \"Baking\" is the process of calculating the features\n  - Baking is done with the `bake()` function.\n  - We used our prepped recipe when we bake.\n  - Whereas we only prep a recipe with training data, we can use a prepped recipe to bake features from any dataset (training, validation, or test).\n\n--------------------------------------------------------------------------------\n\nWe will work with recipes extensively when model building starting in unit 3\n\nFor now, we will only use the recipe to indicate roles as a gentle introduction.\n\nWe will expand on this recipe in unit 3\n\nRecipe syntax is very similar to generic `tidyverse` syntax (created by same group)\n\n- Actually a subset of `tidyverse` functions\n- Less flexible/powerful but focused on our needs and easier to learn\n- You will eventually know both\n\n--------------------------------------------------------------------------------\n\nRecipes are used in Modeling scripts (which is a third type of script after cleaning and modeling EDA scripts)\n\n- Lets reload training again to pretend we are starting a new script\n\n```{r}\n data_trn <- read_csv(file.path(path_data, \"ames_clean_class_trn.csv\"), \n                      col_types = cols()) |> \n  class_ames() |> # <1> \n  glimpse()\n```\n1. Remember our function for classing! \n\n--------------------------------------------------------------------------------\n\nRecipes can be used to indicate the outcome and predictors that will be used in the model\n\n- Can use `.` to indicate all predictors\n  - Currently, our preferred method for larger data sets\n  - We can exclude some predictors later by changing their role, removing them with a later recipe step ($step\\_rm()$), or specifying a more precise formula when we fit the model\n  - See [Roles in Recipes](https://recipes.tidymodels.org/articles/Roles.html) for more info\n\n- Can use specific names of predictors along with $+$\n  - This is our preferred method when we have a smaller set of predictors (We will show you both approaches)\n  - e.g., `sale_price ~ lot_area + year_built + overall_qual`\n\n- Do NOT indicate interactions here\n  - All predictors are combined with `+`\n  - Interactions are specified by a later explicit feature engineering step\n  \n```{r}\nrec <- recipe(sale_price ~ ., data = data_trn)\n\nrec\n\nsummary(rec)\n```\n\n--------------------------------------------------------------------------------\n\n### Prepping and Baking a Recipe\n\nLet's make a feature matrix from the training set\n\nThere are two discrete (and important) steps\n\n - `prep()`\n - `bake()`\n\nFirst we prep the recipe using the training data\n\n```{r}\nrec_prep <- rec |> # <1>\n  prep(training = data_trn) #<2>\n```\n1. We start by prepping our raw/original recipe (`rec`)\n2. We use the `prep()` function on on the training data.  Recipes are ALWAYS prepped using training data.  This makes sure that are recipes will always only use information from the training set when making features for any subsequent dataset.  As part of the prepping process, `prep()` calculates and saves the features for the training data for later use\n\n--------------------------------------------------------------------------------\n\nSecond, we bake the training data using this prepped recipe to get a feature matrix from it.  \n\nWe set `new_data = NULL` because don't need features for new data.  Instead, we want features from the training data that were used to prep the recipe\n```{r}\nfeat_trn <- rec_prep |> \n  bake(new_data = NULL)\n```\n\n--------------------------------------------------------------------------------\n\nFinally, we should generally at least glimpse and/or skim (and typically do some more EDA) on our features to make sure our recipe is doing what we expect.\n\n- glimpse\n```{r}\nfeat_trn |> glimpse()\n```\n\n--------------------------------------------------------------------------------\n\n- skim\n\n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\nWe can now use our features from training to train models, but that will take place in the next unit!\n\nWe could also use the prepped recipe to bake validation or test data to evaluate trained models.  That too will happen in the next unit!","srcMarkdownNoYaml":"\n\n::: {.content-visible unless-format=\"revealjs\"}\n# Exploratory Data Analysis\n:::\n::: {.content-visible when-format=\"revealjs\"}\n# IAML Unit 2: Exploratory Data Analysis\n:::\n\n## Learning Objectives\n  \n- Stages of Analysis  \n- Best practices for data storage, variable classing, data dictionaries\n- Problems and solutions regarding data leakage\n- Key goals and techniques cleaning EDA\n  - Tidying names and response labels\n  - Appropriate visualizations based on variable class\n  - Summary statistics based on variable class\n- Proper splitting for training/validation and test sets\n- Key goals and techniques modeling EDA\n  - Appropriate visualizations based on variable class\n  - Summary statistics based on variable class\n- Introductory use of recipes for feature engineering\n\n--------------------------------------------------------------------------------\n\n## Overview of Exploratory Data Analysis\n\n### Stages of Data Analysis and Model Development\n\nThese are the main stages of data analysis for machine learning and the data that are used\n\n1. EDA: Cleaning (full dataset)\n2. EDA: Split data into training, validation and test set(s)\n3. EDA: Modeling (training sets)\n\n4. Model Development: Feature engineering (training sets)\n5. Model Development: Fit many models configurations (training set)\n6. Model Development: Calculate performance metric for many models configurations (validation sets)\n\n7. Model Development: Select final/best model configuration based on performance metric (validation sets)\n8. Final Model Evaluation: Fit best model configuration (use both training and validation sets)\n9. Final Model Evaluation: Evaluate final model configuration using performance metric (test sets)\n10. Implementation: Fit best model configuration to ALL data (training, validation, and test sets) if you plan to use it for applications.\n\n--------------------------------------------------------------------------------\n\nThe earlier stages are highly iterative:\n\n- You may iterate some through EDA stages 1-3 if you find further errors to clean in stage 3 [**But make sure you resplit into the same sets**]\n- You will iterate many times though stages 3-6 as you learn more about your data both through EDA for modeling and evaluating actual models in validation\n\nYou will NOT iterate back to earlier stages after you select a final model configuration in Stage 7\n\n- Stages 7 - 10 are performed ONLY ONCE\n- Only one model configuration is selected and re-fit and only that model is brought into test for evaluation\n- Any more than this is essentially equivalent to p-hacking in traditional analyses\n- Step 10 only happens if you plan to implement the model in some application\n\n--------------------------------------------------------------------------------\n\n## Best Practices and Other Recommendations\n\n### Data file formats\n\nWe generally store data as CSV [comma-separated value] files\n\n- Easy to **view directly** in a text editor\n- Easy to **share** because others can use/import into any data analysis platform\n- Works with **version control** (e.g. git, svn)\n- use `read_csv()` and `write_csv()`\n\nExceptions include: \n\n- We **may** consider binary (.rds) format for very big files because read/write can be slow for csv files.  \n- Binary file format provides a very modest additional protection for sensitive data (which we also don't share)\n- use `read_rds()` and `write_rds()`\n\n\nSee [chapter 7 - Data Import](https://r4ds.hadley.nz/data-import) in @RDS for more details and advanced techniques for importing data using `read_csv()`\n\n--------------------------------------------------------------------------------\n\n### Classing Variables\nWe store and class variables in R based on their data type (level of measurement).\n\n- See Wikipedia definitions for [levels of measurement](https://en.wikipedia.org/wiki/Level_of_measurement) for a bit more precision that we will provide here.\n\nCoarsely, there are four levels:\n\n- nominal:  qualitative categories, no inherent order (e.g., marital status, sex, car color)\n- ordinal: qualitative categories (sometimes uses number), inherent order but not equidistant spacing (e.g., Likert scale; education level)\n- interval and ratio (generally treated the same in social sciences): quantitative scores, ordered, equidistant spacing.  Ratio has true 0.  (e.g., temperature in Celsius vs. Kelvin scales)\n\nWe generally refer to nominal and ordinal variables as categorical and interval/ratio as quantitative or numeric \n\n--------------------------------------------------------------------------------\n\nFor **nominal** variables\n\n- We store (in csv files) these variables as character class with descriptive text labels for the levels\n  - Easier to share/document\n  - Reduces errors\n- We class these variables in R as factors when we load them (using `read_csv()`)\n- In some cases, we should pay attention to the order of the levels of the variable. e.g., \n  - For a dichotomous outcome variable, the positive/event level of dichotomous factor outcome should be first level of the factor\n  - The order of levels may also matter for factor predictors (e.g., `step_dummy()` uses first level as reference).\n\n--------------------------------------------------------------------------------\n\nFor **ordinal** variables: \n\n- We store (in csv files) these variables as character class with descriptive text labels for the levels\n\t- Easier to share/document\n\t- Reduces errors\n- We class these variables in R as factors (just like nominal variables)\n  - It is easier to do EDA with these variables classes as factors\n  - We use standard factors (not ordered)\n- Confirm that the order of the levels is set up correctly.  This is very important for ordinal variables.\n- During feature engineering stage, we can then either\n  - Treat as a nominal variable and create features using `step_dummy()`\n  - Treat as an interval variable using `step_ordinalscore()`\n\n\nSimilar EDA approaches are used with both nominal and ordinal variable\n\nOrdinal variables may show non-linear relations b/c they may not be evenly spaced.  In these instances, we can use feature engineering approaches that are also used for nominal variables\n\n--------------------------------------------------------------------------------\n\nFor **interval and ratio** variables:\n\n- We store these variables as numeric\n- We class these variables as numeric (either integer or double - let R decide) during the read and clean stage (They are typically already in this class when read in)\n\nSimilar EDA approaches are used with both interval and ratio variables\n\nSimilar feature engineering approaches are used with both\n\n--------------------------------------------------------------------------------\n\n### Data Dictionaries\n\nYou should always make a data dictionary for use with your data files.  \n\n- Ideally, these are created during the planning phase of your study, prior to the start of data collection\n- Still useful if created at the start of data analysis\n\nData dictionaries:\n\n- Help you keep track of your variables and their characteristics (e.g., valid ranges, valid responses)\n- Can be used by you to check your data during EDA\n- Can be provided to others when you share your data (data are not generally useful to others without a data dictionary)\n\nWe will see a variety of data dictionaries throughout the course.   Many are not great as you will learn.\n\n--------------------------------------------------------------------------------\n\n### The Ames Housing Prices Dataset\n\nWe will use the Ames Housing Prices dataset as a running example this unit (and some future units and application assignments as well)\n\n- You can [read more](http://jse.amstat.org/v19n3/decock.pdf) about the original dataset created by Dean DeCock \n\n- The data set contains data from home sales of individual residential property in Ames, Iowa\nfrom 2006 to 2010\n\n- The original data set includes 2930 observations of sales price and a large number of explanatory\nvariables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous)\n\n- This is the [original data dictionary](data/ames_data_dictionary.pdf)\n\n- The challenge with this dataset is to build the best possible prediction model for the\nsale price of the homes.\n\n--------------------------------------------------------------------------------\n\n### Packages and Conflicts\nFirst, lets set up our environment with functions from important packages.  I strongly recommend reviewing our recommendations for best practices regarding [managing function conflicts](https://jjcurtin.github.io/book_dwvt/conflicts.html) now. It will save you a lot of headaches in the future.\n\nWe first load our two main packages (which will conflict with each other but intentionally by the package creators)\n\nNext we load packages for other functions that we will use regularly.  There are five things to note RE best practices\n\n1. If we will use a lot of functions from a package (e.g., `tidyverse`, `tidymodels`), we attach the full package\n2. If we will use only several functions from a package (but plan to use them repeatedly), we use the `include.only` parameter to just attach those functions.\n3.  At times, if we plan to use a single function from a package only 1-2x times, we may not even attach that function at all.  Instead, we just call it using its namespace (i.e.  `packagename::functionname`)\n4. If a package has a function that conflicts with our primary packages and we don't plan to use that function, we load the package but exclude the function.  If we really needed it, we can call it with its namespace as per option 3 above.\n5. Pay attention to conflicts that were allowed to make sure you understand and accept them. (I left the package messages and warnings in the book this time to see them. I will hide them to avoid cluttering book in later units but you should always review them.)\n\n```{r}\n#| message: false\n#| warning: false\n\nlibrary(tidyverse) \nlibrary(tidymodels) \n\noptions(conflicts.policy = \"depends.ok\")\n\nlibrary(janitor, include.only = \"clean_names\") # <1>\nlibrary(cowplot, include.only = \"plot_grid\") # <2> \nlibrary(kableExtra, exclude = \"group_rows\") # <3> \n```\n1. As an alternative, we could have skipped loading the package and instead called the function as `janitor::clean_names()`\n2. Same is true for `cowplot` package\n3. When loading `kableExtra` (which we use often), you will always need to exclude `groups_rows()` to prevent a conflict with `dplyr` package in the tidyverse\n\n--------------------------------------------------------------------------------\n\n### Source and Other Environment Settings\nWe will also source (from github) libraries of functions that we use commonly for exploratory data analyses.  You should review these function scripts: \n\n- [fun_ml.R](https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\n- [fun_eda.R](https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true)\n- [fun_plots.R](https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true)\n```{r}\n#| message: false\n#| warning: false\n\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\nsource(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n```\n\n--------------------------------------------------------------------------------\n\nFinally, we tune our environment a bit more by setting plot themes and print options that we prefer\n```{r}\ntheme_set(theme_classic())\noptions(tibble.width = Inf, tibble.print_max = Inf)\n```\n\nAnd we set a relative path to our data.  This assumes you are using an RStudio project with the path to the data relative to that project file.  I've provided more detail [elsewhere](https://jjcurtin.github.io/book_dwvt/file_and_path_management.html) on best practices for managing files and paths.\n```{r}\npath_data <- \"data\"\n```\n\n--------------------------------------------------------------------------------\n\n### Read and Glimpse Dataframe\nLets read in the data and glimpse the subset of observations we will work with in Units 2-3 and the first two application assignments.  \n\n```{r}\ndata_all <- read_csv(here::here(path_data, \"ames_raw_class.csv\"),   # <1>\n                     show_col_types = FALSE) |> # <2> \n  glimpse() # <3>\n```\n1. First we read data using a relative path and the `here::here()` function.  This is a replacement for `file.path()` that works better for both interactive use and rendering in Quarto when using projects.\n2.  We use `show_col_types = FALSE` to suppresse messages that aren't important at this point prior to EDA.\n3. It is good practice to always `glimpse()` data after you read it.\n\n--------------------------------------------------------------------------------\n\nDataset Notes: \n\n- Dataset has N = 1955 rather than 2930.\n  - I have held out remaining observations to serve as a test set for a friendly competition in Unit 3\n  - I will judge your models' performance with this test set at that time!\n  - More on the importance of held out test sets as we progress through the course\n\n- This full dataset has 81 variables.  For the lecture examples in units 2-3 we will only use a subset of the predictors\n\n- You will use different predictors in the next two application assignments\n\n--------------------------------------------------------------------------------\n\nHere we select the variables we will use for lecture\n\n```{r}\ndata_all <- data_all |> \n  select(SalePrice,\n         `Gr Liv Area`, \n         `Lot Area`, \n         `Year Built`, \n         `Overall Qual`, \n         `Garage Cars`,\n         `Garage Qual`,\n         `MS Zoning`,\n         `Lot Config` ,\n         `Bldg Type`) |> # <1> \n  glimpse()\n```\n1. Notice that the dataset used non-standard variable names that include spaces. We need to use back-ticks around the variable names to allow us reference those variables. We will fix this during the cleaning process and you should never use spaces in variable names when setting up your own data!!! \n\n--------------------------------------------------------------------------------\n Exploratory Data Analysis for Data Cleaning\n\nEDA *could* be done using either `tidyverse` packages and functions or `tidymodels` (mostly using the `recipes` package.)\n\n- We prefer to use the richer set of functions available in the tidyverse (and `dplyr` and `purrr` packages in particular).\n\n- We will reserve the use of recipes for feature engineering only when we are building features for models that we will fit in our training sets and evaluation in our validation and test sets.  \n\n--------------------------------------------------------------------------------\n\n### Data Leakage Issues\nData leakage refers to a mistake made by the developer of a machine learning model in which they accidentally share information between their training set and held-out validation or test sets\n\n- Training sets are used to fit models with different configurations\n- Validation sets are used to select the best model among those with different configurations (not needed if you only have one configuration)\n- Test sets are used to evaluate a best model\n\n- When splitting data-sets into training, validation and test sets, the goal is to ensure that no data (or information more broadly) are shared between the three sets\n  - No data or information from test should influence either fitting or selecting models \n  - Test should only be used once to evaluate a best/final model\n  - Train and validation set also must be segregated (although validation sets may be used to evaluate many model configurations)\n  - Information necessary for transformations and other feature engineering (e.g., means/sds for centering/scaling, procedures for missing data imputation) must all be based only on training data.\n  - Data leakage is common if you are not careful.\n\nIn particular, if we begin to use test data or information about test during model fitting\n\n- We risk overfitting\n- This is essentially the equivalent of p-hacking in traditional analyses\n- Our estimate of model performance will be too optimistic, which could have harmful real-world consequences.\n\n--------------------------------------------------------------------------------\n\n### Tidy variable names\n\nUse snake case for variable names\n\n- `clean_names()` from `janitor` package is useful for this.\n- May need to do further correction of variable names using `rename()`\n- See more details about tidy names for objects (e.g., variables, dfs, functions) per [Tidy Style Guide](https://style.tidyverse.org/syntax.html#object-names)\n\n\n```{r}\ndata_all <- data_all |> \n  clean_names(\"snake\")\n\ndata_all |> names()\n```\n\n--------------------------------------------------------------------------------\n\n### Explore variable classes\n\nAt this point, we should class all of our variables as either numeric or factor\n\n- Interval and ratio variables use numeric classes (dbl or int)\n- Nominal and ordinal variable use factor class\n- Useful for variable selection later (e.g., `where(is.numeric)`, `where(is.factor)`)\n\nSubsequent cleaning steps are clearer if we have this established/confirmed now\n\n--------------------------------------------------------------------------------\n\nWe have a number of nominal or ordinal variables that are classed as character. \n\nWe have one ordinal variable (`overall_qual`) that is classed as numeric (because the levels were coded with numbers rather than text)\n\n- `read_csv()` thought was numeric by the levels are coded using numbers\n- The data dictionary indicates that valid values range from 1 - 10.\n\n```{r}\ndata_all |> glimpse()\n```\n\n--------------------------------------------------------------------------------\n\nWe can the recode `overall_qual` first and set its levels\n\nWe can recode all the character variables to factor in one step.  Most are nominal.  We will handle the order for `garage_qual` later.\n\n\n```{r}\noq_levels <- 1:10 # <1>\n\ndata_all <-  data_all |> \n  mutate(overall_qual = factor(overall_qual, \n                               levels = oq_levels)) |> # <2>\n  mutate(across(where(is.character), factor)) |>  # <3>\n  glimpse()\n```\n1. It is always best to explicitly set the levels of an ordinal factor in the order you prefer.  It is not necessary here because `overall_qual` was numeric and therefore sorts in the expected order.  However, if it had been numbers stored as characters, it could sort incorrectly (e.g., 1, 10, 2, 3, ...).  And obviously if the orders levels were names, the order would have to be specified.\n2. We indicate the levels here.\n3. We use a mutate to re-class all character data to factors.  I prefer `factor()` to `forcats::fct()` because factor orders the levels alphabetically.  Be aware that this *could* change if your code is used in a region of the world where this sorting is different.  I still prefer this to the alternative (in `fct()`) that orders by the order the levels are found in your data.\n\n--------------------------------------------------------------------------------\n\n### Skimming the data\n\n`skim()` from the `skimr` package is a wonderful and customizable function for summary statistics\n\n- It is highly customizable so we can write our own versions for our own needs\n- We use different versions for cleaning and modeling EDA\n- For cleaning EDA, we just remove some stats that we don't want to see at this time\n- We can get many of the summary stats for cleaning in one call \n- We have a custom skim defined in the `fun_eda.R` function library that we use regularly.  Here is the code but you can use the function directly if you sourced `fun_eda.R` (as we did above)\n\n```{r}\n#| eval: false\n\nskim_some <- skim_with(numeric = sfl(mean = NULL, sd = NULL, p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL))\n```\n\n--------------------------------------------------------------------------------\n\nHere is what we get with our new `skim_some()` function\n\n- We will refer to this again for each characteristic we want to review for instructional purposes \n- We can already see that we can use `skim_some()` to confirm that we only have numeric and factor classes\n```{r}\ndata_all |> \n  skim_some()\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar 1\n\n- Write functions whenever you will repeat code often. You can now reuse `skim_some()`\n- `skim_with()` is an example of a function factory - a function that is used to create a new function\n  - `partial()` and `compose()` are two other function factories we will use at times\n  - More details on function factories is available in [Advanced R](https://adv-r.hadley.nz/function-factories.html)\n  \n:::\n\n::: {.callout-tip}\n# Coding sidebar 2\n  \n- Gather useful functions together in a script that you can reuse.\n- All of the reusable functions in this and later units are available to you in one of my [public github repositories](https://github.com/jjcurtin/lab_support). \n- You can load these functions into your workspace directly from github using `source()`. For example: `source(\"https://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\")`\n- You should start to gather your favorite custom functions together in your own script(s).  \n- You can save your own scripts in a local file and load them into your workspace using `source()` or you can make your own github repo so you can begin to share your code with others!\n:::\n\n--------------------------------------------------------------------------------\n\n### Missing Data - All variables\n\n`skim_some()` provides us with missing data counts and complete data proportions for each variable\n\n\n```{r}\ndata_all |> \n  skim_some() |> \n  select(skim_variable, n_missing, complete_rate) # <1>\n```\n1. `skim_some()` returns a dataframe so you can select only the subset of columns to focus its output on what you want.   Or just print it all!\n\n--------------------------------------------------------------------------------\n\nYou likely should view the full observation for missing values\n\nWe will show you a few methods to do this in your rendered output\n\n- `print()` will print only 20 rows and the number of columns that will display for width of page\n  - Set `options()` if you will do a lot of printing and want full dataframe printed\n- Use `kbl()` from `kableExtra` package for formatted tables (two methods below)\n\n\nDon't forget that you can also use `view()` interactively in R Studio\n\n--------------------------------------------------------------------------------\n\n**Option 1 (Simple)**:  Use print() with `options()`\n\n```{r}\noptions(tibble.width = Inf, tibble.print_max = Inf) # <1>\n\ndata_all |> filter(is.na(garage_cars)) |> \n  print()\n```\n1. This sets print to print all rows and columns.  Note that we set these options at the start of the unit b.c. we like to see our full tibbles.  If we want only a subset of the first (or last) rows, we use `head()` or `tail()`\n\n--------------------------------------------------------------------------------\n\nHere are some more advanced options using `kbl()` for the df with many rows\n\n- `kable()` tables from `knitr` package and `kableExtra` extensions (including `kbl()`) are very useful during EDA and also final publication quality tables\n- use `library(kableExtra)`\n- see [vignettes for kableExtra](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html)\n\n**Option 2 (more advanced)**: Use a function for kables that we created.  Code is displayed here but the function is available to you if you source fun_eda.R from Github\n\n```{r}\n#| eval: false\n\n# Might want to use height = \"100%\" if only printing a few rows\nprint_kbl <- function(data, height = \"500px\") { # <1>\n  data |> \n    kbl(align = \"r\") |> \n    kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) |> \n    scroll_box(height = height, width = \"100%\") # <2>\n}\n```\n1. Defaults to a output box of height = \"500px\".  Can set to other values if preferred.\n2. Might want to use `height = \"100%\"` if only printing a few rows. \n\n--------------------------------------------------------------------------------\n\nLet's use this function to see its output\n```{r}\ndata_all |> filter(is.na(garage_qual)) |> \n  print_kbl()\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\n\n- In the above example, we created a function (`print_kbl()`) from scratch (rather than using a function factory)\n- See [functions chapter](https://r4ds.hadley.nz/functions.html) in @RDS for help.\n- See [functionals chapter](https://adv-r.hadley.nz/functionals.html) in @AR. \n\n:::\n\n--------------------------------------------------------------------------------\n\n**Option 3 (Most advanced)**: Line by line kable table. You can make this as complicated and customized as you like. We use kable (and kableExtra) for publication quality tables.  This is a simple example of options\n\n```{r}\ndata_all |> filter(is.na(garage_qual)) |> \n  kbl(align = \"r\") |> \n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) |> \n  scroll_box(height = \"500px\", width = \"100%\")\n```\n\n--------------------------------------------------------------------------------\n\nIn this instance, if we consult our [data dictionary](data/ames_data_dictionary.pdf), we see that `NA` for `garage_qual` should be coded as \"no garage\".  We will correct this in our data set.  \n\nThis is a pretty poor choice on the part of the researchers who created the dataset because it becomes impossible to distinguish between `NA` that means no garage vs. true NA for the variable.  In fact, if you later do really careful EDA on the full data set with all variables, you will see this problem likely exists in this dataset\n\nAnyway, let's correct all the `NA` for `garage_qual` to \"no_garage\" using `mutate()` \n\n```{r}\ndata_all <- data_all |> \n  mutate(garage_qual = fct_expand(garage_qual, \"no_garage\"), # <1>\n         garage_qual = replace_na(garage_qual, \"no_garage\")) # <2>\n\n```\n1. First add a new level to the factor\n2. Then recode NA to that new level\n\nWe will leave the `NA` for `garage_cars` as `NA` because its not clear if that is truly missing or not, based on further EDA not shown here.\n\n--------------------------------------------------------------------------------\n\nWe have one other issue with `garage_qual`.  It is an ordinal variable but we never reviewed the order of its levels.  The data dictionary indicates the levels are ordered (best to worst) as:\n\n- Ex (excellent)\n- Gd (good)\n- TA (typical/average)\n- Fa (fair)\n- Po (poor)\n\nAnd we might assume that no garage is even worse than a poor garage.  Lets see what they are.\n\n```{r}\ndata_all$garage_qual |> levels()\n```\n\nTo fix this, we can use `forcats::fct_relevel()`.\n```{r}\ngq_levels <- c(\"no_garage\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\") # <1>\ndata_all <- data_all |> \n  mutate(garage_qual = fct_relevel(garage_qual, gq_levels)) # <2>\n\ndata_all$garage_qual |> levels() # <3>\n```\n1. Make a vector that indicates the valid levels in order\n2. Pass that into `fct_relevel()`.  See `?fct_relevel` for other ways to adjust the levels of a factor.\n3. Confirm that the levels are now correct\n\n--------------------------------------------------------------------------------\n\n### Explore Min/Max Response for Numeric Variables\n\nWe should explore mins and maxes for all numeric variables to detect out of valid range numeric responses\n\n- Could also do this for ordinal variables that are coded with numbers\n  - e.g., `overall_qual` (1-10) vs. `garage_qual` (no_garage, Po, Fa, TA, Gd, Ex)\n- This is only a temporary mutation of `overall_qual` for this check.  We don't assign to new df to an object\n- We can use `skim_some()` again\n  - p0 = min\n  - p100 = max\n\n```{r}\ndata_all |>\n  mutate(overall_qual = as.numeric(overall_qual)) |> \n  skim_some() |> \n  filter(skim_type == \"numeric\") |>  # <1>\n  select(skim_variable, numeric.p0, numeric.p100) # <2>\n```\n1. Select only numeric variables since min/max only apply to them\n2. Select relevant stats (min/max)\n\n--------------------------------------------------------------------------------\n\n### Explore All Responses for Categorical Variables\n\nWe should explore all unique responses for nominal variables\n\nMight also do this for ordinal variables that are coded with labels vs. numbers.  \n\n```{r}\ndata_all |> \n  select(where(is.factor)) |>\n  walk(\\(column) print(levels(column)))\n```\n\n\n::: {.callout-tip}\n# Coding sidebar\n\n- On the previous page, we demonstrated the use of an anonymous function (`\\(column) print(levels(column))`), which is a function we use once that we don't bother to assign a name (since we won't reuse it).  We often use anonymous functions when using the functions from the `purrr` package (e.g., `map()`, `walk()`) \n- We use `walk()` from the `purrr` package to apply our anonymous function to all columns of the data frame at once\n- Just copy this code for now\n- We will see simpler uses later that will help you understand iteration with `purrr` functions\n- See the chapter on [iteration](https://r4ds.hadley.nz/iteration) in *R for Data Science (2e)* for more info on `map()` and `walk()`\n\n:::\n\n--------------------------------------------------------------------------------\n\n### Tidy Responses for Categorical Variables\n\nFeature engineering with nominal and ordinal variables typically involves \n\n- Converting to factors (already did this!)\n- Often creating dummy features from these factors\n\nThis feature engineering will use response labels for naming new features\n\n- Therefore, it is a good idea to have the responses snake-cased and cleaned up a bit so that these new feature names are clean/clear.\n\nHere is an easy way to convert responses for character variables to snake case using a function (`tidy_responses()`) we share in `fun_eda.R` (reproduced here). \n\n- This uses regular expressions (regex), which will will learn about in a later unit on text processing.\n- You could expand this cleaning function if you encounter other issues that need to be cleaned in the factor levels.\n```{r}\n#| eval: false\n\ntidy_responses <- function(column){\n  # replace all non-alphanumeric with _\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\W\", \"_\"))\n  # replace whitespace with _\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\s+\", \"_\"))\n  # replace multiple _ with single _\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\_+\", \"_\"))\n  #remove _ at end of string\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\_$\", \"\"))\n  # remove _ at start of string\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\^_\", \"\"))\n  # convert to lowercase\n  column <- fct_relabel(column, tolower)\n  factor(column)\n}\n```\n\n--------------------------------------------------------------------------------\n\nLet's use the function\n\n```{r}\ndata_all <- data_all |> \n  mutate(across(where(is.factor), tidy_responses)) # <1>\n```\n1. We use the tidy selection helper function to limit our mutate to only factors.  See [more details](https://tidyselect.r-lib.org/reference/language.html) on the tidy selection helpers like `all_of()` and `where()`\n\n--------------------------------------------------------------------------------\n\nAlas, these response labels were pretty poorly chosen so some didn't convert well.  And some are really hard to understand too.\n\n- Avoid this problem and choose good response labels from the start for your own data\n- Here, we show you what we got from using `tidy_responses()`\n```{r}\ndata_all |> \n  select(where(is.factor)) |>\n  walk(\\(column) print(levels(column)))\n```\n\n--------------------------------------------------------------------------------\n\nLets clean them up a bit more manually\n\n```{r}\ndata_all <- data_all |> \n  mutate(ms_zoning = fct_recode(ms_zoning,\n                                res_low = \"rl\",\n                                res_med = \"rm\",\n                                res_high = \"rh\",\n                                float = \"fv\",\n                                agri = \"a_agr\",\n                                indus = \"i_all\",\n                                commer = \"c_all\"),\n         bldg_type = fct_recode(bldg_type,   # <1>\n                                one_fam = \"1fam\",\n                                two_fam = \"2fmcon\",\n                                town_end = \"twnhse\",\n                                town_inside = \"twnhs\"))\n                                \n```\n1. Note that I did not need to list all levels in the recode.  Only the levels I wanted to change.\n\nThe full dataset is now clean!\n\n--------------------------------------------------------------------------------\n\n### Train/Validate/Test Splits\n\nThe final task we typically do as part of the data preparation process is to split the full dataset into training, validation and test sets.\n\n- Test sets are \"typically\" between 20-30% of your full dataset\n  - There are costs and benefits to larger test sets\n  - We will learn about these costs/benefits in the unit on resampling\n  - I have already held out the test set\n  \n- There are many approaches to validation sets\n  - For now (until unit 5) we will use a single validation set approach\n  - We will use 25% of the remaining data (after holding out the test set) as a validation set for this example\n\n- It is typical to split data on the outcome within strata\n  - For a categorical outcome, this makes the proportions of the response categories more balanced across the train, validation, and test sets\n  - For a numeric outcome, we first break up the distribution into temporary bins (see `breaks = 4` below) and then we split within these bins\n- **IMPORTANT**: Set a seed so that you can reproduce these splits if you later do more cleaning\n\n\n```{r}\nset.seed(20110522)\nsplits <- data_all |> \n  initial_split(prop = 3/4, strata = \"sale_price\", breaks = 4)\n```\n\n::: {.callout-tip}\n# Coding Note\n`initial_split()` is used to create two resamples of your data.  This was used here because we already had a test set held out (outside of the code presented).  We use `analysis()` and `assessment()` to extract these two resamples.   \n\nWe will later use `initial_validation_split()` to create 3 resamples: training, validation, and test.  We will extract those three resamples using `training()`, `validation()`, and `testing()`\n:::\n\n--------------------------------------------------------------------------------\n\nWe then extract the training set from the splits and save it\n\n- Training sets are used for \"analysis\"- hence the name of the function\n```{r}\nsplits |> \n  analysis() |> # <1> \n  glimpse() |> \n  write_csv(here::here(path_data, \"ames_clean_class_trn.csv\"))\n```\n1. `analysis()` pulls out the training set from our splits of `data_all`\n\n--------------------------------------------------------------------------------\n\nWe will not need the validation set for modeling EDA\n\n- It should NOT be used for anything other than calculating performance metrics for our many model configurations to select the best model configuration\n- We do NOT do Modeling EDA or Model Fitting with the validation set\n- Save it in this clean form for easy use when you need it\n- We use the validation set to \"assess\" (calculate performance metrics for selection) models that we have fit in training sets - hence the name of the function\n\n```{r}\nsplits |> \n  assessment() |> # <1> \n  glimpse() |> \n  write_csv(here::here(path_data, \"ames_clean_class_val.csv\"))\n```\n1. `assessment()` pulls out the validation set from our splits of `data_all`\n\n--------------------------------------------------------------------------------\n\n## Exploratory Data Analysis for Modeling\n\nNow let's begin our Modeling EDA\n\nWe prefer to write separate scripts for Cleaning vs. Modeling EDA (but not displayed here)\n\n- This keeps these two processes separate in our minds\n- Cleaning EDA is done with full dataset but Modeling EDA is only done with a training set - NEVER use validation or test set\n- You will use two separate scripts for the application assignment for this unit\n\n--------------------------------------------------------------------------------\n\nLets re-load (and glimpse) our training set to pretend we are at the start of a new script.\n\n\n```{r}\n data_trn <- read_csv(here::here(path_data, \"ames_clean_class_trn.csv\")) |> \n  glimpse()\n```\n\n--------------------------------------------------------------------------------\n\nWe have some work to do (again)\n\n- Notice that `overall_qual` is back to being classed as numeric (dbl).  \n- Notice that your factors are back to character\n  - This is because csv files don't save anything other than the values (labels for factors).  They are the cleaned labels though!\n- You should class all variables using the same approach as before (often just a copy/paste). \n\n```{r}\n data_trn <- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           show_col_types = FALSE) |>  # <1>\n  mutate(across(where(is.character), factor)) |> # <2>\n  mutate(overall_qual = factor(overall_qual, levels = 1:10),  # <3>\n         garage_qual = fct_relevel(garage_qual, c(\"no_garage\", \"po\", \"fa\", \n                                                  \"ta\", \"gd\", \"ex\"))) |>  # <4> \n  glimpse()\n```\n1. use col_types = cols() to suppress messages about default class assignments\n2. use `mutate()` with `across()` to change all character variables to factors\n3. use `mutate()` with `factor()` to change numeric variable to factor.\n4. use `mutate()` with `fct_relevel()` to explicitly set levels of an ordinal factor.  Also notice the warning about the unknown level.  Always explore warnings!  In this instance, its fine.  There were only two observations with ex and neither ended up in the training split.   Still best to include this level to note it exists!\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\nWe will likely re-class the Ames dataset many times (for training, validation, test).  We could copy/paste these mutates each time but whenever you do something more than twice, I recommend writing a function.  We might write this one to re-class the ames variables\n:::\n\n```{r}\nclass_ames <- function(df){\n  \n  df |>\n    mutate(across(where(is.character), factor)) |> \n    mutate(overall_qual = factor(overall_qual, levels = 1:10), \n           garage_qual = fct_relevel(garage_qual, c(\"no_garage\", \"po\", \"fa\", \n                                                    \"ta\", \"gd\", \"ex\")))\n}\n```\n\n--------------------------------------------------------------------------------\n\nNow we can use this function every time we read in one of the Ames datasets\n\n```{r}\n#| warning: false\n\ndata_trn <- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           show_col_types = FALSE) |> \n  class_ames() |> # <1> \n  glimpse()\n```\n1. Using our new function!\n\n--------------------------------------------------------------------------------\n\nThere are 3 basic types of Modeling EDA you should always do\n\n1. Explore missingness for predictors\n2. Explore univariate distributions for outcome and predictors\n3. Explore bivariate relationships between predictors and outcome\n\nAs a result of this exploration, we will:\n\n- Identify promising predictors\n- Determine appropriate feature engineering for those predictors (e.g., transformations)\n- Identify outliers and consider how to handle when model building\n- Consider how to handle imputation for missing data (if any)\n\n--------------------------------------------------------------------------------\n\n### Overall Summary of Feature Matrix\n\nBefore we dig into individual variables and their distributions and relationships with the outcome, it's nice to start with a big picture of the dataset\n\n- We use another customized version of `skim()` from the `skimr` package to provide this\n- Just needed to augment it with skewness and kurtosis statistics for numeric variables\n- and remove histogram b/c we don't find that small histogram useful\n- included in `fun_eda.R` on github\n```{r}\n#| eval: false\n\nskew_na <- partial(e1071::skewness, na.rm = TRUE)\nkurt_na <- partial(e1071::kurtosis, na.rm = TRUE)\n\nskim_all <- skimr::skim_with(numeric = skimr::sfl(skew = skew_na, \n                                                  kurtosis = kurt_na, \n                                                  hist = NULL))\n```\n\n--------------------------------------------------------------------------------\n\nCareful review of this output provides a great orientation to our data\n\n```{r}\ndata_trn |> \n  skim_all()\n```\n\n--------------------------------------------------------------------------------\n\n### Univariate Distributions\n\nExploration of univariate distributions are useful to \n\n- Understand variation and distributional shape\n- May suggest need to consider transformations as part of feature engineering\n- Can identify univariate outliers (valid but disconnected from distribution so not detected in cleaning)\n\n\nWe generally select different visualizations and summary statistics for categorical vs. numeric variables\n\n--------------------------------------------------------------------------------\n\n### Barplots for Categorical Variables (Univariate)\n\nThe primary visualization for categorical variables is the bar plot\n\n- We use it for both nominal and ordinal variables\n- Define and customize it within a function for repeated use.  \n- We share this and all the remaining plots used in this unit in `fun_plot.R`. Source it to use them without having to re-code each time\n```{r}\n#| eval: false\n\nplot_bar <- function(df, x){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, \n                                     size = x_label_size, vjust = 0.5, \n                                     hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\nWhen defining functions, generally put data as first argument so you can pipe in data using tidy pipelines\n\nThere are pros and cons to writing functions that accept variable names that are quoted vs. unquoted\n\n- It depends a bit on how you will use them.\n- .data[[argument]] is used in functions with quoted arguments\n- embracing {{}} is used for unquoted arguments\n- For these plot functions, I use quoted variable names and then pipe those into `map()` to make multiple plots (see below)\n- see `?vignette(\"programming\")` or info on [tidy evaluation](https://r4ds.hadley.nz/functions.html#indirection-and-tidy-evaluation) in @RDS for more details\n\n:::\n\n--------------------------------------------------------------------------------\n\nBar plots reveal low frequency responses for nominal and ordinal variables\n\n- See `bldg_type`\n\n```{r}\ndata_trn |> plot_bar(\"bldg_type\")\n```\n\n--------------------------------------------------------------------------------\n\nBar plots can display distributional shape for ordinal variables.  May suggest the \nneed for transformations if we later treat the ordinal variable as numeric\n\n- See `overall_qual`.  Though it is not very skewed.\n\n```{r}\ndata_trn |> plot_bar(\"overall_qual\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\n\nWe can make all of our plots iteratively using `map()` from the `purrr` package.\n\n```{r}\ndata_trn |> \n  select(where(is.factor)) |> # <1> \n  names() |> # <2> \n  map(\\(name) plot_bar(df = data_trn, x = name)) |> # <3> \n  plot_grid(plotlist = _, ncol = 2) # <4>\n```\n1. Select only the factor columns\n2. Get their names as strings (that is why we use quoted variables in these plot functions\n3. Use `map()` to iterative `plot_bar()` over every column.  (see [iteration](https://r4ds.hadley.nz/iteration.html) in @RDS)\n4. Use `plot_grid()` from `cowplot` package to display the list of plots in a grid\n\n:::\n\n--------------------------------------------------------------------------------\n\n### Tables for Categorical Variables (Univariate)\n\nWe tend to prefer visualizations vs. summary statistics for EDA.   However, tables can be useful.\n\nHere is a function that was described in @RDS that we like because\n\n- It includes counts and proportions\n- It includes NA as a category\n\nWe have included it in `fun_eda.R` for your use.\n```{r}\n#| eval: false\n\ntab <- function(df, var, sort = FALSE) {\n  df |>  dplyr::count({{ var }}, sort = sort) |> \n    dplyr::mutate(prop = n / sum(n))\n} \n```\n\n--------------------------------------------------------------------------------\n\nTables can be used to identify responses that have very low frequency and to think about the need to handle missing values\n\n- See `ms_zoning`\n- May want to collapse low frequency (or low percentage) categories to reduce the number of features needed to represent the predictor\n\n```{r}\ndata_trn |> tab(ms_zoning)\n```\n\n--------------------------------------------------------------------------------\n\n- We could also view the table sorted if we prefer\n```{r}\ndata_trn |> tab(ms_zoning, sort = TRUE)\n```\n\n--------------------------------------------------------------------------------\n\nbut could see all this detail with plot as well\n\n```{r}\ndata_trn |> plot_bar(\"ms_zoning\")\n```\n\n--------------------------------------------------------------------------------\n\n### Histograms for Numeric Variables (Univariate)\n\nHistograms are a useful/common visualization for numeric variables\n\nLet's define a histogram function (included in `fun_plots.r`)\n\n- Bin size should be explored a bit to find best representation\n- Somewhat dependent on n (my default here is based on this training set)\n- This is one of the limitations of histograms\n```{r}\n#| eval: false\n\nplot_hist <- function(df, x, bins = 100){\n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_histogram(bins = bins) +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\nLet's look at `sale_price`\n\n- It is positively skewed\n- May suggest units (dollars) are not interval in nature (makes sense)\n- Could cause problems for some algorithms (e.g., lm) when features are normal\n```{r}\ndata_trn |> plot_hist(\"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\n### Smoothed Frequency Polygons for Numeric Variables (Univariate) \n\nFrequency polygons are also commonly used\n\n- Define a frequency polygon function and use it (included in `fun_plots.r`)\n\n```{r}\n#| eval: false\n\nplot_freqpoly <- function(df, x, bins = 50){\n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_freqpoly(bins = bins) +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\n- Bins may matter again\n\n```{r}\ndata_trn |> plot_freqpoly(\"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\n### Simple Boxplots for Numeric Variables (Univariate) \n\nBoxplots display \n\n- Median as line\n- 25%ile and 75%ile as hinges\n- Highest and lowest points within 1.5 * IQR (interquartile-range: difference between scores at 25% and 75%iles)\n- Outliers outside of 1.5 * IQR\n\nDefine a boxplot function and use it (included in `fun_plots.r`)\n```{r}\n#| eval: false\n\nplot_boxplot <- function(df, x){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1))\n}\n```\n\n--------------------------------------------------------------------------------\n\nHere is the plot for `sale_price`\n```{r}\ndata_trn |> plot_boxplot(\"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\n### Combined Boxplot and Violin Plots for Numeric Variables (Univariate)\n\nThe combination of a boxplot and violin plot is particularly useful\n\n- This is our favorite\n- Get all the benefits of the boxplot\n- Can clearly see shape of distribution given the violin plot overlay\n- Can also clearly see the tails\n\n\nDefine a combined plot (included in `fun_plots.r`)\n\n```{r}\n#| eval: false\n\nplot_box_violin <- function(df, x){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_violin(aes(y = 0), fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1))\n}\n```\n\n--------------------------------------------------------------------------------\n\nHere is the plot for `sale_price`\n\n- In this instance, the skew is NOT due to only a few outliers\n\n```{r}\ndata_trn |> plot_box_violin(\"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\n\n- You can make figures for all numeric variables at once using `select()` and `map()` as before\n\n```{r}\ndata_trn |> \n  select(where(is.numeric)) |> # <1> \n  names() |> \n  map(\\(name) plot_box_violin(df = data_trn, x = name)) |> \n  plot_grid(plotlist = _, ncol = 2)\n```\n1. Now select numeric rather than factor but otherwise same as previous example\n\n:::\n\n--------------------------------------------------------------------------------\n\n### Summary Statistics for Numeric Variables (Univariate)\n\n`skim_all()` provided all the summary statistics you likely needed for numeric variables\n\n- mean & median (p50)\n- sd & IQR (see difference between p25 and p75)\n- skew & kurtosis\n\nYou can get skim of only numeric variables if you like\n```{r}\ndata_trn |> \n  skim_all() |> \n  filter(skim_type == \"numeric\")\n```\n\n--------------------------------------------------------------------------------\n\n### Bivariate Relationships with Outcome\n\nBivariate relationships with the outcome are useful to detect\n\n- Which predictors display some relationship with the outcome\n- What feature engineering (transformations) might maximize that relationship\n- Are there any bivariate (model) outliers\n\nAgain, we prefer visualizations but summary statistics are also available\n\n--------------------------------------------------------------------------------\n\n### Scatterplots for Numeric Variables (Bivariate)\n\nScatterplots are the preferred visualization when both variables are numeric\n\n\nDefine a scatterplot function (included in `fun_plots.r`)\n\n- add a simple line\n- add a LOWESS line (Locally Weighted Scatterplot Smoothing)\n- These lines are useful for considering shape of relationship\n```{r}\n#| eval: false\n\nplot_scatter <- function(df, x, y){\n  df |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"green\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\nLet's consider relationship between `gr_liv_area` and `sale_price`\n\n- Care most about influential points (both model outlier and leverage)\n- Can be *typically* spotted in bivariate plots (but could do more sophisticated assessments)\n- We might:\n  - retain as is\n  - drop\n  - bring to fence\n\nIf bivariate outliers are detected, you should return to cleaning mode to verify that they aren't result of scoring/coding errors.  If they are:\n\n  - Fix in full dataset\n  - Use same train/test split after fixing\n  \n```{r}\ndata_trn |> plot_scatter(\"gr_liv_area\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\nHere is another example where the relationship might be non-linear\n\n```{r}\ndata_trn |> plot_scatter(\"year_built\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\nA transformation of `sale_price` might help the relationship with $year\\_built$ but might hurt `gr_liv_area`\n\nMaybe need to transform both `sale_price` and `gr_liv_area` as both were skewed\n\nThis might require some more EDA but here is a start\n\n- Quick and temporary Log (base e) of `sale_price`\n- This doesn't seem promising by itself\n\n```{r}\n  \ndata_trn |> \n  mutate(sale_price = log(sale_price)) |> \n  plot_scatter(\"gr_liv_area\", \"sale_price\")\n\ndata_trn |> \n  mutate(sale_price = log(sale_price)) |>\n  plot_scatter(\"year_built\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\nCan make scatterplots for ordinal variables as well\n\n- But other (perhaps better) options also exist for this combination of variable classes.\n- Use `as.numeric()` to allow for lm and LOWESS lines on otherwise categorical variable\n```{r}\ndata_trn |> \n  mutate(overall_qual = as.numeric(overall_qual)) |> \n  plot_scatter(\"overall_qual\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\n::: {.callout-tip}\n# Coding sidebar\nUse `jitter()` with x to help with overplotting\n\n```{r}\ndata_trn |> \n  mutate(overall_qual = jitter(as.numeric(overall_qual))) |> \n  plot_scatter(\"overall_qual\", \"sale_price\")\n```\n\n:::\n\n--------------------------------------------------------------------------------\n\n### Correlations & Correlation Plots for Numeric Variables (Bivariate)\n\nCorrelations are useful summary statistics for numeric variables\n\nSome statistical algorithms are sensitive to high correlations among features (multi-collinearity)\n\nAt best, highly correlated features add unnecessary flexibility and can lead to overfitting\n\n--------------------------------------------------------------------------------\n\nWe can visualize correlations among predictors/features using `corrplot.mixed()` from `corrplot` package\n\n- Best for numeric variables\n- Can include ordinal or two level nominal variables if transformed to numeric\n- Can include nominal variables with > 2 levels if first transformed appropriately (e.g., dummy features, not demonstrated yet)\n- Works best with relatively small set of variables\n\n--------------------------------------------------------------------------------\n\n```{r}\ndata_trn |> \n  mutate(overall_qual = as.numeric(overall_qual),\n         garage_qual = as.numeric(garage_qual)) |> \n  select(where(is.numeric)) |> \n  cor(use = \"pairwise.complete.obs\") |> \n  corrplot::corrplot.mixed() # <1>\n```\n1. Note use of namespace (`corrplot::corrplot.mixed()`) to call this function from `corrplot` package\n\n--------------------------------------------------------------------------------\n\n### Grouped Box + Violin Plots for Categorical and Numeric (Bivariate)\n\nA grouped version of the combined box and violin plot is our preferred visualization for relationship between categorical and numeric variables (included in `fun_plots.r`)\n\n- Often best when feature is categorical and outcome is numeric but can reverse\n- Can use with both nominal and ordinal categorical variable\n- @RDS also describes use of grouped frequency polygons for this combination of variable classes\n\n\n```{r}\n#| eval: false\n\nplot_grouped_box_violin <- function(df, x, y){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\nHere is the relationship between `overall_qual` and `sale_price`\n\n- Tend to prefer this over the scatterplot (with `as.numeric()`) for ordinal variables\n- Increasing spread of `sale_price` at higher levels of `overall_qual` is clearer in this plot\n\n```{r}\ndata_trn |> plot_grouped_box_violin(\"overall_qual\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\nHere is a grouped box + violin with a nominal variable\n\n- More variation and skew in `sale_price` for one family homes (additional features, moderators?)\n- Position of townhouse (interior vs. exterior) seems to matter (don't collapse?)\n\n```{r}\ndata_trn |> plot_grouped_box_violin(\"bldg_type\", \"sale_price\")\n```\n\n--------------------------------------------------------------------------------\n\nWhen we have a categorical predictor and a numeric outcome, we often want to see both the relationship between the variables AND the variability on the categorical variable alone.   \n\nWe like this combined plot enough when doing EDA to provide a specific function (included in `fun_plots.r`)!  \n\nIt is our go to for understanding the potential effect of a categorical predictor\n\n```{r}\n#| eval: false\n\nplot_categorical <- function(df, x, y, ordered = FALSE){\n  if (ordered) {\n    df <- df |>\n      mutate(!!x := fct_reorder(.data[[x]], .data[[y]]))\n  }\n  \n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  p_bar <- df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_bar()  +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n  \n  p_box <- df |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n  \n  return(list(p_bar, p_box))\n}\n```\n\n--------------------------------------------------------------------------------\n\n`sale_price` by `bldg_type`\n\n```{r}\ndata_trn |> plot_categorical(\"bldg_type\", \"sale_price\") |> \n  plot_grid(plotlist = _, ncol = 1)\n```\n\n--------------------------------------------------------------------------------\n\n### Stacked Barplots for Categorical (Bivariate)\n\nStacked Barplots:\n\n- Can be useful with both nominal and ordinal variables\n- Can create with either raw counts or percentages.  \n  - Displays different perspective (particularly with uneven distributions across levels)\n  - Depends on your question\n- Often, you will place the outcome on the x-axis and the feature is coded by fill\n\n```{r}\n#| eval: false\n\nplot_grouped_barplot_count <- function(df, x, y){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[y]], fill = .data[[x]])) +\n    geom_bar(position = \"stack\") +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\nplot_grouped_barplot_percent <- function(df, x, y){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[y]], fill = .data[[x]])) +\n    geom_bar(position = \"fill\") +\n    labs(y = \"Proportion\") +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n```\n\n--------------------------------------------------------------------------------\n\nFor example, if we wanted to learn about how `bldg_type` varies by `lot_config`, see these plots\n\n```{r}\ndata_trn |> plot_grouped_barplot_count(\"lot_config\", \"bldg_type\")\n```\n\n```{r}\ndata_trn |> plot_grouped_barplot_percent(\"lot_config\", \"bldg_type\")\n```\n\n--------------------------------------------------------------------------------\n\nMay want to plot both ways\n\n```{r}\ndata_trn |> plot_grouped_barplot_percent(\"lot_config\", \"bldg_type\")\n```\n\n```{r}\ndata_trn |> plot_grouped_barplot_percent(\"bldg_type\", \"lot_config\")\n```\n\n--------------------------------------------------------------------------------\n\n### Tile Plot for Ordinal Categorical (Bivariate)\n\nTile plots may be useful if both categorical variables are ordinal \n\n```{r}\n#| eval: false\n  \nplot_tile <- function(df, x, y){\n  df |>\n    count(.data[[x]], .data[[y]]) |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_tile(mapping = aes(fill = n))\n}\n```\n\n--------------------------------------------------------------------------------\n\n```{r}\ndata_trn |> plot_tile(\"overall_qual\", \"garage_qual\")\n```\n\n--------------------------------------------------------------------------------\n\nYou might also consider a scatterplot with jitter in this instance\n```{r}\ndata_trn |> \n  mutate(overall_qual = jitter(as.numeric(overall_qual)),\n         garage_qual = as.numeric(garage_qual)) |> \n  plot_scatter(\"overall_qual\", \"garage_qual\")\n```\n\n--------------------------------------------------------------------------------\n\n### Two-way Tables for Categorical Variables (Bivariate)\n\nTwo-way tables are sometimes a useful summary statistic for two categorical variables.  We can use a 2-variable form of the tab function, `tab2()`, from my function scripts for this\n\nFor example, the relationship between `bldg_type` and `lot_config`\n```{r}\ndata_trn |> tab2(bldg_type, lot_config)\n```\n\n--------------------------------------------------------------------------------\n\n## Working with Recipes\n\nRecipes are used for feature engineering in `tidymodels` using the [`recipes`](https://recipes.tidymodels.org/) package\n\n- Used for transforming raw predictors into features used in our models\n- Describes all steps to make feature matrix.  For example:\n  - Transforming factors into \"dummy\" features if needed\n  - Linear and non-linear transformations (e.g., log, box-cox)\n  - Polynomials and interactions (i.e., `x1 * x1` or `x1 * x2`)\n  - Missing value imputations\n- Proper use of recipes is an important tool to prevent data leakage between train and either validation or test.\n- Recipes use only information from the training set in all feature engineering\n- Consider example of standardizing `x1` for a feature in train vs. validation and test.  Must use mean and sd from TRAIN to standardize `x1` in train, validate, and test.  VERY IMPORTANT.\n\n--------------------------------------------------------------------------------\n\nWe use recipes in a two step process - `prep()` and `bake()`\n  \n- \"Prepping\" a recipe involves calculating any statistics needed for the transformations that will be applied to engineer features (e.g., mean and standard deviation to normalize a numeric variable).\n  - Prepping is done with the `prep()` function.\n  - Prepping is **always** done only with training data.  A \"prepped\" recipe does not derive any statistics from validation or test sets.\n\n- \"Baking\" is the process of calculating the features\n  - Baking is done with the `bake()` function.\n  - We used our prepped recipe when we bake.\n  - Whereas we only prep a recipe with training data, we can use a prepped recipe to bake features from any dataset (training, validation, or test).\n\n--------------------------------------------------------------------------------\n\nWe will work with recipes extensively when model building starting in unit 3\n\nFor now, we will only use the recipe to indicate roles as a gentle introduction.\n\nWe will expand on this recipe in unit 3\n\nRecipe syntax is very similar to generic `tidyverse` syntax (created by same group)\n\n- Actually a subset of `tidyverse` functions\n- Less flexible/powerful but focused on our needs and easier to learn\n- You will eventually know both\n\n--------------------------------------------------------------------------------\n\nRecipes are used in Modeling scripts (which is a third type of script after cleaning and modeling EDA scripts)\n\n- Lets reload training again to pretend we are starting a new script\n\n```{r}\n data_trn <- read_csv(file.path(path_data, \"ames_clean_class_trn.csv\"), \n                      col_types = cols()) |> \n  class_ames() |> # <1> \n  glimpse()\n```\n1. Remember our function for classing! \n\n--------------------------------------------------------------------------------\n\nRecipes can be used to indicate the outcome and predictors that will be used in the model\n\n- Can use `.` to indicate all predictors\n  - Currently, our preferred method for larger data sets\n  - We can exclude some predictors later by changing their role, removing them with a later recipe step ($step\\_rm()$), or specifying a more precise formula when we fit the model\n  - See [Roles in Recipes](https://recipes.tidymodels.org/articles/Roles.html) for more info\n\n- Can use specific names of predictors along with $+$\n  - This is our preferred method when we have a smaller set of predictors (We will show you both approaches)\n  - e.g., `sale_price ~ lot_area + year_built + overall_qual`\n\n- Do NOT indicate interactions here\n  - All predictors are combined with `+`\n  - Interactions are specified by a later explicit feature engineering step\n  \n```{r}\nrec <- recipe(sale_price ~ ., data = data_trn)\n\nrec\n\nsummary(rec)\n```\n\n--------------------------------------------------------------------------------\n\n### Prepping and Baking a Recipe\n\nLet's make a feature matrix from the training set\n\nThere are two discrete (and important) steps\n\n - `prep()`\n - `bake()`\n\nFirst we prep the recipe using the training data\n\n```{r}\nrec_prep <- rec |> # <1>\n  prep(training = data_trn) #<2>\n```\n1. We start by prepping our raw/original recipe (`rec`)\n2. We use the `prep()` function on on the training data.  Recipes are ALWAYS prepped using training data.  This makes sure that are recipes will always only use information from the training set when making features for any subsequent dataset.  As part of the prepping process, `prep()` calculates and saves the features for the training data for later use\n\n--------------------------------------------------------------------------------\n\nSecond, we bake the training data using this prepped recipe to get a feature matrix from it.  \n\nWe set `new_data = NULL` because don't need features for new data.  Instead, we want features from the training data that were used to prep the recipe\n```{r}\nfeat_trn <- rec_prep |> \n  bake(new_data = NULL)\n```\n\n--------------------------------------------------------------------------------\n\nFinally, we should generally at least glimpse and/or skim (and typically do some more EDA) on our features to make sure our recipe is doing what we expect.\n\n- glimpse\n```{r}\nfeat_trn |> glimpse()\n```\n\n--------------------------------------------------------------------------------\n\n- skim\n\n```{r}\nfeat_trn |> skim_all()\n```\n\n--------------------------------------------------------------------------------\n\nWe can now use our features from training to train models, but that will take place in the next unit!\n\nWe could also use the prepped recipe to bake validation or test data to evaluate trained models.  That too will happen in the next unit!"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["book.css"],"output-file":"l02_exploratory_data_analysis.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","bibliography":["refs.bib"],"callout-icon":false,"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}