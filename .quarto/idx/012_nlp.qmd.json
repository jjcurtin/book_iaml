{"title":"Natural Language Processing: Text Processing and Feature Engineering","markdown":{"yaml":{"output":"html_document","editor_options":{"chunk_output_type":"console"}},"headingText":"Natural Language Processing: Text Processing and Feature Engineering","containsRefs":false,"markdown":"\n\n\n## Overview of Unit\n\n### Learning Objectives\n\n- Objective 1\n- Objective 2\n\n-----\n\n### Readings\n\n- @SMLTAR [Chapter 2: Tokenization](https://smltar.com/tokenization.html)\n- @SMLTAR [Chapter 5: Word Embeddings](https://smltar.com/embeddings.html)\n\nNOTES: Please read the above chapters more with an eye toward concepts and issues \nrather than code.  I will demonstrate a minimum set of functions to accomplish\nthe NLP modeling tasks for this unit.\n\nAlso know that the entire @SMLTAR [book] is really mandatory reading.  I would also strongly recommend this entire @TMR [book](https://www.tidytextmining.com/).  Both  will be important references at a minimum.\n\nPost questions to the readings channel in Slack\n\n-----\n\n### Lecture Videos\n\n- [Lecture 1: General Text (Pre-) Processing - the stringr package](https://mediaspace.wisc.edu/media/unit_12_lecture_1/1_mq7a05cw) ~ 9 mins\n- [Lecture 2: General Text (Pre-) Processing - regular expressions](https://mediaspace.wisc.edu/media/unit_12_lecture_2/1_jijno1uc) ~ 13 mins\n- [Lecture 3: The IMDB Reviews Dataset](https://mediaspace.wisc.edu/media/unit_12_lecture_3/1_33gjwtbv) ~ 6 mins\n- [Lecture 4: Tokenization- Part 1](https://mediaspace.wisc.edu/media/unit_12_lecture_4/1_ean14ejo) ~ 27 mins\n- [Lecture 5: Tokenization- Part 2](https://mediaspace.wisc.edu/media/unit_12_lecture_5/1_z3ywipyc) ~ 13 mins\n- [Lecture 6: Stopwords](https://mediaspace.wisc.edu/media/unit_12_lecture_6/1_flyszeb5) ~ 12 mins\n- [Lecture 7: Stemming](https://mediaspace.wisc.edu/media/unit_12_lecture_7/1_r8rfzgxy) ~12 mins\n- [Lecture 8: Bag of Words](https://mediaspace.wisc.edu/media/unit_12_lecture_8/1_g9hesmnb) ~19 mins\n- [Lecture 9: NLP in Action - Part 1](https://mediaspace.wisc.edu/media/unit_12_lecture_9/1_qvb4fkaj) ~ 17 mins\n- [Lecture 10: NLP in Action - Part 2](https://mediaspace.wisc.edu/media/unit_12_lecture_10/1_x7jcxtaf) ~ 20 mins\n\nPost questions to the video-lectures channel in Slack\n\n-----\n\n### Application Assignment\n\n- [data](application_assignments/unit_12/alcohol_tweets.csv)\n- [qmd shell](https://raw.githubusercontent.com/jjcurtin/book_iaml/main/application_assignments/unit_12/hw_unit_12_nlp.qmd)\n- [GloVE embeddings for twitter](https://dionysus.psych.wisc.edu/iaml/glove_twitter.csv)\n- [solution]()\n\n\n Submit the application assignment [here](https://canvas.wisc.edu/courses/395546/assignments/2187686) and complete the [unit quiz](https://canvas.wisc.edu/courses/395546/quizzes/514056) by 8 pm on Wednesday, April 17th\n \n\n-----\n\n## General Text (Pre-) Processing\n\nIn order to work with text, we need to be able to manipulate text.  We have two \nsets of tools to master:\n\n- The `stringr` [package](https://cran.r-project.org/web/packages/stringr/index.html)\n- Regular expressions (regex)\n\n### The stringr package\n\nThere are many functions in the `stringr` package that are very useful for searching\nand manipulating text.   \n\n- `stringr` is included in tidyverse\n- I recommend keeping the [stringr cheatsheet](pdfs/cheatsheet_strings.pdf) open whenever you are working with text until you learn these functions well.\n\n-----\n\nAll functions in `stringr` start with `str_` and take a vector of strings as the first argument.\n\\\n\\\nHere is a simple vector of strings to use as an example\n```{r}\nlibrary(tidyverse)\n\nx <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\n```\n\n- Length of each string\n```{r}\nstr_length(x)\n```\n\n- Collapse all strings in vector into one long string that is comma separated\n\n```{r}\nstr_c(x, collapse = \", \")\n```\n\n- Get substring based on position (start and end position)\n```{r}\nstr_sub(x, 1, 2)\n```\n\n-----\n\nMost `stringr` functions work with regular expressions, a concise language for describing patterns of text. \n\\\n\\\nFor example, the regular expression \"[aeiou]\" matches any single character that is a vowel.  \n\n- Here we use `str_subset()` to return the strings that contain vowels (doesnt include \"why\")\n\n```{r}\nstr_subset(x, \"[aeiou]\")\n```\n\n- Here we count the vowels in each string\n```{r}\nstr_count(x, \"[aeiou]\") \n```\n\n-----\n\nThere are eight main verbs that work with patterns:\n\n1 `str_detect(x, pattern)` tells you if there is any match to the pattern in each string\n\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_detect(x, \"[aeiou]\")\n```\n\n-----\n\n2 `str_count(x, pattern)` counts the number of patterns\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_count(x, \"[aeiou]\")\n```\n\n-----\n\n3 `str_subset(x, pattern)` extracts the matching components\n\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_subset(x, \"[aeiou]\")\n```\n\n-----\n\n4 `str_locate(x, pattern)` gives the position of the match\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_locate(x, \"[aeiou]\")\n```\n\n-----\n\n5 `str_extract(x, pattern)` extracts the text of the match\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_extract(x, \"[aeiou]\")\n```\n\n-----\n\n6 `str_match(x, pattern)` extracts parts of the match defined by parentheses. In this case, the characters on either side of the vowel \n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_match(x, \"(.)[aeiou](.)\")\n```\n\n-----\n\n7 `str_replace(x, pattern, replacement)` replaces the matches with new text\n\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_replace(x, \"[aeiou]\", \"?\")\n```\n\n-----\n\n8 `str_split(x, pattern)` splits up a string into multiple pieces.\n```{r}\nstr_split(c(\"a,b\", \"c,d,e\"), \",\")\n```\n\n-----\n\n###  Regular Expressions\n\nRegular expressions are a way to specify or search for patterns of strings using a sequence of characters. By combining a selection of simple patterns, we can capture quite complicated strings.\n\\\n\\\nThe `stringr` package uses regular expressions extensively\n\\\n\\\nThe regular expressions are passed as the pattern = argument. Regular expressions can be used to detect, locate, or extract parts of a string.\n\n-----\n\nJulia Silge has put together a wonderful [tutorial/primer on the use of regular expressions](https://smltar.com/regexp.html#literal-characters). After reading it, I finally had a solid grasp on them.  Rather than grab sections, I will direct you to it (and review it live in our filmed lectures).  She does it much better than I could!\n\\\n\\\nYou might consider installing the `RegExplain` [package](https://www.garrickadenbuie.com/project/regexplain/) using devtools if you want more support working with regular expressions.  They are powerful but they are complicated to learn initially\n\\\n\\\nThere is also a very helpful [cheatsheet](pdfs/cheatsheert_regex.pdf) for regular expressions\n\\\n\\\nAnd finally, there is a great @RDS chapter on [strings](https://r4ds.had.co.nz/strings.html) more generally, which covers both `stringr` and regex.\n\n-----\n\n## The IMDB Dataset\n\n```{r}\n#| include: false\n\n# set up environment.  Now hidden from view\n\noptions(conflicts.policy = \"depends.ok\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\ntidymodels_conflictRules()\n\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\nlibrary(xfun, include.only = \"cache_rds\")\n\ncl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))\ndoParallel::registerDoParallel(cl)\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\npath_data <- \"./data\"\n\nrerun_setting <- FALSE \n``` \n\nNow that we have a basic understanding of how to manipulation raw text, we can get set up for NLP and introduce a guiding example for this unit\n\nWe can start with our normal cast of characters RE packages, source, and settings (not displayed here)\n\nHowever, we will also install a few new ones that are specific to working with text.\n\n```{r}\nlibrary(tidytext)\nlibrary(textrecipes)  #step_- functions for NLP\nlibrary(SnowballC)  \nlibrary(stopwords)\n```\n\n-----\n\nThe IMDB Reviews dataset is a classic NLP dataset that is used for sentiment analysis\n\\\n\\\nIt contains: \n\n- 25,000 movie reviews in train and test\n- Balanced on positive and negative sentiment (labeled outcome)\n- For more info, see the [website](http://ai.stanford.edu/~amaas/data/sentiment/)\n\n-----\n\nLet's start by loading the dataset and adding an identifier for each review (i.e., **document**, `doc_num`)\n```{r}\ndata_trn <- read_csv(here::here(path_data, \"imdb_trn.csv\"), \n                     show_col_types = FALSE) |>\n  rowid_to_column(var = \"doc_num\") |> \n  mutate(sentiment = fct(sentiment, levels = c(\"neg\", \"pos\"))) \n\ndata_trn  |>\n  skim_some()\n```\n\n-----\n\nLet's look at our outcome\n\n```{r}\ndata_trn |> tab(sentiment)\n```\n\n-----\n\nTo get a better sense of the dataset, We can view first five negative reviews from the training set\n```{r}\ndata_trn |> \n  filter(sentiment == \"neg\") |> \n  slice(1:5) |> \n  pull(text) |> \n  print_kbl() \n```\n\n-----\n\nand the first five positive reviews from the training set\n```{r}\ndata_trn |> \n  filter(sentiment == \"pos\") |> \n  slice(1:5) |> \n  pull(text) |> \n  print_kbl()\n```\n\n-----\n\nYou need to spend a LOT of time reviewing the text before you begin to process it.\n\\\n\\\nI have NOT done this yet!\n\\\n\\\nMy models will be sub-optimal!\n\n-----\n\n## Tokens\n\nMachine learning algorithms cannot work with raw text (documents) directly\n\\\n\\\nWe must feature engineer these documents to allow them to serve as input to statistical algorithms\n\\\n\\\nThe first step for most NLP feature engineering methods is to represent text (documents) as \ntokens (words, ngrams)\n\\\n\\\nGiven that **tokenization** is often one of our first steps for extracting features from text, it is important to consider carefully what happens during this step and its implications for your subsequent modeling\n\n-----\n\nIn tokenization, we take input documents (text strings) and a token type (a meaningful \nunit of text, such as a word) and split the document into pieces (tokens) that correspond to the type\n\\\n\\\nWe can tokenize text into a variety of token types: \n\n- characters\n- words (most common - our focus;  unigrams)\n- sentences\n- lines\n- paragraphs\n- n-grams (bigrams, trigrams)\n\n-----\n\nAn n-gram consists of a sequence of n items from a given sequence of text. Most often, it is a group of n words (bigrams, trigrams)  \n\\\n\\\nn-grams retain word order which would otherwise be lost if we were just using words as the token type\n\\\n\\\n\"I am not happy\"\n\\\n\\\nTokenized by word, yields:\n\n- I\n- am\n- not\n- happy\n\n\\\n\\\nTokenized by 2-gram words:\n\n- I am\n- am not\n- not happy\n\n-----\n\nWe will be using tokenizer functions from the `tokenizers` package.  Three in particular are:\n\n- `tokenize_words(x, lowercase = TRUE, stopwords = NULL, strip_punct = TRUE, strip_numeric = FALSE, simplify = FALSE)`\n- `tokenize_ngrams(x, lowercase = TRUE, n = 3L, n_min = n, stopwords = character(), ngram_delim = \" \", simplify = FALSE)`\n- `tokenize_regex(x, pattern = \"\\\\s+\", simplify = FALSE)`\n\n-----\n\nHowever, we will be accessing these functions through wrappers: \n\n- `tidytext::unnest_tokens(tbl, output, input, token = \"words\", format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\"), to_lower = TRUE, drop = TRUE, collapse = NULL)` for tidyverse data exploration of tokens within tibbles\n- `textrecipes::step_tokenize()` for tokenization in our recipes\n\n-----\n\nWord-level tokenization by `tokenize_words()` is done by finding word boundaries as follows:\n\n- Break at the start and end of text, unless the text is empty\n- Do not break within CRLF (new line characters)\n- Otherwise, break before and after new lines (including CR and LF)\n- Do not break between most letters\n- Do not break letters across certain punctuation\n- Do not break within sequences of digits, or digits adjacent to letters (“3a,” or “A3”)\n- Do not break within sequences, such as “3.2” or “3,456.789.”\n- Do not break between Katakana\n- Do not break from extenders\n- Do not break within emoji zwj sequences\n- Do not break within emoji flag sequences\n- Ignore Format and Extend characters, except after sot, CR, LF, and new line\n- Keep horizontal whitespace together\n- Otherwise, break everywhere (including around ideographs, e.g., @, %, >)\n\n-----\n\nLet's start with using `tokenize_words()` to get a sense of how it works by default\n\n- Splits on spaces\n- Converts to lowercase by default (does it matter? MAYBE!!!)\n- Retains apostrophes but drops other punctuation (`,`, `.`, `!`) and some symbols (e.g., `-` `\\\\`,  `@`) by default.  Does not drop `_` (Do you need punctuation?  !!!!)\n- Retains numbers (by default) and decimals but drops `+` appended to `4+`\n- Has trouble with URLs and email address (do you need this?)\n- Often, these issues may NOT matter\n```{r}\n\"Here is a sample document to tokenize.  How EXCITING (I _love_ it).  Sarah has spent 4 or 4.1 or 4P or 4+ or >4 years developing her pre-processing and NLP skills.  You can learn more about tokenization here: https://smltar.com/tokenization.html or by emailing me at jjcurtin@wisc.edu\" |> \n  tokenizers::tokenize_words()\n```\n\n-----\n\nSome of these behaviors can be altered from their defaults\n\n- `lowercase = TRUE`\n- `strip_punc = TRUE`\n- `strip_numeric = FALSE`\n\n-----\n\nSome of these issues can be corrected by pre-processing the text\n\n\n```{r}\nstr_replace(\"jjcurtin@wisc.edu\", \"@\", \"_at_\")\n```\n\n-----\n\nIf you need finer control, you can use `tokenize_regex()` and then do further processing with `stringr` functions and regex\n\\\n\\\nNow it may be easier to build up from here (e.g., ):\n\n- `str_to_lower(word)`\n- `str_replace(word, \".$\", \"\")`\n```{r}\n\"Here is a sample document to tokenize.  How EXCITING (I _love_ it).  Sarah has spent 4 or 4.1 or 4P or 4+ years developing her pre-processing and NLP skills.  You can learn more about tokenization here: https://smltar.com/tokenization.html or by emailing me at jjcurtin@wisc.edu\" |> \n  tokenizers::tokenize_regex(pattern = \"\\\\s+\")\n```\n\n-----\n\nYou can explore the tokens that will be formed using `unnest_tokens()` and basic tidyverse data wrangling using a tidied format of your documents as part of your EDA\n\n- We unnest to 1 token (word) per row (tidy format)\n- We keep track of `doc_num` (added earlier)\n\n\\\n\\\nHere, we tokenize the IMDB training set.  \n\n- Using defaults\n- Can change other default for `tokenize_*()` by passing into function via `...`\n- Can set `drop = TRUE` (default) to discard the original document column (text)\n- Its pretty fast!\n\n```{r}\ntokens <- data_trn |> \n  unnest_tokens(word, text, token = \"words\", to_lower = TRUE, drop = FALSE) |> \n  glimpse()\n```\n\\\n\\\n**Coding sidebar:** You can take a much deeper dive into tidyverse text processing in [chapter 1](https://www.tidytextmining.com/tidytext.html) of @TMR.   \n\n-----\n\nLet's get oriented by reviewing the tokens from the first document\n\n- Raw form\n```{r}\ndata_trn$text[1]\n```\n\n- Tokenized by word and tidied\n```{r}\ntokens |> \n  filter(doc_num == 1) |> \n  select(word) |> \n  print(n = Inf)\n```\n\n-----\n\nConsidering all the tokens across all documents\n\n- There are almost 6 million words\n```{r}\nlength(tokens$word)\n```\n\n- The total unique *vocabulary* is around 85 thousand words\n\n```{r}\nlength(unique(tokens$word))\n```\n\n-----\n\nWord frequency is VERY skewed\n\n- These are the counts for the most frequent 750 words\n- there are 84,000 additional infrequent words in the right tail not shown here!\n```{r}\ntokens |>\n  count(word, sort = TRUE) |> \n  slice(1:750) |> \n  mutate(word = reorder(word, -n)) |>\n  ggplot(aes(word, n)) +\n    geom_col() +\n    xlab(\"Words\") +\n    ylab(\"Raw Count\") +\n    theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())\n```\n\n-----\n\nNow let's review the 100 most common words\n\n- We SHOULD review MUCH deeper than this\n- Some of our feature engineering approaches (e.g., BoW) will use the first 5K - 20K tokens\n- Some of our feature engineering approaches (e.g., word embeddings) may use ALL tokens\n- Some of these are likely not very informative (the, a, of, to, is).  We will return to those words in a bit when we consider **stopwords**\n- Notice `br`.   Why is it so common?\n```{r}\ntokens |>\n  count(word, sort = TRUE) |>\n  print(n = 100)\n```\n\n-----\n\nHere is the first document that has the `br` token in it. \n\\\n\\\nIt is html code for a line break.\n```{r}\ntokens |> \n  filter(word == \"br\") |> \n  slice(1) |> \n  pull(text)\n```\n\n-----\n\nLet's clean it in the raw documents and re-tokenize\n\nYou should always check your replacements CAREFULLY before doing them for unexpected matches and side-effects\n\n```{r}\ndata_trn <- data_trn |> \n  mutate(text = str_replace_all(text, \"<br /><br />\", \" \"))\n\ntokens <- data_trn |> \n  unnest_tokens(word, text, drop = FALSE)\n```\n\\\n\\\nWe should continue to review MUCH deeper into the common tokens to detect other\ntokenization errors.  I will not demonstrate that here.\n\n-----\n\nWe should also review the least common tokens\n\n- Worth searching to make sure they haven't resulted from tokenization errors\n- Can use this to tune our pre-processing and tokenization\n- BUT, not as important b/c these will be dropped by some of our feature engineering\napproaches (BoW) and may not present too much problem to others (embeddings)\n\n```{r}\ntokens |>\n  count(word) |> \n  arrange(n) |> \n  print(n = 200)\n```\n\n-----\n\nWhat is the deal with `_*_`?\n\n```{r}\ntokens |> \n  filter(word == \"_anything_\") |> \n  slice(1) |> \n  pull(text)\n```\n\n```{r}\ntokens |> \n  filter(word == \"_love_\") |> \n  slice(1) |> \n  pull(text)\n```\n\n-----\n\nLet's find all the tokens that start or end with `_`\n\\\n\\\nA few of these are even repeatedly used\n\n```{r}\ntokens |> \n  filter(str_detect(word, \"^_\") | str_detect(word, \"_$\")) |> \n  count(word, sort = TRUE) |>\n  print(n = Inf)\n```\n\n-----\n\nNow we can clean the raw documents again.  This works but there is probably a better regex using `^_` and `_$`\n\n```{r}\ndata_trn <- data_trn |> \n  mutate(text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n```\n\n-----\n\nLet's take another look and uncommon tokens\n\n```{r}\ndata_trn |> \n  unnest_tokens(word, text, drop = FALSE) |> \n  count(word) |> \n  arrange(n) |> \n  print(n = 200)\n```\n\nLots of numbers.  **Probably?** not that important for our classification problem.  \n\\\n\\\nLet's strip them for demonstration purposes at least using `strip_numeric = TRUE`\n\\\n\\\nThis is likey good for unigrams but wouldn't be good/possible for bigrams (break sequence)\n\n```{r}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE) |>   \n  count(word) |> \n  arrange(n) |> \n  print(n = 200)\n```\n\n-----\n\nThe tokenizer didn't get rid of numbers connected to text\n\n- How do we want to handle these?\n- Could add a space to make them two words? \n- We will leave them as is\n\n-----\n\nOther issues?\n\n- Mis-spellings\n- Repeated letters for emphasize or effect?\n- Did caps matter (default tokenization was to convert to lowercase)?\n- Domain knowledge is useful here though multiple model configurations can be considered\n- strings of words that aren't meaningful (\"Welcome to Facebook\")\n\n-----\n\nIn the above workflow, we:\n\n1. tokenize\n2. review tokens for issues\n3. clean raw text documents\n4. re-tokenize\n\n\\\n\\\nIn some instances, it may be easier to clean the token and then put them back together\n\n1. tokenize\n2. review tokens for issues\n3. clean tokens\n4. recreate document\n5. tokenize\n\n-----\n\nIf this latter workflow feels easier (i.e., easier to regex into a token than a document), \nwe will need code to put the tokens back together into a document\n\\\n\\\nHere is an example using the first three documents (`slice(1:3)1) and no cleaning\n\n\n1. Tokenize\n```{r}\ntokens <- \n  data_trn |>\n  slice(1:3) |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE)\n```\n\n2. Now we can do further cleaning with tokens\n\n`INSERT CLEANING CHUNKS HERE`\n\n3. Then put back together.  Could also collapse into `text_cln` to retain original text column\n```{r}\ndata_trn_cln <-\n  tokens |>  \n  group_by(doc_num, sentiment) |> \n  summarize(text = str_c(word, collapse = \" \"),\n            .groups = \"drop\") \n```\n\n-----\n\nLets see what we have\n\n- Now all lower case\n- No numbers, punctuation, etc.\n- and any other cleaning we **could** have done with the tokens along the way....\n```{r}\ndata_trn_cln |> \n  pull(text) |>\n  print_kbl() \n```\n\n-----\n\n## Stop words\n\nNot all words are equally informative or useful to our model depending on the \nnature of our problem\n\\\n\\\nVery common words often may carry little or no meaningful information\n\\\n\\\nThese words are called **stop words**\n\\\n\\\nIt is common advice and practice to remove stop words for various NLP tasks\n\\\n\\\nNotice some of the top most frequent words among our tokens from IMDB reviews\n```{r}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE) |> \n  count(word, sort = TRUE) |> \n  print(n = 100)\n```\n\n-----\n\nStop words can have different roles in a **corpus** (a set of documents)\n\\\n\\\nFor our purposes, we generally care about two different types of stop words:\n\n- Global\n- Subject-specific\n\n\n\\\n\\\nGlobal stop words almost always have very little value for our modeling goals\n\\\n\\\nThese are frequent words like \"the\", “of” and “and” in English.\n\\\n\\\nIt is typically pretty safe to remove these and you can find them in pre-made lists of stop words (see below)\n\n-----\n\nSubject-specific stop words are words that are common and uninformative given the subject or context within which your text/documents were collected and your modeling goals.  \n\\\n\\\nFor example, given our goal to classify movie reviews as positive or negative, subject-specific stop words might include:\n\n- movie\n- film\n- movies\n\n-----\n\nWe likely we see others if we expand our review of commons words a bit more (which we should!)\n\n- character\n- actor\n- actress\n- director\n- cast\n- scene\n\nThese are not general stop words but they will be common in this dataset and the **may* be uninformative RE our classification goal\n\n-----\n\nSubject-specific stop words may improve performance if you have the domain expertise to create a good list\n\\\n\\\nHOWEVER, you should think carefully about your goals and method.  For example, if you are using bigrams rather than single word (unigram) tokens, you might retain words like actor or director because them may be informative in bigrams\n\n- bad actor\n- great director\n\n\\\n\\\nThough it might be sufficient to just retain **bad** and **great** \n\n-----\n\nThe `stopwords` package contains many lists of stopwords.  \n\n- We can access these lists using through that package\n- Those lists are also available with `get_stopwords()` in the `tidytext` package (my preference)\n- `get_stopwords()` returns a tibble with two columns (see below)\n\n-----\n\nTwo commonly used stop word lists are:\n\n- snowball (175 words)\n```{r}\nstop_snowball <- \n  get_stopwords(source = \"snowball\") |> \n  print(n = 50)  # review the first 50 words\n```\n\n-----\n\n- smart (571 words)\n```{r}\nstop_smart <-\n  get_stopwords(source = \"smart\") |> \n  print(n = 50)  # review the first 50 words\n```\n\n-----\n\nsmart is mostly a super-set of snowball except for these words which are only in snowball\n\\\n\\\nStop word lists aren't perfect.  Why does smart contain `he's` but not `she's`?\n```{r}\nsetdiff(pull(stop_snowball, word),\n        pull(stop_smart, word))\n```\n\n-----\n\nIt is common and appropriate to start with a pre-made word list or set of lists and combine, add, and/or remove words based on your specific needs\n\n- You can add global words that you feel are missed\n- You can add subject specific words\n- You can remove global words that might be relevant to your problem\n\n-----\n\nIn the service of simplicity, we will use the union of the two previous pre-made \nglobal lists without any additional subject specific lists\n\n```{r}\nall_stops <- union(pull(stop_snowball, word), pull(stop_smart, word))\n```\n\n-----\n\nWe can remove stop words as part of tokenization using `stopwords = all_stops`\n\\\n\\\nLet's see our two 100 tokens now\n```{r}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE,\n                stopwords = all_stops) |> \n  count(word) |> \n  arrange(desc(n)) |> \n  print(n = 100)\n```\n\n-----\n\nWhat if we were doing bigrams instead?\n\n- token = \"ngrams\"\n- n = 2,\n- n_min = 2\n- NOTE: can't strip numeric (would break the sequence of words)\n- NOTE: There are more bigrams than unigrams (more features for BoW!)\n```{r}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                token = \"ngrams\",\n                stopwords = all_stops, \n                n = 2,\n                n_min = 2) |> \n  count(word) |> \n  arrange(desc(n)) |> \n  print(n = 100)\n```\n\\\n\\\nLooks like we are starting to get some signal\n\n-----\n\n## Stemming\n  \nDocuments often contain different versions of one base word\n\\\n\\\nWe refer to the common base as the stem\n\\\n\\\nOften, we may want to treat the different versions of the stem as the same token. \nThis can reduce the total number of tokens that we need to use for features later\nwhich can lead to a better performing model\n\\\n\\\nFor example, do we need to distinguish between movie vs. movies or actor vs. actors or should we collapse those pairs into a single token?\n\n-----\n\nThere are many different algorithms that can stem words for us (i.e., collapse multiple\nversions into the same base).  However, we will focus on only one here as an introduction to the concept and approach for stemming\n\\\n\\\nThis is the Porter method and a current implementation of it is available in the \nusing `wordStem()` in the `SnowballC` package\n\n-----\n\nThe goal of stemming is to reduce the dimensionality (size) of our vocabulary\n\\\n\\\nWhenever we can combine words that \"belong\" together with respect to our goal,\nwe may improve the performance of our model\n\\\n\\\nHowever, stemming is hard and it will also invariably combine words that shouldn't\nbe combined\n\\\n\\\nStemming is useful when it suceeds more than it fails or when it succees more with \nimportant words/tokens\n\n-----\n\nHere are examples of when it helps to reduce our vocabulary given our task\n\n```{r}\nwordStem(c(\"movie\", \"movies\"))\n\nwordStem(c(\"actor\", \"actors\", \"actress\", \"actresses\"))\n\nwordStem(c(\"wait\", \"waits\", \"waiting\", \"waited\"))\n\nwordStem(c(\"play\", \"plays\", \"played\", \"playing\"))\n```\n\n-----\n\nSometimes it works partially, likely with still some benefit\n\n```{r}\nwordStem(c(\"go\", \"gone\", \"going\"))\n\nwordStem(c(\"send\", \"sending\", \"sent\"))\n\nwordStem(c(\"fishing\", \"fished\", \"fisher\"))\n```\n\n-----\n\nBut it clearly makes salient errors too\n```{r}\nwordStem(c(\"university\", \"universal\", \"universe\"))\n\nwordStem(c(\"is\", \"are\", \"was\"))\n\nwordStem(c(\"he\", \"his\", \"him\"))\n\nwordStem(c(\"like\", \"liking\", \"likely\"))\n\nwordStem(c(\"mean\", \"meaning\"))\n```\n\n-----\n\nOf course, the errors are more important if they are with words that contain predictive signal\n\\\n\\\nTherefore, we should look at how it works with our text\n\\\n\\\nTo stem our tokens if we only care about unigrams:\n\n- We first tokenize as before (including removal of stop words)\n- Then we stem, for now using `wordStem()`\n- We are mutating the stemmed words into a new column, `stem`, so we can compare its effect\n\n```{r}\ntokens <- \n  data_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE,\n                stopwords = all_stops) |> \n  mutate(stem = wordStem(word)) |> \n  select(word, stem)\n```\n\n-----\n\nLet's compare vocabulary size\n\\  \n\\\nStemming produced a sizable reduction in vocabulary size\n\n```{r}\nlength(unique(tokens$word))\n\nlength(unique(tokens$stem))\n```\n\n-----\n\nLet's compare frequencies of the top 100 words vs. stems\n\n- Here are our normal (not stemmed words).  Notice vocabulary size\n```{r}\nword_tokens <-\n  tokens |> \n  tab(word) |> \n  arrange(desc(n)) |> \n  slice(1:100) |> \n  select(word, n_word = n)\n\nstem_tokens <-\n  tokens |> \n  tab(stem) |> \n  arrange(desc(n)) |> \n  slice(1:100) |> \n  select(stem, n_stem = n)\n\nword_tokens |> \n  bind_cols(stem_tokens)\n```\n\n-----\n\nStemming is often routinely used as part of an NLP pipeline.  \n\n- Often without thought\n- We should consider carefully if it will help or hurt\n- We should likely formally evaluate it (via model configurations), sometimes without much comment about when it is helpful or not. We encourage you to think of stemming as a pre-processing step in text modeling, one that must be thought through and chosen (or not) with good judgment.\n\n-----\n\nIn this example, we focused on unigrams.  \n\\\n\\\nIf we had wanted bigrams, we would have needed a different order of steps\n\n- Extract words\n- Stem\n- Put back together\n- Extract bigrams\n- Remove stop words\n\n\\\n\\\nThink carefully about what you are doing and what your goals are!\n\\\n\\\nYou can read more about stemming and related (more complicated but possibly more precise)\nprocedure called lemmazation in a [chapter](https://smltar.com/stemming.html#how-to-stem-text-in-r) from @SMLTAR\n\n-----\n\n## Bag of Words\n\nNow that we understand how to tokenize our documents, we can begin to consider how to feature engineer using these tokens\n\\\n\\\nThe **Bag-of-words (BoW)** method  \n\n- Provides one way of representing tokens within text data when modeling text with machine learning algorithms.\n- Is simple to understand and implement \n- Works well for problems such as document classification\n\n-----\n\nBoW is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n- A **vocabulary** of known \"words\" (I put words in quote because our tokens will sometimes be something other than a word)\n- A measure of the occurrence or frequency of these known words.\n\n\\\n\\\nIt is called a “bag” of words because information about the order or structure of words in the document is discarded. BoW is only concerned with occurrence or frequency of known words in the document, not where in the document they occur.\n\\\n\\\nBoW assumes that documents that contain the same content are similar and that we can learn something about the document by its content alone.\n\n-----\n\nBoW approaches vary on two primary characteristics:\n\n- What the token type is\n  - Word is most common\n  - Also common to use bigrams, combinations of unigrams (words) and bigrams\n  - Other options exist (e.g.,trigram)\n- How occurrence frequent of the word/token is measured\n  - Binary (presence or absence)\n  - Raw count\n  - Term frequency\n  - Term frequency - inverse document frequency (tf-idf)\n  - and other less common options\n  \n-----\n\nLets start with a very simple example\n\n- Two documents\n- Tokenization to words\n- Lowercase, strip punctuation, did not remove any stopwords, no stemming or lemmatization\n- Binary measurement (1 = yes, 0 = no)\n\n\n```{r}\n#| echo: false\n\ntibble::tribble(\n  ~Document, ~i, ~loved, ~that, ~movie, ~am, ~so, ~happy, ~was, ~not, ~good,\n  \"I loved that movie! I am so so so happy.\", 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n  \"That movie was not good.\", 0, 0, 1, 1, 0, 0, 0, 1, 1, 1) |>\n  print_kbl()\n```\n\n-----\n\nThis matrix is referred to as a **Document-Term Matrix (DTM)**\n\n- Rows are documents\n- Columns are terms (tokens)\n- Combination of all terms/tokens is called the **vocabulary**\n- The terms columns (or a subset) will be used as features in our statistical algorithm to predict some outcome (not displayed)\n\n-----\n\nYou will also see the use of **raw counts** for measurement of the cell value for each term\n\n```{r}\n#| echo: false\n\ntibble::tribble(\n  ~Document, ~i, ~loved, ~that, ~movie, ~am, ~so, ~happy, ~was, ~not, ~good,\n  \"I loved that movie! I am so so so happy.\", 2, 1, 1, 1, 1, 3, 1, 0, 0, 0,\n  \"That movie was not good.\", 0, 0, 1, 1, 0, 0, 0, 1, 1, 1) |>\n  print_kbl()\n```\n\nBoth binary and raw count measures are biased (increased) for longer documents\n\n-----\n\nThe bias based on document length motivates the use of **term frequency**\n\n- Term frequency is NOT a simple frequency count (that is raw count)\n- Instead it is the raw count divided by the document length\n- This removes the bias based on document length\n\n```{r}\n## echo: false\n\ntibble::tribble(\n  ~Document, ~i, ~loved, ~that, ~movie, ~am, ~so, ~happy, ~was, ~not, ~good,\n  \"I loved that movie! I am so so so happy.\", .2, .1, .1, .1, .1, .3, .1, 0, 0, 0,\n  \"That movie was not good.\", 0, 0, .2, .2, 0, 0, 0, .2, .2, .2) |>\n  print_kbl()\n```\n\n-----\n\nTerm frequency can be dominated by frequently occurring words that may not be as important to understand the document as are rarer but more domain specific words\n\\\n\\\nThis was the motivation for removing stopwords but stopword removal may not be sufficient\n\\\n\\\n**Term Frequency - Inverse Document Frequency (tf-idf)** was developed to address this issue\n\\\n\\\nTF-IDF scales the term frequency by the inverse document frequency\n\n- Term frequency tells us that word is frequently used in the current document\n- IDF indexes how rare the word is across documents (higer IDF == more rare)\n\n\\\n\\\nThis emphasizes words used in specific documents that are not commonly used otherwise\n\n-----\n\n$IDF = log(\\frac{total\\:number\\:of\\:documents}{documents\\:containing\\:the\\:word})$\n\\\n\\\nThis results in larger values for words that arent used in many documents.  e.g., \n\n- for a word that appears in only 1 of 1000 documents, idf = log(1000/1) = 3\n- for a word that appears in 1000 of 1000 documents, idf = log(1000/1000) = 0\n\n\\\n\\\nNote that word that appears in no documents would result in a division by zero.  Therefore it is common to add 1 to the denominator of idf\n\n-----\n\nWe should be aware of some of the limitations of BoW:\n\n- Vocabulary: The vocabulary requires careful thought  \n  - Choice of stop words (global and subject specific) can matter\n  - Choice about size (number of tokens) can matter\n\n- Sparsity: \n  - Sparse representations (features that contain mostly zeros) are hard to model for computational reasons (space and time)\n  - Sparse representations present a challenge to extract signal in a large representational space\n  \n- Meaning: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). \n  - Context and meaning can offer a lot to the model\n  - “this is interesting” vs “is this interesting”\n  - “old bike” vs “used bike”\n\n```{r}\n#| include: false\nrm(tokens)\n```\n\n-----\n\n## Bringing it all together\n\n### General\n\nWe will now explore a series of model configurations to predict the sentiment (positive vs negative) of IMDB.com reviews\n\n- To keep things simple, all model configurations will use only one statistical algorithm - glmnet\n  - Within algorithm, configurations will differ by `penatly` and `dials::mixture' - see grid below\n- We will consider BoW features derived by TF-IDF only\n- We will remove global stop words from some configurations\n- We will stem words from some configurations\n- These BoW configurations will also differ by token type\n    - Word/unigram\n    - A combination of uni- and bigrams\n    - Prior to casting the tf-idf document-term matrix, we will filter tokens to various sizes - see grid below\n- We will also train a word embedding model to demonstrate how to implement that feature\nengineering technique in tidymodels\n\n-----\n\nWe are applying these feature engineering steps blindly.  YOU should not.\n\\\n\\\nYou will want to explore the impact of your feature engineering either\n\n  - During EDA with unnest_tokens()\n  - After making your recipe by using it to make a feature matrix\n  - Likely some of both\n  \n-----\n\nLets start fresh with our training data\n\n```{r}\ndata_trn <- read_csv(here::here(path_data, \"imdb_trn.csv\"),\n                     show_col_types = FALSE) |> \n  rowid_to_column(var = \"doc_num\") |> \n  mutate(sentiment = factor(sentiment, levels = c(\"neg\", \"pos\"))) |> \n  glimpse()\n```\n\n-----\n\nAnd do our (very minimal) cleaning\n```{r}\ndata_trn <- data_trn |> \n  mutate(text = str_replace_all(text, \"<br /><br />\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n```\n\n-----\n\nWe will select among model configurations using a validation split resampled accuracy to ease computational costs\n\n```{r}\nset.seed(123456)\nsplits <- data_trn |> \n  validation_split(strata = sentiment)\n```\n\n-----\n\nWe will use a simple union of stop words for some configurations:\n\n- It **could** help to edit the global list (or use a smaller list like snowball only)\n- It **could** help to have subject specific stop words too AND they might need to be different for words vs. bigrams\n```{r}\nall_stops <- union(pull(get_stopwords(source = \"smart\"), word), pull(get_stopwords(source = \"snowball\"), word))\n```\n\n-----\n\nAll of our model configurations will be tuned on `penalty`, `mixture` and `max_tokens`\n```{r}\ngrid_tokens <- grid_regular(penalty(range = c(-7, 3)), \n                            mixture(), \n                            max_tokens(range = c(5000, 10000)), \n                            levels = c(20, 6, 2))\n```\n\n### Words (unigrams)\n\nWe will start by fitting a BoW model configuration for word tokens\n\\\n\\\nRecipe for Word Tokens.  NOTE:\n\n- `token = \"words\"`  (default)\n- `max_tokens = tune()`  - We are now set to tune recipes!!!\n  `options = list(stopwords = all_stops)` - Passing in options to `tokenizers::tokenize_words()`\n```{r}\nrec_word <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\") |>    \n  step_tokenfilter(text, max_tokens = tune::tune()) |>\n  step_tfidf(text)  |> \n  step_normalize(all_predictors())\n```\n\n-----\n\nTuning hyperparameters for Word Tokens\n```{r}\nfits_word <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), \n                 mixture = tune::tune()) |> \n      set_engine(\"glmnet\") |> \n      tune_grid(preprocessor = rec_word, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_word\")\n```\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n```{r}\nautoplot(fits_word)\n```\n\n-----\n\nDisplay performance of best configuration for Word Tokens.  \n\\\n\\\nWow, pretty good!\n```{r}\nshow_best(fits_word)\n```\n\n-----\n\n### Words (unigrams) Excluding Stop Words\n\nLets try a configuration that removes stop words\n\\\n\\\nRecipe for Word Tokens excluding Stop Words.  NOTE:\n\n- `options = list(stopwords = all_stops)` - Passing in options to `tokenizers::tokenize_words()`\n```{r}\nrec_word_nsw <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\",   # included for clarity\n                options = list(stopwords = all_stops)) |>\n  step_tokenfilter(text, max_tokens = tune::tune()) |>\n  step_tfidf(text)  |> \n  step_normalize(all_predictors())\n```\n\n-----\n\nTuning hyperparameters for Word Tokens excluding Stop Words\n```{r}\nfits_word_nsw <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), \n                 mixture = tune::tune()) |> \n      set_engine(\"glmnet\") |> \n      tune_grid(preprocessor = rec_word_nsw, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_word_nws\")\n```\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n```{r}\nautoplot(fits_word_nsw)\n```\n\n-----\n\nDisplay performance of best configuration for Word Tokens excluding Stop Words.  \n\n- Looks like our mindless use of a large list of global stop words didn't help.   \n- We might consider the more focused snowball list\n- We might consider subject specific stop words\n- For this demonstration, we will just move forward without stop words\n  \n```{r}\nshow_best(fits_word_nsw)\n```\n\n\n-----\n\n### Stemmed Words (unigrams)\n\nNow we will try using stemmed words\n\\\n\\\nRecipe for Stemmed Word Tokens.  NOTE: `step_stem()`\n```{r}\nrec_stemmed_word <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\") |>    # included for clarity\n  step_stem(text) |> \n  step_tokenfilter(text, max_tokens = tune()) |>\n  step_tfidf(text)  |> \n  step_normalize(all_predictors())\n```\n\n-----\n\nTuning hyperparameters for Stemmed Word Tokens\n```{r}\n\nfits_stemmed_word <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), mixture = tune()) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_stemmed_word, \n              resamples = splits, \n              grid = grid_tokens, \n              metrics = metric_set(accuracy))\n},\n  rerun = rerun_setting, \n  dir = \"cache/012/\",\n  file = \"fits_stemmed_word\")\n```\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n```{r}\nautoplot(fits_stemmed_word)\n```\n\n-----\n\nDisplay performance of best configuration for Stemmed Word Tokens\n\\\n\\\nNot much change\n```{r}\nshow_best(fits_stemmed_word)\n```\n\n-----\n\n### ngrams (unigrams and bigrams)\n\nNow we try both unigrams and bigrams\n\\\n\\\nRecipe for unigrams and bigrams. NOTES:\n\n  - `token = \"ngrams\"`\n  - `options = list(n = 2, n_min = 1)`  - includes uni (1) and bi(2) grams\n  - not stemmed (stemming doesn't work by default for bigrams)\n```{r}\nrec_ngrams <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"ngrams\",\n                options = list(n = 2, n_min = 1)) |>\n  step_tokenfilter(text, max_tokens =  tune::tune()) |>\n  step_tfidf(text) |> \n  step_normalize(all_predictors())\n```\n\n-----\n\nTuning hyperparameters for ngrams\n```{r}\n\nfits_ngrams <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), mixture = tune::tune()) |> \n      set_engine(\"glmnet\") |> \n      tune_grid(preprocessor = rec_ngrams, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(yardstick::accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_ngrams\")\n```\n\n------\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n```{r}\nautoplot(fits_ngrams)\n```\n\n-----\n\nDisplay performance of best configuration\n\\\n\\\nOur best model yet!\n\n```{r}\nshow_best(fits_ngrams)\n```\n\n\n-----\n\n### Word Embeddings\n\nBoW is an introductory approach for feature engineering.  \n\\\n\\\nAs you have read, word embeddings are a common alternative that addresses some of the limitations of BoW.  Word embeddings are also well-support in the `tidyrecipes` package.\n\\\n\\\nLet's switch gears away from document term matrices and BoW to word embeddings\n\n-----\n\nYou can find pre-trained word embeddings on the web\n\n- [GloVe](https://nlp.stanford.edu/projects/glove/)\n- [Word2Vec](https://code.google.com/archive/p/word2vec/)\n- [Fasttek](https://fasttext.cc/docs/en/english-vectors.html)\n\nBelow, we download and open pre-trained GloVe embeddings\n\n- I chose a smaller set of embeddings to ease computational cost\n  - Wikipedia 2014 + Gigaword 5\n  - 6B tokens, 400K vocab, uncased, 50d\n\n```{r}\ntemp <- tempfile()\noptions(timeout = max(300, getOption(\"timeout\")))  # need more time to download big file\ndownload.file(\"https://nlp.stanford.edu/data/glove.6B.zip\", temp)\nunzip(temp, files = \"glove.6B.50d.txt\")\nglove_embeddings <- read_delim(here::here(\"glove.6B.50d.txt\"),\n                               delim = \" \",\n                               col_names = FALSE) \n```\n\n```{r}\n#| include: false\n\n# unlink(temp) # remove downloaded zip \n# unlink(\"glove.6B.50d.txt)\n```\n\n-----\n\nRecipe for GloVe embedding. NOTES:\n\n- `token = \"words\"`\n- No need to filter tokens\n- New step is `step_word_embeddings(text, embeddings = glove_embeddings)`\n```{r}\n# rec_glove <-  recipe(sentiment ~ text, data = data_trn) |>\n#   step_tokenize(text, \n#                 engine = \"tokenizers\", token = \"words\",   # included for clarity\n#                 options = list(stopwords = all_stops)) |>\n#   step_word_embeddings(text, embeddings = glove_embeddings)\n```\n\n-----\n\nHyperparameter grid for GloVe embedding (no need for `max_tokens`)\n```{r}\n# grid_glove <- grid_regular(penalty(range = c(-7, 3)), \n#                             dials::mixture(), \n#                             levels = c(20, 6))\n```\n\n-----\n\nTuning hyperparameters for GloVe embedding\n```{r}\n# fits_glove <-\n#   logistic_reg(penalty = tune(), \n#              mixture = tune()) |> \n#   set_engine(\"glmnet\") |> \n#   tune_grid(preprocessor = rec_glove, \n#             resamples = splits, \n#             grid = grid_glove, \n#             metrics = metric_set(accuracy))\n```\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n```{r}\n# autoplot(fits_glove)\n```\n\n-----\n\nDisplay performance of best configuration for GloVe embedding\n```{r}\n# show_best(fits_glove)\n```\n\n-----\n\n### Best Configuration\n\n- Fit best model configuration to training set\n```{r}\nrec_final <-\n  recipe(sentiment ~ text, data = data_trn) |>\n    step_tokenize(text, \n                  engine = \"tokenizers\", token = \"ngrams\",\n                  options = list(n = 2, n_min = 1)) |>\n    step_tokenfilter(text, max_tokens = select_best(fits_ngrams)$max_tokens) |>\n    step_tfidf(text) |> \n    step_normalize(all_predictors()) \n```\n\n```{r}\nrec_final_prep <- rec_final |>\n  prep(data_trn)\nfeat_trn <- rec_final_prep |> \n  bake(NULL) \n```\n\n-----\n\n```{r}\nfit_final <-\n  logistic_reg(penalty = select_best(fits_ngrams)$penalty, \n             mixture = select_best(fits_ngrams)$mixture) |> \n  set_engine(\"glmnet\") |> \n  fit(sentiment ~ ., data = feat_trn)\n```\n\n-----\n\n-----\n\n- Open and clean test to make features\n```{r}\ndata_test <- read_csv(here::here(path_data, \"imdb_test.csv\"),\n                      show_col_types = FALSE) |> \n  rowid_to_column(var = \"doc_num\") |> \n  mutate(sentiment = factor(sentiment, levels = c(\"neg\", \"pos\"))) |> \n  glimpse()\n\ndata_test <- data_test |> \n  mutate(text = str_replace_all(text, \"<br /><br />\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n\nfeat_test <- rec_final_prep |> \n  bake(data_test)\n```\n\n-----\n\n- Predict into test\n\n```{r}\naccuracy_vec(feat_test$sentiment, predict(fit_final, feat_test)$.pred_class)\n```\n\n- Confusion matrix\n```{r}\ncm <- tibble(truth = feat_test$sentiment,\n             estimate = predict(fit_final, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\nautoplot(cm, type = \"heatmap\")\n```\n\n-----\n\nAnd lets end by calculating Permutation feature importance scores in test set using DALEX\n\n```{r}\nlibrary(DALEX, exclude= \"explain\")\nlibrary(DALEXtra)\n```\n\n-----\n\nWe are going to sample only a subset of the test set to keep the computational costs lower for this example.\n\n```{r}\nset.seed(12345)\nfeat_subtest <- feat_test |> \n  slice_sample(prop = .05) # 5% of data \n```\n\n-----\n\nNow we can get a df for the features (without the outcome) and a separate vector for the outcome.  \n```{r}\nx <- feat_subtest |> select(-sentiment)\n```\n\\\n\\\nFor outcome, we need to convert to 0/1 (if classification), and then pull the vector out of the dataframe\n\n```{r}\ny <- feat_subtest |> \n  mutate(sentiment = if_else(sentiment == \"pos\", 1, 0)) |> \n  pull(sentiment)\n```\n\n-----\n\nWe also need a specific predictor function that will work with the DALEX package\n\n```{r}\npredict_wrapper <- function(model, newdata) {\n  predict(model, newdata, type = \"prob\") |> \n    pull(.pred_pos)\n}\n```\n\n-----\n\nWe will also need an `explainer` object based on our model and data\n\n```{r}\nexplain_test <- explain_tidymodels(fit_final, # our model object \n                                   data = x, # df with features without outcome\n                                   y = y, # outcome vector\n                                   # our custom predictor function\n                                   predict_function = predict_wrapper)\n```\n\n-----\n\nFinally, we need to define a custom function for our performance metric as well\n\n```{r}\naccuracy_wrapper <- function(observed, predicted) {\n  observed <- fct(if_else(observed == 1, \"pos\", \"neg\"),\n                  levels = c(\"pos\", \"neg\"))\n  predicted <- fct(if_else(predicted > .5, \"pos\", \"neg\"), \n                   levels  = c(\"pos\", \"neg\"))\n  accuracy_vec(observed, predicted)\n}\n```\n\\\n\\\nWe are now ready to calculate feature importance metrics\n\n-----\n\nOnly doing 1 permutation for each feature to keep computational costs lower for this demonstration.  In real, life do more!\n\n```{r}\n#| label: permutations\n\nset.seed(123456)\nimp_permute <- cache_rds(\n  expr = {model_parts(explain_test, \n                      type = \"raw\", \n                      loss_function = accuracy_wrapper,\n                      B = 1)\n  },\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"imp_permute\"\n)\n```\n\n-----\n\nPlot top 30 in an informative display\n```{r}\nimp_permute |> \n  filter(variable != \"_full_model_\",\n         variable != \"_baseline_\") |> \n  mutate(variable = fct_reorder(variable, dropout_loss)) |> \n  slice_head(n = 30) |> \n  print()\n```\n\n```{r}\nimp_permute |> \n  filter(variable != \"_full_model_\",\n         variable != \"_baseline_\") |> \n  mutate(variable = fct_reorder(variable, dropout_loss, \n                                .desc = TRUE)) |> \n  slice_tail(n = 30) |> \n  print()\n```\n\n```{r}\n#| fig-height: 4\n\n# full_model <- imp_permute |>  \n#    filter(variable == \"_full_model_\")\n  \n# imp_permute |> \n#   filter(variable != \"_full_model_\",\n#          variable != \"_baseline_\") |> \n#   mutate(variable = fct_reorder(variable, dropout_loss)) |> \n#   arrange(desc(dropout_loss) |>\n#   slice(n = 30) |> \n#   ggplot(aes(dropout_loss, variable)) +\n#     geom_vline(data = full_model, aes(xintercept = dropout_loss),\n#                linewidth = 1.4, lty = 2, alpha = 0.7) +\n#    geom_boxplot(fill = \"#91CBD765\", alpha = 0.4) +\n#    theme(legend.position = \"none\") +\n#    labs(x = \"accuracy\", y = NULL)\n```\n\n\n\n# NLP Discussion\n\n## Announcements\n\n- Last application assignment done!\n- Remaining units\n  - Bring it together for applications\n  - Ethics and Algorithmic Fairness\n  - Reading and discussion only for both units.  Discussion requires you!\n  \n- Two weeks left!\n  - next tuesday (lab on NLP)\n  - next thursday (discussion on applications)\n  - following tuesday (discussion on ethics/fairness)\n  - following thursday (concepts final review; 50 minutes only)\n\n- Early start to final application assignment (assigned next thursday, April 25th) and due Wednesday, May 8th at 8pm\n- Concepts exam at lab time during finals period (Tuesday, May 7th, 11-12:15 in this room)\n\n-----\n\n## Single nominal variable\n\n- Our algorithms need features coded with numbers\n  - How did we generally do this?\n  - What about algorithms like random forest?\n  \n-----\n\n##  Single nominal variable Example\n\n- Current emotional state\n  - angry, afraid, sad, excited, happy, calm\n\n- How represented with one-hot?\n\n-----\n\n## What about document of words (e.g., sentence rather than single word)\n\n- I watched a scary movie last night and couldn't get to sleep because I was so afraid\n- I went out with my friends to a bar and slept poorly because I drank too much\n\n\\\n\\\n\n- How represented with bag of words?\n- binary, count, tf-idf\n\n\\\n\\\n\n- What are the problems with these approaches\n  - relationships between features or similarity between full vector across observations \n  - context/order\n  - dimensionality\n  - sparsity\n\n-----\n\n## N-grams vs. simple (1-gram) BOW\n\n- How different?\n\n\\\n\\\n\n- some context/order\n- but still no relationship/meaning or similarity\n- even higher dimesions\n\n-----\n\n## Linguistic Inquiry and Word Count (LIWC)\n\n- Tausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and computerized text analysis methods. Journal of Language and Social Psychology, 29(1), 24–54. https://doi.org/\n\n\\\n\\\n\n- Meaningful features? (based on domain expertise)\n- Lower dimensional\n- Very limited breadth\n\n\\\n\\\n- Can add domain specific dictionaries\n\n-----\n\n## Word Embeddings\n\n- Encodes word meaning\n- Words that are similar get similar vectors\n- Meaning derived from context in various text corpora\n- Lower dimensional\n- Less sparse\n\n-----\n\n## Examples\n\n\n- Affect Words\n\n![](figs/nlp_1.PNG){height=5in}\n\n-----\n\n![](figs/nlp_2.PNG){height=5in}\n\n-----\n \n![](figs/nlp_3.PNG){height=5in}\n\n\n\n- Two more general\n\n- word2vec (google)\n  - CBOW\n  - skipgram\n\n![](figs/nlp_4.PNG){height=5in}\n\n-----\n\n![](figs/nlp_5.PNG){height=5in}\n\n-----\n\n![](figs/nlp_6.PNG){height=5in}\n\n\n-----\n\n## fasttext (facebook ai team)\n\n- n-grams (e.g., 3-6 character representations)\n- word vector is sum of its ngrams\n- can handle low frequecynor even novel wordsp\n\n## Other Methods\n\n- Other approaches\n  - glove\n  - elmo\n  - BERT\n:w\n","srcMarkdownNoYaml":"\n\n# Natural Language Processing: Text Processing and Feature Engineering\n\n## Overview of Unit\n\n### Learning Objectives\n\n- Objective 1\n- Objective 2\n\n-----\n\n### Readings\n\n- @SMLTAR [Chapter 2: Tokenization](https://smltar.com/tokenization.html)\n- @SMLTAR [Chapter 5: Word Embeddings](https://smltar.com/embeddings.html)\n\nNOTES: Please read the above chapters more with an eye toward concepts and issues \nrather than code.  I will demonstrate a minimum set of functions to accomplish\nthe NLP modeling tasks for this unit.\n\nAlso know that the entire @SMLTAR [book] is really mandatory reading.  I would also strongly recommend this entire @TMR [book](https://www.tidytextmining.com/).  Both  will be important references at a minimum.\n\nPost questions to the readings channel in Slack\n\n-----\n\n### Lecture Videos\n\n- [Lecture 1: General Text (Pre-) Processing - the stringr package](https://mediaspace.wisc.edu/media/unit_12_lecture_1/1_mq7a05cw) ~ 9 mins\n- [Lecture 2: General Text (Pre-) Processing - regular expressions](https://mediaspace.wisc.edu/media/unit_12_lecture_2/1_jijno1uc) ~ 13 mins\n- [Lecture 3: The IMDB Reviews Dataset](https://mediaspace.wisc.edu/media/unit_12_lecture_3/1_33gjwtbv) ~ 6 mins\n- [Lecture 4: Tokenization- Part 1](https://mediaspace.wisc.edu/media/unit_12_lecture_4/1_ean14ejo) ~ 27 mins\n- [Lecture 5: Tokenization- Part 2](https://mediaspace.wisc.edu/media/unit_12_lecture_5/1_z3ywipyc) ~ 13 mins\n- [Lecture 6: Stopwords](https://mediaspace.wisc.edu/media/unit_12_lecture_6/1_flyszeb5) ~ 12 mins\n- [Lecture 7: Stemming](https://mediaspace.wisc.edu/media/unit_12_lecture_7/1_r8rfzgxy) ~12 mins\n- [Lecture 8: Bag of Words](https://mediaspace.wisc.edu/media/unit_12_lecture_8/1_g9hesmnb) ~19 mins\n- [Lecture 9: NLP in Action - Part 1](https://mediaspace.wisc.edu/media/unit_12_lecture_9/1_qvb4fkaj) ~ 17 mins\n- [Lecture 10: NLP in Action - Part 2](https://mediaspace.wisc.edu/media/unit_12_lecture_10/1_x7jcxtaf) ~ 20 mins\n\nPost questions to the video-lectures channel in Slack\n\n-----\n\n### Application Assignment\n\n- [data](application_assignments/unit_12/alcohol_tweets.csv)\n- [qmd shell](https://raw.githubusercontent.com/jjcurtin/book_iaml/main/application_assignments/unit_12/hw_unit_12_nlp.qmd)\n- [GloVE embeddings for twitter](https://dionysus.psych.wisc.edu/iaml/glove_twitter.csv)\n- [solution]()\n\n\n Submit the application assignment [here](https://canvas.wisc.edu/courses/395546/assignments/2187686) and complete the [unit quiz](https://canvas.wisc.edu/courses/395546/quizzes/514056) by 8 pm on Wednesday, April 17th\n \n\n-----\n\n## General Text (Pre-) Processing\n\nIn order to work with text, we need to be able to manipulate text.  We have two \nsets of tools to master:\n\n- The `stringr` [package](https://cran.r-project.org/web/packages/stringr/index.html)\n- Regular expressions (regex)\n\n### The stringr package\n\nThere are many functions in the `stringr` package that are very useful for searching\nand manipulating text.   \n\n- `stringr` is included in tidyverse\n- I recommend keeping the [stringr cheatsheet](pdfs/cheatsheet_strings.pdf) open whenever you are working with text until you learn these functions well.\n\n-----\n\nAll functions in `stringr` start with `str_` and take a vector of strings as the first argument.\n\\\n\\\nHere is a simple vector of strings to use as an example\n```{r}\nlibrary(tidyverse)\n\nx <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\n```\n\n- Length of each string\n```{r}\nstr_length(x)\n```\n\n- Collapse all strings in vector into one long string that is comma separated\n\n```{r}\nstr_c(x, collapse = \", \")\n```\n\n- Get substring based on position (start and end position)\n```{r}\nstr_sub(x, 1, 2)\n```\n\n-----\n\nMost `stringr` functions work with regular expressions, a concise language for describing patterns of text. \n\\\n\\\nFor example, the regular expression \"[aeiou]\" matches any single character that is a vowel.  \n\n- Here we use `str_subset()` to return the strings that contain vowels (doesnt include \"why\")\n\n```{r}\nstr_subset(x, \"[aeiou]\")\n```\n\n- Here we count the vowels in each string\n```{r}\nstr_count(x, \"[aeiou]\") \n```\n\n-----\n\nThere are eight main verbs that work with patterns:\n\n1 `str_detect(x, pattern)` tells you if there is any match to the pattern in each string\n\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_detect(x, \"[aeiou]\")\n```\n\n-----\n\n2 `str_count(x, pattern)` counts the number of patterns\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_count(x, \"[aeiou]\")\n```\n\n-----\n\n3 `str_subset(x, pattern)` extracts the matching components\n\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_subset(x, \"[aeiou]\")\n```\n\n-----\n\n4 `str_locate(x, pattern)` gives the position of the match\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_locate(x, \"[aeiou]\")\n```\n\n-----\n\n5 `str_extract(x, pattern)` extracts the text of the match\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_extract(x, \"[aeiou]\")\n```\n\n-----\n\n6 `str_match(x, pattern)` extracts parts of the match defined by parentheses. In this case, the characters on either side of the vowel \n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_match(x, \"(.)[aeiou](.)\")\n```\n\n-----\n\n7 `str_replace(x, pattern, replacement)` replaces the matches with new text\n\n```{r}\n# x <- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\nstr_replace(x, \"[aeiou]\", \"?\")\n```\n\n-----\n\n8 `str_split(x, pattern)` splits up a string into multiple pieces.\n```{r}\nstr_split(c(\"a,b\", \"c,d,e\"), \",\")\n```\n\n-----\n\n###  Regular Expressions\n\nRegular expressions are a way to specify or search for patterns of strings using a sequence of characters. By combining a selection of simple patterns, we can capture quite complicated strings.\n\\\n\\\nThe `stringr` package uses regular expressions extensively\n\\\n\\\nThe regular expressions are passed as the pattern = argument. Regular expressions can be used to detect, locate, or extract parts of a string.\n\n-----\n\nJulia Silge has put together a wonderful [tutorial/primer on the use of regular expressions](https://smltar.com/regexp.html#literal-characters). After reading it, I finally had a solid grasp on them.  Rather than grab sections, I will direct you to it (and review it live in our filmed lectures).  She does it much better than I could!\n\\\n\\\nYou might consider installing the `RegExplain` [package](https://www.garrickadenbuie.com/project/regexplain/) using devtools if you want more support working with regular expressions.  They are powerful but they are complicated to learn initially\n\\\n\\\nThere is also a very helpful [cheatsheet](pdfs/cheatsheert_regex.pdf) for regular expressions\n\\\n\\\nAnd finally, there is a great @RDS chapter on [strings](https://r4ds.had.co.nz/strings.html) more generally, which covers both `stringr` and regex.\n\n-----\n\n## The IMDB Dataset\n\n```{r}\n#| include: false\n\n# set up environment.  Now hidden from view\n\noptions(conflicts.policy = \"depends.ok\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\ntidymodels_conflictRules()\n\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\nlibrary(xfun, include.only = \"cache_rds\")\n\ncl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))\ndoParallel::registerDoParallel(cl)\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\npath_data <- \"./data\"\n\nrerun_setting <- FALSE \n``` \n\nNow that we have a basic understanding of how to manipulation raw text, we can get set up for NLP and introduce a guiding example for this unit\n\nWe can start with our normal cast of characters RE packages, source, and settings (not displayed here)\n\nHowever, we will also install a few new ones that are specific to working with text.\n\n```{r}\nlibrary(tidytext)\nlibrary(textrecipes)  #step_- functions for NLP\nlibrary(SnowballC)  \nlibrary(stopwords)\n```\n\n-----\n\nThe IMDB Reviews dataset is a classic NLP dataset that is used for sentiment analysis\n\\\n\\\nIt contains: \n\n- 25,000 movie reviews in train and test\n- Balanced on positive and negative sentiment (labeled outcome)\n- For more info, see the [website](http://ai.stanford.edu/~amaas/data/sentiment/)\n\n-----\n\nLet's start by loading the dataset and adding an identifier for each review (i.e., **document**, `doc_num`)\n```{r}\ndata_trn <- read_csv(here::here(path_data, \"imdb_trn.csv\"), \n                     show_col_types = FALSE) |>\n  rowid_to_column(var = \"doc_num\") |> \n  mutate(sentiment = fct(sentiment, levels = c(\"neg\", \"pos\"))) \n\ndata_trn  |>\n  skim_some()\n```\n\n-----\n\nLet's look at our outcome\n\n```{r}\ndata_trn |> tab(sentiment)\n```\n\n-----\n\nTo get a better sense of the dataset, We can view first five negative reviews from the training set\n```{r}\ndata_trn |> \n  filter(sentiment == \"neg\") |> \n  slice(1:5) |> \n  pull(text) |> \n  print_kbl() \n```\n\n-----\n\nand the first five positive reviews from the training set\n```{r}\ndata_trn |> \n  filter(sentiment == \"pos\") |> \n  slice(1:5) |> \n  pull(text) |> \n  print_kbl()\n```\n\n-----\n\nYou need to spend a LOT of time reviewing the text before you begin to process it.\n\\\n\\\nI have NOT done this yet!\n\\\n\\\nMy models will be sub-optimal!\n\n-----\n\n## Tokens\n\nMachine learning algorithms cannot work with raw text (documents) directly\n\\\n\\\nWe must feature engineer these documents to allow them to serve as input to statistical algorithms\n\\\n\\\nThe first step for most NLP feature engineering methods is to represent text (documents) as \ntokens (words, ngrams)\n\\\n\\\nGiven that **tokenization** is often one of our first steps for extracting features from text, it is important to consider carefully what happens during this step and its implications for your subsequent modeling\n\n-----\n\nIn tokenization, we take input documents (text strings) and a token type (a meaningful \nunit of text, such as a word) and split the document into pieces (tokens) that correspond to the type\n\\\n\\\nWe can tokenize text into a variety of token types: \n\n- characters\n- words (most common - our focus;  unigrams)\n- sentences\n- lines\n- paragraphs\n- n-grams (bigrams, trigrams)\n\n-----\n\nAn n-gram consists of a sequence of n items from a given sequence of text. Most often, it is a group of n words (bigrams, trigrams)  \n\\\n\\\nn-grams retain word order which would otherwise be lost if we were just using words as the token type\n\\\n\\\n\"I am not happy\"\n\\\n\\\nTokenized by word, yields:\n\n- I\n- am\n- not\n- happy\n\n\\\n\\\nTokenized by 2-gram words:\n\n- I am\n- am not\n- not happy\n\n-----\n\nWe will be using tokenizer functions from the `tokenizers` package.  Three in particular are:\n\n- `tokenize_words(x, lowercase = TRUE, stopwords = NULL, strip_punct = TRUE, strip_numeric = FALSE, simplify = FALSE)`\n- `tokenize_ngrams(x, lowercase = TRUE, n = 3L, n_min = n, stopwords = character(), ngram_delim = \" \", simplify = FALSE)`\n- `tokenize_regex(x, pattern = \"\\\\s+\", simplify = FALSE)`\n\n-----\n\nHowever, we will be accessing these functions through wrappers: \n\n- `tidytext::unnest_tokens(tbl, output, input, token = \"words\", format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\"), to_lower = TRUE, drop = TRUE, collapse = NULL)` for tidyverse data exploration of tokens within tibbles\n- `textrecipes::step_tokenize()` for tokenization in our recipes\n\n-----\n\nWord-level tokenization by `tokenize_words()` is done by finding word boundaries as follows:\n\n- Break at the start and end of text, unless the text is empty\n- Do not break within CRLF (new line characters)\n- Otherwise, break before and after new lines (including CR and LF)\n- Do not break between most letters\n- Do not break letters across certain punctuation\n- Do not break within sequences of digits, or digits adjacent to letters (“3a,” or “A3”)\n- Do not break within sequences, such as “3.2” or “3,456.789.”\n- Do not break between Katakana\n- Do not break from extenders\n- Do not break within emoji zwj sequences\n- Do not break within emoji flag sequences\n- Ignore Format and Extend characters, except after sot, CR, LF, and new line\n- Keep horizontal whitespace together\n- Otherwise, break everywhere (including around ideographs, e.g., @, %, >)\n\n-----\n\nLet's start with using `tokenize_words()` to get a sense of how it works by default\n\n- Splits on spaces\n- Converts to lowercase by default (does it matter? MAYBE!!!)\n- Retains apostrophes but drops other punctuation (`,`, `.`, `!`) and some symbols (e.g., `-` `\\\\`,  `@`) by default.  Does not drop `_` (Do you need punctuation?  !!!!)\n- Retains numbers (by default) and decimals but drops `+` appended to `4+`\n- Has trouble with URLs and email address (do you need this?)\n- Often, these issues may NOT matter\n```{r}\n\"Here is a sample document to tokenize.  How EXCITING (I _love_ it).  Sarah has spent 4 or 4.1 or 4P or 4+ or >4 years developing her pre-processing and NLP skills.  You can learn more about tokenization here: https://smltar.com/tokenization.html or by emailing me at jjcurtin@wisc.edu\" |> \n  tokenizers::tokenize_words()\n```\n\n-----\n\nSome of these behaviors can be altered from their defaults\n\n- `lowercase = TRUE`\n- `strip_punc = TRUE`\n- `strip_numeric = FALSE`\n\n-----\n\nSome of these issues can be corrected by pre-processing the text\n\n\n```{r}\nstr_replace(\"jjcurtin@wisc.edu\", \"@\", \"_at_\")\n```\n\n-----\n\nIf you need finer control, you can use `tokenize_regex()` and then do further processing with `stringr` functions and regex\n\\\n\\\nNow it may be easier to build up from here (e.g., ):\n\n- `str_to_lower(word)`\n- `str_replace(word, \".$\", \"\")`\n```{r}\n\"Here is a sample document to tokenize.  How EXCITING (I _love_ it).  Sarah has spent 4 or 4.1 or 4P or 4+ years developing her pre-processing and NLP skills.  You can learn more about tokenization here: https://smltar.com/tokenization.html or by emailing me at jjcurtin@wisc.edu\" |> \n  tokenizers::tokenize_regex(pattern = \"\\\\s+\")\n```\n\n-----\n\nYou can explore the tokens that will be formed using `unnest_tokens()` and basic tidyverse data wrangling using a tidied format of your documents as part of your EDA\n\n- We unnest to 1 token (word) per row (tidy format)\n- We keep track of `doc_num` (added earlier)\n\n\\\n\\\nHere, we tokenize the IMDB training set.  \n\n- Using defaults\n- Can change other default for `tokenize_*()` by passing into function via `...`\n- Can set `drop = TRUE` (default) to discard the original document column (text)\n- Its pretty fast!\n\n```{r}\ntokens <- data_trn |> \n  unnest_tokens(word, text, token = \"words\", to_lower = TRUE, drop = FALSE) |> \n  glimpse()\n```\n\\\n\\\n**Coding sidebar:** You can take a much deeper dive into tidyverse text processing in [chapter 1](https://www.tidytextmining.com/tidytext.html) of @TMR.   \n\n-----\n\nLet's get oriented by reviewing the tokens from the first document\n\n- Raw form\n```{r}\ndata_trn$text[1]\n```\n\n- Tokenized by word and tidied\n```{r}\ntokens |> \n  filter(doc_num == 1) |> \n  select(word) |> \n  print(n = Inf)\n```\n\n-----\n\nConsidering all the tokens across all documents\n\n- There are almost 6 million words\n```{r}\nlength(tokens$word)\n```\n\n- The total unique *vocabulary* is around 85 thousand words\n\n```{r}\nlength(unique(tokens$word))\n```\n\n-----\n\nWord frequency is VERY skewed\n\n- These are the counts for the most frequent 750 words\n- there are 84,000 additional infrequent words in the right tail not shown here!\n```{r}\ntokens |>\n  count(word, sort = TRUE) |> \n  slice(1:750) |> \n  mutate(word = reorder(word, -n)) |>\n  ggplot(aes(word, n)) +\n    geom_col() +\n    xlab(\"Words\") +\n    ylab(\"Raw Count\") +\n    theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())\n```\n\n-----\n\nNow let's review the 100 most common words\n\n- We SHOULD review MUCH deeper than this\n- Some of our feature engineering approaches (e.g., BoW) will use the first 5K - 20K tokens\n- Some of our feature engineering approaches (e.g., word embeddings) may use ALL tokens\n- Some of these are likely not very informative (the, a, of, to, is).  We will return to those words in a bit when we consider **stopwords**\n- Notice `br`.   Why is it so common?\n```{r}\ntokens |>\n  count(word, sort = TRUE) |>\n  print(n = 100)\n```\n\n-----\n\nHere is the first document that has the `br` token in it. \n\\\n\\\nIt is html code for a line break.\n```{r}\ntokens |> \n  filter(word == \"br\") |> \n  slice(1) |> \n  pull(text)\n```\n\n-----\n\nLet's clean it in the raw documents and re-tokenize\n\nYou should always check your replacements CAREFULLY before doing them for unexpected matches and side-effects\n\n```{r}\ndata_trn <- data_trn |> \n  mutate(text = str_replace_all(text, \"<br /><br />\", \" \"))\n\ntokens <- data_trn |> \n  unnest_tokens(word, text, drop = FALSE)\n```\n\\\n\\\nWe should continue to review MUCH deeper into the common tokens to detect other\ntokenization errors.  I will not demonstrate that here.\n\n-----\n\nWe should also review the least common tokens\n\n- Worth searching to make sure they haven't resulted from tokenization errors\n- Can use this to tune our pre-processing and tokenization\n- BUT, not as important b/c these will be dropped by some of our feature engineering\napproaches (BoW) and may not present too much problem to others (embeddings)\n\n```{r}\ntokens |>\n  count(word) |> \n  arrange(n) |> \n  print(n = 200)\n```\n\n-----\n\nWhat is the deal with `_*_`?\n\n```{r}\ntokens |> \n  filter(word == \"_anything_\") |> \n  slice(1) |> \n  pull(text)\n```\n\n```{r}\ntokens |> \n  filter(word == \"_love_\") |> \n  slice(1) |> \n  pull(text)\n```\n\n-----\n\nLet's find all the tokens that start or end with `_`\n\\\n\\\nA few of these are even repeatedly used\n\n```{r}\ntokens |> \n  filter(str_detect(word, \"^_\") | str_detect(word, \"_$\")) |> \n  count(word, sort = TRUE) |>\n  print(n = Inf)\n```\n\n-----\n\nNow we can clean the raw documents again.  This works but there is probably a better regex using `^_` and `_$`\n\n```{r}\ndata_trn <- data_trn |> \n  mutate(text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n```\n\n-----\n\nLet's take another look and uncommon tokens\n\n```{r}\ndata_trn |> \n  unnest_tokens(word, text, drop = FALSE) |> \n  count(word) |> \n  arrange(n) |> \n  print(n = 200)\n```\n\nLots of numbers.  **Probably?** not that important for our classification problem.  \n\\\n\\\nLet's strip them for demonstration purposes at least using `strip_numeric = TRUE`\n\\\n\\\nThis is likey good for unigrams but wouldn't be good/possible for bigrams (break sequence)\n\n```{r}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE) |>   \n  count(word) |> \n  arrange(n) |> \n  print(n = 200)\n```\n\n-----\n\nThe tokenizer didn't get rid of numbers connected to text\n\n- How do we want to handle these?\n- Could add a space to make them two words? \n- We will leave them as is\n\n-----\n\nOther issues?\n\n- Mis-spellings\n- Repeated letters for emphasize or effect?\n- Did caps matter (default tokenization was to convert to lowercase)?\n- Domain knowledge is useful here though multiple model configurations can be considered\n- strings of words that aren't meaningful (\"Welcome to Facebook\")\n\n-----\n\nIn the above workflow, we:\n\n1. tokenize\n2. review tokens for issues\n3. clean raw text documents\n4. re-tokenize\n\n\\\n\\\nIn some instances, it may be easier to clean the token and then put them back together\n\n1. tokenize\n2. review tokens for issues\n3. clean tokens\n4. recreate document\n5. tokenize\n\n-----\n\nIf this latter workflow feels easier (i.e., easier to regex into a token than a document), \nwe will need code to put the tokens back together into a document\n\\\n\\\nHere is an example using the first three documents (`slice(1:3)1) and no cleaning\n\n\n1. Tokenize\n```{r}\ntokens <- \n  data_trn |>\n  slice(1:3) |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE)\n```\n\n2. Now we can do further cleaning with tokens\n\n`INSERT CLEANING CHUNKS HERE`\n\n3. Then put back together.  Could also collapse into `text_cln` to retain original text column\n```{r}\ndata_trn_cln <-\n  tokens |>  \n  group_by(doc_num, sentiment) |> \n  summarize(text = str_c(word, collapse = \" \"),\n            .groups = \"drop\") \n```\n\n-----\n\nLets see what we have\n\n- Now all lower case\n- No numbers, punctuation, etc.\n- and any other cleaning we **could** have done with the tokens along the way....\n```{r}\ndata_trn_cln |> \n  pull(text) |>\n  print_kbl() \n```\n\n-----\n\n## Stop words\n\nNot all words are equally informative or useful to our model depending on the \nnature of our problem\n\\\n\\\nVery common words often may carry little or no meaningful information\n\\\n\\\nThese words are called **stop words**\n\\\n\\\nIt is common advice and practice to remove stop words for various NLP tasks\n\\\n\\\nNotice some of the top most frequent words among our tokens from IMDB reviews\n```{r}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE) |> \n  count(word, sort = TRUE) |> \n  print(n = 100)\n```\n\n-----\n\nStop words can have different roles in a **corpus** (a set of documents)\n\\\n\\\nFor our purposes, we generally care about two different types of stop words:\n\n- Global\n- Subject-specific\n\n\n\\\n\\\nGlobal stop words almost always have very little value for our modeling goals\n\\\n\\\nThese are frequent words like \"the\", “of” and “and” in English.\n\\\n\\\nIt is typically pretty safe to remove these and you can find them in pre-made lists of stop words (see below)\n\n-----\n\nSubject-specific stop words are words that are common and uninformative given the subject or context within which your text/documents were collected and your modeling goals.  \n\\\n\\\nFor example, given our goal to classify movie reviews as positive or negative, subject-specific stop words might include:\n\n- movie\n- film\n- movies\n\n-----\n\nWe likely we see others if we expand our review of commons words a bit more (which we should!)\n\n- character\n- actor\n- actress\n- director\n- cast\n- scene\n\nThese are not general stop words but they will be common in this dataset and the **may* be uninformative RE our classification goal\n\n-----\n\nSubject-specific stop words may improve performance if you have the domain expertise to create a good list\n\\\n\\\nHOWEVER, you should think carefully about your goals and method.  For example, if you are using bigrams rather than single word (unigram) tokens, you might retain words like actor or director because them may be informative in bigrams\n\n- bad actor\n- great director\n\n\\\n\\\nThough it might be sufficient to just retain **bad** and **great** \n\n-----\n\nThe `stopwords` package contains many lists of stopwords.  \n\n- We can access these lists using through that package\n- Those lists are also available with `get_stopwords()` in the `tidytext` package (my preference)\n- `get_stopwords()` returns a tibble with two columns (see below)\n\n-----\n\nTwo commonly used stop word lists are:\n\n- snowball (175 words)\n```{r}\nstop_snowball <- \n  get_stopwords(source = \"snowball\") |> \n  print(n = 50)  # review the first 50 words\n```\n\n-----\n\n- smart (571 words)\n```{r}\nstop_smart <-\n  get_stopwords(source = \"smart\") |> \n  print(n = 50)  # review the first 50 words\n```\n\n-----\n\nsmart is mostly a super-set of snowball except for these words which are only in snowball\n\\\n\\\nStop word lists aren't perfect.  Why does smart contain `he's` but not `she's`?\n```{r}\nsetdiff(pull(stop_snowball, word),\n        pull(stop_smart, word))\n```\n\n-----\n\nIt is common and appropriate to start with a pre-made word list or set of lists and combine, add, and/or remove words based on your specific needs\n\n- You can add global words that you feel are missed\n- You can add subject specific words\n- You can remove global words that might be relevant to your problem\n\n-----\n\nIn the service of simplicity, we will use the union of the two previous pre-made \nglobal lists without any additional subject specific lists\n\n```{r}\nall_stops <- union(pull(stop_snowball, word), pull(stop_smart, word))\n```\n\n-----\n\nWe can remove stop words as part of tokenization using `stopwords = all_stops`\n\\\n\\\nLet's see our two 100 tokens now\n```{r}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE,\n                stopwords = all_stops) |> \n  count(word) |> \n  arrange(desc(n)) |> \n  print(n = 100)\n```\n\n-----\n\nWhat if we were doing bigrams instead?\n\n- token = \"ngrams\"\n- n = 2,\n- n_min = 2\n- NOTE: can't strip numeric (would break the sequence of words)\n- NOTE: There are more bigrams than unigrams (more features for BoW!)\n```{r}\ndata_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                token = \"ngrams\",\n                stopwords = all_stops, \n                n = 2,\n                n_min = 2) |> \n  count(word) |> \n  arrange(desc(n)) |> \n  print(n = 100)\n```\n\\\n\\\nLooks like we are starting to get some signal\n\n-----\n\n## Stemming\n  \nDocuments often contain different versions of one base word\n\\\n\\\nWe refer to the common base as the stem\n\\\n\\\nOften, we may want to treat the different versions of the stem as the same token. \nThis can reduce the total number of tokens that we need to use for features later\nwhich can lead to a better performing model\n\\\n\\\nFor example, do we need to distinguish between movie vs. movies or actor vs. actors or should we collapse those pairs into a single token?\n\n-----\n\nThere are many different algorithms that can stem words for us (i.e., collapse multiple\nversions into the same base).  However, we will focus on only one here as an introduction to the concept and approach for stemming\n\\\n\\\nThis is the Porter method and a current implementation of it is available in the \nusing `wordStem()` in the `SnowballC` package\n\n-----\n\nThe goal of stemming is to reduce the dimensionality (size) of our vocabulary\n\\\n\\\nWhenever we can combine words that \"belong\" together with respect to our goal,\nwe may improve the performance of our model\n\\\n\\\nHowever, stemming is hard and it will also invariably combine words that shouldn't\nbe combined\n\\\n\\\nStemming is useful when it suceeds more than it fails or when it succees more with \nimportant words/tokens\n\n-----\n\nHere are examples of when it helps to reduce our vocabulary given our task\n\n```{r}\nwordStem(c(\"movie\", \"movies\"))\n\nwordStem(c(\"actor\", \"actors\", \"actress\", \"actresses\"))\n\nwordStem(c(\"wait\", \"waits\", \"waiting\", \"waited\"))\n\nwordStem(c(\"play\", \"plays\", \"played\", \"playing\"))\n```\n\n-----\n\nSometimes it works partially, likely with still some benefit\n\n```{r}\nwordStem(c(\"go\", \"gone\", \"going\"))\n\nwordStem(c(\"send\", \"sending\", \"sent\"))\n\nwordStem(c(\"fishing\", \"fished\", \"fisher\"))\n```\n\n-----\n\nBut it clearly makes salient errors too\n```{r}\nwordStem(c(\"university\", \"universal\", \"universe\"))\n\nwordStem(c(\"is\", \"are\", \"was\"))\n\nwordStem(c(\"he\", \"his\", \"him\"))\n\nwordStem(c(\"like\", \"liking\", \"likely\"))\n\nwordStem(c(\"mean\", \"meaning\"))\n```\n\n-----\n\nOf course, the errors are more important if they are with words that contain predictive signal\n\\\n\\\nTherefore, we should look at how it works with our text\n\\\n\\\nTo stem our tokens if we only care about unigrams:\n\n- We first tokenize as before (including removal of stop words)\n- Then we stem, for now using `wordStem()`\n- We are mutating the stemmed words into a new column, `stem`, so we can compare its effect\n\n```{r}\ntokens <- \n  data_trn |> \n  unnest_tokens(word, text, \n                drop = FALSE,\n                strip_numeric = TRUE,\n                stopwords = all_stops) |> \n  mutate(stem = wordStem(word)) |> \n  select(word, stem)\n```\n\n-----\n\nLet's compare vocabulary size\n\\  \n\\\nStemming produced a sizable reduction in vocabulary size\n\n```{r}\nlength(unique(tokens$word))\n\nlength(unique(tokens$stem))\n```\n\n-----\n\nLet's compare frequencies of the top 100 words vs. stems\n\n- Here are our normal (not stemmed words).  Notice vocabulary size\n```{r}\nword_tokens <-\n  tokens |> \n  tab(word) |> \n  arrange(desc(n)) |> \n  slice(1:100) |> \n  select(word, n_word = n)\n\nstem_tokens <-\n  tokens |> \n  tab(stem) |> \n  arrange(desc(n)) |> \n  slice(1:100) |> \n  select(stem, n_stem = n)\n\nword_tokens |> \n  bind_cols(stem_tokens)\n```\n\n-----\n\nStemming is often routinely used as part of an NLP pipeline.  \n\n- Often without thought\n- We should consider carefully if it will help or hurt\n- We should likely formally evaluate it (via model configurations), sometimes without much comment about when it is helpful or not. We encourage you to think of stemming as a pre-processing step in text modeling, one that must be thought through and chosen (or not) with good judgment.\n\n-----\n\nIn this example, we focused on unigrams.  \n\\\n\\\nIf we had wanted bigrams, we would have needed a different order of steps\n\n- Extract words\n- Stem\n- Put back together\n- Extract bigrams\n- Remove stop words\n\n\\\n\\\nThink carefully about what you are doing and what your goals are!\n\\\n\\\nYou can read more about stemming and related (more complicated but possibly more precise)\nprocedure called lemmazation in a [chapter](https://smltar.com/stemming.html#how-to-stem-text-in-r) from @SMLTAR\n\n-----\n\n## Bag of Words\n\nNow that we understand how to tokenize our documents, we can begin to consider how to feature engineer using these tokens\n\\\n\\\nThe **Bag-of-words (BoW)** method  \n\n- Provides one way of representing tokens within text data when modeling text with machine learning algorithms.\n- Is simple to understand and implement \n- Works well for problems such as document classification\n\n-----\n\nBoW is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n- A **vocabulary** of known \"words\" (I put words in quote because our tokens will sometimes be something other than a word)\n- A measure of the occurrence or frequency of these known words.\n\n\\\n\\\nIt is called a “bag” of words because information about the order or structure of words in the document is discarded. BoW is only concerned with occurrence or frequency of known words in the document, not where in the document they occur.\n\\\n\\\nBoW assumes that documents that contain the same content are similar and that we can learn something about the document by its content alone.\n\n-----\n\nBoW approaches vary on two primary characteristics:\n\n- What the token type is\n  - Word is most common\n  - Also common to use bigrams, combinations of unigrams (words) and bigrams\n  - Other options exist (e.g.,trigram)\n- How occurrence frequent of the word/token is measured\n  - Binary (presence or absence)\n  - Raw count\n  - Term frequency\n  - Term frequency - inverse document frequency (tf-idf)\n  - and other less common options\n  \n-----\n\nLets start with a very simple example\n\n- Two documents\n- Tokenization to words\n- Lowercase, strip punctuation, did not remove any stopwords, no stemming or lemmatization\n- Binary measurement (1 = yes, 0 = no)\n\n\n```{r}\n#| echo: false\n\ntibble::tribble(\n  ~Document, ~i, ~loved, ~that, ~movie, ~am, ~so, ~happy, ~was, ~not, ~good,\n  \"I loved that movie! I am so so so happy.\", 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n  \"That movie was not good.\", 0, 0, 1, 1, 0, 0, 0, 1, 1, 1) |>\n  print_kbl()\n```\n\n-----\n\nThis matrix is referred to as a **Document-Term Matrix (DTM)**\n\n- Rows are documents\n- Columns are terms (tokens)\n- Combination of all terms/tokens is called the **vocabulary**\n- The terms columns (or a subset) will be used as features in our statistical algorithm to predict some outcome (not displayed)\n\n-----\n\nYou will also see the use of **raw counts** for measurement of the cell value for each term\n\n```{r}\n#| echo: false\n\ntibble::tribble(\n  ~Document, ~i, ~loved, ~that, ~movie, ~am, ~so, ~happy, ~was, ~not, ~good,\n  \"I loved that movie! I am so so so happy.\", 2, 1, 1, 1, 1, 3, 1, 0, 0, 0,\n  \"That movie was not good.\", 0, 0, 1, 1, 0, 0, 0, 1, 1, 1) |>\n  print_kbl()\n```\n\nBoth binary and raw count measures are biased (increased) for longer documents\n\n-----\n\nThe bias based on document length motivates the use of **term frequency**\n\n- Term frequency is NOT a simple frequency count (that is raw count)\n- Instead it is the raw count divided by the document length\n- This removes the bias based on document length\n\n```{r}\n## echo: false\n\ntibble::tribble(\n  ~Document, ~i, ~loved, ~that, ~movie, ~am, ~so, ~happy, ~was, ~not, ~good,\n  \"I loved that movie! I am so so so happy.\", .2, .1, .1, .1, .1, .3, .1, 0, 0, 0,\n  \"That movie was not good.\", 0, 0, .2, .2, 0, 0, 0, .2, .2, .2) |>\n  print_kbl()\n```\n\n-----\n\nTerm frequency can be dominated by frequently occurring words that may not be as important to understand the document as are rarer but more domain specific words\n\\\n\\\nThis was the motivation for removing stopwords but stopword removal may not be sufficient\n\\\n\\\n**Term Frequency - Inverse Document Frequency (tf-idf)** was developed to address this issue\n\\\n\\\nTF-IDF scales the term frequency by the inverse document frequency\n\n- Term frequency tells us that word is frequently used in the current document\n- IDF indexes how rare the word is across documents (higer IDF == more rare)\n\n\\\n\\\nThis emphasizes words used in specific documents that are not commonly used otherwise\n\n-----\n\n$IDF = log(\\frac{total\\:number\\:of\\:documents}{documents\\:containing\\:the\\:word})$\n\\\n\\\nThis results in larger values for words that arent used in many documents.  e.g., \n\n- for a word that appears in only 1 of 1000 documents, idf = log(1000/1) = 3\n- for a word that appears in 1000 of 1000 documents, idf = log(1000/1000) = 0\n\n\\\n\\\nNote that word that appears in no documents would result in a division by zero.  Therefore it is common to add 1 to the denominator of idf\n\n-----\n\nWe should be aware of some of the limitations of BoW:\n\n- Vocabulary: The vocabulary requires careful thought  \n  - Choice of stop words (global and subject specific) can matter\n  - Choice about size (number of tokens) can matter\n\n- Sparsity: \n  - Sparse representations (features that contain mostly zeros) are hard to model for computational reasons (space and time)\n  - Sparse representations present a challenge to extract signal in a large representational space\n  \n- Meaning: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). \n  - Context and meaning can offer a lot to the model\n  - “this is interesting” vs “is this interesting”\n  - “old bike” vs “used bike”\n\n```{r}\n#| include: false\nrm(tokens)\n```\n\n-----\n\n## Bringing it all together\n\n### General\n\nWe will now explore a series of model configurations to predict the sentiment (positive vs negative) of IMDB.com reviews\n\n- To keep things simple, all model configurations will use only one statistical algorithm - glmnet\n  - Within algorithm, configurations will differ by `penatly` and `dials::mixture' - see grid below\n- We will consider BoW features derived by TF-IDF only\n- We will remove global stop words from some configurations\n- We will stem words from some configurations\n- These BoW configurations will also differ by token type\n    - Word/unigram\n    - A combination of uni- and bigrams\n    - Prior to casting the tf-idf document-term matrix, we will filter tokens to various sizes - see grid below\n- We will also train a word embedding model to demonstrate how to implement that feature\nengineering technique in tidymodels\n\n-----\n\nWe are applying these feature engineering steps blindly.  YOU should not.\n\\\n\\\nYou will want to explore the impact of your feature engineering either\n\n  - During EDA with unnest_tokens()\n  - After making your recipe by using it to make a feature matrix\n  - Likely some of both\n  \n-----\n\nLets start fresh with our training data\n\n```{r}\ndata_trn <- read_csv(here::here(path_data, \"imdb_trn.csv\"),\n                     show_col_types = FALSE) |> \n  rowid_to_column(var = \"doc_num\") |> \n  mutate(sentiment = factor(sentiment, levels = c(\"neg\", \"pos\"))) |> \n  glimpse()\n```\n\n-----\n\nAnd do our (very minimal) cleaning\n```{r}\ndata_trn <- data_trn |> \n  mutate(text = str_replace_all(text, \"<br /><br />\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n```\n\n-----\n\nWe will select among model configurations using a validation split resampled accuracy to ease computational costs\n\n```{r}\nset.seed(123456)\nsplits <- data_trn |> \n  validation_split(strata = sentiment)\n```\n\n-----\n\nWe will use a simple union of stop words for some configurations:\n\n- It **could** help to edit the global list (or use a smaller list like snowball only)\n- It **could** help to have subject specific stop words too AND they might need to be different for words vs. bigrams\n```{r}\nall_stops <- union(pull(get_stopwords(source = \"smart\"), word), pull(get_stopwords(source = \"snowball\"), word))\n```\n\n-----\n\nAll of our model configurations will be tuned on `penalty`, `mixture` and `max_tokens`\n```{r}\ngrid_tokens <- grid_regular(penalty(range = c(-7, 3)), \n                            mixture(), \n                            max_tokens(range = c(5000, 10000)), \n                            levels = c(20, 6, 2))\n```\n\n### Words (unigrams)\n\nWe will start by fitting a BoW model configuration for word tokens\n\\\n\\\nRecipe for Word Tokens.  NOTE:\n\n- `token = \"words\"`  (default)\n- `max_tokens = tune()`  - We are now set to tune recipes!!!\n  `options = list(stopwords = all_stops)` - Passing in options to `tokenizers::tokenize_words()`\n```{r}\nrec_word <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\") |>    \n  step_tokenfilter(text, max_tokens = tune::tune()) |>\n  step_tfidf(text)  |> \n  step_normalize(all_predictors())\n```\n\n-----\n\nTuning hyperparameters for Word Tokens\n```{r}\nfits_word <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), \n                 mixture = tune::tune()) |> \n      set_engine(\"glmnet\") |> \n      tune_grid(preprocessor = rec_word, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_word\")\n```\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n```{r}\nautoplot(fits_word)\n```\n\n-----\n\nDisplay performance of best configuration for Word Tokens.  \n\\\n\\\nWow, pretty good!\n```{r}\nshow_best(fits_word)\n```\n\n-----\n\n### Words (unigrams) Excluding Stop Words\n\nLets try a configuration that removes stop words\n\\\n\\\nRecipe for Word Tokens excluding Stop Words.  NOTE:\n\n- `options = list(stopwords = all_stops)` - Passing in options to `tokenizers::tokenize_words()`\n```{r}\nrec_word_nsw <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\",   # included for clarity\n                options = list(stopwords = all_stops)) |>\n  step_tokenfilter(text, max_tokens = tune::tune()) |>\n  step_tfidf(text)  |> \n  step_normalize(all_predictors())\n```\n\n-----\n\nTuning hyperparameters for Word Tokens excluding Stop Words\n```{r}\nfits_word_nsw <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), \n                 mixture = tune::tune()) |> \n      set_engine(\"glmnet\") |> \n      tune_grid(preprocessor = rec_word_nsw, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_word_nws\")\n```\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n```{r}\nautoplot(fits_word_nsw)\n```\n\n-----\n\nDisplay performance of best configuration for Word Tokens excluding Stop Words.  \n\n- Looks like our mindless use of a large list of global stop words didn't help.   \n- We might consider the more focused snowball list\n- We might consider subject specific stop words\n- For this demonstration, we will just move forward without stop words\n  \n```{r}\nshow_best(fits_word_nsw)\n```\n\n\n-----\n\n### Stemmed Words (unigrams)\n\nNow we will try using stemmed words\n\\\n\\\nRecipe for Stemmed Word Tokens.  NOTE: `step_stem()`\n```{r}\nrec_stemmed_word <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"words\") |>    # included for clarity\n  step_stem(text) |> \n  step_tokenfilter(text, max_tokens = tune()) |>\n  step_tfidf(text)  |> \n  step_normalize(all_predictors())\n```\n\n-----\n\nTuning hyperparameters for Stemmed Word Tokens\n```{r}\n\nfits_stemmed_word <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune(), mixture = tune()) |> \n    set_engine(\"glmnet\") |> \n    tune_grid(preprocessor = rec_stemmed_word, \n              resamples = splits, \n              grid = grid_tokens, \n              metrics = metric_set(accuracy))\n},\n  rerun = rerun_setting, \n  dir = \"cache/012/\",\n  file = \"fits_stemmed_word\")\n```\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n```{r}\nautoplot(fits_stemmed_word)\n```\n\n-----\n\nDisplay performance of best configuration for Stemmed Word Tokens\n\\\n\\\nNot much change\n```{r}\nshow_best(fits_stemmed_word)\n```\n\n-----\n\n### ngrams (unigrams and bigrams)\n\nNow we try both unigrams and bigrams\n\\\n\\\nRecipe for unigrams and bigrams. NOTES:\n\n  - `token = \"ngrams\"`\n  - `options = list(n = 2, n_min = 1)`  - includes uni (1) and bi(2) grams\n  - not stemmed (stemming doesn't work by default for bigrams)\n```{r}\nrec_ngrams <-  recipe(sentiment ~ text, data = data_trn) |>\n  step_tokenize(text, \n                engine = \"tokenizers\", token = \"ngrams\",\n                options = list(n = 2, n_min = 1)) |>\n  step_tokenfilter(text, max_tokens =  tune::tune()) |>\n  step_tfidf(text) |> \n  step_normalize(all_predictors())\n```\n\n-----\n\nTuning hyperparameters for ngrams\n```{r}\n\nfits_ngrams <- cache_rds(\n  expr = {\n    logistic_reg(penalty = tune::tune(), mixture = tune::tune()) |> \n      set_engine(\"glmnet\") |> \n      tune_grid(preprocessor = rec_ngrams, \n                resamples = splits, \n                grid = grid_tokens, \n                metrics = metric_set(yardstick::accuracy))\n\n},\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"fits_ngrams\")\n```\n\n------\n\nConfirm that the range of hyperparameters we considered was sufficient\n\n```{r}\nautoplot(fits_ngrams)\n```\n\n-----\n\nDisplay performance of best configuration\n\\\n\\\nOur best model yet!\n\n```{r}\nshow_best(fits_ngrams)\n```\n\n\n-----\n\n### Word Embeddings\n\nBoW is an introductory approach for feature engineering.  \n\\\n\\\nAs you have read, word embeddings are a common alternative that addresses some of the limitations of BoW.  Word embeddings are also well-support in the `tidyrecipes` package.\n\\\n\\\nLet's switch gears away from document term matrices and BoW to word embeddings\n\n-----\n\nYou can find pre-trained word embeddings on the web\n\n- [GloVe](https://nlp.stanford.edu/projects/glove/)\n- [Word2Vec](https://code.google.com/archive/p/word2vec/)\n- [Fasttek](https://fasttext.cc/docs/en/english-vectors.html)\n\nBelow, we download and open pre-trained GloVe embeddings\n\n- I chose a smaller set of embeddings to ease computational cost\n  - Wikipedia 2014 + Gigaword 5\n  - 6B tokens, 400K vocab, uncased, 50d\n\n```{r}\ntemp <- tempfile()\noptions(timeout = max(300, getOption(\"timeout\")))  # need more time to download big file\ndownload.file(\"https://nlp.stanford.edu/data/glove.6B.zip\", temp)\nunzip(temp, files = \"glove.6B.50d.txt\")\nglove_embeddings <- read_delim(here::here(\"glove.6B.50d.txt\"),\n                               delim = \" \",\n                               col_names = FALSE) \n```\n\n```{r}\n#| include: false\n\n# unlink(temp) # remove downloaded zip \n# unlink(\"glove.6B.50d.txt)\n```\n\n-----\n\nRecipe for GloVe embedding. NOTES:\n\n- `token = \"words\"`\n- No need to filter tokens\n- New step is `step_word_embeddings(text, embeddings = glove_embeddings)`\n```{r}\n# rec_glove <-  recipe(sentiment ~ text, data = data_trn) |>\n#   step_tokenize(text, \n#                 engine = \"tokenizers\", token = \"words\",   # included for clarity\n#                 options = list(stopwords = all_stops)) |>\n#   step_word_embeddings(text, embeddings = glove_embeddings)\n```\n\n-----\n\nHyperparameter grid for GloVe embedding (no need for `max_tokens`)\n```{r}\n# grid_glove <- grid_regular(penalty(range = c(-7, 3)), \n#                             dials::mixture(), \n#                             levels = c(20, 6))\n```\n\n-----\n\nTuning hyperparameters for GloVe embedding\n```{r}\n# fits_glove <-\n#   logistic_reg(penalty = tune(), \n#              mixture = tune()) |> \n#   set_engine(\"glmnet\") |> \n#   tune_grid(preprocessor = rec_glove, \n#             resamples = splits, \n#             grid = grid_glove, \n#             metrics = metric_set(accuracy))\n```\n\n-----\n\nConfirm that the range of hyperparameters we considered was sufficient\n```{r}\n# autoplot(fits_glove)\n```\n\n-----\n\nDisplay performance of best configuration for GloVe embedding\n```{r}\n# show_best(fits_glove)\n```\n\n-----\n\n### Best Configuration\n\n- Fit best model configuration to training set\n```{r}\nrec_final <-\n  recipe(sentiment ~ text, data = data_trn) |>\n    step_tokenize(text, \n                  engine = \"tokenizers\", token = \"ngrams\",\n                  options = list(n = 2, n_min = 1)) |>\n    step_tokenfilter(text, max_tokens = select_best(fits_ngrams)$max_tokens) |>\n    step_tfidf(text) |> \n    step_normalize(all_predictors()) \n```\n\n```{r}\nrec_final_prep <- rec_final |>\n  prep(data_trn)\nfeat_trn <- rec_final_prep |> \n  bake(NULL) \n```\n\n-----\n\n```{r}\nfit_final <-\n  logistic_reg(penalty = select_best(fits_ngrams)$penalty, \n             mixture = select_best(fits_ngrams)$mixture) |> \n  set_engine(\"glmnet\") |> \n  fit(sentiment ~ ., data = feat_trn)\n```\n\n-----\n\n-----\n\n- Open and clean test to make features\n```{r}\ndata_test <- read_csv(here::here(path_data, \"imdb_test.csv\"),\n                      show_col_types = FALSE) |> \n  rowid_to_column(var = \"doc_num\") |> \n  mutate(sentiment = factor(sentiment, levels = c(\"neg\", \"pos\"))) |> \n  glimpse()\n\ndata_test <- data_test |> \n  mutate(text = str_replace_all(text, \"<br /><br />\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \" _\", \" \"),\n         text = str_replace_all(text, \"^_\", \"\"),\n         text = str_replace_all(text, \"_\\\\.\", \"\\\\.\"),\n         text = str_replace_all(text, \"\\\\(_\", \"\\\\(\"),\n         text = str_replace_all(text, \":_\", \": \"),\n         text = str_replace_all(text, \"_{3,}\", \" \"))\n\nfeat_test <- rec_final_prep |> \n  bake(data_test)\n```\n\n-----\n\n- Predict into test\n\n```{r}\naccuracy_vec(feat_test$sentiment, predict(fit_final, feat_test)$.pred_class)\n```\n\n- Confusion matrix\n```{r}\ncm <- tibble(truth = feat_test$sentiment,\n             estimate = predict(fit_final, feat_test)$.pred_class) |> \n  conf_mat(truth, estimate)\n\nautoplot(cm, type = \"heatmap\")\n```\n\n-----\n\nAnd lets end by calculating Permutation feature importance scores in test set using DALEX\n\n```{r}\nlibrary(DALEX, exclude= \"explain\")\nlibrary(DALEXtra)\n```\n\n-----\n\nWe are going to sample only a subset of the test set to keep the computational costs lower for this example.\n\n```{r}\nset.seed(12345)\nfeat_subtest <- feat_test |> \n  slice_sample(prop = .05) # 5% of data \n```\n\n-----\n\nNow we can get a df for the features (without the outcome) and a separate vector for the outcome.  \n```{r}\nx <- feat_subtest |> select(-sentiment)\n```\n\\\n\\\nFor outcome, we need to convert to 0/1 (if classification), and then pull the vector out of the dataframe\n\n```{r}\ny <- feat_subtest |> \n  mutate(sentiment = if_else(sentiment == \"pos\", 1, 0)) |> \n  pull(sentiment)\n```\n\n-----\n\nWe also need a specific predictor function that will work with the DALEX package\n\n```{r}\npredict_wrapper <- function(model, newdata) {\n  predict(model, newdata, type = \"prob\") |> \n    pull(.pred_pos)\n}\n```\n\n-----\n\nWe will also need an `explainer` object based on our model and data\n\n```{r}\nexplain_test <- explain_tidymodels(fit_final, # our model object \n                                   data = x, # df with features without outcome\n                                   y = y, # outcome vector\n                                   # our custom predictor function\n                                   predict_function = predict_wrapper)\n```\n\n-----\n\nFinally, we need to define a custom function for our performance metric as well\n\n```{r}\naccuracy_wrapper <- function(observed, predicted) {\n  observed <- fct(if_else(observed == 1, \"pos\", \"neg\"),\n                  levels = c(\"pos\", \"neg\"))\n  predicted <- fct(if_else(predicted > .5, \"pos\", \"neg\"), \n                   levels  = c(\"pos\", \"neg\"))\n  accuracy_vec(observed, predicted)\n}\n```\n\\\n\\\nWe are now ready to calculate feature importance metrics\n\n-----\n\nOnly doing 1 permutation for each feature to keep computational costs lower for this demonstration.  In real, life do more!\n\n```{r}\n#| label: permutations\n\nset.seed(123456)\nimp_permute <- cache_rds(\n  expr = {model_parts(explain_test, \n                      type = \"raw\", \n                      loss_function = accuracy_wrapper,\n                      B = 1)\n  },\n  rerun = rerun_setting,\n  dir = \"cache/012/\",\n  file = \"imp_permute\"\n)\n```\n\n-----\n\nPlot top 30 in an informative display\n```{r}\nimp_permute |> \n  filter(variable != \"_full_model_\",\n         variable != \"_baseline_\") |> \n  mutate(variable = fct_reorder(variable, dropout_loss)) |> \n  slice_head(n = 30) |> \n  print()\n```\n\n```{r}\nimp_permute |> \n  filter(variable != \"_full_model_\",\n         variable != \"_baseline_\") |> \n  mutate(variable = fct_reorder(variable, dropout_loss, \n                                .desc = TRUE)) |> \n  slice_tail(n = 30) |> \n  print()\n```\n\n```{r}\n#| fig-height: 4\n\n# full_model <- imp_permute |>  \n#    filter(variable == \"_full_model_\")\n  \n# imp_permute |> \n#   filter(variable != \"_full_model_\",\n#          variable != \"_baseline_\") |> \n#   mutate(variable = fct_reorder(variable, dropout_loss)) |> \n#   arrange(desc(dropout_loss) |>\n#   slice(n = 30) |> \n#   ggplot(aes(dropout_loss, variable)) +\n#     geom_vline(data = full_model, aes(xintercept = dropout_loss),\n#                linewidth = 1.4, lty = 2, alpha = 0.7) +\n#    geom_boxplot(fill = \"#91CBD765\", alpha = 0.4) +\n#    theme(legend.position = \"none\") +\n#    labs(x = \"accuracy\", y = NULL)\n```\n\n\n\n# NLP Discussion\n\n## Announcements\n\n- Last application assignment done!\n- Remaining units\n  - Bring it together for applications\n  - Ethics and Algorithmic Fairness\n  - Reading and discussion only for both units.  Discussion requires you!\n  \n- Two weeks left!\n  - next tuesday (lab on NLP)\n  - next thursday (discussion on applications)\n  - following tuesday (discussion on ethics/fairness)\n  - following thursday (concepts final review; 50 minutes only)\n\n- Early start to final application assignment (assigned next thursday, April 25th) and due Wednesday, May 8th at 8pm\n- Concepts exam at lab time during finals period (Tuesday, May 7th, 11-12:15 in this room)\n\n-----\n\n## Single nominal variable\n\n- Our algorithms need features coded with numbers\n  - How did we generally do this?\n  - What about algorithms like random forest?\n  \n-----\n\n##  Single nominal variable Example\n\n- Current emotional state\n  - angry, afraid, sad, excited, happy, calm\n\n- How represented with one-hot?\n\n-----\n\n## What about document of words (e.g., sentence rather than single word)\n\n- I watched a scary movie last night and couldn't get to sleep because I was so afraid\n- I went out with my friends to a bar and slept poorly because I drank too much\n\n\\\n\\\n\n- How represented with bag of words?\n- binary, count, tf-idf\n\n\\\n\\\n\n- What are the problems with these approaches\n  - relationships between features or similarity between full vector across observations \n  - context/order\n  - dimensionality\n  - sparsity\n\n-----\n\n## N-grams vs. simple (1-gram) BOW\n\n- How different?\n\n\\\n\\\n\n- some context/order\n- but still no relationship/meaning or similarity\n- even higher dimesions\n\n-----\n\n## Linguistic Inquiry and Word Count (LIWC)\n\n- Tausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and computerized text analysis methods. Journal of Language and Social Psychology, 29(1), 24–54. https://doi.org/\n\n\\\n\\\n\n- Meaningful features? (based on domain expertise)\n- Lower dimensional\n- Very limited breadth\n\n\\\n\\\n- Can add domain specific dictionaries\n\n-----\n\n## Word Embeddings\n\n- Encodes word meaning\n- Words that are similar get similar vectors\n- Meaning derived from context in various text corpora\n- Lower dimensional\n- Less sparse\n\n-----\n\n## Examples\n\n\n- Affect Words\n\n![](figs/nlp_1.PNG){height=5in}\n\n-----\n\n![](figs/nlp_2.PNG){height=5in}\n\n-----\n \n![](figs/nlp_3.PNG){height=5in}\n\n\n\n- Two more general\n\n- word2vec (google)\n  - CBOW\n  - skipgram\n\n![](figs/nlp_4.PNG){height=5in}\n\n-----\n\n![](figs/nlp_5.PNG){height=5in}\n\n-----\n\n![](figs/nlp_6.PNG){height=5in}\n\n\n-----\n\n## fasttext (facebook ai team)\n\n- n-grams (e.g., 3-6 character representations)\n- word vector is sum of its ngrams\n- can handle low frequecynor even novel wordsp\n\n## Other Methods\n\n- Other approaches\n  - glove\n  - elmo\n  - BERT\n:w\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":"html_document","warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["book.css"],"output-file":"012_nlp.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","bibliography":["refs.bib"],"callout-icon":false,"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}