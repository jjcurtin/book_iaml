{
  "hash": "2d0a70afa8bddfb7785354e7b7d95138",
  "result": {
    "engine": "knitr",
    "markdown": "---\neditor_options: \n  chunk_output_type: console\n---\n\n\n# IAML Unit 4: Discussion\n\n\n## Anouncements\n\n- Please meet with TA or me if you can't generate predictions from your models\n- And the winner is.....\n\n--------------------------------------------------------------------------------\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/print_kbl.R?raw=true\",\n                     sha1 = \"021a7f7cddc1f0ffcd0613e57b94c81246b84f7b\")\nread_csv(here::here(\"./application_assignments/competitions/2025_unit_04.csv\"),\n         show_col_types = FALSE) |>\n  mutate(acc_test = round(acc_test, 3)) |> \n  print_kbl()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; \"><table class=\"table table-striped table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> name </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> acc_test </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> Tanchone </td>\n   <td style=\"text-align:right;\"> 0.79 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Hwang </td>\n   <td style=\"text-align:right;\"> 0.78 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Diao </td>\n   <td style=\"text-align:right;\"> 0.78 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Lin </td>\n   <td style=\"text-align:right;\"> 0.78 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Villwock </td>\n   <td style=\"text-align:right;\"> 0.78 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Higgins </td>\n   <td style=\"text-align:right;\"> 0.77 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Khoury </td>\n   <td style=\"text-align:right;\"> 0.77 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Luo </td>\n   <td style=\"text-align:right;\"> 0.76 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Janssen </td>\n   <td style=\"text-align:right;\"> 0.76 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Yu </td>\n   <td style=\"text-align:right;\"> 0.76 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Zhao </td>\n   <td style=\"text-align:right;\"> 0.76 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Cha </td>\n   <td style=\"text-align:right;\"> 0.75 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Yin </td>\n   <td style=\"text-align:right;\"> 0.75 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Baker </td>\n   <td style=\"text-align:right;\"> 0.75 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Lau </td>\n   <td style=\"text-align:right;\"> 0.75 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Xu </td>\n   <td style=\"text-align:right;\"> 0.75 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Vo </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Wong </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Holenarasipura </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Zhang </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Ahmed </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Hey </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Jolly </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Murray </td>\n   <td style=\"text-align:right;\"> 0.70 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> Cox </td>\n   <td style=\"text-align:right;\"> 0.65 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n--------------------------------------------------------------------------------\n\nKaggle Competitions\n\n- [Titanic](https://www.kaggle.com/competitions/titanic/overview)\n- [Ames](https://www.kaggle.com/competitions/i2ml-ames/overview)\n- [Other competitions](https://www.kaggle.com/competitions?sortOption=numTeams)\n\n--------------------------------------------------------------------------------\n\nQuiz review!\n\n--------------------------------------------------------------------------------\n\n## Comparisons across algorithms\n\nLogistic regression models DGP condition probabilities using logistic function\n\n- Get parameter estimates for effects of X \n- Makes strong assumptions about shape of DGP - linear on log-odds(Y)\n- Yields linear decision boundary\n- Better for binary outcomes but can do more than two levels (with some effort)\n  - (**Briefly describe: multiple one vs other classes models approach**)\n- Needs numeric features but can dummy code categorical variables (as with lm)\n- Problems when classes are fully separable (or even mostly separable)\n\n--------------------------------------------------------------------------------\n\nLDA uses Bayes theorem to estimate condition probability\n\n- LDA models the distributions of the Xs separately for each class\n- Then uses Bayes theorem to estimate $Pr(Y = k | X)$ for each k and assigns the  observation to the class with the highest probability\n\n$Pr(Y = k|X) = \\frac{\\pi_k * f_k(X)}{\\sum_{l = 1}^{K} f_l(X)}$\n\nwhere\n\n- $\\pi_k$ is the prior probability that an observation comes from class k (estimated from frequencies of k in training)\n- $f_k(X)$ is the density function of X for an observation from class k\n  -  $f_k(X)$ is large if there is a high probability that an observation in class k has that set of values for X and small if that probability is low\n  - $f_k(X)$ is difficult to estimate unless we make some simplifying assumptions\n  - X is multivariate normal\n  - Common covariance matrix ($\\sum$) across K classesj\n  - With these assumptions, we can estimate $\\pi_k$, $\\mu_k$, and $\\sigma^2$ from the training set and calculate $Pr(Y = k|X)$ for each k\n  \n--------------------------------------------------------------------------------\n\n- Parametric model but parameters not useful for interpretation of effects of X\n- Linear decision boundary  \n- Assumptions about multivariate normal X and common $\\sum$\n- Dummy features may not work well given assumption about normally distributed?\n- May require smaller sample sizes to fit than logistic regression if assumptions met\n- Can natively handle more than two level for outcome  \n  \n-------------------------------------------------------------------------------- \n  \nQDA relaxes one restrictive assumption of LDA\n  \n- Still required multivariate normal X\n- **But it allows each class to have its own $\\sum$**\n- This makes it:\n  - More flexible\n  - Able to model non-linear decision boundaries including 2-way interactions (see formula for discriminant in @ISL)\n  - How handle interactions by relaxing common $\\sum$? \n  - But requires substantial increase in parameter estimation (more potential to overfit)\n- Still problems with dummy features (not normal; product terms?)\n- Can natively handle more than 2 levels of outcome like LDA\n- Compare to LDA and Logistic Regression on bias-variance trade off?\n\nRDA may be better than both LDA and QDA?  More on idea of blending after elastic net\n\n--------------------------------------------------------------------------------\n\nKNN works similar to regression\n\n- But now looks at percentage of observations for each class among nearest neighbors to estimate conditional probabilities\n- Doesn't make assumptions about Xs or $\\sum$ for LDA and/or QDA\n- Not limited to linear decision boundaries like logistic and LDA\n- Very flexible - low bias but high variance?\n- K can be adjusted to impact bias-variance trade-off\n- KNN can handle more than two level outcomes natively\n- KNN can be computationally cost with big N (and many X)\n  - Can down-sample training data to reduce this problem\n\n--------------------------------------------------------------------------------\n\nSummary\n\n- Both logistic and LDA are linear functions of X and therefore produce linear decision boundaries\n\n- LDA makes additional assumptions about X (multivariate normal and common $\\sum$) beyond logistic regression.  Relative performance is based on the quality of this assumption\n\n- QDA relaxes the LDA assumption about common $\\sum$ (and RDA can relax it partially)\n  - This also allows for nonlinear decision boundaries including 2-way interactions among features\n  - QDA is therefore more flexible, which means possibly less bias but more potential for overfitting\n\n- Both QDA and LDA assume multivariate normal X so *may* not accommodate categorical predictors very well.  Logistic and KNN do accommodate categorical predictors\n\n- KNN is non-parametric and therefore the most flexible\n  - Can also handle interactions and non-linear effects natively (with feature engineering)\n  - Increased overfitting, decreased bias?\n  - Not very interpretable.  But LDA/QDA, although parametric, aren't as interpretable as logistic regression\n\n- Logistic regression fails when classes are perfectly separated (but does that ever happen?) and is less stable when classes are well separated\n\n- LDA, KNN, and QDA naturally accommodate more than two classes  \n  - Logistic requires additional tweak\n  \n- Sample size issues\n  - Logistic regression requires relatively large sample sizes.  \n  - LDA may perform better than logistic regression with smaller sample sizes if assumptions are met (**QDA?**)\n  - KNN can be computationally very costly with large sample sizes (and large number of X) but could always downsample training set.\n\n--------------------------------------------------------------------------------\n\n## Interactions in LDA and QDA\n\n- Simulate multivariate normal distribution for X (`x1` and `x2`) using MASS package\n- Separately for trn and val\n- NOTE: I first did this with uniform distributions on X and the models fit more poorly.  Why?\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(discrim, exclude = \"smoothness\")\nset.seed(5433)\nmeans <- c(0, 0)\nsigma <- diag(2) * 100\ndata_trn <- MASS::mvrnorm(n = 300, mu = means, Sigma = sigma) |>  \n    magrittr::set_colnames(str_c(\"x\", 1:length(means))) |>  \n    as_tibble()\n\ndata_val <- MASS::mvrnorm(n = 3000, mu = means, Sigma = sigma) |>  \n    magrittr::set_colnames(str_c(\"x\", 1:length(means))) |>  \n    as_tibble()\n```\n:::\n\n \n--------------------------------------------------------------------------------\n\n- Write function for interactive DGP based on `x1` and `x2`\n- Will map this over rows of d\n- Can specify any values for b\n- b[4] will be interaction parameter estimate\n\n::: {.cell}\n\n```{.r .cell-code}\nb <- c(0, 0, 0, .5)\n\ncalc_p <- function(x, b){\n   exp(b[1] + b[2]*x$x1 + b[3]*x$x2 + b[4]*x$x1*x$x2) /\n     (1 + exp(b[1] + b[2]*x$x1 + b[3]*x$x2 + b[4]*x$x1*x$x2))\n}\n```\n:::\n\n\n--------------------------------------------------------------------------------\n\n- Add p and then observed classes to trn and val \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_trn <- data_trn |> \n  mutate(p = calc_p(data_trn, b)) |> \n  mutate(y = rbinom(nrow(data_trn), 1, p),\n         y = factor(y, levels = 0:1, labels = c(\"neg\", \"pos\")))\n\nhead(data_trn, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 Ã— 4\n        x1     x2        p y    \n     <dbl>  <dbl>    <dbl> <fct>\n 1   9.85   -5.53 1.52e-12 neg  \n 2  -2.48    5.64 9.21e- 4 neg  \n 3  -4.74  -13.7  1.00e+ 0 pos  \n 4  -7.02   12.6  6.09e-20 neg  \n 5   0.942  -3.48 1.63e- 1 pos  \n 6   0.151   2.78 5.52e- 1 neg  \n 7   7.74   -6.84 3.24e-12 neg  \n 8  -4.85  -10.1  1.00e+ 0 pos  \n 9  -0.937   2.03 2.78e- 1 neg  \n10 -16.5     6.08 1.83e-22 neg  \n```\n\n\n:::\n\n```{.r .cell-code}\ndata_val <- data_val |> \n  mutate(p = calc_p(data_val, b)) |> \n  mutate(y = rbinom(nrow(data_val), 1, p),\n         y = factor(y, levels = 0:1, labels = c(\"neg\", \"pos\")))\n```\n:::\n\n\n--------------------------------------------------------------------------------\n\n- Lets look at what an interactive DGP looks like for two features and a binary outcome\n- Parameter estimates set up a \"cross-over\" interaction\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val |> \n  ggplot(mapping = aes(x = x1, y = x2, color = y)) +\n    geom_point(size = 2, alpha = .5)\n```\n\n::: {.cell-output-display}\n![](d04_classification_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n--------------------------------------------------------------------------------\n\n- Fit models in trn \n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lda <- \n  discrim_linear() |> \n  set_engine(\"MASS\") |> \n  fit(y ~ x1 + x2, data = data_trn)\n\nfit_lda_int <- \n  discrim_linear() |> \n  set_engine(\"MASS\") |> \n  fit(y ~ x1 + x2 + x1*x2, data = data_trn)\n\nfit_qda <- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) |> \n  set_engine(\"klaR\") |> \n  fit(y ~ x1 + x2, data = data_trn)\n\nfit_qda_int <- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) |> \n  set_engine(\"klaR\") |> \n  fit(y ~ x1 + x2 + x1*x2, data = data_trn)\n```\n:::\n\n\n--------------------------------------------------------------------------------\n\n- Additive LDA model decision boundary and performance in val\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val |> \n  plot_decision_boundary(fit_lda, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400)\n```\n\n::: {.cell-output-display}\n![](d04_classification_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n--------------------------------------------------------------------------------\n\n-  Interactive LDA model decision boundary and performance in val\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val |> \n  plot_decision_boundary(fit_lda_int, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400) \n```\n\n::: {.cell-output-display}\n![](d04_classification_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n--------------------------------------------------------------------------------\n\n- Additive QDA model decision boundary\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val |> \n  plot_decision_boundary(fit_qda, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400)\n```\n\n::: {.cell-output-display}\n![](d04_classification_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n- Interactive QDA model decision boundary\n\n--------------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_val |> \n  plot_decision_boundary(fit_qda_int, x_names = c(\"x1\", \"x2\"), y_name = \"y\",\n                         n_points = 400) \n```\n\n::: {.cell-output-display}\n![](d04_classification_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n \n- Costs for QDA vs. LDA interactive in this example and more generally with more features?\n- What if you were using RDA, which can model the full range of models between linear and quadratic?\n\n--------------------------------------------------------------------------------\n\n## Categorical predictors\n\n- All algorithms so far require numeric features\n- Ordinal can be made numeric sometimes by just substituting ordered vector (i.e. 1, 2, 3, etc)\n- Nominal needs something more\n- Our go to method is dummy features\n  - What is big problem with dummy features?\n  - Collapsing levels?\n  - Difference between dummy coding and one-hot encoding\n  - What is \"dummy variable trap?\" with one-hot\n  - Issue of binary scores for LDA/QDA\n  \n- Target encoding example\n  - Country of origin for car example (but maybe think of many countries?)\n  - Why not data leakage?\n  - Problems with step_mutate()\n  - Can manually do it with our current resampling\n  - See `step_lencode_*()` in [embed package](https://embed.tidymodels.org/)\n  \n--------------------------------------------------------------------------------\n\n## DGP and Errors\n\n- DGP on probability\n- what is irreducible error for classification?\n- DGP on X1 - draw it with varying degrees of error  \n\n--------------------------------------------------------------------------------\n\n- DGP and error on two features\n\n![](figs/unit4_two_class_decision_boundry.png){height=4in}\n\n--------------------------------------------------------------------------------\n\n## Bayes Classifier\n \nThe previous figure below displays simulated data for a classification problem for K = 2 classes as a function of `X1` and `X2`\n\nThe Bayes classifier assigns each observation its most likely class given its conditional probabilities for the values for `X1` and `X2`\n\n- $Pr(Y = k | X = x_0) for\\:k = 1:K$\n- For K = 2, this means assigning to the class with Pr > .50\n- This decision boundary for the two class problem is displayed in the figure\n\n--------------------------------------------------------------------------------\n\nThe Bayes classifier provides the minimum error rate for test data\n\n- Error rate for any $x_0$ will be $1 - max (Pr( Y = k | X = x_0))$\n- Overall error rate will be the average of this across all possible X\n- This is the irreducible error for classification problems\n- This is a theoretical model b/c (except for simulated data), we don't know the conditional probabilities based on X\n- Many classification models try to estimate these conditionals\n\n\n--------------------------------------------------------------------------------\n\n- Probability vs. odds vs. log-odds\n- How to interpret parameter estimates (effects of X)\n\n--------------------------------------------------------------------------------\n\nPCA\n\n- https://setosa.io/ev/principal-component-analysis/\n- https://www.cs.cmu.edu/~elaw/papers/pca.pdf\n",
    "supporting": [
      "d04_classification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"d04_classification_files/libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"d04_classification_files/libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}