# Regularization and Penalized Models

## Overview of Unit

**Learning Objectives**

* Subsetting approaches: Forward, Backward, Best Subset (covered in reading only)
* Cost and Loss functions
  * What are they and how are they used
  * What are the specific formulas for linear model, logistic regression, and variants of glmnet (ridge, LASSO, full elasticnet)
* What is regularization
  * What are its benefits?
  * What are its costs?
* How does lambda affect bias-variance trade-off in glmnet
* What does alpha do?
* Feature engineering approaches for dimensionality reduction: PCA (covered in reading only)
* Other algorithms that do feature selection/dimensionality reduction: PCR and PLS (covered in reading only)
* Contrasts of PCA, PCR, PLS, and glmnet/LASSO for dimensionality reduction (covered in reading only)

**Readings**

* @ISL [Chapter 6, pp 225 - 267](https://dionysus.psych.wisc.edu/iaml/pdfs/ISLRv2.pdf)

**Lecture Videos**

* [Lecture 1: An Introduction to Penalized/Regularized Algorithms]()
* [Lecture 2: Cost Functions]()
* [Lecture 3: Ridge Regression]()
* [Lecture 4: LASSO]()
* [Lecture 5: The Elastic net]()
* [Lecture 6: Emprical Example - Many good predictors]()
* [Lecture 7: Emprical Example - Good and zero predictors]()
* [Lecture 8: Emprical Example - LASSO for covariate selection]()

* [Discussion]()

**Coding Assignment**

* [data](homework/unit_6/ames_full_cln.csv)
* [data dictionary](homework/unit_6/ames_data_dictionary)
* [rmd shell](homework/unit_6/hw_unit_6_regularization.Rmd)
* [fun_modeling.R](https://dionysus.psych.wisc.edu:5001/sharing/4KTOoge00)
* [solution](homework/unit_6/key_unit_6_regularization.html) 


Submit the application assignment [here]() and complete the [unit quiz]() by 8 pm on Wednesday, February 28th :w

