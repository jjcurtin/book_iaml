[
  {
    "objectID": "index.html#course-syllabus",
    "href": "index.html#course-syllabus",
    "title": "Introduction to Applied Machine Learning",
    "section": "Course Syllabus",
    "text": "Course Syllabus\n\nInstructor\nJohn Curtin\n\nOffice hours: Thursdays, 1-2 pm or by appointment in room 326\nEmail: jjcurtin@wisc.edu (but please use Slack DM or channel posts for all course communications during this semester)\n\n\n\nTeaching Assistants\nMichelle Marji\n\nOffice hours: INSERT HOURS\nEmail: michelle.marji@wisc.edu\n\nKendra Wyant\n\nOffice hours: INSERT HOURS\nEmail: kpaquette2@wisc.edu\n\n\n\nCourse Website\nhttps://jjcurtin.github.io/book_iaml/\n\n\nCommunications\nAll course communications will occur in the course’s Slack workspace (https://iaml-2024.slack.com/). You should have received an invitation to join the workspace. If you have difficulty joining, please contact me by my email above. The TAs and I will respond to all Slack messages within 1 business day (and often much quicker). Please plan accordingly (e.g., weekend messages may not receive a response until Monday). For general questions about class, coding assignments, etc., please post the question to the appropriate public channel. If you have the question, you are probably not alone. For issues relevant only to you (e.g., class absences, accommodations, etc.), you can send a direct message in Slack to me. However, I may share the DM with the TAs unless you request otherwise. In general, we prefer that all course communication occur within Slack rather than by email so that it is centralized in one location.\n\n\nMeeting Times\nThe scheduled course meeting times are Tuesdays and Thursdays from 11:00 - 12:15 pm. Tuesdays are generally used by the TAs to discuss application issues from the homework or in the course more generally. Thursdays are generally led by John and used to discuss topics from the video lectures and readings.\nAll required videos, readings, and application assignments are described on the course website at the beginning of each unit.\n\n\nCourse Description\nThis course is designed to introduce students to a variety of computational approaches in machine learning. The course is designed with two key foci. First, students will focus on the application of common, “out-of-the-box” statistical learning algorithms that have good performance and are implemented in tidymodels in R. Second, students will focus on the application of these approaches in the context of common questions in behavioral science in academia and industry.\n\n\nRequisites\nStudents are required to have completed Psychology 610 with a grade of B or better or a comparable course with my consent.\n\n\nLearning Outcomes\n\nStudents will develop and refine best practices for data wrangling, general programming, and analysis in R.\nStudents will distinguish among a variety of machine learning settings: supervised learning vs. unsupervised learning, regression vs. classification\nStudents will be able to implement a broad toolbox of well-supported machine-learning methods: decision trees, nearest neighbor, general and generalized linear models, penalized models including ridge, lasso, and elastic-nets, neural nets, random forests.\nStudents will develop expertise with common feature extraction techniques for quantitative and categorical predictors.\nStudents will be able to use natural language processing approaches to extract meaningful features from text data.\nStudents will know how to characterize how well their regression and classification models perform and they will employ appropriate methodology for evaluating their: cross validation, ROC and PR curves, hypothesis testing.\nStudents will learn to apply their skills to common learning problems in psychology and behavioral sciences more generally.\n\n\n\nCourse Topics\n\nOverview of Machine Learning Concepts and Uses\nData wrangling in R using tidyverse and tidymodels\nIterations, functions, simulations in R\nRegression models\nClassification models\nModel performance metrics\nROCs\nCross validation and other resampling methods\nModel selection and regularization\nApproaches to parallel processing\nFeature engineering techniques\nNatural language processing\nTree based methods\nBagging and boosting\nNeural networks\nDimensionality reduction and feature selection\nExplanatory methods including variable importance, partial dependence plots, etc\nEthics and privacy issues\n\n\n\nSchedule\nThe course is organized around 14 weeks of academic instruction covering the following topics:\n\nIntroduction to course and machine learning\nExploratory data analysis\nRegression models\nClassification models\nResampling methods for model selection and evaluation\nRegularization and penalized models\nMidterm exam/project\nModel comparisons\nAdvanced performance metrics\nAdvanced models: Random forests and ensemble methods (bagging, boosting, stacking)\nAdvanced models: Neural networks\nNatural Language Processing I: Text processing and feature engineering\nApplications\nEthics\n\n\nThe final exam is during exam week on Tuesday May 7th from 11 - 12:15 in our normal classroom.\nThe final project is due during exam week on Wednesday May 8th at 8 pm.\n\n\n\nRequired Textbooks and Software\nAll required textbooks are freely available online (though hard copies can also be purchased if desired). There are eight required textbooks for the course. The primary text for which we will read many chapters is:\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. [-@ISL]. An Introduction to Statistical Learning: With Applications in R (2021; 2nd Edition). (website)\n\nWe will also read sections to chapters in each of the following texts:\n\nWickham, H. & Grolemund, G. [-@RDS]. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (1st ed.). Sebastopol, CA: O’Reilly Media, Inc. (website)\nHvitfeldt, E. & Silge, J.. [-@SMLTAR] Supervised Machine Learning for Text Analysis in R (website)\nKuhn, M. & Johnson, K. [-@APM]. Applied Predictive Modeling. New York, NYL Springer Science. (pdf)\nKuhn, M., & Johnson, K. [-@FES]. Feature Engineering and Selection: A Practical Approach for Predictive Models (1 edition). Boca Raton, FL: Chapman and Hall/CRC. (website)\nKuhn, M. & Silge, J. [-@TID] Tidy Modeling with R. (website)\nMolnar, C. [-@IML] Intepretable Machine Learning: A Guide for Makiong Black Box Models Explainable. website\nSilge, J., & Robinson, D. [-@TMR]. Text Mining with R: A Tidy Approach (1rst ed.). Beijing; Boston: O’Reilly Media. (website)\nWickham, H. [-@TSG]. The Tidy Style Guide. (website)\n\nAdditional articles will be assigned and provided by pdf through the course website.\nAll data processing and analysis will be accomplished using R (and we recommend the RStudio IDE). R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS.\n\n\nGrading\n\nQuizzes (13 anticipated): 15%\nApplication assignments (11 anticipated): 25%\nMidterm application exam: 15%\nMidterm conceptual exam: 15%\nFinal application exam: 15%\nFinal conceptual exam: 15%\n\nFinal letter grades may be curved upward, but a minimum guarantee is made of an A for 93 or above, AB for 88 - 92, B for 83 - 87, BC for 78 - 82, C for 70 - 77, D for 60-69, and F for &lt; 60.\n\n\nExams, Application Assignments and Quizzes\n\nThe midterm application exam will be due during the 7th week of the course on Wednesday, March 6th at 8 pm.\nThe midterm conceptual exam will be administered during class on Thursday, March 7th.\nThe final exam is during exam week on Tuesday May 7th from 11 - 12:15 in our normal classroom.\nThe final project is due during exam week on Wednesday May 8th at 8 pm.\nApproximately weekly quizzes will be administered through Canvas and due each Wednesday at 8 pm\nApproximately weekly application assignments will be submitted via Canvas and due each Wednesday at 8 pm.\n\n\n\nApplication Assignments\nThe approximately weekly application assignments are due on Wednesdays at 8 pm through Canvas. These assignments are to be done individually. Please do not share answers or code. You are also encouraged to make use of online resources (e.g., stack overflow) for assistance. All assignments will be completed using R markdown to provide both the code and documentation as might be provided to your mentor or employer to fully describe your solution. Late assignments are not accepted because problem solutions are provided immediately after the due date. Application assignments are graded on a three-point scale (0 = not completed, 1 = partially completed and/or with many errors, 2 = fully completed and at least mostly correct). Grades for each assignment will be posted by the following Monday at the latest.\n\n\nChatGPT\nI suspect you have all seen discussions of all that ChatGPT can do by now and its impact on teaching and assessment. I believe that AI like ChatGPT will eventually become an incredible tool for data scientists and programmers. As such, I view these advances with excitement. Of course, I don’t plan to assign a grade to ChatGPT so I want to make sure that we are clear on when you can and when you cannot use it. Given that I expect AI like ChatGPT to become a useful tool in our workflow as professionals, now is the time to start to learn how it can help. Therefore, you are free to use it during any of our application assignments AND the application questions on the mid-term and final exams. Code from ChatGPT is unlikely to be sufficient in either context (and my testing suggests it can be flat out wrong in some instances!) but I suspect that it will still be useful. In contrast, you CAN NOT use ChatGPT to answer the conceptual questions on the two exams or the weekly quizzes. Those questions are designed to assess your working knowledge about concepts and best practices. That information must be in YOUR head. I am not yet certain how I will administer that part of those exams (in class vs. HonorLock), but I wanted to be 100% clear that use of ChatGPT to answer those questions will be considered cheating and handled as such if detected. There will be a zero tolerance policy for such cheating. It will be reported to the Dean of Students on first offense.\n\n\nStudent Ethics\nThe members of the faculty of the Department of Psychology at UW-Madison uphold the highest ethical standards of teaching and research. They expect their students to uphold the same standards of ethical conduct. By registering for this course, you are implicitly agreeing to conduct yourself with the utmost integrity throughout the semester.\nIn the Department of Psychology, acts of academic misconduct are taken very seriously. Such acts diminish the educational experience for all involved – students who commit the acts, classmates who would never consider engaging in such behaviors, and instructors. Academic misconduct includes, but is not limited to, cheating on assignments and exams, stealing exams, sabotaging the work of classmates, submitting fraudulent data, plagiarizing the work of classmates or published and/or online sources, acquiring previously written papers and submitting them (altered or unaltered) for course assignments, collaborating with classmates when such collaboration is not authorized, and assisting fellow students in acts of misconduct. Students who have knowledge that classmates have engaged in academic misconduct should report this to the instructor.\n\n\nDiversity and Inclusion\nInstitutional statement on diversity: “Diversity is a source of strength, creativity, and innovation for UW-Madison. We value the contributions of each person and respect the profound ways their identity, culture, background, experience, status, abilities, and opinion enrich the university community. We commit ourselves to the pursuit of excellence in teaching, research, outreach, and diversity as inextricably linked goals.\nThe University of Wisconsin-Madison fulfills its public mission by creating a welcoming and inclusive community for people from every background – people who as students, faculty, and staff serve Wisconsin and the world.” https://diversity.wisc.edu/"
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "Introduction to Applied Machine Learning",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nBy enrolling in this course, each student assumes the responsibilities of an active participant in UW-Madison’s community of scholars in which everyone’s academic work and behavior are held to the highest academic integrity standards. Academic misconduct compromises the integrity of the university. Cheating, fabrication, plagiarism, unauthorized collaboration, and helping others commit these acts are examples of academic misconduct, which can result in disciplinary action. This includes but is not limited to failure on the assignment/course, disciplinary probation, or suspension. Substantial or repeated cases of misconduct will be forwarded to the Office of Student Conduct & Community Standards for additional review. For more information, refer to http://studentconduct.wiscweb.wisc.edu/academic-integrity\n\nAccommodations Polices\nMcBurney Disability Resource Center syllabus statement: “The University of Wisconsin-Madison supports the right of all enrolled students to a full and equal educational opportunity. The Americans with Disabilities Act (ADA), Wisconsin State Statute (36.12), and UW-Madison policy (Faculty Document 1071) require that students with disabilities be reasonably accommodated in instruction and campus life. Reasonable accommodations for students with disabilities is a shared faculty and student responsibility. Students are expected to inform faculty [me] of their need for instructional accommodations by the end of the third week of the semester, or as soon as possible after a disability has been incurred or recognized. Faculty [I], will work either directly with the student [you] or in coordination with the McBurney Center to identify and provide reasonable instructional accommodations. Disability information, including instructional accommodations as part of a student’s educational record, is confidential and protected under FERPA.” http://mcburney.wisc.edu/facstaffother/faculty/syllabus.php\nUW-Madison students who have experienced sexual misconduct (which can include sexual harassment, sexual assault, dating violence and/or stalking) also have the right to request academic accommodations. This right is afforded them under Federal legislation (Title IX). Information about services and resources (including information about how to request accommodations) is available through Survivor Services, a part of University Health Services: https://www.uhs.wisc.edu/survivor-services/\n\n\nComplaints\nOccasionally, a student may have a complaint about a TA or course instructor. If that happens, you should feel free to discuss the matter directly with the TA or instructor. If the complaint is about the TA and you do not feel comfortable discussing it with the individual, you should discuss it with the course instructor. Complaints about mistakes in grading should be resolved with the TA and/or instructor in the great majority of cases. If the complaint is about the instructor (other than ordinary grading questions) and you do not feel comfortable discussing it with the individual, make an appointment to speak to the Associate Chair for Graduate Studies, Professor Shawn Green, cshawngreen@wisc.edu.\nIf you have concerns about climate or bias in this class, or if you wish to report an incident of bias or hate that has occurred in class, you may contact the Chair of the Department, Professor Allyson Bennett (allyson.j.bennett@wisc.edu) or the Chair of the Psychology Department Climate & Diversity Committee, Martha Alibali (martha.alibali@wisc.edu). You may also use the University’s bias incident reporting system\n\n\nPrivacy of Student Information & Digital Tools\nThe privacy and security of faculty, staff and students’ personal information is a top priority for UW-Madison. The university carefully reviews and vets all campus-supported digital tools used to support teaching and learning, to help support success through learning analytics, and to enable proctoring capabilities. UW-Madison takes necessary steps to ensure that the providers of such tools prioritize proper handling of sensitive data in alignment with FERPA, industry standards and best practices. Under the Family Educational Rights and Privacy Act (FERPA which protects the privacy of student education records), student consent is not required for the university to share with school officials those student education records necessary for carrying out those university functions in which they have legitimate educational interest. 34 CFR 99.31(a)(1)(i)(B). FERPA specifically allows universities to designate vendors such as digital tool providers as school officials, and accordingly to share with them personally identifiable information from student education records if they perform appropriate services for the university and are subject to all applicable requirements governing the use, disclosure and protection of student data.\n\n\nPrivacy of Student Records & the Use of Audio Recorded Lectures\nSee information about privacy of student records and the usage of audio-recorded lectures.\nLecture materials and recordings for this course are protected intellectual property at UW-Madison. Students in this course may use the materials and recordings for their personal use related to participation in this class. Students may also take notes solely for their personal use. If a lecture is not already recorded, you are not authorized to record my lectures without my permission unless you are considered by the university to be a qualified student with a disability requiring accommodation. [Regent Policy Document 4-1] Students may not copy or have lecture materials and recordings outside of class, including posting on internet sites or selling to commercial entities. Students are also prohibited from providing or selling their personal notes to anyone else or being paid for taking notes by any person or commercial firm without the instructor’s express written permission. Unauthorized use of these copyrighted lecture materials and recordings constitutes copyright infringement and may be addressed under the university’s policies, UWS Chapters 14 and 17, governing student academic and non-academic misconduct.\n\n\nAcademic Calendar & Religious Observances\nStudents who wish to inquire about religious observance accommodations for exams or assignments should contact the instructor within the first two weeks of class, following the university’s policy on religious observance conflicts"
  },
  {
    "objectID": "001_overview.html#overview-of-unit",
    "href": "001_overview.html#overview-of-unit",
    "title": "1  Overview of Machine Learning",
    "section": "1.1 Overview of Unit",
    "text": "1.1 Overview of Unit\nLearning Objectives\n\nUnderstand uses for machine learning models\nBecome familiar with key terminology (presented in bold throughout this unit)\nUnderstand differences between models\n\nSupervised vs. unsupervised\nRegression vs. classification\nOptions for statistical algorithms\nFeatures vs. predictors\n\nRelationships between:\n\nData generating processes\nStatistical algorithms\nModel flexibility\nModel interpretability\nPrediction vs. explanation\n\nUnderstand Bias-Variance Trade-off\n\nReducible and irreducible error\nWhat is bias and variance?\nWhat affects bias and variance?\nWhat is overfitting and how does it relate to bias, variance, and also p-hacking\nUse of training and test sets to assess bias and variance\n\n\nReadings\n\nPSY 752 course syllabus\n@ISL Chapter 2, pp 15 - 42\n@Yarkoni2017 paper\n\nLecture & Discussion Videos\n\nLecture 1: Overview; ~ 40 mins\nLecture 2: Terminology; ~ 19 mins\nLecture 3: An Empirical Example; ~ 28 mins\nDiscussion\n\nPost questions/discuss readings or lectures in Slack\nThe unit quiz is due by 5 pm on Wednesday January 25th"
  },
  {
    "objectID": "001_overview.html#an-introductory-framework-for-machine-learning",
    "href": "001_overview.html#an-introductory-framework-for-machine-learning",
    "title": "1  Overview of Machine Learning",
    "section": "1.2 An Introductory Framework for Machine Learning",
    "text": "1.2 An Introductory Framework for Machine Learning\nMachine (Statistical) learning techniques have developed in parallel in statistics and computer science\nTechniques can be coarsely divided into supervised and unsupervised approaches\n\nSupervised approaches involve models that predict an outcome using features\nUnsupervised approaches involve finding structure (e.g., clusters, factors) among a set of variables without any specific outcome specified\nHowever supervised approaches often use unsupervised approaches in early stages as part of feature engineering\nThis course will focus primarily on supervised machine learning problems\n\nExamples of supervised approaches include:\n\nPredicting relapse day-by-day among recovering patients with substance use disorders based on cellular communications and GPS.\nScreening someone as positive or negative for substance use disorder based on their Facebook activity\nPredicting the sale price of a house based on characteristics of the house and its neighborhood\n\nExamples of unsupervised approaches include:\n\nDetermining the factor structure of a set of personality items\nIdentifying subgroups among patients with alcohol use disorder based on demographics, use history, addiction severity, and other patient characteristics\nIdentifying the common topics present in customer reviews of some new product or app\n\n\nSupervised machine learning approaches can be categorized as either regression or classification techniques\n\nRegression techniques involve numeric (quantitative) outcomes. Regression techniques are NOT limited to regression or the general linear model. There are many more types of statistical models that are appropriate for numeric outcomes.\nClassification techniques involve nominal (categorical) outcomes.\nMost regression and classification techniques can handle categorical predictors\n\nAmong the earlier supervised model examples, predicting sale price was a regression technique and screening individuals as positive or negative for substance use disorder was a classification technique"
  },
  {
    "objectID": "001_overview.html#more-details-on-supervised-techniques",
    "href": "001_overview.html#more-details-on-supervised-techniques",
    "title": "1  Overview of Machine Learning",
    "section": "1.3 More Details on Supervised Techniques",
    "text": "1.3 More Details on Supervised Techniques\nFor supervised machine learning problems, we assume \\(Y\\) (outcome) is a function of some data generating process (DGP, f) involving a set of Xs (features) plus the addition of random error (\\(\\epsilon\\)) that is independent of X and with mean of 0\n\\(Y = f(X) + \\epsilon\\)\n\nTerminology sidebar: Throughout the course we will distinguish between the raw predictors available in a dataset and the features that are derived from those raw predictors through various transformations.\n\nWe estimate f (the DGP) for two main reasons: prediction and/or inference (i.e., explanation per Yarkoni and Westfall, 2017)\n\\(\\hat{Y} = \\hat{f}(X)\\)\nFor prediction, we are most interested in the accuracy of \\(\\hat{Y}\\) and typically treat \\(\\hat{f}\\) as a black box\nFor inference, we are typically interested in the way that \\(Y\\) is affected by \\(X\\)\n\nWhich predictors are associated with \\(Y\\)?\nWhat is the relationship between the outcome and the features associated with each predictor. Is the overall relationship between a predictor and Y positive, negative, dependent on other predictors? What is the shape of relationship (e.g., linear or more complex)?\nDoes the model as a whole improve prediction beyond a null model (no features from predictors) or beyond a compact model?\nWe care about good (low error) predictions even when we care about inference (we want small \\(\\epsilon\\))\n\nParameter estimates from models that don’t predict well may be incorrect or at least imprecise\nThey will also be tested with low power\n\n\n\nModel error includes both reducible and irreducible error.\n\nIf we consider both \\(X\\) and \\(\\hat{f}\\) to be fixed, then:\n\n\\(E(Y - \\hat{Y})^2 = (f(X) + \\epsilon - \\hat{f}(X))^2\\)\n\\(E(Y - \\hat{Y})^2 = [f(X) - \\hat{f}(X)]^2 + Var(\\epsilon)\\)\n\n\\([f(X) - \\hat{f}(X)]^2\\) is reducible and \\(Var(\\epsilon)\\) is irreducible\nIrreducible error results from other important \\(X\\) that we fail to measure and from measurement error in \\(X\\) and \\(Y\\)\nIrreducible error serves as an (unknown) bounds for model accuracy (without collecting additional Xs)\nReducible error results from a mismatch between \\(\\hat{f}\\) and the true f\n\nThis course will focus on techniques to estimate f with the goal of minimizing reducible error\n\n\n1.3.1 How Do We Estimate f?\nTo estimate f:\n\nWe need a sample of n observations of Y and X that we will call our training set\nThere are two types of statistical algorithms that we can use for \\(\\hat{f}\\):\n\nParametric algorithms\nNon-parametric algorithms\n\n\n\nParametric algorithms:\n\nFirst, make an assumption about the functional form or shape of f.\n\nFor example, the general linear model assumes: \\(f(X) = \\beta_0 + \\beta_1*X_1 + \\beta_2*X2 + ... + \\beta_p*X_p\\)\nNext, a model using that algorithm is fit to the training set. In other words, the parameter estimates (e.g., \\(\\beta_0, \\beta_1\\)) are derived to minimize some cost function (e.g., mean squared error for the linear model)\nParametric algorithms reduce the problem of estimating f down to one of only estimating some set of parameters for a chosen model\nParametric algorithms often yield more interpretable models\nBut they are often not very flexible. If you chose the wrong algorithm (shape for \\(\\hat{f}\\) that does not match f) the model will not fit well in the training set (and more importantly not in the new test set either)\n\nTerminology sidebar: A training set is a subset of your full dataset that is used to fit a model. In contrast, a validation set is a subset that has not been included in the training set and is used to select a best model from among competing model configurations. A test set is a third subset of the full dataset that has not been included in either the training or validation sets and is used for evaluating the performance of your fitted final/best model.\n\nNon-parametric algorithms:\n\nDo not make any assumption about the form/shape of f\nCan fit well for a wide variety of forms/shapes for f\nThis flexibility comes with costs\n\nThey generally require larger n in the training set than parametric algorithms to achieve comparable performance\nThey may overfit the training set (low error in training set but much higher error in new validation or test sets)\nThey are often less interpretable\n\n\n\nGenerally:\n\nFlexibility and interpretability are inversely related\nModels need to be flexible enough to fit f well\nAdditional flexibility beyond this can produce overfitting\nParametric algorithms are generally less flexible than non-parametric algorithms\nParametric algorithms can become more flexible by increasing the number of features (p from 610/710; e.g., using more predictors, more complex, non-linear forms to when deriving features from predictors)\nParametric algorithms can be made less flexible through regularization. There are techniques to make some non-parametric algorithms less flexible as well\nYou want the sweet spot for prediction. You may want even less flexible for inference in increase interpretability.\n\n\n\n\n\n1.3.2 How Do We Assess Model Performance?\nThere is no universally best statistical algorithm\n\nDepends on the true f and your goal (prediction or inference)\nWe often compare multiple statistical algorithms (various parametric and non-parametric options) and model configurations more generally (combinations of different algorithms with different sets of features)\nWhen comparing models/configurations, we need to both fit these models and then select the best one\n\n\nBest needs to be defined with respect to some performance metric in new (validation or test set) data\n\nThere are many performance metrics you might use\nRoot Mean squared error (RMSE) is common for regression problems\nAccuracy is common for classification problems\n\nWe will learn many other performance metrics in a later unit\n\nTwo types of performance problems are typical\n\nModels are biased if they don’t adequately represent the true f\n\nBiased models will perform poorly in both training and test sets\n\nModels are overfit if they are too flexible and begin to fit the noise in the training set\n\nThese models will perform well (too well actually) in the training set but poorly in test or validation sets\nThey will show high variance such that the model and its predictions change drastically depending on the training set where it is fit\n\n\n\nMore generally, these two problems are largely inversely related\n\nThis is known as the Bias-Variance trade-off\nWe previously discussed reducible and irreducible error\n\nReducible error can be parsed into components due to bias and variance\nGoal is to minimize the sum of bias and variance error (i.e., the reducible error overall)\nWe will often trade off a little bias if it provides a big reduction in variance"
  },
  {
    "objectID": "001_overview.html#key-terminology-in-context",
    "href": "001_overview.html#key-terminology-in-context",
    "title": "1  Overview of Machine Learning",
    "section": "1.4 Key Terminology in Context",
    "text": "1.4 Key Terminology in Context\nMachine learning has emerged in parallel from developments in statistics and computer science. As a result, there is a lot of terminology and often multiple terms used for the same concept. This is not my fault! However, to help us avoid confusion, I will try to use one set of terms. In the following paragraphs, I identify these key terms in context (along with other synonymous terms used by others) and highlight them in bold.\nThere are two broad approaches in machine learning - unsupervised and supervised approaches. This course focuses primarily on developing supervised models.\n\nDeveloping a supervised machine learning model to predict or explain an outcome (also called DV, label, output) typically entails:\n\nFitting models with multiple candidate model configurations\nAssessing each model and selecting a best configuration\nEvaluating how well a model with that best configuration will perform with new observations.\n\n\nCandidate model configurations can vary with respect to the statistical algorithm used. Statistical algorithms can be coarsely categorized as parametric or non-parametric. But we will mostly focus on a more granular description of the specific algorithm itself (e.g., linear model, generalized linear model, multi-level linear model, elastic net, LASSO, ridge regression, neural network, KNN, random forest).\nThe set of candidate model configurations often includes variations of the same statistical algorithm with different hyperparameter (also called tuning parameter) values that control aspects of the algorithm’s operation. For example, the hyperparameter lambda controls the degree of regularization in penalized regression algorithms such as LASSO, ridge and elastic net. We will learn more about hyperparameters and their effects later in this course.\nThe set of candidate model configurations can vary with respect to the features that are included. A recipe describes how to transform raw data for predictors (also called IVs) into features (also called regressors, inputs) that are included in the feature matrix (also called design matrix, model matrix). This process of transforming predictors into features in a feature matrix is called feature engineering.\nCrossing variation on statistical algorithms, hyperparameter values, and alternative sets of features can increase the number of candidate model configurations dramatically; developing a machine learning model can easily involve fitting thousands of model configurations. In most implementations of machine learning, the number of candidate model configurations nearly ensures that some fitted models will overfit the dataset in which they are developed such that they capitalize on noise that is unique to the dataset in which they were fit. For this reason, model configurations are assessed and selected on the basis of their relative performance for new data (observations that were not involved in the fitting process).\n\nWe have ONE full dataset but we use resampling techniques to form subsets of that dataset to enable us to assess models’ performance in new data. Cross-validation and bootstrapping are both examples of classes of resampling techniques that we will learn in this course. Broadly, resampling techniques create multiple subsets of data random samples that consist of subsets of the full dataset. These different subsets can be used for model fitting, model selection, and model evaluation.\nTraining sets are subsets that are used for model fitting (also called model training). During model fitting, models with each candidate model configuration are fit to the data in the training set. For example, during fitting, model parameters are estimated for regression algorithms, and weights are established for neural network algorithms. Some non-parametric algorithms, like k-nearest neighbors, do not estimate parameters but simply “memorize” the training sets for subsequent predictions.\nValidation sets are subsets that are used for model selection (or, more accurately, for model configuration selection). During model selection, each (fitted) model — one for every candidate model configuration — is used to make predictions for observations in a validation set that, importantly, does not overlap with the model’s training set. On the basis of each model’s performance in the validation set, the relatively best model configuration (i.e., the configuration of the model that performs best relative to all other model configurations) is identified and selected. If you have only one model configuration, validation set(s) are not needed because there is no need to select among model configurations.\nTest sets are subsets that are used for model evaluation. Generally, a model with the previously identified best configuration is re-fit to all available data other than the test set. This fitted model is used to predict observations in the test set to estimate how well this model is expected to perform for new observations."
  },
  {
    "objectID": "001_overview.html#an-example-of-overfitting-and-the-bias-variance-trade-off",
    "href": "001_overview.html#an-example-of-overfitting-and-the-bias-variance-trade-off",
    "title": "1  Overview of Machine Learning",
    "section": "1.5 An Example of Overfitting and the Bias-Variance Trade-off",
    "text": "1.5 An Example of Overfitting and the Bias-Variance Trade-off\n\n1.5.1 Overview of Example\nThe concepts of overfitting and the bias-variance trade-off are critical to understand\n\nIt is also important to understand how model flexibility can affect both the bias and variance of that model’s performance\nIt can help to make these abstract concepts concrete by exploring real models that are fit in actual data\nWe will conduct a very simple simulation to demonstrate these concepts\n\nThe code in this example is secondary to understanding the concepts of overfitting, bias, and the bias-variance trade-off\n\nWe will not display much of it so that you can maintain focus on the concepts\nYou will have plenty of time to learn the underlying\n\n\nWhen modeling, our goal is typically to approximate the data generating process (DGP) as close as possible, but in the real world we never know the true DGP.\nA key advantage of many simulations is that we do know the DGP because we define it ourselves.\n\nFor example, in this simulation, we know that y is a cubic function of x and noise (random error).\nIn fact, we know the exact equation for calculating y as a function of x.\n\n\\(y = 1100 - 4.0 * x - 0.4 * x^2 + 0.1 * (x - h)^3 + noise\\), where:\n\nb0 = 1100\nb1 = -4.0\nb2 = -0.4\nb3 = 0.1\nh = -20.0\nnoise has mean = 0 and sd = 150\n\n\n\nWe will attempt to model this cubic DGP with three different model configurations\n\nA simple linear model that uses only x as a feature\nA (20th order) polynomial linear model that uses 20 polynomials of x as features\nA (20th order) polynomial LASSO model that uses the same 20 polynomials of x as features but “regularizes” to remove unimportant features from the model\n\nr-colorize(\"If the DGP for y is a cubic function of x, what do we know about the expected bias for our three candidate model configurations in this example?\", \"red\") 1\n\n\n1.5.2 Stimulation Steps\nWith that introduction complete, lets start our simulation of the bias-variance trade-off\n\nWe get four random samples of training data to fit models\n\n\nIn other words, we are simulating four separate studies where the researcher in each study is trying to develop a prediction model for y using a sample of training data\nHere are plots of these four simulated training sets with a dotted line for the data generating process (DGP)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe get one more large random sample (N = 5000) with the same DGP to use as a test set to evaluate all the models that will be fit in the training sample across all of the simulations.\n\n\n\n\n\n\n\n\nWe fit the three model configurations in each of the four simulated training sets\nWe use the resulting models to make predictions for observations in the same training set in which they were fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr-colorize(\"Look in any training set.  Can you see evidence of bias for any model configuration?\", \"red\") 2\nr-colorize(\"Can you see any evidence of overfitting for any model configuration?\", \"red\") 3\n\n\nWe use these same models (3 model configurations fit to 4 different training sets) to make predictions for new observations in the test set\n\n\nRemember that the test set has NEW observations of x and y that weren’t used for fitting any of the models.\nLets look at each model configurations performance in test separately\n\n\n\nHere are predictions from the four simple linear models (fit in each of the four training sets) in the test set\n\n\n\n\n\n\nr-colorize(\"Can you see evidence of bias for the simple linear models?\", \"red\") 4\nr-colorize(\"How much variance across the simple linear models is present?\", \"red\") 5\n\n\nHere are predictions from the four polynomial linear models in the test set\n\n\n\n\n\n\nr-colorize(\"Are they systematically biased?\", \"red\") 6\nr-colorize(\"How does the variance of these models compare to the variance of the simple linear models?\", \"red\") 7\nr-colorize(\"How does this demonstrate the connection between model overfitting and model variance?\", \"red\") 8\n\n\nHere are predictions from the four polynomial LASSO models in the test set\n\n\n\n\n\n\nr-colorize(\"How does their bias compare to the simple and polynomial linear models?\", \"red\") 9\nr-colorize(\"How does their variance compare to the simple and polynomial linear models?\", \"red\") 10\n\n\nNow we will quantify the performance of these models in training and test sets with the root mean square error performance metric. This is the standard deviation of the error when comparing the predicted values for y to the actual values (ground truth) for y.\n\nr-colorize(\"What do we expect about RMSE for the three models in train and test?\", \"red\") 11\n\nTo better understand this:\n\nCompare RMSE across the three model configurations within the training sets (turquoise line)\nCompare how RMSE changes for each model configuration across its training set and the test set\nCompare RMSE across the three model configurations within the test set (red line)?\nSpecifically compare the performance of simple linear model (least flexible) with the polynomial linear model (most flexible)\n\n\n\n\n\n\n\nr-colorize(\"Would these observations about bias and variance of these three model configurations always be the same regardless of the DGP?\", \"red\") 12"
  },
  {
    "objectID": "001_overview.html#discussion---tuesday",
    "href": "001_overview.html#discussion---tuesday",
    "title": "1  Overview of Machine Learning",
    "section": "1.6 Discussion - Tuesday",
    "text": "1.6 Discussion - Tuesday\n\n1.6.1 Course Overview\n\nIntroductions (preferred name, pronouns, program/department and year)\nStructure of course\n\nSame flow each week\n\nWeek starts on Thursdays at 12:15 pm\nAssignments include:\n\nPre-recorded lectures\nWeb book material\nReadings (@ISL and other sources; All free)\nApplication assignment\n\nAsychronous discussion and questions on Slack\nLab section on Tuesdays at 11:00 am - address previous weeks code\nQuiz and application assignments due Wednesdays at 8 pm\nWrap-up discussion and conceptual questions on Thursdays at 11 am. Not a lecture\n\nSelf-paced (except for due dates and discussion)\nOffice hours\n\nJohn - Thursdays, 1 – 2 pm\nMatt - Tuesdays 1:30 - 2:30 pm\nSarah - Mondays 11 - 12 pm\npersonal appointments & Slack\n\n\nThe web book\n\nPrimary source for all course materials\nOrganized by units (with syllabus at front)\nLinks to pre-recorded lectures, readings, and quiz\nProvides primary source for code examples (all you need for this course)\nLecture follows book\n\nCourse as guided learning\n\nConcepts in lectures and readings\nApplications in web book and application assignment\nDiscussion section is discussion/questions (not pre-planned lecture)\nSlack CAN be a good source for discussion and questions as well\nGrades are secondary (quizzes, application assignments, exams)\n\nWhy these tools?\n\nrmarkdown?\n\nTool for collaboration\nInteractive with static product (knit html)\nApplication assignments & web book (bookdown - a superset of R Markdown)\n\ntidyverse?\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures\nUnmatched for data wrangling, EDA, and data visualization\n\ntidymodels?\n\nConsistent interface to hundreds of statistical algorithms\nUnder VERY active development (young but maturing)\nWell-supported and documented (tidymodels.org)\nBe careful with other web documentation\nData wrangling in dplyr vs. recipes\n\n\nWhy me?\n\nPrimary tools in our research program\nModel for progression from 610/710\nCan do AND can collaborate with CS and expert data scientists\n\nEnvironment\n\nSafe and respectful\nVERY encouraging of questions, discussion, and tangents\nHave fun\nAccomodations and Complaints\n\nChatGPT\n\nYay! May develop into an amazing tool in your workflow\nUse as tool (like Stack Overflow) for applications (application assignments, application questions on exams)\nCheck carefully - it can be wrong even when it looks right\nDo NOT use for conceptual questions (quizzes, conceptual exam questions). This type of info needs to be in your head to be effective data scientist.\n\nAcademic Integrity\n\nDo not cheat! Only you lose.\nNo collaboration with classmates, peers, previous students on anything graded\nAll cheating reported to Department and Dean of Students. If homework or quizzes, zero on all them because I can’t trust them. If exam, zero on exam.\n\n\n\n\n1.6.2 Association vs. Prediction\n\nr-colorize(\"What is the difference between association vs. prediction?\", \"red\") 13\nMuch research in psychology demonstrates association but calls it prediction!\nAssociation (sometimes substantially) overestimates the predictive strength of our models\n\nCoefficients are derived to mimimize SSE (or maximize R2)\nR2 from GLM (using one sample) indexes how well on average any GLM that is fit to a sample will account for variance in that sample when specific coefficients are estimated in the same sample they are evaluated\nR2 does NOT tell you how well a specific GLM (including its coefficients) will work with new data for prediction\nR2 itself is positively biased even as estimate of how well a sample specific GLM will predict in that sample (vs. adjusted R2 and other corrections)\n\n\n\n\n1.6.3 Prediction vs. Explanation\n\nr-colorize(\"Examples of valuable prediction without explanation?\", \"red\")\nr-colorize(\"Can you have explanation without prediction?\", \"red\")\nPrediction models can provide insight or tests of explanatory theories (e.g., do causes actually predict in new data; variable importance)\nGoal of scientific psychology is to understand human behavior. It involves both explaining behavior (i.e., identifying causes) and predicting (yet to be observed) behaviors.\n\nWe overemphasize explanatory goals in this department, IMO\n\n\n\n\n1.6.4 Data Generating Process\n\n\\(Y = f(X) + \\epsilon\\)\nboth function and Xs are generally unknown\n\n\n\n1.6.5 Bias, Overfitting, and Variance\n\nWhat are they and how are they related?\nr-colorize(\"Describe problem of p-hacking with respect to overfitting?\", \"red\") 14\n\n\n\n1.6.6 Cross Validation\n\nr-colorize(\"What is it?\", \"red\")\nr-colorize(\"How are replication and cross-validation different?\", \"red\")"
  },
  {
    "objectID": "001_overview.html#discussion---thursday",
    "href": "001_overview.html#discussion---thursday",
    "title": "1  Overview of Machine Learning",
    "section": "1.7 Discussion - Thursday",
    "text": "1.7 Discussion - Thursday\n\n1.7.1 Housekeeping\n\nQuizzes - scoring, submission deadline, feedback, use of Q10\nNew unit - reading, lectures, application assignment, quiz\nCourse load - comparable to 610/710\n\n\n\n1.7.2 Basic framework and terminology\n\nr-colorize(\"Supervised vs. unsupervised machine learning?\", \"red\")\nr-colorize(\"Supervised regression vs classification?\", \"red\")\n\n\n\nr-colorize(\"What is a data generating process?\", \"red\")\n\n\\(Y = f(X) + \\epsilon\\)\nBoth function and Xs are generally unknown\n\\(\\hat{Y} = \\hat{f}(X)\\)\n\nr-colorize(\"Why do we estimate the data generating process?\", \"red\")\n\n\nReducible vs. Irreducible error?\n\nOur predictions will have error\nYou learned to estimate parameters in the GLM to mimimize error in 610\nBut error remained non-zero (in your sample and more importantly with same model in new samples) unless you perfectly estimated the DGP\nThat error can be divided into two sources\nIrreducible error comes from measurement error in X, Y and missing X because predictors (causes) not measured.\n\nIrreducible without collecting new predictors and/or with new measures\nIt places a ceiling on the performance of the best model you can develop with your available data\n\nReducible error comes from mismatch between \\(\\hat{f}(X)\\) and the true \\(f(X)\\).\n\nWe can reduce this without new data.\n\nJust need better model (\\(\\hat{f}(X)\\)).\n\nYou didn’t consider this (much) in 610 because you were limited to one statistical algorithm (GLM) AND it didnt have hyperparameters.\nYou did reduce error by coding predictors (feature engineering) differently (interactions \\(X1*X2\\), polynomial terms \\(X^2\\), power transformations of X) This course will teach you methods to decrease reducible error (and validly estimate total error of that best model)\n\n\n\n\nr-colorize(\"What are the **three general steps** by which we estimate and evaluate the data generating process with a sample of data?  Lets use all this vocabulary!\", \"red\")\n\nCandidate model configurations\n\nStatistical algorithms\nHyperparameters\nFeatures (vs. predictors?), feature matrix, feature engineering, recipe (tidymodels specific)\n\nModel fitting (training), selection, and evaluation\nResampling techniques\n\ncross validation techniques (k-fold)\nboostrapping\n\nTraining, validation, and test sets (terms vary in literature!)\n\n\n\n\n1.7.3 Bias-variance tradeoff\n\nWhat is bias, overfitting, and variance?\nBias and variance are general concepts to understand during any estimation process\n\nEstimate mean, median, standard deviation\nParameter estimates in GLM\nEstimate DGP - \\(\\hat{Y} = \\hat{f}(X)\\)\n\n\n\nUseful conceptual examples of bias-variance\n\nExample 1: Darts from @Yarkoni2017\n\n\n\nExample 2: Models (e.g. many scales made by Acme Co) to measure my weight\n\n\n1.7.3.1 Bias\nBiased models are generally less complex models than the data-generating process for your outcome\n\nBiased models lead to errors in prediction because the model will systematically over- or under-predict outcomes (scores or probabilities) for specific values of predictor(s) (bad for prediction goals!)\nParameter estimates from biased models may over or under-estimate the true effect of a predictor (bad for explanatory goals!)\n\n\nr-colorize(\"Are GLMs biased models?\", \"red\") 15\n\nBias seems like a bad thing.\n\nBoth bias and variance (due to overfitting) are sources of (reducible) prediction errors (and imprecise/inaccurate parameter estimates). They are also often inversely related (i.e., the trade-off).\nA model configuration needs to be flexible enough to represent the true DGP.\n\nAny more flexibility will lead to overfitting.\n\nAny less flexibility will lead to bias.\n\nIdeally, if your model configuration is perfectly matched to the DGP, it will have very low bias and very low variance (holding other factors like N constant)\nThe world is complex. In many instances,\n\nWe can’t perfectly represent the DGP\nWe trade off a little bias for big reduction in variance to produce the most accurate predictions (and stable parameter estimates across samples for explanatory goals)\nOr we trade off a little variance (slightly more flexible model) to get a big reduction in bias\nEither way, we get models that predict well and may be useful for explanatory goals\n\n\n\n\n1.7.3.2 Overfitting - Variance\nr-colorize(\"Consider example of p = n - 1 in general linear model.  What happens in this situation?  How is this related to overfitting and model flexibility?\", \"red\") 16\nFactors that increase overfitting\n\nSmall N\nComplex models (e.g, many predictors, p relative to n, non-parametric models)\nWeak effects of predictors (lots of noise available to “overfit”)\nCorrelated predictors (for some algorithms like the GLM)\nChoosing between many model configurations (e.g. different predictors or predictor sets, transformations, types of statistical models) - lets return to this when we consider p-hacking\n\n\nYou might have noticed that many of the above factors contribute to the standard error of a parameter estimate/model coefficient from the GLM\n\nSmall N\nBig p\nSmall R^2^ (weak effects)\nCorrelated predictors\n\nThe standard error increases as model overfitting increases due to these factors\nr-colorize(\"Explain the link between model variance/overfitting, standard errors, and sampling distributions?\", \"red\") 17\n\nr-colorize(\"Describe problem of p-hacking with respect to overfitting?\", \"red\") 18\n\nParameter estimates from an overfit model are specific to the sample within which they were trained and are not true for other samples or the population as a whole\n\nParameter estimates from overfit models have big (TRUE) SE and so they may be VERY different in other samples\n\nThough if the overfitting is due to fitting many models, it won’t be reflected in the SE from any one model because each model doesn’t know the other models exist! p-hacking!!\n\nWith traditional (one-sample) statistics, this can lead us to incorrect conclusions about the effect of predictors associated with these parameter estimates (bad for explanatory goals!).\nIf the parameter estimates are very different sample to sample (and different from the true population parameters), this means the model will predict poorly in new samples (bad for prediction goals!). We fix this by using resampling to evaluate model performance."
  },
  {
    "objectID": "001_overview.html#footnotes",
    "href": "001_overview.html#footnotes",
    "title": "1  Overview of Machine Learning",
    "section": "",
    "text": "The simple linear model will be biased b/c it can only represent y as a linear function of x. The two polynomial models will be generally unbiased b/c they have x represented with 20th order polynomials. LASSO will be slightly biased due to regularization but more on that in a later unit↩︎\nThe simple linear model is clearly biased. It systemically underestimates Y in some portions of the X distribution and overestimates Y in other portions of the X distribution.\nThis is true across training sets.↩︎\nThe polynomial linear model appears to overfit the data in the training set. In other words, it seems to follow both the signal/DGP and the noise. However, you can’t be certain of this with only one training set and without knowing the DGP.\nIt is possible that the wiggles in the prediction line represent the real DGP.\nYou will need a test set to be certain about the degree of overfitting.↩︎\nYes, consistent with what we saw in the training sets, the simple linear model systematically overestimates Y in some places and underestimates it in others. The DGP is clearly NOT linear but this simple model can only make linear predictions. It is a fairly biased model.\nThis bias will make a large contribution to the reducible error of the model↩︎\nThere is not much variance in the prediction lines across the models that were fit to different training sets. The slopes are very close across models and the intercepts only vary by a small amount. The simple linear model configuration does not appear to have high variance and model variance will not contribute much to its reducible error.↩︎\nThere is not much systematic bias. The overall function is generally cubic as is the DGP.\nBias will not contribute much to the model’s reducible error.↩︎\nThere is much higher model variance for this polynomial linear model relative to the simple linear model. Although all four models generally predict Y as a cubic function of X, there is also a non-systematic wiggle that is different for each of the models.↩︎\nModel variance is a result of overfitting to the training set. If a model fits noise in its training set, that noise will be different in every dataset. Therefore, you end up with different models depending on the training set in which they are fit. And none of those models will do well with new data as you can see in this test set because noise is random and different in each dataset.↩︎\nIt has low bias much like the polynomial linear model. It is able to capture the true cubic DGP fairly well. The regularization process slightly reduced the magnitude of the cubic (the prediction line is a little straighter than it should be) but not by much.↩︎\nIt has low variance, much like the simple linear model. All four models, fit in different training sets resulted in very similar prediction lines.↩︎\nThe simple linear model is biased and won’t even fit train for this reason. However, it’s not very flexible so it won’t be overfit to train and therefore should fit comparably in test. The polynomial linear model won’t be biased at all given that the DGP is polynomial. However, it is overly flexible (20th order) and so will substantially overfit the training data such that it will show high variance and its performance will be poor in test. The polynomial LASSO will be the sweet spot in bias-variance trade-off. It has a little bias but not much. However, it is not as flexible due to regularization by lambda so it won’t be overfit to its training set. Therefore, it should do well in the test set.↩︎\nNo. A model configuration needs to be flexible enough and/or well designed to represent the DGP for the data that you are modeling. The two polynomial models in this example were each able to represent a cubic DGP. The simple linear model was not. The polynomial linear model was too flexible for a cubic given that it had 20 polynomials of X. Therefore, it was overfit to its training set and had high variance. However, if the DGP was a different shape, the story would be different. If the DGP was linear the simple linear model would not have been biased and would have performed best. If this DGP was some other form (step function), it may be that none of the models would work well.↩︎\nAssociation quantifies the relationship between variables within a sample (predictors-outcome). Prediction requires using an established model to predict (future?) outcomes for new (“out-of-sample,”held-out”) participants. ↩︎\nWhen you p-hack, you are overfitting the training set (your sample). You try out many, many different model configurations and choose the one you like best rather than what works well in new data. This model likely capitalizes on noise in your sample. It won’t fit well in another sample. In other words, your conclusions are not linked to the true DGP and would be different if you used a different sample. In a different vein, your significance test is wrong. The SE does not reflect the model variance that resulted from testing many different configurations b/c your final model didn’t “know” about the other models. Statistically invalid conclusion!↩︎\n“GLM parameter estimates are BLUE - best linear unbiased estimators. Parameter estimates from any sample are unbiased estimates of the linear model coefficients for population model but if DGP is not linear, this linear model will produce biased predictions and have biased parameter estimates.↩︎\nThe model will perfectly fit the sample data even when there is no relationship between the predictors and the outcome. e.g., Any two points can be fit perfectly with one predictor (line), any three points can be fit perfectly with two predictors (plane). This model will NOT predict well in new data. This model is overfit because n-1 predictors is too flexible for the linear model↩︎\nAll parameter estimates have a sampling distribution. This is the distribution of estimates that you would get if you repeatedly fit the same model to new samples. When a model is overfit, that means that aspects of the model (its parameter estimates, its predictions) will vary greatly from sample to sample. This is represented by a large standard error (the SD of the sampling distribution) for the model’s parameter estimates. It also means that the predictions you will make in new data will be very different depending on the sample that was used to estimate the parameters.↩︎\nWhen you p-hack, you are overfitting the training set (your sample). You try out many, many different model configurations and choose the one you like best rather than what works well in new data. This model likely capitalizes on noise in your sample. It won’t fit well in another sample. In other words, your conclusions are not linked to the true DGP and would be different if you used a different sample. In a different vein, your significance test is wrong. The SE does not reflect the model variance that resulted from testing many different configurations b/c your final model didn’t “know” about the other models. Statistically invalid conclusion!↩︎"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "20  Test",
    "section": "",
    "text": "demo &lt;- function(x)\nOr we can use\n\\(Y\\)\nY"
  }
]