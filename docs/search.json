[
  {
    "objectID": "002_exploratory_data_analysis.html#overview-of-unit",
    "href": "002_exploratory_data_analysis.html#overview-of-unit",
    "title": "2  Exploratory Data Analysis",
    "section": "2.1 Overview of Unit",
    "text": "2.1 Overview of Unit\n\n2.1.1 Learning Objectives\n\nStages of Analysis\nBest practices for data storage, variable classing, data dictionaries\nProblems and solutions regarding data leakage\nKey goals and techniques cleaning EDA\n\nTidying names and response labels\nAppropriate visualizations based on variable class\nSummary statistics based on variable class\n\nProper splitting for training/validation and test sets\nKey goals and techniques modeling EDA\n\nAppropriate visualizations based on variable class\nSummary statistics based on variable class\n\nIntroductory use of recipes for feature engineering\n\n\n\n\n2.1.2 Readings\n[NOTE: These are short chapters. You are reading to understand the framework of visualizing data in R. Don’t feel like you have to memorize the details. These are reference materials that you can turn back to when you need to write code!]\n\nWickham, Çetinkaya-Rundel, and Grolemund (2023) Chapter 1, Data Visualization\nWickham, Çetinkaya-Rundel, and Grolemund (2023) Chapter 9, Layers\nWickham, Çetinkaya-Rundel, and Grolemund (2023) Chapter 10, Exploratory Data Analysis\n\n\n\n2.1.3 Lecture Videos\n\nLecture 1: Stages of Data Analysis and Model Development ~ 10 mins\nLecture 2: Best Practices and Other Recommendations ~ 27 mins\nLecture 3: EDA for Data Cleaning ~ 41 mins\nLecture 4: EDA for Modeling - Univariate ~ 24 mins\nLecture 5: EDA for Modeling - Bivariate ~ 20 mins\nLecture 6: Working with Recipes ~ 13 mins\n\n\nDiscussion\n\nPost questions or discuss readings or lectures in Slack\n\n\n\n2.1.4 Application Assignment\n\ndata\ndata dictionary\ncleaning EDA: qmd\nmodeling EDA: qmd\nsolutions: knit cleaning EDA; knit modeling EDA\n\nNote: the qmd files may not be viewable but can be downloaded through your browser (e.g., right-click to save)\nSubmit the application assignment by 8 pm on Wednesday, January 31st\n\n\n2.1.5 Quiz\nSubmit the unit quiz by 8 pm on Wednesday, January 31st"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#overview-of-exploratory-data-analysis",
    "href": "002_exploratory_data_analysis.html#overview-of-exploratory-data-analysis",
    "title": "2  Exploratory Data Analysis",
    "section": "2.2 Overview of Exploratory Data Analysis",
    "text": "2.2 Overview of Exploratory Data Analysis\n\n2.2.1 Stages of Data Analysis and Model Development\nThese are the main stages of data analysis for machine learning and the data that are used\n\nEDA: Cleaning (full dataset)\nEDA: Split data into training, validation and test set(s)\nEDA: Modeling (training sets)\nModel Building: Feature engineering (training sets)\nModel Building: Fit many models configurations (training set)\nModel Building: Evaluate many models configurations (validation sets)\nFinal Model Evaluation: Select final/best model configuration (validation sets)\nFinal Model Evaluation: Fit best model configuration (use both training and validation sets)\nFinal Model Evaluation: Evaluate final model configuration (test sets)\nFinal Model Evaluation: Fit best model configuration to ALL data (training, validation, and test sets) if you plan to use it for applications.\n\n\nThe earlier stages are highly iterative:\n\nYou may iterate some through EDA stages 1-3 if you find further errors to clean in stage 3 [But make sure you resplit into the same sets]\nYou will iterate many times though stages 3-6 as you learn more about your data both through EDA for modeling and evaluating actual models in validation\n\nYou will NOT iterate back to earlier stages after you select a final model configuration\n\nStages 7 - 10 are performed ONLY ONCE\nOnly one model configuration is selected and re-fit and only that model is brought into test for evaluation\nAny more than this is essentially equivalent to p-hacking in traditional analyses\nStep 10 only happens if you plan to use the model in some application"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#best-practices-and-other-recommendations",
    "href": "002_exploratory_data_analysis.html#best-practices-and-other-recommendations",
    "title": "2  Exploratory Data Analysis",
    "section": "2.3 Best Practices and Other Recommendations",
    "text": "2.3 Best Practices and Other Recommendations\n\n2.3.1 Data file formats\nWe generally store data as CSV [comma-separated value] files\n\nEasy to view directly in a text editor\nEasy to share because others can use/import into any data analysis platform\nWorks with version control (e.g. git, svn)\nuse read_csv() and write_csv()\n\nExceptions include:\n\nWe may consider binary (.rds) format for very big files because read/write can be slow for csv files.\n\nBinary file format provides a very modest additional protection for sensitive data (which we also don’t share)\nuse read_rds() and write_rds()\n\nSee chapter 7 - Data Import in Wickham, Çetinkaya-Rundel, and Grolemund (2023) for more details and advanced techniques for importing data using read_csv()\n\n\n\n2.3.2 Classing Variables\nWe store and class variables in R based on their data type (level of measurement).\n\nSee Wikipedia definitions for levels of measurement for a bit more precision that we will provide here.\n\nCoarsely, there are four levels:\n\nnominal: qualitative categories, no inherent order (e.g., marital status, sex, car color)\nordinal: qualitative categories (sometimes uses number), inherent order but not equidistant spacing (e.g., Likert scale; education level)\ninterval and ratio (generally treated the same in social sciences): quantitative scores, ordered, equidistant spacing. Ratio has true 0. (e.g., temperature in Celsius vs. Kelvin scales)\n\nWe generally refer to nominal and ordinal variables as categorical and interval/ratio as quantitative or numeric\n\nFor nominal variables\n\nWe store (in csv files) these variables as character class with descriptive text labels for the levels\n\nEasier to share/document\nReduces errors\n\nWe class these variables in R as factors when we load them (using read_csv())\nIn some cases, we should pay attention to the order of the levels of the variable. e.g.,\n\nFor a dichotomous outcome variable, the positive/event level of dichotomous factor outcome should be first level of the factor\nThe order of levels may also matter for factor predictors (e.g., step_dummy() uses first level as reference).\n\n\n\nFor ordinal variables:\n\nWe store (in csv files) these variables as character class with descriptive text labels for the levels\n\nEasier to share/document\nReduces errors\n\nWe class these variables in R as factors (just like nominal variables)\n\nIt is easier to do EDA with these variables classes as factors\nWe use standard factors (not ordered)\n\nConfirm that the order of the levels is set up correctly. This is very important for ordinal variables.\nDuring feature engineering stage, we can then either\n\nTreat as a nominal variable and create features using step_dummy()\nTreat as an interval variable using step_ordinalscore()\n\n\nSimilar EDA approaches are used with both nominal and ordinal variable\nOrdinal variables may show non-linear relations b/c they may not be evenly spaced. In these instances, we can use feature engineering approaches that are also used for nominal variables\n\nFor interval and ratio variables:\n\nWe store these variables as numeric\nWe class these variables as numeric (either integer or double - let R decide) during the read and clean stage (They are typically already in this class when read in)\n\nSimilar EDA approaches are used with both interval and ratio variables\nSimilar feature engineering approaches are used with both\n\n\n\n2.3.3 Data Dictionaries\nYou should always make a data dictionary for use with your data files.\n\nIdeally, these are created during the planning phase of your study, prior to the start of data collection\nStill useful if created at the start of data analysis\n\nData dictionaries:\n\nhelp you keep track of your variables and their characteristics (e.g., valid ranges, valid responses)\ncan be used by you to check your data during EDA\ncan be provided to others when you share your data (data are not generally useful to others without a data dictionary)\n\nWe will see a variety of data dictionaries throughout the course. Many are not great as you will learn.\n\n\n\n2.3.4 The Ames Housing Prices Dataset\nWe will use the Ames Housing Prices dataset as a running example this unit (and some future units and application assignments as well)\n\nYou can read more about the original dataset created by Dean DeCock\nThe data set contains data from home sales of individual residential property in Ames, Iowa from 2006 to 2010\nThe original data set includes 2930 observations of sales price and a large number of explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous)\nThis is the original data dictionary\nThe challenge with this dataset is to build the best possible prediction model for the sale price of the homes.\n\n\n\n\n2.3.5 Packages and Conflicts\nFirst, lets set up our environment with functions from important packages. I strongly recommend reviewing our recommendations for best practices regarding managing function conflicts now. It will save you a lot of headaches in the future.\n\nWe set a conflicts policy that will produce errors if we have unanticipated conflicts.\nWe source a library of functions that we use for common tasks in machine learning.\n\nThis includes a function (tidymodels_conflictRules()) that sets conflict rules to allow us to attach tidymodels functions without conflicts with tidyverse functions.\n\nYou can review that function to see what it does (search for that function name at the link)\n\nThen we use that function\n\n\noptions(conflicts.policy = \"depends.ok\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\ntidymodels_conflictRules()\n\n\nNext we load packages for functions that we will use regularly. There are five things to note RE best practices\n\nIf we will use a lot of functions from a package (e.g., tidyverse, tidymodels), we attach the full package\nIf we will use only several functions from a package (but plan to use them repeatedly), we use the include.only parameter to just attach those functions.\nAt times, if we plan to use a single function from a package only 1-2x times, we may not even attach that function at all. Instead, we just call it using its namespace (i.e. packagename::functionname)\nIf a package has a function that conflicts with our primary packages and we don’t plan to use that function, we load the package but exclude the function. If we really needed it, we can call it with its namespace as per option 3 above.\nPay attention to conflicts that were allowed to make sure you understand and accept them. (I left the package messages and warnings in the book this time to see them. I will hide them to avoid cluttering book in later units but you should always review them.)\n\n\nlibrary(janitor, include.only = \"clean_names\") # <1>\nlibrary(cowplot, include.only = \"plot_grid\") # <2> \nlibrary(kableExtra, exclude = \"group_rows\") # <3> \nlibrary(tidyverse) \nlibrary(tidymodels) # <4>\n\n\nAs an alternative, we could have skipped loading the package and instead called the function as janitor::clean_names()\nSame is true for cowplot package\nWhen loading kableExtra (which we use often), you will always need to exclude groups_rows() to prevent a conflict with dplyr package in the tidyverse\nLoading tidymodels will produce conflicts unless you source and call my function tidymodels_conflictRules() (see above)\n\n\n\n\n2.3.6 Source and Other Environment Settings\nWe will also source (from github) two other libraries of functions that we use commonly for exploratory data analyses. You should review these function scripts (fun_eda.R; fun_plots.R to see the code for these functions.\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\n\nFinally, we tune our environment a bit more by setting plot themes and print options that we prefer\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf, tibble.print_max = Inf)\n\nAnd we set a relative path to our data. This assumes you are using an RStudio project with the path to the data relative to that project file. I’ve provided more detail elsewhere on best practices for managing files and paths.\n\npath_data <- \"data\"\n\n\n\n\n2.3.7 Read and Glimpse Dataframe\nLets read in the data and glimpse the subset of observations we will work with in Units 2-3 and the first two application assignments.\n\ndata_all <- read_csv(here::here(path_data, \"ames_raw_class.csv\"),   # <1>\n                     col_types = cols()) |> # <2> \n  glimpse() # <3>\n\nRows: 1,955\nColumns: 81\n$ PID               <chr> \"0526301100\", \"0526350040\", \"0526351010\", \"052710501…\n$ `MS SubClass`     <chr> \"020\", \"020\", \"020\", \"060\", \"120\", \"120\", \"120\", \"06…\n$ `MS Zoning`       <chr> \"RL\", \"RH\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\"…\n$ `Lot Frontage`    <dbl> 141, 80, 81, 74, 41, 43, 39, 60, 75, 63, 85, NA, 47,…\n$ `Lot Area`        <dbl> 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, …\n$ Street            <chr> \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pav…\n$ Alley             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Lot Shape`       <chr> \"IR1\", \"Reg\", \"IR1\", \"IR1\", \"Reg\", \"IR1\", \"IR1\", \"Re…\n$ `Land Contour`    <chr> \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"HLS\", \"Lvl\", \"Lv…\n$ Utilities         <chr> \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"A…\n$ `Lot Config`      <chr> \"Corner\", \"Inside\", \"Corner\", \"Inside\", \"Inside\", \"I…\n$ `Land Slope`      <chr> \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gt…\n$ Neighborhood      <chr> \"NAmes\", \"NAmes\", \"NAmes\", \"Gilbert\", \"StoneBr\", \"St…\n$ `Condition 1`     <chr> \"Norm\", \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"No…\n$ `Condition 2`     <chr> \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Nor…\n$ `Bldg Type`       <chr> \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"TwnhsE\", \"TwnhsE\", …\n$ `House Style`     <chr> \"1Story\", \"1Story\", \"1Story\", \"2Story\", \"1Story\", \"1…\n$ `Overall Qual`    <dbl> 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6…\n$ `Overall Cond`    <dbl> 5, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 2, 5, 6, 6…\n$ `Year Built`      <dbl> 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993…\n$ `Year Remod/Add`  <dbl> 1960, 1961, 1958, 1998, 2001, 1992, 1996, 1999, 1994…\n$ `Roof Style`      <chr> \"Hip\", \"Gable\", \"Hip\", \"Gable\", \"Gable\", \"Gable\", \"G…\n$ `Roof Matl`       <chr> \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg…\n$ `Exterior 1st`    <chr> \"BrkFace\", \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"CemntBd…\n$ `Exterior 2nd`    <chr> \"Plywood\", \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"CmentBd…\n$ `Mas Vnr Type`    <chr> \"Stone\", \"None\", \"BrkFace\", \"None\", \"None\", \"None\", …\n$ `Mas Vnr Area`    <dbl> 112, 0, 108, 0, 0, 0, 0, 0, 0, 0, 0, 0, 603, 0, 350,…\n$ `Exter Qual`      <chr> \"TA\", \"TA\", \"TA\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"TA\", \"TA\"…\n$ `Exter Cond`      <chr> \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\"…\n$ Foundation        <chr> \"CBlock\", \"CBlock\", \"CBlock\", \"PConc\", \"PConc\", \"PCo…\n$ `Bsmt Qual`       <chr> \"TA\", \"TA\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\"…\n$ `Bsmt Cond`       <chr> \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\"…\n$ `Bsmt Exposure`   <chr> \"Gd\", \"No\", \"No\", \"No\", \"Mn\", \"No\", \"No\", \"No\", \"No\"…\n$ `BsmtFin Type 1`  <chr> \"BLQ\", \"Rec\", \"ALQ\", \"GLQ\", \"GLQ\", \"ALQ\", \"GLQ\", \"Un…\n$ `BsmtFin SF 1`    <dbl> 639, 468, 923, 791, 616, 263, 1180, 0, 0, 0, 637, 36…\n$ `BsmtFin Type 2`  <chr> \"Unf\", \"LwQ\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Un…\n$ `BsmtFin SF 2`    <dbl> 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0, 0, 0, 0,…\n$ `Bsmt Unf SF`     <dbl> 441, 270, 406, 137, 722, 1017, 415, 994, 763, 789, 6…\n$ `Total Bsmt SF`   <dbl> 1080, 882, 1329, 928, 1338, 1280, 1595, 994, 763, 78…\n$ Heating           <chr> \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"Gas…\n$ `Heating QC`      <chr> \"Fa\", \"TA\", \"TA\", \"Gd\", \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"Gd\"…\n$ `Central Air`     <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n$ Electrical        <chr> \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\"…\n$ `1st Flr SF`      <dbl> 1656, 896, 1329, 928, 1338, 1280, 1616, 1028, 763, 7…\n$ `2nd Flr SF`      <dbl> 0, 0, 0, 701, 0, 0, 0, 776, 892, 676, 0, 0, 1589, 67…\n$ `Low Qual Fin SF` <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Gr Liv Area`     <dbl> 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655,…\n$ `Bsmt Full Bath`  <dbl> 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0…\n$ `Bsmt Half Bath`  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Full Bath`       <dbl> 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 1, 1, 2, 2…\n$ `Half Bath`       <dbl> 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0…\n$ `Bedroom AbvGr`   <dbl> 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 4, 4, 1, 2, 3, 3…\n$ `Kitchen AbvGr`   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ `Kitchen Qual`    <chr> \"TA\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"TA\"…\n$ `TotRms AbvGrd`   <dbl> 7, 5, 6, 6, 6, 5, 5, 7, 7, 7, 5, 4, 12, 8, 8, 4, 7, …\n$ Functional        <chr> \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Ty…\n$ Fireplaces        <dbl> 2, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1…\n$ `Fireplace Qu`    <chr> \"Gd\", NA, NA, \"TA\", NA, NA, \"TA\", \"TA\", \"TA\", \"Gd\", …\n$ `Garage Type`     <chr> \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"A…\n$ `Garage Yr Blt`   <dbl> 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993…\n$ `Garage Finish`   <chr> \"Fin\", \"Unf\", \"Unf\", \"Fin\", \"Fin\", \"RFn\", \"RFn\", \"Fi…\n$ `Garage Cars`     <dbl> 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2…\n$ `Garage Area`     <dbl> 528, 730, 312, 482, 582, 506, 608, 442, 440, 393, 50…\n$ `Garage Qual`     <chr> \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\"…\n$ `Garage Cond`     <chr> \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\"…\n$ `Paved Drive`     <chr> \"P\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n$ `Wood Deck SF`    <dbl> 210, 140, 393, 212, 0, 0, 237, 140, 157, 0, 192, 0, …\n$ `Open Porch SF`   <dbl> 62, 0, 36, 34, 0, 82, 152, 60, 84, 75, 0, 54, 36, 12…\n$ `Enclosed Porch`  <dbl> 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `3Ssn Porch`      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Screen Porch`    <dbl> 0, 120, 0, 0, 0, 144, 0, 0, 0, 0, 0, 140, 210, 0, 0,…\n$ `Pool Area`       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Pool QC`         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence             <chr> NA, \"MnPrv\", NA, \"MnPrv\", NA, NA, NA, NA, NA, NA, NA…\n$ `Misc Feature`    <chr> NA, NA, \"Gar2\", NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Misc Val`        <dbl> 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Mo Sold`         <dbl> 5, 6, 6, 3, 4, 1, 3, 6, 4, 5, 2, 6, 6, 6, 6, 6, 2, 1…\n$ `Yr Sold`         <dbl> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010…\n$ `Sale Type`       <chr> \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\"…\n$ `Sale Condition`  <chr> \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"N…\n$ SalePrice         <dbl> 215000, 105000, 172000, 189900, 213500, 191500, 2365…\n\n\n\nFirst we read data using a relative path and the here::here() function. This is a replacement for file.path() that works better for both interactive use and rendering in Quarto when using projects.\nWe use col_types = cols() to let R guess the correct class for each column. This suppresses messages that aren’t important at this point prior to EDA.\nIt is good practice to always glimpse() data after you read it.\n\n\nDataset Notes:\n\nDataset has N = 1955 rather than 2930.\n\nI have held out remaining observations to serve as a test set for a friendly competition in Unit 3\nI will judge your models’ performance with this test set at that time!\nMore on the importance of held out test sets as we progress through the course\n\nThis full dataset has 81 variables. For the lecture examples in units 2-3 we will only use a subset of the predictors\nYou will use different predictors in the next two application assignments\n\n\nHere we select the variables we will use for lecture\n\ndata_all <- data_all |> \n  select(SalePrice,\n         `Gr Liv Area`, \n         `Lot Area`, \n         `Year Built`, \n         `Overall Qual`, \n         `Garage Cars`,\n         `Garage Qual`,\n         `MS Zoning`,\n         `Lot Config` ,\n         `Bldg Type`) |> # <1> \n  glimpse()\n\nRows: 1,955\nColumns: 10\n$ SalePrice      <dbl> 215000, 105000, 172000, 189900, 213500, 191500, 236500,…\n$ `Gr Liv Area`  <dbl> 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 14…\n$ `Lot Area`     <dbl> 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 100…\n$ `Year Built`   <dbl> 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, 1…\n$ `Overall Qual` <dbl> 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, 7…\n$ `Garage Cars`  <dbl> 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2…\n$ `Garage Qual`  <chr> \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"…\n$ `MS Zoning`    <chr> \"RL\", \"RH\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"…\n$ `Lot Config`   <chr> \"Corner\", \"Inside\", \"Corner\", \"Inside\", \"Inside\", \"Insi…\n$ `Bldg Type`    <chr> \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"TwnhsE\", \"TwnhsE\", \"Tw…\n\n\n\nNotice that the dataset used non-standard variable names that include spaces. We need to use back-ticks around the variable names to allow us reference those variables. We will fix this during the cleaning process and you should never use spaces in variable names when setting up your own data!!!"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-data-cleaning",
    "href": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-data-cleaning",
    "title": "2  Exploratory Data Analysis",
    "section": "2.4 Exploratory Data Analysis for Data Cleaning",
    "text": "2.4 Exploratory Data Analysis for Data Cleaning\nEDA could be done using either tidyverse packages and functions or tidymodels (mostly using the recipes package.)\n\nWe prefer to use the richer set of functions available in the tidyverse (and dplyr and purrr packages in particular).\nWe will reserve the use of recipes for feature engineering only when we are building features for models that we will fit in our training sets and evaluation in our validation and test sets.\n\n\n\n2.4.1 Data Leakage Issues\nData leakage refers to a mistake made by the developer of a machine learning model in which they accidentally share information between their training set and held-out validation or test sets\n\nTraining sets are used to fit models with different configurations\nValidation sets are used to select the best model among those with different configurations (not needed if you only have one configuration)\nTest sets are used to evaluate a best model\nWhen splitting data-sets into training, validation and test sets, the goal is to ensure that no data (or information more broadly) are shared between the three sets\n\nNo data or information from test should influence either fitting or selecting models\nTest should only be used once to evaluate a best/final model\nTrain and validation set also must be segregated (although validation sets may be used to evaluate many model configurations)\nInformation necessary for transformations and other feature engineering (e.g., means/sds for centering/scaling, procedures for missing data imputation) must all be based only on training data.\nData leakage is common if you are not careful.\n\n\nIn particular, if we begin to use test data or information about test during model fitting\n\nWe risk overfitting\nThis is essentially the equivalent of p-hacking in traditional analyses\nOur estimate of model performance will be too optimistic, which could have harmful real-world consequences.\n\n\n\n\n2.4.2 Tidy variable names\nUse snake case for variable names\n\nclean_names() from janitor package is useful for this.\nMay need to do further correction of variable names using rename()\nSee more details about tidy names for objects (e.g., variables, dfs, functions) per Tidy Style Guide\n\n\ndata_all <- data_all |> \n  clean_names(\"snake\")\n\ndata_all |> names()\n\n [1] \"sale_price\"   \"gr_liv_area\"  \"lot_area\"     \"year_built\"   \"overall_qual\"\n [6] \"garage_cars\"  \"garage_qual\"  \"ms_zoning\"    \"lot_config\"   \"bldg_type\"   \n\n\n\n\n\n2.4.3 Explore variable classes\nAt this point, we should class all of our variables as either numeric or factor\n\nInterval and ratio variables use numeric classes (dbl or int)\nNominal and ordinal variable use factor class\nUseful for variable selection later (e.g., where(is.numeric), where(is.factor))\n\nSubsequent cleaning steps are clearer if we have this established/confirmed now\n\nWe have a number of nominal or ordinal variables that are classed as character.\nWe have one ordinal variable (overall_qual) that is classed as numeric (because the levels were coded with numbers rather than text)\n\nread_csv() thought was numeric by the levels are coded using numbers\nThe data dictionary indicates that valid values range from 1 - 10.\n\n\ndata_all |> glimpse()\n\nRows: 1,955\nColumns: 10\n$ sale_price   <dbl> 215000, 105000, 172000, 189900, 213500, 191500, 236500, 1…\n$ gr_liv_area  <dbl> 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 1465…\n$ lot_area     <dbl> 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 10000…\n$ year_built   <dbl> 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, 199…\n$ overall_qual <dbl> 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, 7, …\n$ garage_cars  <dbl> 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, …\n$ garage_qual  <chr> \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA…\n$ ms_zoning    <chr> \"RL\", \"RH\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL…\n$ lot_config   <chr> \"Corner\", \"Inside\", \"Corner\", \"Inside\", \"Inside\", \"Inside…\n$ bldg_type    <chr> \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"TwnhsE\", \"TwnhsE\", \"Twnh…\n\n\n\nWe can the recode overall_qual first and set its levels\nWe can recode all the character variables to factor in one step. Most are nominal. We will handle the order for garage_qual later.\n\noq_levels <- 1:10 # <1>\n\ndata_all <-  data_all |> \n  mutate(overall_qual = factor(overall_qual, \n                               levels = oq_levels)) |> # <2>\n  mutate(across(where(is.character), factor)) |>  # <3>\n  glimpse()\n\nRows: 1,955\nColumns: 10\n$ sale_price   <dbl> 215000, 105000, 172000, 189900, 213500, 191500, 236500, 1…\n$ gr_liv_area  <dbl> 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 1465…\n$ lot_area     <dbl> 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 10000…\n$ year_built   <dbl> 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, 199…\n$ overall_qual <fct> 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, 7, …\n$ garage_cars  <dbl> 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, …\n$ garage_qual  <fct> TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, T…\n$ ms_zoning    <fct> RL, RH, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, R…\n$ lot_config   <fct> Corner, Inside, Corner, Inside, Inside, Inside, Inside, I…\n$ bldg_type    <fct> 1Fam, 1Fam, 1Fam, 1Fam, TwnhsE, TwnhsE, TwnhsE, 1Fam, 1Fa…\n\n\n\nIt is always best to explicitly set the levels of an ordinal factor in the order you prefer. It is not necessary here because overall_qual was numeric and therefore sorts in the expected order. However, if it had been numbers stored as characters, it could sort incorrectly (e.g., 1, 10, 2, 3, …). And obviously if the orders levels were names, the order would have to be specified.\nWe indicate the levels here.\nWe use a mutate to re-class all character data to factors. I prefer factor() to forcats::fct() because factor orders the levels alphabetically. Be aware that this could change if your code is used in a region of the world where this sorting is different. I still prefer this to the alternative (in fct()) that orders by the order the levels are found in your data.\n\n\n\n\n2.4.4 Skimming the data\nskim() from the skimr package is a wonderful and customizable function for summary statistics\n\nIt is highly customizable so we can write our own versions for our own needs\nWe use different versions for cleaning and modeling EDA\nFor cleaning EDA, we just remove some stats that we don’t want to see at this time\nWe can get many of the summary stats for cleaning in one call\nWe have a custom skim defined in the fun_eda.R function library that we use regularly. Here is the code but you can use the function directly if you sourced fun_eda.R (as we did above)\n\n\nskim_some <- skim_with(numeric = sfl(mean = NULL, sd = NULL, p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL))\n\n\nHere is what we get with our new skim_some() function\n\nWe will refer to this again for each characteristic we want to review for instructional purposes\nWe can already see that we can use skim_some() to confirm that we only have numeric and factor classes\n\n\ndata_all |> \n  skim_some()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndata_all\n\n\n\n\nNumber of rows\n\n\n1955\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n5\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\noverall_qual\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n10\n\n\n5: 556, 6: 487, 7: 403, 8: 233\n\n\n\n\ngarage_qual\n\n\n109\n\n\n0.94\n\n\nFALSE\n\n\n5\n\n\nTA: 1745, Fa: 79, Gd: 16, Po: 4\n\n\n\n\nms_zoning\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n7\n\n\nRL: 1530, RM: 297, FV: 91, C (: 19\n\n\n\n\nlot_config\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n5\n\n\nIns: 1454, Cor: 328, Cul: 114, FR2: 55\n\n\n\n\nbldg_type\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n5\n\n\n1Fa: 1631, Twn: 145, Dup: 77, Twn: 64\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\np0\n\n\np100\n\n\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n12789\n\n\n745000\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n438\n\n\n5642\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n1476\n\n\n215245\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1875\n\n\n2010\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n0\n\n\n4\n\n\n\n\n\n\n\n\nCoding sidebar 1:\n\nWrite functions whenever you will repeat code often. You can now reuse skim_some()\nskim_with() is an example of a function factory - a function that is used to create a new function\n\npartial() and compose() are two other function factories we will use at times\nMore details on function factories is available in Advanced R\n\n\n\nCoding sidebar 2:\n\nGather useful functions together in a script that you can reuse.\nAll of the reusable functions in this and later units are available to you in one of my public github repositories.\nYou can load these functions into your workspace directly from github using devtools::source_url(). For example: devtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\")\nYou should start to gather your favorite custom functions together in your own script(s).\n\nYou can save your own scripts in a local file and load them into your workspace using source() or you can make your own github repo so you can begin to share your code with others!\n\n\n\n\n2.4.5 Missing Data - All variables\nskim_some() provides us with missing data counts and complete data proportions for each variable\n\ndata_all |> \n  skim_some() |> \n  select(skim_variable, n_missing, complete_rate) # <1>\n\n# A tibble: 10 × 3\n   skim_variable n_missing complete_rate\n   <chr>             <int>         <dbl>\n 1 overall_qual          0         1    \n 2 garage_qual         109         0.944\n 3 ms_zoning             0         1    \n 4 lot_config            0         1    \n 5 bldg_type             0         1    \n 6 sale_price            0         1    \n 7 gr_liv_area           0         1    \n 8 lot_area              0         1    \n 9 year_built            0         1    \n10 garage_cars           1         0.999\n\n\n\nskim_some() returns a dataframe so you can select only the subset of columns to focus its output on what you want. Or just print it all!\n\n\nYou likely should view the full observation for missing values\nWe will show you a few methods to do this in your rendered output\n\nprint() will print only 20 rows and the number of columns that will display for width of page\n\nSet options() if you will do a lot of printing and want full dataframe printed\n\nUse kbl() from kableExtra package for formatted tables (two methods below)\n\nDon’t forget that you can also use View() interactively in R Studio\n\nOption 1 (Simple): Use print() with options()\n\noptions(tibble.width = Inf, tibble.print_max = Inf) # <1>\n\ndata_all |> filter(is.na(garage_cars)) |> \n  print()\n\n# A tibble: 1 × 10\n  sale_price gr_liv_area lot_area year_built overall_qual garage_cars\n       <dbl>       <dbl>    <dbl>      <dbl> <fct>              <dbl>\n1     150909        1828     9060       1923 5                     NA\n  garage_qual ms_zoning lot_config bldg_type\n  <fct>       <fct>     <fct>      <fct>    \n1 <NA>        RM        Inside     1Fam     \n\n\n\nThis sets print to print all rows and columns. Note that we set these options at the start of the unit b.c. we like to see our full tibbles. If we want only a subset of the first (or last) rows, we use head() or tail()\n\n\nHere are some more advanced options using kbl() for the df with many rows\n\nkable() tables from knitr package and kableExtra extensions (including kbl()) are very useful during EDA and also final publication quality tables\nuse library(kableExtra)\nsee vignettes for kableExtra\n\nOption 2 (more advanced): Use a function for kables that we created. Code is displayed here but the function is available to you if you source fun_eda.R from Github\n\n# Might want to use height = \"100%\" if only printing a few rows\nprint_kbl <- function(data, height = \"500px\") { # <1>\n  data |> \n    kbl(align = \"r\") |> \n    kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) |> \n    scroll_box(height = height, width = \"100%\") # <2>\n}\n\n\nDefaults to a output box of height = “500px”. Can set to other values if preferred.\nMight want to use height = \"100%\" if only printing a few rows.\n\n\nLet’s use this function to see its output\n\ndata_all |> filter(is.na(garage_qual)) |> \n  print_kbl()\n\n\n\n \n  \n    sale_price \n    gr_liv_area \n    lot_area \n    year_built \n    overall_qual \n    garage_cars \n    garage_qual \n    ms_zoning \n    lot_config \n    bldg_type \n  \n \n\n  \n    115000 \n    864 \n    10500 \n    1971 \n    4 \n    0 \n    NA \n    RL \n    FR2 \n    1Fam \n  \n  \n    128950 \n    1225 \n    9320 \n    1959 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    84900 \n    1728 \n    13260 \n    1962 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    116500 \n    858 \n    7207 \n    1958 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    76500 \n    1306 \n    5350 \n    1940 \n    3 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    76500 \n    2256 \n    9045 \n    1910 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    2fmCon \n  \n  \n    159900 \n    1560 \n    12900 \n    1912 \n    6 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    55000 \n    1092 \n    5600 \n    1930 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    2fmCon \n  \n  \n    93369 \n    1884 \n    6449 \n    1907 \n    4 \n    0 \n    NA \n    C (all) \n    Inside \n    1Fam \n  \n  \n    94000 \n    1020 \n    6342 \n    1875 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    136000 \n    1832 \n    10773 \n    1967 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    100000 \n    1664 \n    9825 \n    1965 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    90000 \n    960 \n    6410 \n    1958 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    100000 \n    1666 \n    9839 \n    1931 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    139000 \n    1824 \n    9400 \n    1971 \n    6 \n    0 \n    NA \n    RL \n    Corner \n    Duplex \n  \n  \n    76000 \n    1092 \n    1476 \n    1970 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    Twnhs \n  \n  \n    75500 \n    630 \n    1491 \n    1972 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    TwnhsE \n  \n  \n    88250 \n    1092 \n    1900 \n    1970 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    TwnhsE \n  \n  \n    136000 \n    1792 \n    9000 \n    1974 \n    5 \n    0 \n    NA \n    RL \n    FR2 \n    Duplex \n  \n  \n    142000 \n    1114 \n    13072 \n    2004 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    82500 \n    708 \n    5330 \n    1940 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    129000 \n    1464 \n    9900 \n    1910 \n    5 \n    0 \n    NA \n    RM \n    Corner \n    1Fam \n  \n  \n    94550 \n    1701 \n    7627 \n    1920 \n    4 \n    0 \n    NA \n    RM \n    Corner \n    2fmCon \n  \n  \n    103000 \n    1447 \n    10134 \n    1910 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    37900 \n    968 \n    5925 \n    1910 \n    3 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    113000 \n    1452 \n    4456 \n    1920 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    2fmCon \n  \n  \n    58500 \n    816 \n    3300 \n    1910 \n    4 \n    0 \n    NA \n    C (all) \n    Inside \n    1Fam \n  \n  \n    34900 \n    720 \n    7879 \n    1920 \n    4 \n    0 \n    NA \n    C (all) \n    Inside \n    1Fam \n  \n  \n    60000 \n    800 \n    6120 \n    1936 \n    2 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    62500 \n    2128 \n    3000 \n    1922 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    Duplex \n  \n  \n    97500 \n    1864 \n    5852 \n    1902 \n    7 \n    0 \n    NA \n    RM \n    Corner \n    2fmCon \n  \n  \n    70000 \n    892 \n    5160 \n    1923 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    179000 \n    1200 \n    10800 \n    1987 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    179000 \n    1200 \n    10800 \n    1987 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    61000 \n    904 \n    10020 \n    1922 \n    1 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    118000 \n    698 \n    9405 \n    1947 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    99900 \n    864 \n    4060 \n    1922 \n    5 \n    0 \n    NA \n    RL \n    Corner \n    1Fam \n  \n  \n    119900 \n    1678 \n    10926 \n    1959 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    112000 \n    833 \n    8780 \n    1985 \n    5 \n    0 \n    NA \n    RL \n    Corner \n    1Fam \n  \n  \n    141000 \n    1080 \n    7500 \n    2004 \n    7 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    106250 \n    1294 \n    10800 \n    1900 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    130000 \n    1800 \n    8513 \n    1961 \n    5 \n    0 \n    NA \n    RL \n    Corner \n    Duplex \n  \n  \n    120000 \n    1027 \n    5400 \n    1920 \n    7 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    95000 \n    1080 \n    5914 \n    1890 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    65000 \n    1588 \n    12205 \n    1949 \n    3 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    129400 \n    1540 \n    6000 \n    1905 \n    5 \n    0 \n    NA \n    RM \n    Corner \n    1Fam \n  \n  \n    160000 \n    1984 \n    8094 \n    1910 \n    6 \n    1 \n    NA \n    RM \n    Inside \n    2fmCon \n  \n  \n    89500 \n    1406 \n    7920 \n    1920 \n    6 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    79900 \n    1198 \n    5586 \n    1920 \n    6 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    82375 \n    1344 \n    10320 \n    1915 \n    3 \n    0 \n    NA \n    RM \n    Inside \n    2fmCon \n  \n  \n    127500 \n    1355 \n    10106 \n    1940 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    80000 \n    1006 \n    9000 \n    1959 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    260000 \n    1518 \n    19550 \n    1940 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    99600 \n    864 \n    9350 \n    1975 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    107500 \n    1347 \n    7000 \n    1910 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    79000 \n    1096 \n    9600 \n    1924 \n    6 \n    0 \n    NA \n    RL \n    Corner \n    1Fam \n  \n  \n    85000 \n    796 \n    8777 \n    1910 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    145900 \n    2200 \n    8777 \n    1900 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    82000 \n    1152 \n    6040 \n    1955 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    82000 \n    1152 \n    6012 \n    1955 \n    4 \n    0 \n    NA \n    RL \n    Corner \n    Duplex \n  \n  \n    118000 \n    1440 \n    12108 \n    1955 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    82500 \n    1152 \n    6845 \n    1955 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    91900 \n    784 \n    6931 \n    1955 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    120000 \n    1053 \n    12180 \n    1938 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    96000 \n    1137 \n    8050 \n    1947 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    98000 \n    864 \n    5604 \n    1925 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    67000 \n    864 \n    8248 \n    1914 \n    3 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    135900 \n    1716 \n    5687 \n    1912 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    119000 \n    1200 \n    8155 \n    1930 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    81000 \n    630 \n    1890 \n    1972 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    Twnhs \n  \n  \n    146000 \n    1100 \n    7500 \n    2006 \n    6 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    64000 \n    670 \n    3500 \n    1945 \n    3 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    103200 \n    882 \n    5500 \n    1956 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    148000 \n    1534 \n    10800 \n    1895 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    110500 \n    866 \n    3880 \n    1945 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    127000 \n    1355 \n    6882 \n    1914 \n    6 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    200500 \n    3086 \n    18030 \n    1946 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    150000 \n    1440 \n    7711 \n    1977 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    86000 \n    605 \n    9098 \n    1920 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    123600 \n    990 \n    8070 \n    1994 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    98500 \n    1195 \n    8741 \n    1946 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    79000 \n    774 \n    4270 \n    1931 \n    3 \n    0 \n    NA \n    RH \n    Inside \n    1Fam \n  \n  \n    200000 \n    3395 \n    10896 \n    1914 \n    6 \n    0 \n    NA \n    RH \n    Inside \n    2fmCon \n  \n  \n    150000 \n    2592 \n    10890 \n    1923 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    115000 \n    1517 \n    8500 \n    1919 \n    5 \n    0 \n    NA \n    RM \n    Corner \n    1Fam \n  \n  \n    150909 \n    1828 \n    9060 \n    1923 \n    5 \n    NA \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    119600 \n    1991 \n    8250 \n    1895 \n    5 \n    0 \n    NA \n    C (all) \n    Inside \n    2fmCon \n  \n  \n    147000 \n    1120 \n    8402 \n    2007 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    93900 \n    1092 \n    1495 \n    1970 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    TwnhsE \n  \n  \n    84500 \n    630 \n    1936 \n    1970 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    Twnhs \n  \n  \n    139500 \n    1142 \n    7733 \n    2005 \n    6 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    132000 \n    1131 \n    13072 \n    2005 \n    6 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    85500 \n    869 \n    5900 \n    1923 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    135000 \n    1192 \n    10800 \n    1949 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    119000 \n    1556 \n    8512 \n    1960 \n    5 \n    0 \n    NA \n    RL \n    Corner \n    Duplex \n  \n  \n    124000 \n    1025 \n    7000 \n    1962 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    64500 \n    1020 \n    4761 \n    1918 \n    3 \n    0 \n    NA \n    C (all) \n    Corner \n    1Fam \n  \n  \n    100000 \n    788 \n    7446 \n    1941 \n    4 \n    0 \n    NA \n    RL \n    Corner \n    1Fam \n  \n  \n    80500 \n    912 \n    6240 \n    1947 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    72000 \n    819 \n    9000 \n    1919 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    117250 \n    914 \n    8050 \n    2002 \n    6 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    81000 \n    1184 \n    8410 \n    1910 \n    5 \n    0 \n    NA \n    RL \n    FR2 \n    1Fam \n  \n  \n    83000 \n    1414 \n    8248 \n    1922 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    102000 \n    1522 \n    6000 \n    1926 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    72000 \n    672 \n    8534 \n    1925 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    115000 \n    1396 \n    9000 \n    1951 \n    5 \n    0 \n    NA \n    C (all) \n    Inside \n    2fmCon \n  \n  \n    78000 \n    936 \n    8520 \n    1916 \n    3 \n    0 \n    NA \n    C (all) \n    Inside \n    1Fam \n  \n  \n    92000 \n    630 \n    1533 \n    1970 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    Twnhs \n  \n  \n    90500 \n    1092 \n    1936 \n    1970 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    Twnhs \n  \n\n\n\n\n\n\nCoding sidebar:\n\nIn the above example, we created a function (print_kbl()) from scratch (rather than using a function factory)\nSee functions chapter in Wickham, Çetinkaya-Rundel, and Grolemund (2023) for help.\nSee functionals chapter in Wickham (2019).\n\n\nOption 3 (Most advanced): Line by line kable table. You can make this as complicated and customized as you like. We use kable (and kableExtra) for publication quality tables. This is a simple example of options\n\ndata_all |> filter(is.na(garage_qual)) |> \n  kbl(align = \"r\") |> \n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) |> \n  scroll_box(height = \"500px\", width = \"100%\")\n\n\n\n \n  \n    sale_price \n    gr_liv_area \n    lot_area \n    year_built \n    overall_qual \n    garage_cars \n    garage_qual \n    ms_zoning \n    lot_config \n    bldg_type \n  \n \n\n  \n    115000 \n    864 \n    10500 \n    1971 \n    4 \n    0 \n    NA \n    RL \n    FR2 \n    1Fam \n  \n  \n    128950 \n    1225 \n    9320 \n    1959 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    84900 \n    1728 \n    13260 \n    1962 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    116500 \n    858 \n    7207 \n    1958 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    76500 \n    1306 \n    5350 \n    1940 \n    3 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    76500 \n    2256 \n    9045 \n    1910 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    2fmCon \n  \n  \n    159900 \n    1560 \n    12900 \n    1912 \n    6 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    55000 \n    1092 \n    5600 \n    1930 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    2fmCon \n  \n  \n    93369 \n    1884 \n    6449 \n    1907 \n    4 \n    0 \n    NA \n    C (all) \n    Inside \n    1Fam \n  \n  \n    94000 \n    1020 \n    6342 \n    1875 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    136000 \n    1832 \n    10773 \n    1967 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    100000 \n    1664 \n    9825 \n    1965 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    90000 \n    960 \n    6410 \n    1958 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    100000 \n    1666 \n    9839 \n    1931 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    139000 \n    1824 \n    9400 \n    1971 \n    6 \n    0 \n    NA \n    RL \n    Corner \n    Duplex \n  \n  \n    76000 \n    1092 \n    1476 \n    1970 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    Twnhs \n  \n  \n    75500 \n    630 \n    1491 \n    1972 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    TwnhsE \n  \n  \n    88250 \n    1092 \n    1900 \n    1970 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    TwnhsE \n  \n  \n    136000 \n    1792 \n    9000 \n    1974 \n    5 \n    0 \n    NA \n    RL \n    FR2 \n    Duplex \n  \n  \n    142000 \n    1114 \n    13072 \n    2004 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    82500 \n    708 \n    5330 \n    1940 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    129000 \n    1464 \n    9900 \n    1910 \n    5 \n    0 \n    NA \n    RM \n    Corner \n    1Fam \n  \n  \n    94550 \n    1701 \n    7627 \n    1920 \n    4 \n    0 \n    NA \n    RM \n    Corner \n    2fmCon \n  \n  \n    103000 \n    1447 \n    10134 \n    1910 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    37900 \n    968 \n    5925 \n    1910 \n    3 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    113000 \n    1452 \n    4456 \n    1920 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    2fmCon \n  \n  \n    58500 \n    816 \n    3300 \n    1910 \n    4 \n    0 \n    NA \n    C (all) \n    Inside \n    1Fam \n  \n  \n    34900 \n    720 \n    7879 \n    1920 \n    4 \n    0 \n    NA \n    C (all) \n    Inside \n    1Fam \n  \n  \n    60000 \n    800 \n    6120 \n    1936 \n    2 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    62500 \n    2128 \n    3000 \n    1922 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    Duplex \n  \n  \n    97500 \n    1864 \n    5852 \n    1902 \n    7 \n    0 \n    NA \n    RM \n    Corner \n    2fmCon \n  \n  \n    70000 \n    892 \n    5160 \n    1923 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    179000 \n    1200 \n    10800 \n    1987 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    179000 \n    1200 \n    10800 \n    1987 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    61000 \n    904 \n    10020 \n    1922 \n    1 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    118000 \n    698 \n    9405 \n    1947 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    99900 \n    864 \n    4060 \n    1922 \n    5 \n    0 \n    NA \n    RL \n    Corner \n    1Fam \n  \n  \n    119900 \n    1678 \n    10926 \n    1959 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    112000 \n    833 \n    8780 \n    1985 \n    5 \n    0 \n    NA \n    RL \n    Corner \n    1Fam \n  \n  \n    141000 \n    1080 \n    7500 \n    2004 \n    7 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    106250 \n    1294 \n    10800 \n    1900 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    130000 \n    1800 \n    8513 \n    1961 \n    5 \n    0 \n    NA \n    RL \n    Corner \n    Duplex \n  \n  \n    120000 \n    1027 \n    5400 \n    1920 \n    7 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    95000 \n    1080 \n    5914 \n    1890 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    65000 \n    1588 \n    12205 \n    1949 \n    3 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    129400 \n    1540 \n    6000 \n    1905 \n    5 \n    0 \n    NA \n    RM \n    Corner \n    1Fam \n  \n  \n    160000 \n    1984 \n    8094 \n    1910 \n    6 \n    1 \n    NA \n    RM \n    Inside \n    2fmCon \n  \n  \n    89500 \n    1406 \n    7920 \n    1920 \n    6 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    79900 \n    1198 \n    5586 \n    1920 \n    6 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    82375 \n    1344 \n    10320 \n    1915 \n    3 \n    0 \n    NA \n    RM \n    Inside \n    2fmCon \n  \n  \n    127500 \n    1355 \n    10106 \n    1940 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    80000 \n    1006 \n    9000 \n    1959 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    260000 \n    1518 \n    19550 \n    1940 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    99600 \n    864 \n    9350 \n    1975 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    107500 \n    1347 \n    7000 \n    1910 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    79000 \n    1096 \n    9600 \n    1924 \n    6 \n    0 \n    NA \n    RL \n    Corner \n    1Fam \n  \n  \n    85000 \n    796 \n    8777 \n    1910 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    145900 \n    2200 \n    8777 \n    1900 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    82000 \n    1152 \n    6040 \n    1955 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    82000 \n    1152 \n    6012 \n    1955 \n    4 \n    0 \n    NA \n    RL \n    Corner \n    Duplex \n  \n  \n    118000 \n    1440 \n    12108 \n    1955 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    82500 \n    1152 \n    6845 \n    1955 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    91900 \n    784 \n    6931 \n    1955 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    120000 \n    1053 \n    12180 \n    1938 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    96000 \n    1137 \n    8050 \n    1947 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    98000 \n    864 \n    5604 \n    1925 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    67000 \n    864 \n    8248 \n    1914 \n    3 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    135900 \n    1716 \n    5687 \n    1912 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    119000 \n    1200 \n    8155 \n    1930 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    81000 \n    630 \n    1890 \n    1972 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    Twnhs \n  \n  \n    146000 \n    1100 \n    7500 \n    2006 \n    6 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    64000 \n    670 \n    3500 \n    1945 \n    3 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    103200 \n    882 \n    5500 \n    1956 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    148000 \n    1534 \n    10800 \n    1895 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    110500 \n    866 \n    3880 \n    1945 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    127000 \n    1355 \n    6882 \n    1914 \n    6 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    200500 \n    3086 \n    18030 \n    1946 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    150000 \n    1440 \n    7711 \n    1977 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    86000 \n    605 \n    9098 \n    1920 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    123600 \n    990 \n    8070 \n    1994 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    98500 \n    1195 \n    8741 \n    1946 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    79000 \n    774 \n    4270 \n    1931 \n    3 \n    0 \n    NA \n    RH \n    Inside \n    1Fam \n  \n  \n    200000 \n    3395 \n    10896 \n    1914 \n    6 \n    0 \n    NA \n    RH \n    Inside \n    2fmCon \n  \n  \n    150000 \n    2592 \n    10890 \n    1923 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    Duplex \n  \n  \n    115000 \n    1517 \n    8500 \n    1919 \n    5 \n    0 \n    NA \n    RM \n    Corner \n    1Fam \n  \n  \n    150909 \n    1828 \n    9060 \n    1923 \n    5 \n    NA \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    119600 \n    1991 \n    8250 \n    1895 \n    5 \n    0 \n    NA \n    C (all) \n    Inside \n    2fmCon \n  \n  \n    147000 \n    1120 \n    8402 \n    2007 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    93900 \n    1092 \n    1495 \n    1970 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    TwnhsE \n  \n  \n    84500 \n    630 \n    1936 \n    1970 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    Twnhs \n  \n  \n    139500 \n    1142 \n    7733 \n    2005 \n    6 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    132000 \n    1131 \n    13072 \n    2005 \n    6 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    85500 \n    869 \n    5900 \n    1923 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    135000 \n    1192 \n    10800 \n    1949 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    119000 \n    1556 \n    8512 \n    1960 \n    5 \n    0 \n    NA \n    RL \n    Corner \n    Duplex \n  \n  \n    124000 \n    1025 \n    7000 \n    1962 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    2fmCon \n  \n  \n    64500 \n    1020 \n    4761 \n    1918 \n    3 \n    0 \n    NA \n    C (all) \n    Corner \n    1Fam \n  \n  \n    100000 \n    788 \n    7446 \n    1941 \n    4 \n    0 \n    NA \n    RL \n    Corner \n    1Fam \n  \n  \n    80500 \n    912 \n    6240 \n    1947 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    72000 \n    819 \n    9000 \n    1919 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    117250 \n    914 \n    8050 \n    2002 \n    6 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    81000 \n    1184 \n    8410 \n    1910 \n    5 \n    0 \n    NA \n    RL \n    FR2 \n    1Fam \n  \n  \n    83000 \n    1414 \n    8248 \n    1922 \n    4 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    102000 \n    1522 \n    6000 \n    1926 \n    5 \n    0 \n    NA \n    RL \n    Inside \n    1Fam \n  \n  \n    72000 \n    672 \n    8534 \n    1925 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    1Fam \n  \n  \n    115000 \n    1396 \n    9000 \n    1951 \n    5 \n    0 \n    NA \n    C (all) \n    Inside \n    2fmCon \n  \n  \n    78000 \n    936 \n    8520 \n    1916 \n    3 \n    0 \n    NA \n    C (all) \n    Inside \n    1Fam \n  \n  \n    92000 \n    630 \n    1533 \n    1970 \n    5 \n    0 \n    NA \n    RM \n    Inside \n    Twnhs \n  \n  \n    90500 \n    1092 \n    1936 \n    1970 \n    4 \n    0 \n    NA \n    RM \n    Inside \n    Twnhs \n  \n\n\n\n\n\n\nIn this instance, if we consult our data dictionary, we see that NA for garage_qual should be coded as “no garage”. We will correct this in our data set.\nThis is a pretty poor choice on the part of the researchers who created the dataset because it becomes impossible to distinguish between NA that means no garage vs. true NA for the variable. In fact, if you later do really careful EDA on the full data set with all variables, you will see this problem likely exists in this dataset\nAnyway, let’s correct all the NA for garage_qual to “no_garage” using mutate()\n\ndata_all <- data_all |> \n  mutate(garage_qual = fct_expand(garage_qual, \"no_garage\"), # <1>\n         garage_qual = replace_na(garage_qual, \"no_garage\")) # <2>\n\n\nFirst add a new level to the factor\nThen recode NA to that new level\n\nWe will leave the NA for garage_cars as NA because its not clear if that is truly missing or not, based on further EDA not shown here.\n\nWe have one other issue with garage_qual. It is an ordinal variable but we never reviewed the order of its levels. The data dictionary indicates the levels are ordered (best to worst) as:\n\nEx (excellent)\nGd (good)\nTA (typical/average)\nFa (fair)\nPo (poor)\n\nAnd we might assume that no garage is even worse than a poor garage. Lets see what they are.\n\ndata_all$garage_qual |> levels()\n\n[1] \"Ex\"        \"Fa\"        \"Gd\"        \"Po\"        \"TA\"        \"no_garage\"\n\n\nTo fix this, we can use forcats::fct_relevel().\n\ngq_levels <- c(\"no_garage\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\") # <1>\ndata_all <- data_all |> \n  mutate(garage_qual = fct_relevel(garage_qual, gq_levels)) # <2>\n\ndata_all$garage_qual |> levels() # <3>\n\n[1] \"no_garage\" \"Po\"        \"Fa\"        \"TA\"        \"Gd\"        \"Ex\"       \n\n\n\nMake a vector that indicates the valid levels in order\nPass that into fct_relevel(). See ?fct_relevel for other ways to adjust the levels of a factor.\nConfirm that the levels are now correct\n\n\n\n\n2.4.6 Explore Min/Max Response for Numeric Variables\nWe should explore mins and maxes for all numeric variables to detect out of valid range numeric responses\n\nCould also do this for ordinal variables that are coded with numbers\n\ne.g., overall_qual (1-10) vs. garage_qual (no_garage, Po, Fa, TA, Gd, Ex)\n\nThis is only a temporary mutation of overall_qual for this check. We don’t assign to new df to an object\nWe can use skim_some() again\n\np0 = min\np100 = max\n\n\n\ndata_all |>\n  mutate(overall_qual = as.numeric(overall_qual)) |> \n  skim_some() |> \n  filter(skim_type == \"numeric\") |>  # <1>\n  select(skim_variable, numeric.p0, numeric.p100) # <2>\n\n# A tibble: 6 × 3\n  skim_variable numeric.p0 numeric.p100\n  <chr>              <dbl>        <dbl>\n1 sale_price         12789       745000\n2 gr_liv_area          438         5642\n3 lot_area            1476       215245\n4 year_built          1875         2010\n5 overall_qual           1           10\n6 garage_cars            0            4\n\n\n\nSelect only numeric variables since min/max only apply to them\nSelect relevant stats (min/max)\n\n\n\n\n2.4.7 Explore All Responses for Categorical Variables\nWe should explore all unique responses for nominal variables\nMight also do this for ordinal variables that are coded with labels vs. numbers.\n\ndata_all |> \n  select(where(is.factor)) |>\n  walk(\\(column) print(levels(column)))\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n[1] \"no_garage\" \"Po\"        \"Fa\"        \"TA\"        \"Gd\"        \"Ex\"       \n[1] \"A (agr)\" \"C (all)\" \"FV\"      \"I (all)\" \"RH\"      \"RL\"      \"RM\"     \n[1] \"Corner\"  \"CulDSac\" \"FR2\"     \"FR3\"     \"Inside\" \n[1] \"1Fam\"   \"2fmCon\" \"Duplex\" \"Twnhs\"  \"TwnhsE\"\n\n\nCoding sidebar:\n\nOn the previous page, we demonstrated the use of an anonymous function (\\(column) print(levels(column))), which is a function we use once that we don’t bother to assign a name (since we won’t reuse it). We often use anonymous functions when using the functions from the purrr package (e.g., map(), walk())\nWe use walk() from the purrr package to apply our anonymous function to all columns of the data frame at once\nJust copy this code for now\nWe will see simpler uses later that will help you understand iteration with purrr functions\nSee the chapter on iteration in R for Data Science (2e) for more info on map() and walk()\n\n\n\n\n2.4.8 Tidy Responses for Categorical Variables\nFeature engineering with nominal and ordinal variables typically involves\n\nConverting to factors (already did this!)\nOften creating dummy features from these factors\n\nThis feature engineering will use response labels for naming new features\n\nTherefore, it is a good idea to have the responses snake-cased and cleaned up a bit so that these new feature names are clean/clear.\n\nHere is an easy way to convert responses for character variables to snake case using a function (tidy_responses()) we share in fun_eda.R (reproduced here).\n\nThis uses regular expressions (regex), which will will learn about in a later unit on text processing.\nYou could expand this cleaning function if you encounter other issues that need to be cleaned in the factor levels.\n\n\ntidy_responses <- function(column){\n  # replace all non-alphanumeric with _\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\W\", \"_\"))\n  # replace whitespace with _\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\s+\", \"_\"))\n  # replace multiple _ with single _\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\_+\", \"_\"))\n  #remove _ at end of string\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\_$\", \"\"))\n  # remove _ at start of string\n  column <- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\^_\", \"\"))\n  # convert to lowercase\n  column <- fct_relabel(column, tolower)\n  factor(column)\n}\n\n\nLet’s use the function\n\ndata_all <- data_all |> \n  mutate(across(where(is.factor), tidy_responses)) # <1>\n\n\nWe use the tidy selection helper function to limit our mutate to only factors. See more details on the tidy selection helpers like all_of() and where()\n\n\nAlas, these response labels were pretty poorly chosen so some didn’t convert well. And some are really hard to understand too.\n\nAvoid this problem and choose good response labels from the start for your own data\nHere, we show you what we got from using tidy_responses()\n\n\ndata_all |> \n  select(where(is.factor)) |>\n  walk(\\(column) print(levels(column)))\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n[1] \"no_garage\" \"po\"        \"fa\"        \"ta\"        \"gd\"        \"ex\"       \n[1] \"a_agr\" \"c_all\" \"fv\"    \"i_all\" \"rh\"    \"rl\"    \"rm\"   \n[1] \"corner\"  \"culdsac\" \"fr2\"     \"fr3\"     \"inside\" \n[1] \"1fam\"   \"2fmcon\" \"duplex\" \"twnhs\"  \"twnhse\"\n\n\n\nLets clean them up a bit more manually\n\ndata_all <- data_all |> \n  mutate(ms_zoning = fct_recode(ms_zoning,\n                                res_low = \"rl\",\n                                res_med = \"rm\",\n                                res_high = \"rh\",\n                                float = \"fv\",\n                                agri = \"a_agr\",\n                                indus = \"i_all\",\n                                commer = \"c_all\"),\n         bldg_type = fct_recode(bldg_type,   # <1>\n                                one_fam = \"1fam\",\n                                two_fam = \"2fmcon\",\n                                town_end = \"twnhse\",\n                                town_inside = \"twnhs\"))\n\n\nNote that I did not need to list all levels in the recode. Only the levels I wanted to change.\n\nThe full dataset is now clean!\n\n\n\n2.4.9 Train/Validate/Test Splits\nThe final task we typically do as part of the data preparation process is to split the full dataset into training, validation and test sets.\n\nTest sets are “typically” between 20-30% of your full dataset\n\nThere are costs and benefits to larger test sets\nWe will learn about these costs/benefits in the unit on resampling\nI have already held out the test set\n\nThere are many approaches to validation sets\n\nFor now (until unit 5) we will use a single validation set approach\nWe will use 25% of the remaining data (after holding out the test set) as a validation set for this example\n\nIt is typical to split data on the outcome within strata\n\nFor a categorical outcome, this makes the proportions of the response categories more balanced across the train, validation, and test sets\nFor a numeric outcome, we first break up the distribution into temporary bins (see breaks = 4 below) and then we split within these bins\n\nIMPORTANT: Set a seed so that you can reproduce these splits if you later do more cleaning\n\n\nset.seed(20110522)\nsplits <- data_all |> \n  initial_split(prop = 3/4, strata = \"sale_price\", breaks = 4)\n\n\nWe then extract the training set from the splits and save it\n\nTraining sets are used for “analysis”- hence the name of the function\n\n\nsplits |> \n  analysis() |> # <1> \n  glimpse() |> \n  write_csv(here::here(path_data, \"ames_clean_class_trn.csv\"))\n\nRows: 1,465\nColumns: 10\n$ sale_price   <dbl> 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  <dbl> 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     <dbl> 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   <dbl> 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual <fct> 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  <dbl> 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  <fct> ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    <fct> res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   <fct> inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    <fct> one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nanalysis() pulls out the training set from our splits of data_all\n\n\nWe will not need the validation set for modeling EDA\n\nIt should NOT be used for anything other than evaluating models to select the best model configuration\nWe do NOT do Modeling EDA or Model Fitting with the validation set\nSave it in this clean form for easy use when you need it\nWe use the validation set to “assess” models that we have fit in training sets - hence the name of the function\n\n\nsplits |> \n  assessment() |> # <1> \n  glimpse() |> \n  write_csv(here::here(path_data, \"ames_clean_class_val.csv\"))\n\nRows: 490\nColumns: 10\n$ sale_price   <dbl> 215000, 189900, 189000, 171500, 212000, 164000, 394432, 1…\n$ gr_liv_area  <dbl> 1656, 1629, 1804, 1341, 1502, 1752, 1856, 1004, 1092, 106…\n$ lot_area     <dbl> 31770, 13830, 7500, 10176, 6820, 12134, 11394, 11241, 168…\n$ year_built   <dbl> 1960, 1997, 1999, 1990, 1985, 1988, 2010, 1970, 1971, 197…\n$ overall_qual <fct> 6, 5, 7, 7, 8, 8, 9, 6, 5, 6, 7, 9, 8, 8, 7, 8, 6, 5, 5, …\n$ garage_cars  <dbl> 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 3, 2, 3, 1, 1, 2, …\n$ garage_qual  <fct> ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, t…\n$ ms_zoning    <fct> res_low, res_low, res_low, res_low, res_low, res_low, res…\n$ lot_config   <fct> corner, inside, inside, inside, corner, inside, corner, c…\n$ bldg_type    <fct> one_fam, one_fam, one_fam, one_fam, town_end, one_fam, on…\n\n\n\nassessment() pulls out the validation set from our splits of data_all"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-modeling",
    "href": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-modeling",
    "title": "2  Exploratory Data Analysis",
    "section": "2.5 Exploratory Data Analysis for Modeling",
    "text": "2.5 Exploratory Data Analysis for Modeling\nNow let’s begin our Modeling EDA\nWe prefer to write separate scripts for Cleaning vs. Modeling EDA (but not displayed here)\n\nThis keeps these two processes separate in our minds\nCleaning EDA is done with full dataset but Modeling EDA is only done with a training set - NEVER use validation or test set\nYou will use two separate scripts for the application assignment for this unit\n\n\nLets re-load (and glimpse) our training set to pretend we are at the start of a new script.\n\n data_trn <- read_csv(here::here(path_data, \"ames_clean_class_trn.csv\")) |> \n  glimpse()\n\nRows: 1465 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): garage_qual, ms_zoning, lot_config, bldg_type\ndbl (6): sale_price, gr_liv_area, lot_area, year_built, overall_qual, garage...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   <dbl> 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  <dbl> 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     <dbl> 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   <dbl> 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual <dbl> 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  <dbl> 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  <chr> \"ta\", \"ta\", \"no_garage\", \"ta\", \"ta\", \"ta\", \"ta\", \"no_gara…\n$ ms_zoning    <chr> \"res_high\", \"res_low\", \"res_low\", \"res_low\", \"res_low\", \"…\n$ lot_config   <chr> \"inside\", \"corner\", \"fr2\", \"fr2\", \"inside\", \"inside\", \"in…\n$ bldg_type    <chr> \"one_fam\", \"one_fam\", \"one_fam\", \"town_inside\", \"town_end…\n\n\n\nWe have some work to do (again)\n\nNotice that overall_qual is back to being classed as numeric (dbl).\n\nNotice that your factors are back to character\n\nThis is because csv files don’t save anything other than the values (labels for factors). They are the cleaned labels though!\n\nYou should class all variables using the same approach as before (often just a copy/paste).\n\n\n data_trn <- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           col_types = cols()) |>  # <1>\n  mutate(across(where(is.character), factor)) |> # <2>\n  mutate(overall_qual = factor(overall_qual, levels = 1:10),  # <3>\n         garage_qual = fct_relevel(garage_qual, c(\"no_garage\", \"po\", \"fa\", \n                                                  \"ta\", \"gd\", \"ex\"))) |>  # <4> \n  glimpse()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `garage_qual = fct_relevel(...)`.\nCaused by warning:\n! 1 unknown level in `f`: ex\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   <dbl> 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  <dbl> 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     <dbl> 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   <dbl> 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual <fct> 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  <dbl> 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  <fct> ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    <fct> res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   <fct> inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    <fct> one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nuse col_types = cols() to suppress messages about default class assignments\nuse mutate() with across() to change all character variables to factors\nuse mutate() with factor() to change numeric variable to factor.\nuse mutate() with fct_relevel() to explicitly set levels of an ordered factor. Also notice the warning about the unknown level. Always explore warnings! In this instance, its fine. There were only two observations with ex and neither ended up in the training split. Still best to include this level to note it exists!\n\n\nCoding sidebar: We will likely re-class the Ames dataset many times (for training, validation, test). We could copy/paste these mutates each time but whenever you do something more than twice, I recommend writing a function. We might write this one to re-class the ames variables\n\nclass_ames <- function(df){\n  \n  df |>\n    mutate(across(where(is.character), factor)) |> \n    mutate(overall_qual = factor(overall_qual, levels = 1:10), \n           garage_qual = fct_relevel(garage_qual, c(\"no_garage\", \"po\", \"fa\", \n                                                    \"ta\", \"gd\", \"ex\")))\n}\n\n\nNow we can use this function every time we read in one of the Ames datasets\n\ndata_trn <- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           col_types = cols()) |> \n  class_ames() |> # <1> \n  glimpse()\n\nRows: 1,465\nColumns: 10\n$ sale_price   <dbl> 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  <dbl> 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     <dbl> 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   <dbl> 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual <fct> 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  <dbl> 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  <fct> ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    <fct> res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   <fct> inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    <fct> one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nUsing our new function!\n\n\nThere are 3 basic types of Modeling EDA you should always do\n\nExplore missingness for predictors\nExplore univariate distributions for outcome and predictors\nExplore bivariate relationships between predictors and outcome\n\nAs a result of this exploration, we will:\n\nIdentify promising predictors\nDetermine appropriate feature engineering for those predictors (e.g., transformations)\nIdentify outliers and consider how to handle when model building\nConsider how to handle imputation for missing data (if any)\n\n\n\n2.5.1 Overall Summary of Feature Matrix\nBefore we dig into individual variables and their distributions and relationships with the outcome, it’s nice to start with a big picture of the dataset\n\nWe use another customized version of skim() from the skimr package to provide this\nJust needed to augment it with skewness and kurtosis statistics for numeric variables\nand remove histogram b/c we don’t find that small histogram useful\nincluded in fun_eda.R on github\n\n\nskew_na <- partial(e1071::skewness, na.rm = TRUE)\nkurt_na <- partial(e1071::kurtosis, na.rm = TRUE)\n\nskim_all <- skimr::skim_with(numeric = skimr::sfl(skew = skew_na, \n                                                  kurtosis = kurt_na, \n                                                  hist = NULL))\n\n\nCareful review of this output provides a great orientation to our data\n\ndata_trn |> \n  skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndata_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n5\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n10\n\n\n5: 424, 6: 350, 7: 304, 8: 176\n\n\n\n\ngarage_qual\n\n\n0\n\n\n1\n\n\n5\n\n\nta: 1312, no_: 81, fa: 57, gd: 13\n\n\n\n\nms_zoning\n\n\n0\n\n\n1\n\n\n7\n\n\nres: 1157, res: 217, flo: 66, com: 13\n\n\n\n\nlot_config\n\n\n0\n\n\n1\n\n\n5\n\n\nins: 1095, cor: 248, cul: 81, fr2: 39\n\n\n\n\nbldg_type\n\n\n0\n\n\n1\n\n\n5\n\n\none: 1216, tow: 108, dup: 63, tow: 46\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\n\n\n\n\n\n\n2.5.2 Univariate Distributions\nExploration of univariate distributions are useful to\n\nUnderstand variation and distributional shape\nMay suggest need to consider transformations as part of feature engineering\nCan identify univariate outliers (valid but disconnected from distribution so not detected in cleaning)\n\nWe generally select different visualizations and summary statistics for categorical vs. numeric variables\n\n\n\n2.5.3 Barplots for Categorical Variables (Univariate)\nThe primary visualization for categorical variables is the bar plot\n\nWe use it for both nominal and ordinal variables\nDefine and customize it within a function for repeated use.\n\nWe share this and all the remaining plots used in this unit in fun_plot.R. Source it to use them without having to re-code each time\n\n\nplot_bar <- function(df, x){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\n\nCoding sidebar: When defining functions, generally put data as first argument so you can pipe in data using tidy pipelines\nThere are pros and cons to writing functions that accept variable names that are quoted vs. unquoted\n\nIt depends a bit on how you will use them.\n.data[[argument]] is used in functions with quoted arguments\nembracing {{}} is used for unquoted arguments\nFor these plot functions, I use quoted variable names and then pipe those into map() to make multiple plots (see below)\nsee ?vignette(\"programming\") or info on tidy evaluation in Wickham, Çetinkaya-Rundel, and Grolemund (2023) for more details\n\n\nBar plots reveal low frequency responses for nominal and ordinal variables\n\nSee bldg_type\n\n\ndata_trn |> plot_bar(\"bldg_type\")\n\n\n\n\n\nBar plots can display distributional shape for ordinal variables. May suggest the need for transformations if we later treat the ordinal variable as numeric\n\nSee overall_qual. Though it is not very skewed.\n\n\ndata_trn |> plot_bar(\"overall_qual\")\n\n\n\n\n\nCoding sidebar:\nWe can make all of our plots iteratively using map() from the `purrr package.\n\ndata_trn |> \n  select(where(is.factor)) |> # <1> \n  names() |> # <2> \n  map(\\(name) plot_bar(df = data_trn, x = name)) |> # <3> \n  plot_grid(plotlist = _, ncol = 2) # <4>\n\n\n\n\n\nSelect only the factor columns\nGet their names as strings (that is why we use quoted variables in these plot functions\nUse map() to iterative plot_bar() over every column. (see iteration in Wickham, Çetinkaya-Rundel, and Grolemund (2023))\nUse plot_grid() from cowplot package to display the list of plots in a grid\n\n\n\n\n2.5.4 Tables for Categorical Variables (Univariate)\nWe tend to prefer visualizations vs. summary statistics for EDA. However, tables can be useful.\nHere is a function that was described in Wickham, Çetinkaya-Rundel, and Grolemund (2023) that we like because\n\nIt includes counts and proportions\nIt includes NA as a category\n\nWe have included it in fun_eda.R for your use.\n\ntab <- function(df, var, sort = FALSE) {\n  df |>  dplyr::count({{ var }}, sort = sort) |> \n    dplyr::mutate(prop = n / sum(n))\n} \n\n\nTables can be used to identify responses that have very low frequency and to think about the need to handle missing values\n\nSee ms_zoning\nMay want to collapse low frequency (or low percentage) categories to reduce the number of features needed to represent the predictor\n\n\ndata_trn |> tab(ms_zoning)\n\n# A tibble: 7 × 3\n  ms_zoning     n     prop\n  <fct>     <int>    <dbl>\n1 agri          2 0.00137 \n2 commer       13 0.00887 \n3 float        66 0.0451  \n4 indus         1 0.000683\n5 res_high      9 0.00614 \n6 res_low    1157 0.790   \n7 res_med     217 0.148   \n\n\n\n\nWe could also view the table sorted if we prefer\n\n\ndata_trn |> tab(ms_zoning, sort = TRUE)\n\n# A tibble: 7 × 3\n  ms_zoning     n     prop\n  <fct>     <int>    <dbl>\n1 res_low    1157 0.790   \n2 res_med     217 0.148   \n3 float        66 0.0451  \n4 commer       13 0.00887 \n5 res_high      9 0.00614 \n6 agri          2 0.00137 \n7 indus         1 0.000683\n\n\n\nbut could see all this detail with plot as well\n\ndata_trn |> plot_bar(\"ms_zoning\")\n\n\n\n\n\n\n\n2.5.5 Histograms for Numeric Variables (Univariate)\nHistograms are a useful/common visualization for numeric variables\nLet’s define a histogram function (included in fun_plots.r)\n\nBin size should be explored a bit to find best representation\nSomewhat dependent on n (my default here is based on this training set)\nThis is one of the limitations of histograms\n\n\nplot_hist <- function(df, x, bins = 100){\n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_histogram(bins = bins) +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n\n\nLet’s look at sale_price\n\nIt is positively skewed\nMay suggest units (dollars) are not interval in nature (makes sense)\nCould cause problems for some algorithms (e.g., lm) when features are normal\n\n\ndata_trn |> plot_hist(\"sale_price\")\n\n\n\n\n\n\n\n2.5.6 Smoothed Frequency Polygons for Numeric Variables (Univariate)\nFrequency polygons are also commonly used\n\nDefine a frequency polygon function and use it (included in fun_plots.r)\n\n\nplot_freqpoly <- function(df, x, bins = 50){\n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_freqpoly(bins = bins) +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n\n\n\nBins may matter again\n\n\ndata_trn |> plot_freqpoly(\"sale_price\")\n\n\n\n\n\n\n\n2.5.7 Simple Boxplots for Numeric Variables (Univariate)\nBoxplots display\n\nMedian as line\n25%ile and 75%ile as hinges\nHighest and lowest points within 1.5 * IQR (interquartile-range: difference between scores at 25% and 75%iles)\nOutliers outside of 1.5 * IQR\n\nDefine a boxplot function and use it (included in fun_plots.r)\n\nplot_boxplot <- function(df, x){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1))\n}\n\n\nHere is the plot for sale_price\n\ndata_trn |> plot_boxplot(\"sale_price\")\n\n\n\n\n\n\n\n2.5.8 Combined Boxplot and Violin Plots for Numeric Variables (Univariate)\nThe combination of a boxplot and violin plot is particularly useful\n\nThis is our favorite\nGet all the benefits of the boxplot\nCan clearly see shape of distribution given the violin plot overlay\nCan also clearly see the tails\n\nDefine a combined plot (included in fun_plots.r)\n\nplot_box_violin <- function(df, x){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_violin(aes(y = 0), fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1))\n}\n\n\nHere is the plot for sale_price\n\nIn this instance, the skew is NOT due to only a few outliers\n\n\ndata_trn |> plot_box_violin(\"sale_price\")\n\n\n\n\n\nCoding sidebar:\n\nYou can make figures for all numeric variables at once using select() and map() as before\n\n\ndata_trn |> \n  select(where(is.numeric)) |> # <1> \n  names() |> \n  map(\\(name) plot_box_violin(df = data_trn, x = name)) |> \n  plot_grid(plotlist = _, ncol = 2)\n\nWarning: Removed 1 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\nNow select numeric rather than factor but otherwise same as previous example\n\n\n\n\n2.5.9 Summary Statistics for Numeric Variables (Univariate)\nskim_all() provided all the summary statistics you likely needed for numeric variables\n\nmean & median (p50)\nsd & IQR (see difference between p25 and p75)\nskew & kurtosis\n\nYou can get skim of only numeric variables if you like\n\ndata_trn |> \n  skim_all() |> \n  filter(skim_type == \"numeric\")\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndata_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\n\n\n\n\n\n\n2.5.10 Bivariate Relationships with Outcome\nBivariate relationships with the outcome are useful to detect\n\nWhich predictors display some relationship with the outcome\nWhat feature engineering (transformations) might maximize that relationship\nAre there any bivariate (model) outliers\n\nAgain, we prefer visualizations but summary statistics are also available\n\n\n\n2.5.11 Scatterplots for Numeric Variables (Bivariate)\nScatterplots are the preferred visualization when both variables are numeric\nDefine a scatterplot function (included in fun_plots.r)\n\nadd a simple line\nadd a LOWESS line (Locally Weighted Scatterplot Smoothing)\nThese lines are useful for considering shape of relationship\n\n\nplot_scatter <- function(df, x, y){\n  df |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"green\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n\n\nLet’s consider relationship between gr_liv_area and sale_price\n\nCare most about influential points (both model outlier and leverage)\nCan be typically spotted in bivariate plots (but could do more sophisticated assessments)\nWe might:\n\nretain as is\ndrop\nbring to fence\n\n\nIf bivariate outliers are detected, you should return to cleaning mode to verify that they aren’t result of scoring/coding errors. If they are:\n\nFix in full dataset\nUse same train/test split after fixing\n\n\ndata_trn |> plot_scatter(\"gr_liv_area\", \"sale_price\")\n\n\n\n\n\nHere is another example where the relationship might be non-linear\n\ndata_trn |> plot_scatter(\"year_built\", \"sale_price\")\n\n\n\n\n\nA transformation of sale_price might help the relationship with \\(year\\_built\\) but might hurt gr_liv_area\nMaybe need to transform both sale_price and gr_liv_area as both were skewed\nThis might require some more EDA but here is a start\n\nQuick and temporary Log (base e) of sale_price\nThis doesn’t seem promising by itself\n\n\ndata_trn |> \n  mutate(sale_price = log(sale_price)) |> \n  plot_scatter(\"gr_liv_area\", \"sale_price\")\n\n\n\ndata_trn |> \n  mutate(sale_price = log(sale_price)) |>\n  plot_scatter(\"year_built\", \"sale_price\")\n\n\n\n\n\nCan make scatterplots for ordered factors as well\n\nBut other (perhaps better) options also exist for this combination of variable classes.\nUse as.numeric() to allow for lm and LOWESS lines on otherwise categorical variable\n\n\ndata_trn |> \n  mutate(overall_qual = as.numeric(overall_qual)) |> \n  plot_scatter(\"overall_qual\", \"sale_price\")\n\n\n\n\n\nCoding sidebar: Use jitter() with x to help with overplotting\n\ndata_trn |> \n  mutate(overall_qual = jitter(as.numeric(overall_qual))) |> \n  plot_scatter(\"overall_qual\", \"sale_price\")\n\n\n\n\n\n\n\n2.5.12 Correlations & Correlation Plots for Numeric Variables (Bivariate)\nCorrelations are useful summary statistics for numeric variables\nSome statistical algorithms are sensitive to high correlations among features (multi-collinearity)\nAt best, highly correlated features add unnecessary flexibility and can lead to overfitting\n\nWe can visualize correlations among predictors/features using corrplot.mixed() from corrplot package\n\nBest for numeric variables\nCan include ordinal or two level nominal variables if transformed to numeric\nCan include nominal variables with > 2 levels if first transformed appropriately (e.g., dummy features, not demonstrated yet)\nWorks best with relatively small set of variables\n\n\n\ndata_trn |> \n  mutate(overall_qual = as.numeric(overall_qual),\n         garage_qual = as.numeric(garage_qual)) |> \n  select(where(is.numeric)) |> \n  cor(use = \"pairwise.complete.obs\") |> \n  corrplot::corrplot.mixed() # <1>\n\n\n\n\n\nNote use of namespace (corrplot::corrplot.mixed()) to call this function from corrplot package\n\n\n\n\n2.5.13 Grouped Box + Violin Plots for Categorical and Numeric (Bivariate)\nA grouped version of the combined box and violin plot is our preferred visualization for relationship between categorical and numeric variables (included in fun_plots.r)\n\nOften best when feature is categorical and outcome is numeric but can reverse\nCan use with both nominal and ordinal categorical variable\nWickham, Çetinkaya-Rundel, and Grolemund (2023) also describes use of grouped frequency polygons for this combination of variable classes\n\n\nplot_grouped_box_violin <- function(df, x, y){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\n\nHere is the relationship between overall_qual and sale_price\n\nTend to prefer this over the scatterplot (with as.numeric()) for ordinal variables\nIncreasing spread of sale_price at higher levels of overall_qual is clearer in this plot\n\n\ndata_trn |> plot_grouped_box_violin(\"overall_qual\", \"sale_price\")\n\n\n\n\n\nHere is a grouped box + violin with a nominal variable\n\nMore variation and skew in sale_price for one family homes (additional features, moderators?)\nPosition of townhouse (interior vs. exterior) seems to matter (don’t collapse?)\n\n\ndata_trn |> plot_grouped_box_violin(\"bldg_type\", \"sale_price\")\n\n\n\n\n\nWhen we have a categorical predictor and a numeric outcome, we often want to see both the relationship between the variables AND the variability on the categorical variable alone.\nWe like this combined plot enough when doing EDA to provide a specific function (included in fun_plots.r)!\nIt is our go to for understanding the potential effect of a categorical predictor\n\nplot_categorical <- function(df, x, y, ordered = FALSE){\n  if (ordered) {\n    df <- df |>\n      mutate(!!x := fct_reorder(.data[[x]], .data[[y]]))\n  }\n  \n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  p_bar <- df |>\n    ggplot(aes(x = .data[[x]])) +\n    geom_bar()  +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n  \n  p_box <- df |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n  \n  return(list(p_bar, p_box))\n}\n\n\nsale_price by bldg_type\n\ndata_trn |> plot_categorical(\"bldg_type\", \"sale_price\") |> \n  plot_grid(plotlist = _, ncol = 1)\n\n\n\n\n\n\n\n2.5.14 Stacked Barplots for Categorical (Bivariate)\nStacked Barplots:\n\nCan be useful with both nominal and ordinal variables\nCan create with either raw counts or percentages.\n\nDisplays different perspective (particularly with uneven distributions across levels)\nDepends on your question\n\nOften, you will place the outcome on the x-axis and the feature is coded by fill\n\n\nplot_grouped_barplot_count <- function(df, x, y){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[y]], fill = .data[[x]])) +\n    geom_bar(position = \"stack\") +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\nplot_grouped_barplot_percent <- function(df, x, y){\n  x_label_size <- if_else(skimr::n_unique(df[[x]]) < 7, 11, 7)\n  \n  df |>\n    ggplot(aes(x = .data[[y]], fill = .data[[x]])) +\n    geom_bar(position = \"fill\") +\n    labs(y = \"Proportion\") +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\n\nFor example, if we wanted to learn about how bldg_type varies by lot_config, see these plots\n\ndata_trn |> plot_grouped_barplot_count(\"lot_config\", \"bldg_type\")\n\n\n\n\n\ndata_trn |> plot_grouped_barplot_percent(\"lot_config\", \"bldg_type\")\n\n\n\n\n\nMay want to plot both ways\n\ndata_trn |> plot_grouped_barplot_percent(\"lot_config\", \"bldg_type\")\n\n\n\n\n\ndata_trn |> plot_grouped_barplot_percent(\"bldg_type\", \"lot_config\")\n\n\n\n\n\n\n\n2.5.15 Tile Plot for Ordered Categorical (Bivariate)\nTile plots may be useful if both categorical variables are ordinal\n\nplot_tile <- function(df, x, y){\n  df |>\n    count(.data[[x]], .data[[y]]) |>\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_tile(mapping = aes(fill = n))\n}\n\n\n\ndata_trn |> plot_tile(\"overall_qual\", \"garage_qual\")\n\n\n\n\n\nYou might also consider a scatterplot with jitter in this instance\n\ndata_trn |> \n  mutate(overall_qual = jitter(as.numeric(overall_qual)),\n         garage_qual = as.numeric(garage_qual)) |> \n  plot_scatter(\"overall_qual\", \"garage_qual\")\n\n\n\n\n\n\n\n2.5.16 Two-way Tables for Categorical Variables (Bivariate)\nTwo-way tables are sometimes a useful summary statistic for two categorical variables. We can use a 2-variable form of the tab function, tab2(), from my function scripts for this\nFor example, the relationship between bldg_type and lot_config\n\ndata_trn |> tab2(bldg_type, lot_config)\n\n   bldg_type corner culdsac fr2 fr3 inside\n      duplex     13       0   3   0     47\n     one_fam    221      73  29   1    892\n    town_end      8       5   3   0     92\n town_inside      0       3   3   0     40\n     two_fam      6       0   1   1     24"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#working-with-recipes",
    "href": "002_exploratory_data_analysis.html#working-with-recipes",
    "title": "2  Exploratory Data Analysis",
    "section": "2.6 Working with Recipes",
    "text": "2.6 Working with Recipes\nRecipes are used for feature engineering in tidymodels using the recipes package\n\nUsed for transforming raw predictors into features used in our models\nDescribes all steps to make feature matrix. For example:\n\nTransforming factors into “dummy” features if needed\nLinear and non-linear transformations (e.g., log, box-cox)\nPolynomials and interactions (i.e., x1 * x1 or x1 * x2)\nMissing value imputations\n\nProper use of recipes is an important tool to prevent data leakage between train and either validation or test.\nRecipes use only information from the training set in all feature engineering\nConsider example of standardizing x1 for a feature in train vs. validation and test. Must use mean and sd from TRAIN to standardize x1 in train, validate, and test. VERY IMPORTANT.\n\n\nWe use recipes in a two step process - prep() and bake()\n\n“Prepping” a recipe involves calculating any statistics needed for the transformations that will be applied to engineer features (e.g., mean and standard deviation to normalize a numeric variable).\n\nPrepping is done with the prep() function.\nPrepping is always done only with training data. A “prepped” recipe does not derive any statistics from validation or test sets.\n\n“Baking” is the process of calculating the features\n\nBaking is done with the bake() function.\nWe used our prepped recipe when we bake.\nWhereas we only prep a recipe with training data, we can use a prepped recipe to bake features from any dataset (training, validation, or test).\n\n\n\nWe will work with recipes extensively when model building starting in unit 3\nFor now, we will only use the recipe to indicate roles as a gentle introduction.\nWe will expand on this recipe in unit 3\nRecipe syntax is very similar to generic tidyverse syntax (created by same group)\n\nActually a subset of tidyverse functions\nLess flexible/powerful but focused on our needs and easier to learn\nYou will eventually know both\n\n\nRecipes are used in Modeling scripts (which is a third type of script after cleaning and modeling EDA scripts)\n\nLets reload training again to pretend we are starting a new script\n\n\n data_trn <- read_csv(file.path(path_data, \"ames_clean_class_trn.csv\"), \n                      col_types = cols()) |> \n  class_ames() |> # <1> \n  glimpse()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `garage_qual = fct_relevel(...)`.\nCaused by warning:\n! 1 unknown level in `f`: ex\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   <dbl> 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  <dbl> 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     <dbl> 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   <dbl> 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual <fct> 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  <dbl> 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  <fct> ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    <fct> res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   <fct> inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    <fct> one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nRemember our function for classing!\n\n\nRecipes can be used to indicate the outcome and predictors that will be used in the model\n\nCan use . to indicate all predictors\n\nCurrently, our preferred method for larger data sets\nWe can exclude some predictors later by changing their role, removing them with a later recipe step (\\(step\\_rm()\\)), or specifying a more precise formula when we fit the model\nSee Roles in Recipes for more info\n\nCan use specific names of predictors along with \\(+\\)\n\nThis is our preferred method when we have a smaller set of predictors (We will show you both approaches)\ne.g., sale_price ~ lot_area + year_built + overall_qual\n\nDo NOT indicate interactions here\n\nAll predictors are combined with +\nInteractions are specified by a later explicit feature engineering step\n\n\n\nrec <- recipe(sale_price ~ ., data = data_trn)\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 9\n\nsummary(rec)\n\n# A tibble: 10 × 4\n   variable     type      role      source  \n   <chr>        <list>    <chr>     <chr>   \n 1 gr_liv_area  <chr [2]> predictor original\n 2 lot_area     <chr [2]> predictor original\n 3 year_built   <chr [2]> predictor original\n 4 overall_qual <chr [3]> predictor original\n 5 garage_cars  <chr [2]> predictor original\n 6 garage_qual  <chr [3]> predictor original\n 7 ms_zoning    <chr [3]> predictor original\n 8 lot_config   <chr [3]> predictor original\n 9 bldg_type    <chr [3]> predictor original\n10 sale_price   <chr [2]> outcome   original\n\n\n\n\n2.6.1 Prepping and Baking a Recipe\nLet’s make a feature matrix from the training set\nThere are two discrete (and important) steps\n\nprep()\nbake()\n\nFirst we prep the recipe using the training data\n\nrec_prep <- rec |> # <1>\n  prep(training = data_trn) #<2>\n\n\nWe start by prepping our raw/original recipe (rec)\nWe use the prep() function on on the training data. Recipes are ALWAYS prepped using training data. This makes sure that are recipes will always only use information from the training set when making features for any subsequent dataset.\n\n\nSecond, we bake the training data using this prepped recipe to get a feature matrix from it.\n\nfeat_trn <- rec_prep |> \n  bake(new_data = data_trn)\n\n\nFinally, we should generally at least glimpse and/or skim (and typically do some more EDA) on our features to make sure our recipe is doing what we expect.\n\nglimpse\n\n\nfeat_trn |> glimpse()\n\nRows: 1,465\nColumns: 10\n$ gr_liv_area  <dbl> 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     <dbl> 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   <dbl> 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual <fct> 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  <dbl> 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  <fct> ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    <fct> res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   <fct> inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    <fct> one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n$ sale_price   <dbl> 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n\n\n\n\nskim\n\n\nfeat_trn |> skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n5\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n10\n\n\n5: 424, 6: 350, 7: 304, 8: 176\n\n\n\n\ngarage_qual\n\n\n0\n\n\n1\n\n\n5\n\n\nta: 1312, no_: 81, fa: 57, gd: 13\n\n\n\n\nms_zoning\n\n\n0\n\n\n1\n\n\n7\n\n\nres: 1157, res: 217, flo: 66, com: 13\n\n\n\n\nlot_config\n\n\n0\n\n\n1\n\n\n5\n\n\nins: 1095, cor: 248, cul: 81, fr2: 39\n\n\n\n\nbldg_type\n\n\n0\n\n\n1\n\n\n5\n\n\none: 1216, tow: 108, dup: 63, tow: 46\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\n\n\n\n\nWe can now use our features from training to train models, but that will take place in the next unit!\nWe could also use the prepped recipe to bake validation or test data to evaluate trained models. That too will happen in the next unit!"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#discussion-topics",
    "href": "002_exploratory_data_analysis.html#discussion-topics",
    "title": "2  Exploratory Data Analysis",
    "section": "2.7 Discussion Topics",
    "text": "2.7 Discussion Topics\n\nHouse keeping\n\nUnit 2 solutions\nQuizzes\nCourse evals for extra credit (to quiz score)!\nUnit 3 homework\n\ntest set predictions\nfree lunch!\n\n\nReview\n\nGoal is to develop model that closely approximates DGP\nGoal is to evaluate (estimate) how close our model is to the DGP (how much error) with as little error as possible\nBias, overfitting/variance for any estimate (model and performance of model)\ncandidate model configurations\nfit, select, evaluate\ntraining, validation, test\n\nReview: 2.2.1 Stages of Data Analysis and Model Development\nBest practices (discuss quickly)\n\ncsv for data sharing, viewing, git (though be careful with data in github or other public repo!)\nvariable values saved as text when nominal and ordinal (self-documenting)\nCreate data dictionary - Documentation is critical!!\nsnake_case for variables and self-documenting names (systematic names too)\n\nReview: 2.3.1 Data Leakage Issues\n\nReview section in webbook\nCleaning EDA is done with full dataset (but univariate). Very limited (variable names, values, find errors)\nModeling EDA is only done with a training set (or even “eyeball” sample) - NEVER use validate or test set\nNever estimate anything with full data set (e.g., missing values, standardize, etc)\nUse recipes, prep (all estimation) with held in data than bake the appropriate set\nPut test aside\nYou work with validation but never explore with validation (will still catch leakage with test but will be mislead to be overly optimistic and spoil test)\n\nFunctions sidenote - fun_modeling.R on github\nReview: 2.4.2 Prepping and Baking a Recipe\n\nReview section in web book\nprep always with held in data, bake with held in & out data.\n\nEDA for modeling\n\nlimitless, just scratched the surface\nDiffers some based on dimenstionality of dataset\nLearning about DGP\n\nunderstand univariate distributions, frequencies\nbivariate relationships\ninteractions (3 or more variables)\npatterns in data\n\n\nExtra topics, time permitting\n\n8.1. Missing data\n- Exclude vs. Impute in training data.  Outcomes?\n- How to impute\n- Missing in validate or test (can't exclude?). Exclude cases with missing outcomes.\n8.2. Outliers - Drop or fix errors! - Goal is always to estimate DGP - Exclude - Retain - Bring to fence - Don’t exclude/change outcome in validate/test\n8.3. Issues with high dimensionality\n- Hard to do predictor level EDA\n- Common choices (normality transformations)\n- observed vs. predicted plots\n- Methods for automated variable selection (glmnet)\n8.4. Distributional Shape\n- Measurement issues (interval scale)\n- Implications for relationships with other variables\n- Solutions?\n8.5. Linearity vs. More Complex Relationships\n- Transformations\n- Choice of statistical algorithm\n- Do you need a linear model?\n8.6. Interactions\n- Domain expertise\n- Visual options for interactions\n- But what do do with high dimensional data?\n- Explanatory vs. prediction goals (algorithms that accommodate interactions)\n8.7. How to handle all of these decisions in the machine learning framework\n- Goal is to develop a model that most closely approximates the DGP\n- How does validation and test help this?\n- Preregistration?\n  - Pre-reg for performance metric, resampling method   \n  - Use of resampling for other decisions\n  - Use of resampling to find correct model to test explanatory goals\n8.8. Model Assumptions\n- Why do we make assumptions?\n  - Inference\n  - Flexibility wrt DGP\n\n\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. https://adv-r.hadley.nz/.\n\n\nWickham, Hadley, Çetinkaya-Rundel Mine, and Garrett Grolemund. 2023. R for Data Science: Visualize, Model, Transform, and Import Data. 2nd ed. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "001_overview.html#overview-of-unit",
    "href": "001_overview.html#overview-of-unit",
    "title": "1  Overview of Machine Learning",
    "section": "1.1 Overview of Unit",
    "text": "1.1 Overview of Unit\n\n1.1.1 Learning Objectives\n\nUnderstand uses for machine learning models\nBecome familiar with key terminology (presented in bold throughout this unit)\nUnderstand differences between models\n\nSupervised vs. unsupervised\nRegression vs. classification\nOptions for statistical algorithms\nFeatures vs. predictors\n\nRelationships between:\n\nData generating processes\nStatistical algorithms\nModel flexibility\nModel interpretability\nPrediction vs. explanation\n\nUnderstand Bias-Variance Trade-off\n\nReducible and irreducible error\nWhat is bias and variance?\nWhat affects bias and variance?\nWhat is overfitting and how does it relate to bias, variance, and also p-hacking\nUse of training and test sets to assess bias and variance\n\n\n\n\n\n1.1.2 Readings\n\nYarkoni and Westfall (2017) paper\nJames et al. (2023) Chapter 2, pp 15 - 42\n\n\n\n1.1.3 Lecture & Discussion Videos\nNotes: You can adjust the playback speed of the videos to meet your needs. Closed captioning is also available for all videos.\n\nLecture 1: An Introductory Framework ~ 9 mins\nLecture 2: More Details on Supervised Techniques ~ 23 mins\nLecture 3: Key Terminology in Context ~ 11 mins\nLecture 4: An Example of Bias-Variance Tradeoff ~ 27 mins\nDiscussion 1\nDiscussion 2\n\n\n\n1.1.4 Quiz and Application Assignment\n\nNo application assignment this unit!\nThe unit quiz is due by 8 pm on Wednesday January 24th"
  },
  {
    "objectID": "001_overview.html#an-introductory-framework-for-machine-learning",
    "href": "001_overview.html#an-introductory-framework-for-machine-learning",
    "title": "1  Overview of Machine Learning",
    "section": "1.2 An Introductory Framework for Machine Learning",
    "text": "1.2 An Introductory Framework for Machine Learning\nMachine (Statistical) learning techniques have developed in parallel in statistics and computer science\nTechniques can be coarsely divided into supervised and unsupervised approaches\n\nSupervised approaches involve models that predict an outcome using features\nUnsupervised approaches involve finding structure (e.g., clusters, factors) among a set of variables without any specific outcome specified\nThis course will focus primarily on supervised machine learning problems\nHowever supervised approaches often use unsupervised approaches in early stages as part of feature engineering\n\n\nExamples of supervised approaches include:\n\nPredicting relapse day-by-day among recovering patients with substance use disorders based on cellular communications and GPS.\nScreening someone as positive or negative for substance use disorder based on their Facebook activity\nPredicting the sale price of a house based on characteristics of the house and its neighborhood\n\nExamples of unsupervised approaches include:\n\nDetermining the factor structure of a set of personality items\nIdentifying subgroups among patients with alcohol use disorder based on demographics, use history, addiction severity, and other patient characteristics\nIdentifying the common topics present in customer reviews of some new product or app\n\n\nSupervised machine learning approaches can be categorized as either regression or classification techniques\n\nRegression techniques involve numeric (quantitative) outcomes.\n\nRegression techniques are NOT limited to “regression” (i.e., the general linear model)\n\nThere are many more types of statistical models that are appropriate for numeric outcomes\n\nClassification techniques involve nominal (categorical) outcomes\nMost regression and classification techniques can handle categorical predictors\n\nAmong the earlier supervised model examples, predicting sale price was a regression technique and screening individuals as positive or negative for substance use disorder was a classification technique"
  },
  {
    "objectID": "001_overview.html#more-details-on-supervised-techniques",
    "href": "001_overview.html#more-details-on-supervised-techniques",
    "title": "1  Overview of Machine Learning",
    "section": "1.3 More Details on Supervised Techniques",
    "text": "1.3 More Details on Supervised Techniques\nFor supervised machine learning problems, we assume \\(Y\\) (outcome) is a function of some data generating process (DGP, \\(f\\)) involving a set of Xs (features) plus the addition of random error (\\(\\epsilon\\)) that is independent of X and with mean of 0\n\\(Y = f(X) + \\epsilon\\)\n\nTerminology sidebar: Throughout the course we will distinguish between the raw predictors available in a dataset and the features that are derived from those raw predictors through various transformations.\n\nWe estimate \\(f\\) (the DGP) for two main reasons: prediction and/or inference (i.e., explanation per Yarkoni and Westfall, 2017)\n\\(\\hat{Y} = \\hat{f}(X)\\)\nFor prediction, we are most interested in the accuracy of \\(\\hat{Y}\\) and typically treat \\(\\hat{f}\\) as a black box\nFor inference, we are typically interested in the way that \\(Y\\) is affected by \\(X\\)\n\nWhich predictors are associated with \\(Y\\)?\nWhich are the strongest/most important predictors of \\(Y\\)\nWhat is the relationship between the outcome and the features associated with each predictor. Is the overall relationship between a predictor and \\(Y\\) positive, negative, dependent on other predictors? What is the shape of relationship (e.g., linear or more complex)?\nDoes the model as a whole improve prediction beyond a null model (no features from predictors) or beyond a compact model?\nWe care about good (low error) predictions even when we care about inference (we want small \\(\\epsilon\\))\n\nThey will also be tested with low power\nParameter estimates from models that don’t predict well may be incorrect or at least imprecise\n\n\n\nModel error includes both reducible and irreducible error.\nIf we consider both \\(X\\) and \\(\\hat{f}\\) to be fixed, then:\n\n\\(E(Y - \\hat{Y})^2 = (f(X) + \\epsilon - \\hat{f}(X))^2\\)\n\\(E(Y - \\hat{Y})^2 = [f(X) - \\hat{f}(X)]^2 + Var(\\epsilon)\\)\n\n\\(Var(\\epsilon)\\) is irreducible\n\nIrreducible error results from other important \\(X\\) that we fail to measure and from measurement error in \\(X\\) and \\(Y\\)\nIrreducible error serves as an (unknown) bounds for model accuracy (without collecting additional Xs)\n\n\\([f(X) - \\hat{f}(X)]^2\\) is reducible\n\nReducible error results from a mismatch between \\(\\hat{f}\\) and the true \\(f\\)\nThis course will focus on techniques to estimate \\(f\\) with the goal of minimizing reducible error\n\n\n\n1.3.1 How Do We Estimate \\(f\\)?\n\nWe need a sample of \\(N\\) observations of \\(Y\\) and \\(X\\) that we will call our training set\nThere are two types of statistical algorithms that we can use for \\(\\hat{f}\\):\n\nParametric algorithms\nNon-parametric algorithms\n\n\n\nParametric algorithms:\n\nFirst, make an assumption about the functional form or shape of \\(f\\).\n\nFor example, the general linear model assumes: \\(f(X) = \\beta_0 + \\beta_1*X_1 + \\beta_2*X2 + ... + \\beta_p*X_p\\)\nNext, a model using that algorithm is fit to the training set. In other words, the parameter estimates (e.g., \\(\\beta_0, \\beta_1\\)) are derived to minimize some cost function (e.g., mean squared error for the linear model)\nParametric algorithms reduce the problem of estimating \\(f\\) down to one of only estimating some set of parameters for a chosen model\nParametric algorithms often yield more interpretable models\nBut they are often not very flexible. If you chose the wrong algorithm (shape for \\(\\hat{f}\\) that does not match \\(f\\)) the model will not fit well in the training set (and more importantly not in the new test set either)\n\nTerminology sidebar: A training set is a subset of your full dataset that is used to fit a model. In contrast, a validation set is a subset that has not been included in the training set and is used to select a best model from among competing model configurations. A test set is a third subset of the full dataset that has not been included in either the training or validation sets and is used for evaluating the performance of your fitted final/best model.\n\nNon-parametric algorithms:\n\nDo not make any assumption about the form/shape of \\(f\\)\nCan fit well for a wide variety of forms/shapes for \\(f\\)\nThis flexibility comes with costs\n\nThey generally require larger \\(N\\) in the training set than parametric algorithms to achieve comparable performance\nThey may overfit the training set. This happens when they begin to fit the noise in the training set. This will yield low error in training set but much higher error in new validation or test sets.\nThey are often less interpretable\n\n\n\nGenerally:\n\nFlexibility and interpretability are inversely related\nModels need to be flexible enough to fit \\(f\\) well\nAdditional flexibility beyond this can produce overfitting\nParametric algorithms are generally less flexible than non-parametric algorithms\nParametric algorithms can become more flexible by increasing the number of features (\\(p\\) from 610/710; e.g., using more predictors, more complex, non-linear forms to when deriving features from predictors)\nParametric algorithms can be made less flexible through regularization. There are techniques to make some non-parametric algorithms less flexible as well\nYou want the sweet spot for prediction. You may want even less flexible for inference in increase interpretability.\n\n\n\n\n\n1.3.2 How Do We Assess Model Performance?\nThere is no universally best statistical algorithm\n\nDepends on the true \\(f\\) and your goal (prediction or inference)\nWe often compare multiple statistical algorithms (various parametric and non-parametric options) and model configurations more generally (combinations of different algorithms with different sets of features)\nWhen comparing models/configurations, we need to both fit these models and then select the best one\n\n\nBest needs to be defined with respect to some performance metric in new (validation or test set) data\n\nThere are many performance metrics you might use\nRoot Mean squared error (RMSE) is common for regression problems\nAccuracy is common for classification problems\n\nWe will learn many other performance metrics in a later unit\n\nTwo types of performance problems are typical\n\nModels are underfit if they don’t adequately represent the true \\(f\\), typically because they have oversimplied the relationship (e.g., linear function fit to quadratic DGP, missing key interaction terms)\n\nUnderfit models will yield biased predictions. In other words, they will systematically either under-predict or over-predict \\(Y\\) in some regions of the function.\nBiased models will perform poorly in both training and test sets\n\nModels are overfit if they are too flexible and begin to fit the noise in the training set.\n\nOverfit models will perform well (too well actually) in the training set but poorly in test or validation sets\nThey will show high variance such that the model and its predictions change drastically depending on the training set where it is fit\n\n\n\nMore generally, these problems and their consequences for model performance are largely inversely related\n\nThis is known as the Bias-Variance trade-off\nWe previously discussed reducible and irreducible error\n\nReducible error can be parsed into components due to bias and variance\nGoal is to minimize the sum of bias and variance error (i.e., the reducible error overall)\nWe will often trade off a little bias if it provides a big reduction in variance\n\n\nBut before we dive further into the Bias-Variance trade-off, lets review some key terminology that we will use throughout this course."
  },
  {
    "objectID": "001_overview.html#key-terminology-in-context",
    "href": "001_overview.html#key-terminology-in-context",
    "title": "1  Overview of Machine Learning",
    "section": "1.4 Key Terminology in Context",
    "text": "1.4 Key Terminology in Context\nIn the following pages:\n\nWe will present the broad steps for developing and evaluating machine learning models\nWe will situate key terms in this context (along with other synonymous terms used by others) and highlight them in bold.\n\nMachine learning has emerged in parallel from developments in statistics and computer science.\n\nAs a result, there is a lot of terminology and often multiple terms used for the same concept. This is not my fault!\n\nI will try to use one set of terms, but you need to be familiar with other terms you will encounter\n\n\nWhen developing a supervised machine learning model to predict or explain an outcome (also called DV, label, output):\n\nOur goal is for the model to match as close as possible (given the limits due to irreducible error) the true data generating process for Y.\nWe typically consider multiple (often many) candidate model configurations to achieve this goal.\n\n\nCandidate model configurations can vary with respect to:\n\nthe statistical algorithm used\nthe algorithm’s hyperparameters\nthe features used in the model to predict the outcome\n\n\nStatistical algorithms can be coarsely categorized as parametric or non-parametric.\nBut we will mostly focus on a more granular description of the specific algorithm itself\nExamples of specific statistical algorithms we will learn in this course include the linear model, generalized linear model, elastic net, LASSO, ridge regression, neural networks, KNN, random forest.\n\nThe set of candidate model configurations often includes variations of the same statistical algorithm with different hyperparameter (also called tuning parameter) values that control aspects of the algorithm’s operation.\n\nExamples include \\(k\\) in the KNN algorithm and \\(lambda\\) in LASSO, Ridge and Elastic Net algorithms.\nWe will learn more about hyperparameters and their effects later in this course.\n\n\nThe set of candidate model configurations can vary with respect to the features that are included.\n\nA recipe describes how to transform raw data for predictors (also called IVs) into features (also called regressors, inputs) that are included in the feature matrix (also called design matrix, model matrix).\n\nThis process of transforming predictors into features in a feature matrix is called feature engineering.\n\n\nCrossing variation on statistical algorithms, hyperparameter values, and alternative sets of features can increase the number of candidate model configurations dramatically\n\ndeveloping a machine learning model can easily involve fitting thousands of model configurations.\nIn most implementations of machine learning, the number of candidate model configurations nearly ensures that some fitted models will overfit the dataset in which they are developed such that they capitalize on noise that is unique to the dataset in which they were fit.\n\nFor this reason, model configurations are assessed and selected on the basis of their relative performance for new data (observations that were not involved in the fitting process).\n\n\nWe have ONE full dataset but we use resampling techniques to form subsets of that dataset to enable us to assess models’ performance in new data.\nCross-validation and bootstrapping are both examples of classes of resampling techniques that we will learn in this course.\nBroadly, resampling techniques create multiple subsets that consist of random samples of the full dataset. These different subsets can be used for model fitting, model selection, and model evaluation.\n\nTraining sets are subsets that are used for model fitting (also called model training). During model fitting, models with each candidate model configuration are fit to the data in the training set. For example, during fitting, model parameters are estimated for regression algorithms, and weights are established for neural network algorithms. Some non-parametric algorithms, like k-nearest neighbors, do not estimate parameters but simply “memorize” the training sets for subsequent predictions.\nValidation sets are subsets that are used for model selection (or, more accurately, for model configuration selection). During model selection, each (fitted) model — one for every candidate model configuration — is used to make predictions for observations in a validation set that, importantly, does not overlap with the model’s training set. On the basis of each model’s performance in the validation set, the relatively best model configuration (i.e., the configuration of the model that performs best relative to all other model configurations) is identified and selected. If you have only one model configuration, validation set(s) are not needed because there is no need to select among model configurations.\nTest sets are subsets that are used for model evaluation. Generally, a model with the previously identified best configuration is re-fit to all available data other than the test set. This fitted model is used to predict observations in the test set to estimate how well this model is expected to perform for new observations.\n\n\nThere are three broad steps to develop and evaluate a machine learning model:\n\nFitting models with multiple candidate model configurations (in training set(s))\nAssessing each model to select the best configuration (in validation set(s))\nEvaluating how well a model with that best configuration will perform with new observations (in test sets(s))"
  },
  {
    "objectID": "001_overview.html#an-example-of-the-bias-variance-trade-off",
    "href": "001_overview.html#an-example-of-the-bias-variance-trade-off",
    "title": "1  Overview of Machine Learning",
    "section": "1.5 An Example of the Bias-Variance Trade-off",
    "text": "1.5 An Example of the Bias-Variance Trade-off\n\n1.5.1 Overview of Example\nThe concepts of underfitting vs. overfitting and the bias-variance trade-off are critical to understand\n\nIt is also important to understand how model flexibility can affect both the bias and variance of that model’s performance\nIt can help to make these abstract concepts concrete by exploring real models that are fit in actual data\nWe will conduct a very simple simulation to demonstrate these concepts\n\nThe code in this example is secondary to understanding the concepts of underfittinng, overfitting, bias, variance, and the bias-variance trade-off\n\nWe will not display much of it so that you can maintain focus on the concepts\nYou will have plenty of time to learn the underlying\n\n\nWhen modeling, our goal is typically to approximate the data generating process (DGP) as close as possible, but in the real world we never know the true DGP.\nA key advantage of many simulations is that we do know the DGP because we define it ourselves.\n\nFor example, in this simulation, we know that \\(Y\\) is a cubic function of \\(X\\) and noise (random error).\nIn fact, we know the exact equation for calculating \\(Y\\) as a function of \\(X\\).\n\n\\(y = 1100 - 4.0 * x - 0.4 * x^2 + 0.1 * (x - h)^3 + noise\\), where:\n\nb0 = 1100\nb1 = -4.0\nb2 = -0.4\nb3 = 0.1\nh = -20.0\nnoise has mean = 0 and sd = 150\n\n\n\nWe will attempt to model this cubic DGP with three different model configurations\n\nA simple linear model that uses only \\(X\\) as a feature\nA (20th order) polynomial linear model that uses 20 polynomials of \\(X\\) as features\nA (20th order) polynomial LASSO model that uses the same 20 polynomials of \\(X\\) as features but “regularizes” to remove unimportant features from the model\n\n\n\n\n\n\n\nQuestion: If the DGP for y is a cubic function of x, what do we know about the expected bias for our three candidate model configurations in this example?\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model will underfit the true DGP and therefore it will be biased b/c \nit can only represent Y as a linear function of X.  \n\nThe two polynomial models will be generally unbiased b/c they have X represented \nwith 20th order polynomials.  \n\nLASSO will be slightly biased due to regularization but more on that in a later unit\n\n\n\n\n\n\n\n\n1.5.2 Stimulation Steps\nWith that introduction complete, lets start our simulation of the bias-variance trade-off\n\nLets simulate four separate research teams, each working to estimate the DGP for Y\n\n\nEach team will get their own random sample of training data (N = 100) to fit models\n\nHere are plots of these four simulated training sets (one for each team) with a dotted line for the data generating process (DGP)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe get one more large random sample (N = 5000) with the same DGP to use as a test set to evaluate all the models that will be fit in the separate training sets across the four teams.\n\n\nWe will let each team use this same test set to keep things simple\nThe key is that the test set contains new observations not present in any of the training sets\n\n\n\n\n\n\n\n\nEach of the four teams fit their three model configurations in their training sets\nThey use the resulting models to make predictions for observations in the same training set in which they were fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can you see evidence of bias for any model configuration? Look in any training set.\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model is clearly biased.  It systemically underestimates Y in \nsome portions of the X distribution and overestimates Y in other portions of the \nX distribution.  This is true across training sets for all teams.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can you see any evidence of overfitting for any model configuration?\n\n\n\n\n\n\n\nShow Answer\nThe polynomial linear model appears to overfit the data in the training set.  In \nother words, it seems to follow both the signal/DGP and the noise.  However, in practice\nnone of the teams could not be certain of this with only their training set.  It is\npossible that the wiggles in the prediction line represent the real DGP.   They need\nto look at the model's performance in the test set to be certain about the degree of\noverfitting.  (Of course, we know because these are simulated data and we know the DGP.)\n\n\n\n\n\n\n\nNow the teams use their 3 trained models to make predictions for new observations in the test set\n\n\nRemember that the test set has NEW observations of X and Y that weren’t used for fitting any of the models.\nLets look at each model configuration’s performance in test separately\n\n\n\nHere are predictions from the four simple linear models (fit in the training sets for each team) in the test set\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can you see evidence of bias for the simple linear models?\n\n\n\n\n\n\n\nShow Answer\nYes, consistent with what we saw in the training sets, the simple linear model\nsystematically overestimates Y in some places and underestimates it in others.  \nThe DGP is clearly NOT linear but this simple model can only make linear predictions.\nIt is a fairly biased model that underfits the true DGP.  This bias will make a \nlarge contribution to the reducible error of the model\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How much variance across the simple linear models is present?\n\n\n\n\n\n\n\nShow Answer\nThere is not much variance in the prediction lines across the models that were \nfit by different teams in different training sets.  The slopes are very close across\nthe different team's models and the intercepts only vary by a small amount.   \nThe simple linear model configuration does not appear to have high variance (across teams) \nand therefore model variance will not contribute much to its reducible error.\n\n\n\n\n\n\n\nHere are predictions from the polynomial linear models from the four teams in the test set\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Are these polynomial models systematically biased?\n\n\n\n\n\n\n\nShow Answer\nThere is not much systematic bias.  The overall function is generally cubic for \nall four teams  - just like the DGP. Bias will not contribute much to the model's \nreducible error.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does the variance of these polynomial models compare to the variance of the simple linear models?\n\n\n\n\n\n\n\nShow Answer\nThere is much higher model variance for this polynomial linear model relative to \nthe simple linear model. Although all four models generally predict Y as a cubic\nfunction of X, there is also a non-systematic wiggle that is different for each \nteam's models.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does this demonstrate the connection between model overfitting and model variance?\n\n\n\n\n\n\n\nShow Answer\nModel variance (across teams) is a result of overfitting to the training set.  \nIf a model fits noise in its training set, that noise will be different in every dataset.\nTherefore, you end up with different models depending on the training set in which they \nare fit.  And none of those models will do well with new data as you can see in this\ntest set because noise is random and different in each dataset.\n\n\n\n\n\n\n\nHere are predictions from the polynomial LASSO models from each team in the test set\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does their bias compare to the simple and polynomial linear models?\n\n\n\n\n\n\n\nShow Answer\nThe LASSO models have low bias much like the polynomial linear model. They are able \nto capture the true cubic DGP fairly well.  The regularization process slightly reduced the\nmagnitude of the cubic (the prediction line is a little straighter than it should be),\nbut not by much.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does their variance compare to the simple and polynomial linear models?\n\n\n\n\n\n\n\nShow Answer\nAll four LASSO models, fit in different training sets, resulted in very similar \nprediction lines. Therefore, these LASSO models have low variance, much like the simple linear model. \nIn contrast, the LASSO model variance is clearly lower than the more flexible \npolynomimal model.\n\n\n\n\n\n\n\nNow we will quantify the performance of these models in training and test sets with the root mean square error performance metric. This is the standard deviation of the error when comparing the predicted values for Y to the actual values (ground truth) for Y.\n\n\n\n\n\n\n\nQuestion: What do we expect about RMSE for the three models in train and test?\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model is underfit to the TRUE DGP.  Therfore it is \nsystematically biased everywhere it is used.  It won't fit well in train or test \nfor this reason.  However, it’s not very flexible so it won’t be overfit to the noise \nin train and therefore should fit comparably in train and test.  \n\nThe polynomial linear model will not be biased at all given that the DGP is polynomial.  \nHowever, it is overly flexible (20th order) and so will substantially overfit the \ntraining data such that it will show high variance and its performance will be poor in test.  \n\nThe polynomial LASSO will be the sweet spot in bias-variance trade-off.  It has \na little bias but not much.  However, it is not as flexible due to regularization\nby lambda so it won’t be overfit to its training set.  Therefore, it should do \nwell in the test set.\n\n\n\n\n\n\nTo better understand this:\n\nCompare RMSE across the three model configurations within the training sets (turquoise line)\nCompare how RMSE changes for each model configuration across its training set and the test set\nCompare RMSE across the three model configurations within the test set (red line)?\nSpecifically compare the performance of simple linear model (least flexible) with the polynomial linear model (most flexible)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Would these observations about bias and variance of these three model configurations always be the same regardless of the DGP?\n\n\n\n\n\n\n\nShow Answer\nNo.  A model configuration needs to be flexible enough and/or well designed to \nrepresent the DGP for the data that you are modeling.   The two polynomial models\nin this example were each able to represent a cubic DGP.  The simple linear model\nwas not.  The polynomial linear model was too flexible for a cubic given that it \nhad 20 polynomials of X.  Therefore, it was overfit to its training set and had \nhigh variance.  However, if the DGP was a different shape, the story would be \ndifferent.  If the DGP was linear the simple linear model would not have been \nbiased and would have performed best. If this DGP was some other form (step function),\nit may be that none of the models would work well."
  },
  {
    "objectID": "001_overview.html#discussion---tuesdaythursday",
    "href": "001_overview.html#discussion---tuesdaythursday",
    "title": "1  Overview of Machine Learning",
    "section": "1.6 Discussion - Tuesday/Thursday",
    "text": "1.6 Discussion - Tuesday/Thursday\n\n1.6.1 Course Overview\n\nIntroductions (preferred name, pronouns, program/department and year)\n\n\n\nStructure of course\n\nSame flow each week\n\nWeek starts on Thursdays at 12:15 pm\nAssignments include:\n\nPre-recorded lectures\nWeb book material\nReadings (James et al. (2023) and other sources; All free)\nApplication assignment\n\nAsychronous discussion and questions on Slack\nLab section on Tuesdays at 11:00 am - address previous week’s code\nQuiz and application assignments due Wednesdays at 8 pm\nWrap-up discussion and conceptual questions on Thursdays at 11 am. Not a lecture\n\nSelf-paced (except for due dates and discussion)\nQuizzes used to encourage and assess reading. Also to guide discussion.\nWorkload similar to 610/710\nOffice hours\n\nJohn - Thursdays, 1 – 2 pm\nMichelle - Wednesdays, 10 - 11 am\nKendra - Mondays, 2:30 - 3:30 pm\nPersonal appointments & Slack\n\n\n\n\n\nThe web book\n\nPrimary source for all course materials\nOrganized by units (with syllabus at front)\nLinks to pre-recorded lectures, readings, and quiz\nProvides primary source for code examples (all you need for this course)\nLecture follows book\n\n\n\n\nCourse as guided learning\n\nConcepts in lectures and readings\nApplications in web book and application assignment\nDiscussion section is discussion/questions (not pre-planned lecture)\nSlack CAN be a good source for discussion and questions as well\nGrades are secondary (quizzes, application assignments, exams)\n\n\n\n\nWhy these tools?\n\nQuarto\n\nScientific publishing system (reproducible code, with output, presentations, papers)\nTool for collaboration\nInteractive with static product (render to html or pdf)\nApplication assignments, web book, and slides\n\ntidyverse?\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures\nUnmatched for data wrangling, EDA, and data visualization\n\ntidymodels?\n\nConsistent interface to hundreds of statistical algorithms\nUnder VERY active development\nWell-supported and documented (tidymodels.org)\nBe careful with other web documentation (often out of date)\n\n\n\n\n\nWhy me?\n\nPrimary tools in our research program\nModel for progression from 610/710\nCan do AND can collaborate with CS and expert data scientists\n\n\n\n\nEnvironment\n\nSafe and respectful\nVERY encouraging of questions, discussion, and tangents\nHave fun\nAccomodations and Complaints\n\n\n\n\nChatGPT\n\nYay! May develop into an amazing tool in your workflow\nUse as tool (like Stack Overflow) for applications (application assignments, application questions on exams)\nCheck carefully - it can be wrong even when it looks right\nDo NOT use for conceptual questions (quizzes, conceptual exam questions). This type of info needs to be in your head to be effective data scientist.\n\nAcademic Integrity\n\nDo not cheat! Only you lose.\nNo collaboration with classmates, peers, previous students on anything graded (including application assignments)\nAll cheating reported to Department and Dean of Students. If application assignment or quizzes, zero on all of them because I can’t trust them. If exam, zero on exam.\n\n\n\n\n\n1.6.2 Association vs. Prediction\n\n\n\n\n\n\nQuestion: What is the difference between association vs. prediction?\n\n\n\n\n\n\n\nShow Answer\nAssociation quantifies the relationship between variables within a sample \n(predictors-outcome).  Prediction requires using an established model to \npredict (future?) outcomes for new (\"out-of-sample, \"held-out\") participants.\n\n\n\n\n\n\n\nMuch research in psychology demonstrates association but calls it prediction!\nAssociation (sometimes substantially) overestimates the predictive strength of our models\n\nCoefficients are derived to mimimize SSE (or maximize \\(R^2\\))\n\\(R^2\\) from GLM (using one sample) indexes how well on average any GLM that is fit to a sample will account for variance in that sample when specific coefficients are estimated in the same sample they are evaluated\n\\(R^2\\) does NOT tell you how well a specific GLM (including its coefficients) will work with new data for prediction\n\\(R^2\\) itself is positively biased even as estimate of how well a sample specific GLM will predict in that sample (vs. adjusted \\(R^2\\) and other corrections)\n\n\n\n\n\n1.6.3 Prediction vs. Explanation\n\nExamples of valuable prediction without explanation?\nCan you have explanation without prediction?\nPrediction models can provide insight or tests of explanatory theories (e.g., do causes actually predict in new data; variable importance)\nGoal of scientific psychology is to understand human behavior. It involves both explaining behavior (i.e., identifying causes) and predicting (yet to be observed) behaviors.\n\nWe overemphasize explanatory goals in this department, IMO\n\nMachine learning is well positioned for prediction, but also for explanation\n\n\n\n\n1.6.4 Basic framework and terminology for machine learning\n\nSupervised vs. unsupervised machine learning?\nSupervised regression vs classification?\n\n\n\n\n1.6.5 Data Generating Process\nWhat is a data generating process?\n\n\\(Y = f(X) + \\epsilon\\)\nboth function and Xs are generally unknown\n\\(\\hat{Y} = \\hat{f}(X)\\)\n\nWhy do we estimate the data generating process?\n\n\n\n1.6.6 Cross Validation\n\nWhat is it and why do we do it?\nHow are replication and cross-validation different?\n\n\nReducible vs. Irreducible error?\n\nOur predictions will have error\nYou learned to estimate parameters in the GLM to minimize error in 610\nBut error remained non-zero (in your sample and more importantly with same model in new samples) unless you perfectly estimated the DGP\nThat error can be divided into two sources\nIrreducible error comes from measurement error in X, Y and missing X because predictors (causes) not measured.\n\nIrreducible without collecting new predictors and/or with new measures\nIt places a ceiling on the performance of the best model you can develop with your available data\n\nReducible error comes from mismatch between \\(\\hat{f}(X)\\) and the true \\(f(X)\\).\n\nWe can reduce this without new data.\n\nJust need better model (\\(\\hat{f}(X)\\)).\n\nYou didn’t consider this (much) in 610 because you were limited to one statistical algorithm (GLM) AND it didn’t have hyperparameters.\nYou did reduce error by coding predictors (feature engineering) differently (interactions \\(X1*X2\\), polynomial terms \\(X^2\\), power transformations of X) This course will teach you methods to decrease reducible error (and validly estimate total error of that best model)\n\n\n\nWhat are the three general steps by which we estimate and evaluate the data generating process with a sample of data? Lets use all this vocabulary!\n\nCandidate model configurations\n\nStatistical algorithms\nHyperparameters\nFeatures (vs. predictors?), feature matrix, feature engineering, recipe (tidymodels specific)\n\nModel fitting (training), selection, and evaluation\nResampling techniques\n\ncross validation techniques (k-fold)\nboostrapping for cross validation\n\nTraining, validation, and test sets (terms vary in literature!)\n\n\n\n\n1.6.7 Bias-variance tradeoff\nWhat is underfitting, overfitting, bias, and variance?\nBias and variance are general concepts to understand during any estimation process\n\nEstimate mean, median, standard deviation\nParameter estimates in GLM\nEstimate DGP - \\(\\hat{Y} = \\hat{f}(X)\\)\n\n\nConceptual example of bias-variance: Darts from Yarkoni and Westfall (2017)\n\n\nSecond Conceptual Example: Models (e.g. many scales made by Acme Co.) to measure my weight\n\n\n\n1.6.8 Bias - A deeper dive\nBiased models are generally less complex models (i.e., underfit) than the data-generating process for your outcome\n\nBiased models lead to errors in prediction because the model will systematically over- or under-predict outcomes (scores or probabilities) for specific values of predictor(s) (bad for prediction goals!)\nParameter estimates from biased models may over or under-estimate the true effect of a predictor (bad for explanatory goals!)\n\n\n\n\n\n\n\n\nQuestion: Are GLMs biased models?\n\n\n\n\n\n\n\nShow Answer\nGLM parameter estimates are BLUE - best **linear** unbiased estimators. Parameter\nestimates from any sample are unbiased estimates of the linear model coefficients\nfor population model but if DGP is not linear, this linear model will produce \nbiased predictions and have biased parameter estimates.\n\n\n\n\n\n\nBias seems like a bad thing.\n\nBoth bias (due to underfitting) and variance (due to overfitting) are sources of (reducible) prediction errors (and imprecise/inaccurate parameter estimates). They are also often inversely related (i.e., the trade-off).\nA model configuration needs to be flexible enough to represent the true DGP.\n\nAny more flexibility will lead to overfitting.\n\nAny less flexibility will lead to underfitting.\n\nIdeally, if your model configuration is perfectly matched to the DGP, it will have very low bias and very low variance (assuming sufficiently large N)\nThe world is complex. In many instances,\n\nWe can’t perfectly represent the DGP\nWe trade off a little bias for big reduction in variance to produce the most accurate predictions (and stable parameter estimates across samples for explanatory goals)\nOr we trade off a little variance (slightly more flexible model) to get a big reduction in bias\nEither way, we get models that predict well and may be useful for explanatory goals\n\n\n\n\n\n1.6.9 Variance - A deeper dive\n\n\n\n\n\n\nQuestion: Consider example of p = n - 1 in general linear model. What happens in this situation? How is this related to overfitting and model flexibility?\n\n\n\n\n\n\n\nShow Answer\nThe  model will perfectly fit the sample data even when there is no relationship\nbetween the predictors and the outcome.  e.g., Any two points can be fit perfectly\nwith one predictor (line), any three points can be fit perfectly with two predictors\n(plane).  This model will NOT predict well in new data.  This model is overfit \nbecause n-1 predictors is too flexible for the linear model. You will fit the \nnoise in the training data.\n\n\n\n\n\n\nFactors that increase overfitting\n\nSmall N\nComplex models (e.g, many predictors, p relative to n, non-parametric models)\nWeak effects of predictors (lots of noise available to overfit)\nCorrelated predictors (for some algorithms like the GLM)\nChoosing between many model configurations (e.g. different predictors or predictor sets, transformations, types of statistical models) - lets return to this when we consider p-hacking\n\n\nYou might have noticed that many of the above factors contribute to the standard error of a parameter estimate/model coefficient from the GLM\n\nSmall N\nBig p\nSmall \\(R^2\\) (weak effects)\nCorrelated predictors\n\nThe standard error increases as model overfitting increases due to these factors\n\n\n\n\n\n\n\nQuestion: Explain the link between model variance/overfitting, standard errors, and sampling distributions?\n\n\n\n\n\n\n\nShow Answer\nAll parameter estimates have a sampling distribution.  This is the distribution \nof estimates that you would get if you repeatedly fit the same model to new samples.\nWhen a model is overfit, that means that aspects of the model (its parameter \nestimates, its predictions) will vary greatly from sample to sample.  This is \nrepresented by a large standard error (the SD of the sampling distribution) for \nthe model's parameter estimates.  It also means that the predictions you will make\nin new data will be very different depending on the sample that was used to \nestimate the parameters.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Describe problem of p-hacking with respect to overfitting?\n\n\n\n\n\n\n\nShow Answer\nWhen you p-hack, you are overfitting the training set (your sample).  You try out\nmany, many different model configurations and choose the one you like best rather\nthan what works well in new data.  This model likely capitalizes on noise in your\nsample.  It won't fit well in another sample.  In other words, your conclusions \nare not linked to the true DGP and would be different if you used a different sample.\nIn a different vein, your significance test is wrong.  The SE does not reflect \nthe model variance that resulted from testing many different configurations b/c \nyour final model didn't \"know\" about the other models.  Statistically invalid \nconclusion!\n\n\n\n\n\n\nParameter estimates from an overfit model are specific to the sample within which they were trained and are not true for other samples or the population as a whole\n\nParameter estimates from overfit models have big (TRUE) SE and so they may be VERY different in other samples\n\nThough if the overfitting is due to fitting many models, it won’t be reflected in the SE from any one model because each model doesn’t know the other models exist! p-hacking!!\n\nWith traditional (one-sample) statistics, this can lead us to incorrect conclusions about the effect of predictors associated with these parameter estimates (bad for explanatory goals!).\nIf the parameter estimates are very different sample to sample (and different from the true population parameters), this means the model will predict poorly in new samples (bad for prediction goals!). We fix this by using resampling to evaluate model performance.\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag.\n\n\nYarkoni, Tal, and Jacob Westfall. 2017. “Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning.” Perspectives on Psychological Science 12 (6): 1100–1122."
  }
]