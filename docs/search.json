[
  {
    "objectID": "index.html#course-syllabus",
    "href": "index.html#course-syllabus",
    "title": "Introduction to Applied Machine Learning",
    "section": "Course Syllabus",
    "text": "Course Syllabus\n\nInstructor\nJohn Curtin\n\nOffice hours: Thursdays, 1-2 pm or by appointment in Brogden 326\nEmail: jjcurtin@wisc.edu (but please use Slack DM or channel posts for all course communications during this semester)\n\n\n\nTeaching Assistants\nMichelle Marji\n\nOffice hours: Wednesdays, 10-11 am in Brogden 391 or by appointment\nEmail: michelle.marji@wisc.edu\n\nKendra Wyant\n\nOffice hours: Mondays, 2:30-3:30 pm in Brodgen 325 or by appointment\nEmail: kpaquette2@wisc.edu\n\n\n\nCourse Website\nhttps://jjcurtin.github.io/book_iaml/\n\n\nCommunications\nAll course communications will occur in the course’s Slack workspace (https://iaml-2024.slack.com/). You should have received an invitation to join the workspace. If you have difficulty joining, please contact me by my email above. The TAs and I will respond to all Slack messages within 1 business day (and often much quicker). Please plan accordingly (e.g., weekend messages may not receive a response until Monday). For general questions about class, coding assignments, etc., please post the question to the appropriate public channel. If you have the question, you are probably not alone. For issues relevant only to you (e.g., class absences, accommodations, etc.), you can send a direct message in Slack to me. However, I may share the DM with the TAs unless you request otherwise. In general, we prefer that all course communication occur within Slack rather than by email so that it is centralized in one location.\n\n\nMeeting Times\nThe scheduled course meeting times are Tuesdays and Thursdays from 11:00 - 12:15 pm. Tuesdays are generally used by the TAs to discuss application issues from the homework or in the course more generally. Thursdays are generally led by John and used to discuss topics from the video lectures and readings.\nAll required videos, readings, and application assignments are described on the course website at the beginning of each unit.\n\n\nCourse Description\nThis course is designed to introduce students to a variety of computational approaches in machine learning. The course is designed with two key foci. First, students will focus on the application of common, “out-of-the-box” statistical learning algorithms that have good performance and are implemented in tidymodels in R. Second, students will focus on the application of these approaches in the context of common questions in behavioral science in academia and industry.\n\n\nRequisites\nStudents are required to have completed Psychology 610 with a grade of B or better or a comparable course with my consent.\n\n\nLearning Outcomes\n\nStudents will develop and refine best practices for data wrangling, general programming, and analysis in R.\nStudents will distinguish among a variety of machine learning settings: supervised learning vs. unsupervised learning, regression vs. classification\nStudents will be able to implement a broad toolbox of well-supported machine-learning methods: decision trees, nearest neighbor, general and generalized linear models, penalized models including ridge, lasso, and elastic-nets, neural nets, random forests.\nStudents will develop expertise with common feature extraction techniques for quantitative and categorical predictors.\nStudents will be able to use natural language processing approaches to extract meaningful features from text data.\nStudents will know how to characterize how well their regression and classification models perform and they will employ appropriate methodology for evaluating their: cross validation, ROC and PR curves, hypothesis testing.\nStudents will learn to apply their skills to common learning problems in psychology and behavioral sciences more generally.\n\n\n\nCourse Topics\n\nOverview of Machine Learning Concepts and Uses\nData wrangling in R using tidyverse and tidymodels\nIterations, functions, simulations in R\nRegression models\nClassification models\nModel performance metrics\nROCs\nCross validation and other resampling methods\nModel selection and regularization\nApproaches to parallel processing\nFeature engineering techniques\nNatural language processing\nTree based methods\nBagging and boosting\nNeural networks\nDimensionality reduction and feature selection\nExplanatory methods including variable importance, partial dependence plots, etc\nEthics and privacy issues\n\n\n\nSchedule\nThe course is organized around 14 weeks of academic instruction covering the following topics:\n\nIntroduction to course and machine learning\nExploratory data analysis\nRegression models\nClassification models\nResampling methods for model selection and evaluation\nRegularization and penalized models\nMidterm exam/project\nAdvanced performance metrics\nModel comparisons\nAdvanced models: Random forests and ensemble methods (bagging, boosting, stacking)\nAdvanced models: Neural networks\nNatural Language Processing I: Text processing and feature engineering\nApplications\nEthics\n\n\nThe final exam is during exam week on Tuesday May 7th from 11 - 12:15 in our normal classroom.\nThe final project is due during exam week on Wednesday May 8th at 8 pm.\n\n\n\nRequired Textbooks and Software\nAll required textbooks are freely available online (though hard copies can also be purchased if desired). There are eight required textbooks for the course. The primary text for which we will read many chapters is:\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. An Introduction to Statistical Learning: With Applications in R (2023; 2nd Edition). (website)\n\nWe will also read sections to chapters in each of the following texts:\n\nWickham, H. & Grolemund, G. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (1st ed.). Sebastopol, CA: O’Reilly Media, Inc. (website)\nHvitfeldt, E. & Silge, J. Supervised Machine Learning for Text Analysis in R (website)\nKuhn, M. & Johnson, K. Applied Predictive Modeling. New York, NYL Springer Science. (website)\nKuhn, M., & Johnson, K. Feature Engineering and Selection: A Practical Approach for Predictive Models (1st ed.). Boca Raton, FL: Chapman and Hall/CRC. (website)\nKuhn, M. & Silge, J. Tidy Modeling with R. (website)\nMolnar, C. Intepretable Machine Learning: A Guide for Makiong Black Box Models Explainable (2nd ed.). (website\nSilge, J., & Robinson, D. Text Mining with R: A Tidy Approach (1st ed.). Beijing; Boston: O’Reilly Media. (website)\nWickham, H. The Tidy Style Guide. (website)\nBoehmke, Brad and Greenwell, Brandon M. (2019). Hands-On Machine Learning with R. Chapman and Hall/CRC. (website)\nNg, Andrew (2018). Machine Learning Yearning: Technical Strategy for AI Engineers in the Age of Deep Learning. DeepLearning.AI. (website)\nWickham, H. (2019). Advanced R. Chapman and Hall/CRC. (website)\n\nAdditional articles will be assigned and provided by pdf through the course website.\nAll data processing and analysis will be accomplished using R (and we recommend the RStudio IDE). R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS.\n\n\nGrading\n\nQuizzes (13 anticipated): 15%\nApplication assignments (11 anticipated): 25%\nMidterm application exam: 15%\nMidterm conceptual exam: 15%\nFinal application exam: 15%\nFinal conceptual exam: 15%\n\nFinal letter grades may be curved upward, but a minimum guarantee is made of an A for 93 or above, AB for 88 - 92, B for 83 - 87, BC for 78 - 82, C for 70 - 77, D for 60-69, and F for &lt; 60.\n\n\nExams, Application Assignments and Quizzes\n\nThe midterm application exam will be due during the 7th week of the course on Wednesday, March 6th at 8 pm.\nThe midterm conceptual exam will be administered during class on Thursday, March 7th.\nThe final exam is during exam week on Tuesday May 7th from 11 - 12:15 in our normal classroom.\nThe final project is due during exam week on Wednesday May 8th at 8 pm.\nApproximately weekly quizzes will be administered through Canvas and due each Wednesday at 8 pm\nApproximately weekly application assignments will be submitted via Canvas and due each Wednesday at 8 pm.\n\n\n\nApplication Assignments\nThe approximately weekly application assignments are due on Wednesdays at 8 pm through Canvas. These assignments are to be done individually. Please do not share answers or code. You are also encouraged to make use of online resources (e.g., stack overflow) for assistance. All assignments will be completed using R markdown to provide both the code and documentation as might be provided to your mentor or employer to fully describe your solution. Late assignments are not accepted because problem solutions are provided immediately after the due date. Application assignments are graded on a three-point scale (0 = not completed, 1 = partially completed and/or with many errors, 2 = fully completed and at least mostly correct). Grades for each assignment will be posted by the following Monday at the latest.\n\n\nChatGPT\nI suspect you have all seen discussions of all that ChatGPT can do by now and its impact on teaching and assessment. I believe that AI like ChatGPT will eventually become an incredible tool for data scientists and programmers. As such, I view these advances with excitement. Of course, I don’t plan to assign a grade to ChatGPT so I want to make sure that we are clear on when you can and when you cannot use it. Given that I expect AI like ChatGPT to become a useful tool in our workflow as professionals, now is the time to start to learn how it can help. Therefore, you are free to use it during any of our application assignments AND the application questions on the mid-term and final exams. Code from ChatGPT is unlikely to be sufficient in either context (and my testing suggests it can be flat out wrong in some instances!) but I suspect that it will still be useful. In contrast, you CANNOT use ChatGPT to answer the conceptual questions on the two exams or the weekly quizzes. Those questions are designed to assess your working knowledge about concepts and best practices. That information must be in YOUR head and I want to be 100% clear that use of ChatGPT to answer those questions will be considered cheating and handled as such if detected. There will be a zero tolerance policy for such cheating. It will be reported to the Dean of Students on first offense.\n\n\nStudent Ethics\nThe members of the faculty of the Department of Psychology at UW-Madison uphold the highest ethical standards of teaching and research. They expect their students to uphold the same standards of ethical conduct. By registering for this course, you are implicitly agreeing to conduct yourself with the utmost integrity throughout the semester.\nIn the Department of Psychology, acts of academic misconduct are taken very seriously. Such acts diminish the educational experience for all involved – students who commit the acts, classmates who would never consider engaging in such behaviors, and instructors. Academic misconduct includes, but is not limited to, cheating on assignments and exams, stealing exams, sabotaging the work of classmates, submitting fraudulent data, plagiarizing the work of classmates or published and/or online sources, acquiring previously written papers and submitting them (altered or unaltered) for course assignments, collaborating with classmates when such collaboration is not authorized, and assisting fellow students in acts of misconduct. Students who have knowledge that classmates have engaged in academic misconduct should report this to the instructor.\n\n\nDiversity and Inclusion\nInstitutional statement on diversity: “Diversity is a source of strength, creativity, and innovation for UW-Madison. We value the contributions of each person and respect the profound ways their identity, culture, background, experience, status, abilities, and opinion enrich the university community. We commit ourselves to the pursuit of excellence in teaching, research, outreach, and diversity as inextricably linked goals.\nThe University of Wisconsin-Madison fulfills its public mission by creating a welcoming and inclusive community for people from every background – people who as students, faculty, and staff serve Wisconsin and the world.” https://diversity.wisc.edu/\n\n\nAcademic Integrity\nBy enrolling in this course, each student assumes the responsibilities of an active participant in UW-Madison’s community of scholars in which everyone’s academic work and behavior are held to the highest academic integrity standards. Academic misconduct compromises the integrity of the university. Cheating, fabrication, plagiarism, unauthorized collaboration, and helping others commit these acts are examples of academic misconduct, which can result in disciplinary action. This includes but is not limited to failure on the assignment/course, disciplinary probation, or suspension. Substantial or repeated cases of misconduct will be forwarded to the Office of Student Conduct & Community Standards for additional review. For more information, refer to http://studentconduct.wiscweb.wisc.edu/academic-integrity\n\n\nAccommodations Polices\nMcBurney Disability Resource Center syllabus statement: “The University of Wisconsin-Madison supports the right of all enrolled students to a full and equal educational opportunity. The Americans with Disabilities Act (ADA), Wisconsin State Statute (36.12), and UW-Madison policy (Faculty Document 1071) require that students with disabilities be reasonably accommodated in instruction and campus life. Reasonable accommodations for students with disabilities is a shared faculty and student responsibility. Students are expected to inform faculty [me] of their need for instructional accommodations by the end of the third week of the semester, or as soon as possible after a disability has been incurred or recognized. Faculty [I], will work either directly with the student [you] or in coordination with the McBurney Center to identify and provide reasonable instructional accommodations. Disability information, including instructional accommodations as part of a student’s educational record, is confidential and protected under FERPA.” http://mcburney.wisc.edu/facstaffother/faculty/syllabus.php\nUW-Madison students who have experienced sexual misconduct (which can include sexual harassment, sexual assault, dating violence and/or stalking) also have the right to request academic accommodations. This right is afforded them under Federal legislation (Title IX). Information about services and resources (including information about how to request accommodations) is available through Survivor Services, a part of University Health Services: https://www.uhs.wisc.edu/survivor-services/\n\n\nComplaints\nOccasionally, a student may have a complaint about a TA or course instructor. If that happens, you should feel free to discuss the matter directly with the TA or instructor. If the complaint is about the TA and you do not feel comfortable discussing it with the individual, you should discuss it with the course instructor. Complaints about mistakes in grading should be resolved with the TA and/or instructor in the great majority of cases. If the complaint is about the instructor (other than ordinary grading questions) and you do not feel comfortable discussing it with the individual, make an appointment to speak to the Associate Chair for Graduate Studies, Professor Shawn Green, cshawngreen@wisc.edu.\nIf you have concerns about climate or bias in this class, or if you wish to report an incident of bias or hate that has occurred in class, you may contact the Chair of the Department, Professor Allyson Bennett (allyson.j.bennett@wisc.edu) or the Chair of the Psychology Department Climate & Diversity Committee, Martha Alibali (martha.alibali@wisc.edu). You may also use the University’s bias incident reporting system\n\n\nPrivacy of Student Information & Digital Tools\nThe privacy and security of faculty, staff and students’ personal information is a top priority for UW-Madison. The university carefully reviews and vets all campus-supported digital tools used to support teaching and learning, to help support success through learning analytics, and to enable proctoring capabilities. UW-Madison takes necessary steps to ensure that the providers of such tools prioritize proper handling of sensitive data in alignment with FERPA, industry standards and best practices. Under the Family Educational Rights and Privacy Act (FERPA which protects the privacy of student education records), student consent is not required for the university to share with school officials those student education records necessary for carrying out those university functions in which they have legitimate educational interest. 34 CFR 99.31(a)(1)(i)(B). FERPA specifically allows universities to designate vendors such as digital tool providers as school officials, and accordingly to share with them personally identifiable information from student education records if they perform appropriate services for the university and are subject to all applicable requirements governing the use, disclosure and protection of student data.\n\n\nPrivacy of Student Records & the Use of Audio Recorded Lectures\nSee information about privacy of student records and the usage of audio-recorded lectures.\nLecture materials and recordings for this course are protected intellectual property at UW-Madison. Students in this course may use the materials and recordings for their personal use related to participation in this class. Students may also take notes solely for their personal use. If a lecture is not already recorded, you are not authorized to record my lectures without my permission unless you are considered by the university to be a qualified student with a disability requiring accommodation. [Regent Policy Document 4-1] Students may not copy or have lecture materials and recordings outside of class, including posting on internet sites or selling to commercial entities. Students are also prohibited from providing or selling their personal notes to anyone else or being paid for taking notes by any person or commercial firm without the instructor’s express written permission. Unauthorized use of these copyrighted lecture materials and recordings constitutes copyright infringement and may be addressed under the university’s policies, UWS Chapters 14 and 17, governing student academic and non-academic misconduct.\n\n\nAcademic Calendar & Religious Observances\nStudents who wish to inquire about religious observance accommodations for exams or assignments should contact the instructor within the first two weeks of class, following the university’s policy on religious observance conflicts"
  },
  {
    "objectID": "001_overview.html#overview-of-unit",
    "href": "001_overview.html#overview-of-unit",
    "title": "1  Overview of Machine Learning",
    "section": "1.1 Overview of Unit",
    "text": "1.1 Overview of Unit\n\n1.1.1 Learning Objectives\n\nUnderstand uses for machine learning models\nBecome familiar with key terminology (presented in bold throughout this unit)\nUnderstand differences between models\n\nSupervised vs. unsupervised\nRegression vs. classification\nOptions for statistical algorithms\nFeatures vs. predictors\n\nRelationships between:\n\nData generating processes\nStatistical algorithms\nModel flexibility\nModel interpretability\nPrediction vs. explanation\n\nUnderstand Bias-Variance Trade-off\n\nReducible and irreducible error\nWhat is bias and variance?\nWhat affects bias and variance?\nWhat is overfitting and how does it relate to bias, variance, and also p-hacking\nUse of training and test sets to assess bias and variance\n\n\n\n\n\n1.1.2 Readings\n\nYarkoni and Westfall (2017) paper\nJames et al. (2023) Chapter 2, pp 15 - 42\n\n\n\n1.1.3 Lecture & Discussion Videos\nNotes: You can adjust the playback speed of the videos to meet your needs. Closed captioning is also available for all videos.\n\nLecture 1: An Introductory Framework ~ 9 mins\nLecture 2: More Details on Supervised Techniques ~ 23 mins\nLecture 3: Key Terminology in Context ~ 11 mins\nLecture 4: An Example of Bias-Variance Tradeoff ~ 27 mins\nDiscussion 1\nDiscussion 2\n\n\n\n1.1.4 Quiz and Application Assignment\n\nNo application assignment this unit!\nThe unit quiz is due by 8 pm on Wednesday January 24th"
  },
  {
    "objectID": "001_overview.html#an-introductory-framework-for-machine-learning",
    "href": "001_overview.html#an-introductory-framework-for-machine-learning",
    "title": "1  Overview of Machine Learning",
    "section": "1.2 An Introductory Framework for Machine Learning",
    "text": "1.2 An Introductory Framework for Machine Learning\nMachine (Statistical) learning techniques have developed in parallel in statistics and computer science\nTechniques can be coarsely divided into supervised and unsupervised approaches\n\nSupervised approaches involve models that predict an outcome using features\nUnsupervised approaches involve finding structure (e.g., clusters, factors) among a set of variables without any specific outcome specified\nThis course will focus primarily on supervised machine learning problems\nHowever supervised approaches often use unsupervised approaches in early stages as part of feature engineering\n\n\nExamples of supervised approaches include:\n\nPredicting relapse day-by-day among recovering patients with substance use disorders based on cellular communications and GPS.\nScreening someone as positive or negative for substance use disorder based on their Facebook activity\nPredicting the sale price of a house based on characteristics of the house and its neighborhood\n\nExamples of unsupervised approaches include:\n\nDetermining the factor structure of a set of personality items\nIdentifying subgroups among patients with alcohol use disorder based on demographics, use history, addiction severity, and other patient characteristics\nIdentifying the common topics present in customer reviews of some new product or app\n\n\nSupervised machine learning approaches can be categorized as either regression or classification techniques\n\nRegression techniques involve numeric (quantitative) outcomes.\n\nRegression techniques are NOT limited to “regression” (i.e., the general linear model)\n\nThere are many more types of statistical models that are appropriate for numeric outcomes\n\nClassification techniques involve nominal (categorical) outcomes\nMost regression and classification techniques can handle categorical predictors\n\nAmong the earlier supervised model examples, predicting sale price was a regression technique and screening individuals as positive or negative for substance use disorder was a classification technique"
  },
  {
    "objectID": "001_overview.html#more-details-on-supervised-techniques",
    "href": "001_overview.html#more-details-on-supervised-techniques",
    "title": "1  Overview of Machine Learning",
    "section": "1.3 More Details on Supervised Techniques",
    "text": "1.3 More Details on Supervised Techniques\nFor supervised machine learning problems, we assume \\(Y\\) (outcome) is a function of some data generating process (DGP, \\(f\\)) involving a set of Xs (features) plus the addition of random error (\\(\\epsilon\\)) that is independent of X and with mean of 0\n\\(Y = f(X) + \\epsilon\\)\n\nTerminology sidebar: Throughout the course we will distinguish between the raw predictors available in a dataset and the features that are derived from those raw predictors through various transformations.\n\nWe estimate \\(f\\) (the DGP) for two main reasons: prediction and/or inference (i.e., explanation per Yarkoni and Westfall, 2017)\n\\(\\hat{Y} = \\hat{f}(X)\\)\nFor prediction, we are most interested in the accuracy of \\(\\hat{Y}\\) and typically treat \\(\\hat{f}\\) as a black box\nFor inference, we are typically interested in the way that \\(Y\\) is affected by \\(X\\)\n\nWhich predictors are associated with \\(Y\\)?\nWhich are the strongest/most important predictors of \\(Y\\)\nWhat is the relationship between the outcome and the features associated with each predictor. Is the overall relationship between a predictor and \\(Y\\) positive, negative, dependent on other predictors? What is the shape of relationship (e.g., linear or more complex)?\nDoes the model as a whole improve prediction beyond a null model (no features from predictors) or beyond a compact model?\nWe care about good (low error) predictions even when we care about inference (we want small \\(\\epsilon\\))\n\nThey will also be tested with low power\nParameter estimates from models that don’t predict well may be incorrect or at least imprecise\n\n\n\nModel error includes both reducible and irreducible error.\nIf we consider both \\(X\\) and \\(\\hat{f}\\) to be fixed, then:\n\n\\(E(Y - \\hat{Y})^2 = (f(X) + \\epsilon - \\hat{f}(X))^2\\)\n\\(E(Y - \\hat{Y})^2 = [f(X) - \\hat{f}(X)]^2 + Var(\\epsilon)\\)\n\n\\(Var(\\epsilon)\\) is irreducible\n\nIrreducible error results from other important \\(X\\) that we fail to measure and from measurement error in \\(X\\) and \\(Y\\)\nIrreducible error serves as an (unknown) bounds for model accuracy (without collecting additional Xs)\n\n\\([f(X) - \\hat{f}(X)]^2\\) is reducible\n\nReducible error results from a mismatch between \\(\\hat{f}\\) and the true \\(f\\)\nThis course will focus on techniques to estimate \\(f\\) with the goal of minimizing reducible error\n\n\n\n1.3.1 How Do We Estimate \\(f\\)?\n\nWe need a sample of \\(N\\) observations of \\(Y\\) and \\(X\\) that we will call our training set\nThere are two types of statistical algorithms that we can use for \\(\\hat{f}\\):\n\nParametric algorithms\nNon-parametric algorithms\n\n\n\nParametric algorithms:\n\nFirst, make an assumption about the functional form or shape of \\(f\\).\n\nFor example, the general linear model assumes: \\(f(X) = \\beta_0 + \\beta_1*X_1 + \\beta_2*X2 + ... + \\beta_p*X_p\\)\nNext, a model using that algorithm is fit to the training set. In other words, the parameter estimates (e.g., \\(\\beta_0, \\beta_1\\)) are derived to minimize some cost function (e.g., mean squared error for the linear model)\nParametric algorithms reduce the problem of estimating \\(f\\) down to one of only estimating some set of parameters for a chosen model\nParametric algorithms often yield more interpretable models\nBut they are often not very flexible. If you chose the wrong algorithm (shape for \\(\\hat{f}\\) that does not match \\(f\\)) the model will not fit well in the training set (and more importantly not in the new test set either)\n\nTerminology sidebar: A training set is a subset of your full dataset that is used to fit a model. In contrast, a validation set is a subset that has not been included in the training set and is used to select a best model from among competing model configurations. A test set is a third subset of the full dataset that has not been included in either the training or validation sets and is used for evaluating the performance of your fitted final/best model.\n\nNon-parametric algorithms:\n\nDo not make any assumption about the form/shape of \\(f\\)\nCan fit well for a wide variety of forms/shapes for \\(f\\)\nThis flexibility comes with costs\n\nThey generally require larger \\(N\\) in the training set than parametric algorithms to achieve comparable performance\nThey may overfit the training set. This happens when they begin to fit the noise in the training set. This will yield low error in training set but much higher error in new validation or test sets.\nThey are often less interpretable\n\n\n\nGenerally:\n\nFlexibility and interpretability are inversely related\nModels need to be flexible enough to fit \\(f\\) well\nAdditional flexibility beyond this can produce overfitting\nParametric algorithms are generally less flexible than non-parametric algorithms\nParametric algorithms can become more flexible by increasing the number of features (\\(p\\) from 610/710; e.g., using more predictors, more complex, non-linear forms to when deriving features from predictors)\nParametric algorithms can be made less flexible through regularization. There are techniques to make some non-parametric algorithms less flexible as well\nYou want the sweet spot for prediction. You may want even less flexible for inference in increase interpretability.\n\n\n\n\n\n1.3.2 How Do We Assess Model Performance?\nThere is no universally best statistical algorithm\n\nDepends on the true \\(f\\) and your goal (prediction or inference)\nWe often compare multiple statistical algorithms (various parametric and non-parametric options) and model configurations more generally (combinations of different algorithms with different sets of features)\nWhen comparing models/configurations, we need to both fit these models and then select the best one\n\n\nBest needs to be defined with respect to some performance metric in new (validation or test set) data\n\nThere are many performance metrics you might use\nRoot Mean squared error (RMSE) is common for regression problems\nAccuracy is common for classification problems\n\nWe will learn many other performance metrics in a later unit\n\nTwo types of performance problems are typical\n\nModels are underfit if they don’t adequately represent the true \\(f\\), typically because they have oversimplied the relationship (e.g., linear function fit to quadratic DGP, missing key interaction terms)\n\nUnderfit models will yield biased predictions. In other words, they will systematically either under-predict or over-predict \\(Y\\) in some regions of the function.\nBiased models will perform poorly in both training and test sets\n\nModels are overfit if they are too flexible and begin to fit the noise in the training set.\n\nOverfit models will perform well (too well actually) in the training set but poorly in test or validation sets\nThey will show high variance such that the model and its predictions change drastically depending on the training set where it is fit\n\n\n\nMore generally, these problems and their consequences for model performance are largely inversely related\n\nThis is known as the Bias-Variance trade-off\nWe previously discussed reducible and irreducible error\n\nReducible error can be parsed into components due to bias and variance\nGoal is to minimize the sum of bias and variance error (i.e., the reducible error overall)\nWe will often trade off a little bias if it provides a big reduction in variance\n\n\nBut before we dive further into the Bias-Variance trade-off, lets review some key terminology that we will use throughout this course."
  },
  {
    "objectID": "001_overview.html#key-terminology-in-context",
    "href": "001_overview.html#key-terminology-in-context",
    "title": "1  Overview of Machine Learning",
    "section": "1.4 Key Terminology in Context",
    "text": "1.4 Key Terminology in Context\nIn the following pages:\n\nWe will present the broad steps for developing and evaluating machine learning models\nWe will situate key terms in this context (along with other synonymous terms used by others) and highlight them in bold.\n\nMachine learning has emerged in parallel from developments in statistics and computer science.\n\nAs a result, there is a lot of terminology and often multiple terms used for the same concept. This is not my fault!\n\nI will try to use one set of terms, but you need to be familiar with other terms you will encounter\n\n\nWhen developing a supervised machine learning model to predict or explain an outcome (also called DV, label, output):\n\nOur goal is for the model to match as close as possible (given the limits due to irreducible error) the true data generating process for Y.\nWe typically consider multiple (often many) candidate model configurations to achieve this goal.\n\n\nCandidate model configurations can vary with respect to:\n\nthe statistical algorithm used\nthe algorithm’s hyperparameters\nthe features used in the model to predict the outcome\n\n\nStatistical algorithms can be coarsely categorized as parametric or non-parametric.\nBut we will mostly focus on a more granular description of the specific algorithm itself\nExamples of specific statistical algorithms we will learn in this course include the linear model, generalized linear model, elastic net, LASSO, ridge regression, neural networks, KNN, random forest.\n\nThe set of candidate model configurations often includes variations of the same statistical algorithm with different hyperparameter (also called tuning parameter) values that control aspects of the algorithm’s operation.\n\nExamples include \\(k\\) in the KNN algorithm and \\(lambda\\) in LASSO, Ridge and Elastic Net algorithms.\nWe will learn more about hyperparameters and their effects later in this course.\n\n\nThe set of candidate model configurations can vary with respect to the features that are included.\n\nA recipe describes how to transform raw data for predictors (also called IVs) into features (also called regressors, inputs) that are included in the feature matrix (also called design matrix, model matrix).\n\nThis process of transforming predictors into features in a feature matrix is called feature engineering.\n\n\nCrossing variation on statistical algorithms, hyperparameter values, and alternative sets of features can increase the number of candidate model configurations dramatically\n\ndeveloping a machine learning model can easily involve fitting thousands of model configurations.\nIn most implementations of machine learning, the number of candidate model configurations nearly ensures that some fitted models will overfit the dataset in which they are developed such that they capitalize on noise that is unique to the dataset in which they were fit.\n\nFor this reason, model configurations are assessed and selected on the basis of their relative performance for new data (observations that were not involved in the fitting process).\n\n\nWe have ONE full dataset but we use resampling techniques to form subsets of that dataset to enable us to assess models’ performance in new data.\nCross-validation and bootstrapping are both examples of classes of resampling techniques that we will learn in this course.\nBroadly, resampling techniques create multiple subsets that consist of random samples of the full dataset. These different subsets can be used for model fitting, model selection, and model evaluation.\n\nTraining sets are subsets that are used for model fitting (also called model training). During model fitting, models with each candidate model configuration are fit to the data in the training set. For example, during fitting, model parameters are estimated for regression algorithms, and weights are established for neural network algorithms. Some non-parametric algorithms, like k-nearest neighbors, do not estimate parameters but simply “memorize” the training sets for subsequent predictions.\nValidation sets are subsets that are used for model selection (or, more accurately, for model configuration selection). During model selection, each (fitted) model — one for every candidate model configuration — is used to make predictions for observations in a validation set that, importantly, does not overlap with the model’s training set. On the basis of each model’s performance in the validation set, the relatively best model configuration (i.e., the configuration of the model that performs best relative to all other model configurations) is identified and selected. If you have only one model configuration, validation set(s) are not needed because there is no need to select among model configurations.\nTest sets are subsets that are used for model evaluation. Generally, a model with the previously identified best configuration is re-fit to all available data other than the test set. This fitted model is used to predict observations in the test set to estimate how well this model is expected to perform for new observations.\n\n\nThere are three broad steps to develop and evaluate a machine learning model:\n\nFitting models with multiple candidate model configurations (in training set(s))\nAssessing each model to select the best configuration (in validation set(s))\nEvaluating how well a model with that best configuration will perform with new observations (in test sets(s))"
  },
  {
    "objectID": "001_overview.html#an-example-of-the-bias-variance-trade-off",
    "href": "001_overview.html#an-example-of-the-bias-variance-trade-off",
    "title": "1  Overview of Machine Learning",
    "section": "1.5 An Example of the Bias-Variance Trade-off",
    "text": "1.5 An Example of the Bias-Variance Trade-off\n\n1.5.1 Overview of Example\nThe concepts of underfitting vs. overfitting and the bias-variance trade-off are critical to understand\n\nIt is also important to understand how model flexibility can affect both the bias and variance of that model’s performance\nIt can help to make these abstract concepts concrete by exploring real models that are fit in actual data\nWe will conduct a very simple simulation to demonstrate these concepts\n\nThe code in this example is secondary to understanding the concepts of underfittinng, overfitting, bias, variance, and the bias-variance trade-off\n\nWe will not display much of it so that you can maintain focus on the concepts\nYou will have plenty of time to learn the underlying\n\n\nWhen modeling, our goal is typically to approximate the data generating process (DGP) as close as possible, but in the real world we never know the true DGP.\nA key advantage of many simulations is that we do know the DGP because we define it ourselves.\n\nFor example, in this simulation, we know that \\(Y\\) is a cubic function of \\(X\\) and noise (random error).\nIn fact, we know the exact equation for calculating \\(Y\\) as a function of \\(X\\).\n\n\\(y = 1100 - 4.0 * x - 0.4 * x^2 + 0.1 * (x - h)^3 + noise\\), where:\n\nb0 = 1100\nb1 = -4.0\nb2 = -0.4\nb3 = 0.1\nh = -20.0\nnoise has mean = 0 and sd = 150\n\n\n\nWe will attempt to model this cubic DGP with three different model configurations\n\nA simple linear model that uses only \\(X\\) as a feature\nA (20th order) polynomial linear model that uses 20 polynomials of \\(X\\) as features\nA (20th order) polynomial LASSO model that uses the same 20 polynomials of \\(X\\) as features but “regularizes” to remove unimportant features from the model\n\n\n\n\n\n\n\nQuestion: If the DGP for y is a cubic function of x, what do we know about the expected bias for our three candidate model configurations in this example?\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model will underfit the true DGP and therefore it will be biased b/c \nit can only represent Y as a linear function of X.  \n\nThe two polynomial models will be generally unbiased b/c they have X represented \nwith 20th order polynomials.  \n\nLASSO will be slightly biased due to regularization but more on that in a later unit\n\n\n\n\n\n\n\n\n1.5.2 Stimulation Steps\nWith that introduction complete, lets start our simulation of the bias-variance trade-off\n\nLets simulate four separate research teams, each working to estimate the DGP for Y\n\n\nEach team will get their own random sample of training data (N = 100) to fit models\n\nHere are plots of these four simulated training sets (one for each team) with a dotted line for the data generating process (DGP)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe get one more large random sample (N = 5000) with the same DGP to use as a test set to evaluate all the models that will be fit in the separate training sets across the four teams.\n\n\nWe will let each team use this same test set to keep things simple\nThe key is that the test set contains new observations not present in any of the training sets\n\n\n\n\n\n\n\n\nEach of the four teams fit their three model configurations in their training sets\nThey use the resulting models to make predictions for observations in the same training set in which they were fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can you see evidence of bias for any model configuration? Look in any training set.\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model is clearly biased.  It systemically underestimates Y in \nsome portions of the X distribution and overestimates Y in other portions of the \nX distribution.  This is true across training sets for all teams.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can you see any evidence of overfitting for any model configuration?\n\n\n\n\n\n\n\nShow Answer\nThe polynomial linear model appears to overfit the data in the training set.  In \nother words, it seems to follow both the signal/DGP and the noise.  However, in practice\nnone of the teams could not be certain of this with only their training set.  It is\npossible that the wiggles in the prediction line represent the real DGP.   They need\nto look at the model's performance in the test set to be certain about the degree of\noverfitting.  (Of course, we know because these are simulated data and we know the DGP.)\n\n\n\n\n\n\n\nNow the teams use their 3 trained models to make predictions for new observations in the test set\n\n\nRemember that the test set has NEW observations of X and Y that weren’t used for fitting any of the models.\nLets look at each model configuration’s performance in test separately\n\n\n\nHere are predictions from the four simple linear models (fit in the training sets for each team) in the test set\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can you see evidence of bias for the simple linear models?\n\n\n\n\n\n\n\nShow Answer\nYes, consistent with what we saw in the training sets, the simple linear model\nsystematically overestimates Y in some places and underestimates it in others.  \nThe DGP is clearly NOT linear but this simple model can only make linear predictions.\nIt is a fairly biased model that underfits the true DGP.  This bias will make a \nlarge contribution to the reducible error of the model\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How much variance across the simple linear models is present?\n\n\n\n\n\n\n\nShow Answer\nThere is not much variance in the prediction lines across the models that were \nfit by different teams in different training sets.  The slopes are very close across\nthe different team's models and the intercepts only vary by a small amount.   \nThe simple linear model configuration does not appear to have high variance (across teams) \nand therefore model variance will not contribute much to its reducible error.\n\n\n\n\n\n\n\nHere are predictions from the polynomial linear models from the four teams in the test set\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Are these polynomial models systematically biased?\n\n\n\n\n\n\n\nShow Answer\nThere is not much systematic bias.  The overall function is generally cubic for \nall four teams  - just like the DGP. Bias will not contribute much to the model's \nreducible error.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does the variance of these polynomial models compare to the variance of the simple linear models?\n\n\n\n\n\n\n\nShow Answer\nThere is much higher model variance for this polynomial linear model relative to \nthe simple linear model. Although all four models generally predict Y as a cubic\nfunction of X, there is also a non-systematic wiggle that is different for each \nteam's models.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does this demonstrate the connection between model overfitting and model variance?\n\n\n\n\n\n\n\nShow Answer\nModel variance (across teams) is a result of overfitting to the training set.  \nIf a model fits noise in its training set, that noise will be different in every dataset.\nTherefore, you end up with different models depending on the training set in which they \nare fit.  And none of those models will do well with new data as you can see in this\ntest set because noise is random and different in each dataset.\n\n\n\n\n\n\n\nHere are predictions from the polynomial LASSO models from each team in the test set\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does their bias compare to the simple and polynomial linear models?\n\n\n\n\n\n\n\nShow Answer\nThe LASSO models have low bias much like the polynomial linear model. They are able \nto capture the true cubic DGP fairly well.  The regularization process slightly reduced the\nmagnitude of the cubic (the prediction line is a little straighter than it should be),\nbut not by much.\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How does their variance compare to the simple and polynomial linear models?\n\n\n\n\n\n\n\nShow Answer\nAll four LASSO models, fit in different training sets, resulted in very similar \nprediction lines. Therefore, these LASSO models have low variance, much like the simple linear model. \nIn contrast, the LASSO model variance is clearly lower than the more flexible \npolynomimal model.\n\n\n\n\n\n\n\nNow we will quantify the performance of these models in training and test sets with the root mean square error performance metric. This is the standard deviation of the error when comparing the predicted values for Y to the actual values (ground truth) for Y.\n\n\n\n\n\n\n\nQuestion: What do we expect about RMSE for the three models in train and test?\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model is underfit to the TRUE DGP.  Therfore it is \nsystematically biased everywhere it is used.  It won't fit well in train or test \nfor this reason.  However, it’s not very flexible so it won’t be overfit to the noise \nin train and therefore should fit comparably in train and test.  \n\nThe polynomial linear model will not be biased at all given that the DGP is polynomial.  \nHowever, it is overly flexible (20th order) and so will substantially overfit the \ntraining data such that it will show high variance and its performance will be poor in test.  \n\nThe polynomial LASSO will be the sweet spot in bias-variance trade-off.  It has \na little bias but not much.  However, it is not as flexible due to regularization\nby lambda so it won’t be overfit to its training set.  Therefore, it should do \nwell in the test set.\n\n\n\n\n\n\nTo better understand this:\n\nCompare RMSE across the three model configurations within the training sets (turquoise line)\nCompare how RMSE changes for each model configuration across its training set and the test set\nCompare RMSE across the three model configurations within the test set (red line)?\nSpecifically compare the performance of simple linear model (least flexible) with the polynomial linear model (most flexible)\n\n\n\n`summarise()` has grouped output by 'model'. You can override using the\n`.groups` argument.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Would these observations about bias and variance of these three model configurations always be the same regardless of the DGP?\n\n\n\n\n\n\n\nShow Answer\nNo.  A model configuration needs to be flexible enough and/or well designed to \nrepresent the DGP for the data that you are modeling.   The two polynomial models\nin this example were each able to represent a cubic DGP.  The simple linear model\nwas not.  The polynomial linear model was too flexible for a cubic given that it \nhad 20 polynomials of X.  Therefore, it was overfit to its training set and had \nhigh variance.  However, if the DGP was a different shape, the story would be \ndifferent.  If the DGP was linear the simple linear model would not have been \nbiased and would have performed best. If this DGP was some other form (step function),\nit may be that none of the models would work well."
  },
  {
    "objectID": "001_overview.html#discussion---tuesdaythursday",
    "href": "001_overview.html#discussion---tuesdaythursday",
    "title": "1  Overview of Machine Learning",
    "section": "1.6 Discussion - Tuesday/Thursday",
    "text": "1.6 Discussion - Tuesday/Thursday\n\n1.6.1 Course Overview\n\nIntroductions (preferred name, pronouns, program/department and year)\n\n\n\nStructure of course\n\nSame flow each week\n\nWeek starts on Thursdays at 12:15 pm\nAssignments include:\n\nPre-recorded lectures\nWeb book material\nReadings (James et al. (2023) and other sources; All free)\nApplication assignment\n\nAsychronous discussion and questions on Slack\nLab section on Tuesdays at 11:00 am - address previous week’s code\nQuiz and application assignments due Wednesdays at 8 pm\nWrap-up discussion and conceptual questions on Thursdays at 11 am. Not a lecture\n\nSelf-paced (except for due dates and discussion)\nQuizzes used to encourage and assess reading. Also to guide discussion.\nWorkload similar to 610/710\nOffice hours\n\nJohn - Thursdays, 1 – 2 pm\nMichelle - Wednesdays, 10 - 11 am\nKendra - Mondays, 2:30 - 3:30 pm\nPersonal appointments & Slack\n\n\n\n\n\nThe web book\n\nPrimary source for all course materials\nOrganized by units (with syllabus at front)\nLinks to pre-recorded lectures, readings, and quiz\nProvides primary source for code examples (all you need for this course)\nLecture follows book\n\n\n\n\nCourse as guided learning\n\nConcepts in lectures and readings\nApplications in web book and application assignment\nDiscussion section is discussion/questions (not pre-planned lecture)\nSlack CAN be a good source for discussion and questions as well\nGrades are secondary (quizzes, application assignments, exams)\n\n\n\n\nWhy these tools?\n\nQuarto\n\nScientific publishing system (reproducible code, with output, presentations, papers)\nTool for collaboration\nInteractive with static product (render to html or pdf)\nApplication assignments, web book, and slides\n\ntidyverse?\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures\nUnmatched for data wrangling, EDA, and data visualization\n\ntidymodels?\n\nConsistent interface to hundreds of statistical algorithms\nUnder VERY active development\nWell-supported and documented (tidymodels.org)\nBe careful with other web documentation (often out of date)\n\n\n\n\n\nWhy me?\n\nPrimary tools in our research program\nModel for progression from 610/710\nCan do AND can collaborate with CS and expert data scientists\n\n\n\n\nEnvironment\n\nSafe and respectful\nVERY encouraging of questions, discussion, and tangents\nHave fun\nAccomodations and Complaints\n\n\n\n\nChatGPT\n\nYay! May develop into an amazing tool in your workflow\nUse as tool (like Stack Overflow) for applications (application assignments, application questions on exams)\nCheck carefully - it can be wrong even when it looks right\nDo NOT use for conceptual questions (quizzes, conceptual exam questions). This type of info needs to be in your head to be effective data scientist.\n\nAcademic Integrity\n\nDo not cheat! Only you lose.\nNo collaboration with classmates, peers, previous students on anything graded (including application assignments)\nAll cheating reported to Department and Dean of Students. If application assignment or quizzes, zero on all of them because I can’t trust them. If exam, zero on exam.\n\n\n\n\n\n1.6.2 Association vs. Prediction\n\n\n\n\n\n\nQuestion: What is the difference between association vs. prediction?\n\n\n\n\n\n\n\nShow Answer\nAssociation quantifies the relationship between variables within a sample \n(predictors-outcome).  Prediction requires using an established model to \npredict (future?) outcomes for new (\"out-of-sample, \"held-out\") participants.\n\n\n\n\n\n\n\nMuch research in psychology demonstrates association but calls it prediction!\nAssociation (sometimes substantially) overestimates the predictive strength of our models\n\nCoefficients are derived to mimimize SSE (or maximize \\(R^2\\))\n\\(R^2\\) from GLM (using one sample) indexes how well on average any GLM that is fit to a sample will account for variance in that sample when specific coefficients are estimated in the same sample they are evaluated\n\\(R^2\\) does NOT tell you how well a specific GLM (including its coefficients) will work with new data for prediction\n\\(R^2\\) itself is positively biased even as estimate of how well a sample specific GLM will predict in that sample (vs. adjusted \\(R^2\\) and other corrections)\n\n\n\n\n\n1.6.3 Prediction vs. Explanation\n\nExamples of valuable prediction without explanation?\nCan you have explanation without prediction?\nPrediction models can provide insight or tests of explanatory theories (e.g., do causes actually predict in new data; variable importance)\nGoal of scientific psychology is to understand human behavior. It involves both explaining behavior (i.e., identifying causes) and predicting (yet to be observed) behaviors.\n\nWe overemphasize explanatory goals in this department, IMO\n\nMachine learning is well positioned for prediction, but also for explanation\n\n\n\n\n1.6.4 Basic framework and terminology for machine learning\n\nSupervised vs. unsupervised machine learning?\nSupervised regression vs classification?\n\n\n\n\n1.6.5 Data Generating Process\nWhat is a data generating process?\n\n\\(Y = f(X) + \\epsilon\\)\nboth function and Xs are generally unknown\n\\(\\hat{Y} = \\hat{f}(X)\\)\n\nWhy do we estimate the data generating process?\n\n\n\n1.6.6 Cross Validation\n\nWhat is it and why do we do it?\nHow are replication and cross-validation different?\n\n\nReducible vs. Irreducible error?\n\nOur predictions will have error\nYou learned to estimate parameters in the GLM to minimize error in 610\nBut error remained non-zero (in your sample and more importantly with same model in new samples) unless you perfectly estimated the DGP\nThat error can be divided into two sources\nIrreducible error comes from measurement error in X, Y and missing X because predictors (causes) not measured.\n\nIrreducible without collecting new predictors and/or with new measures\nIt places a ceiling on the performance of the best model you can develop with your available data\n\nReducible error comes from mismatch between \\(\\hat{f}(X)\\) and the true \\(f(X)\\).\n\nWe can reduce this without new data.\n\nJust need better model (\\(\\hat{f}(X)\\)).\n\nYou didn’t consider this (much) in 610 because you were limited to one statistical algorithm (GLM) AND it didn’t have hyperparameters.\nYou did reduce error by coding predictors (feature engineering) differently (interactions \\(X1*X2\\), polynomial terms \\(X^2\\), power transformations of X) This course will teach you methods to decrease reducible error (and validly estimate total error of that best model)\n\n\n\nWhat are the three general steps by which we estimate and evaluate the data generating process with a sample of data? Lets use all this vocabulary!\n\nCandidate model configurations\n\nStatistical algorithms\nHyperparameters\nFeatures (vs. predictors?), feature matrix, feature engineering, recipe (tidymodels specific)\n\nModel fitting (training), selection, and evaluation\nResampling techniques\n\ncross validation techniques (k-fold)\nboostrapping for cross validation\n\nTraining, validation, and test sets (terms vary in literature!)\n\n\n\n\n1.6.7 Bias-variance tradeoff\nWhat is underfitting, overfitting, bias, and variance?\nBias and variance are general concepts to understand during any estimation process\n\nEstimate mean, median, standard deviation\nParameter estimates in GLM\nEstimate DGP - \\(\\hat{Y} = \\hat{f}(X)\\)\n\n\nConceptual example of bias-variance: Darts from Yarkoni and Westfall (2017)\n\n\nSecond Conceptual Example: Models (e.g. many scales made by Acme Co.) to measure my weight\n\n\n\n1.6.8 Bias - A deeper dive\nBiased models are generally less complex models (i.e., underfit) than the data-generating process for your outcome\n\nBiased models lead to errors in prediction because the model will systematically over- or under-predict outcomes (scores or probabilities) for specific values of predictor(s) (bad for prediction goals!)\nParameter estimates from biased models may over or under-estimate the true effect of a predictor (bad for explanatory goals!)\n\n\n\n\n\n\n\n\nQuestion: Are GLMs biased models?\n\n\n\n\n\n\n\nShow Answer\nGLM parameter estimates are BLUE - best **linear** unbiased estimators. Parameter\nestimates from any sample are unbiased estimates of the linear model coefficients\nfor population model but if DGP is not linear, this linear model will produce \nbiased predictions and have biased parameter estimates.\n\n\n\n\n\n\nBias seems like a bad thing.\n\nBoth bias (due to underfitting) and variance (due to overfitting) are sources of (reducible) prediction errors (and imprecise/inaccurate parameter estimates). They are also often inversely related (i.e., the trade-off).\nA model configuration needs to be flexible enough to represent the true DGP.\n\nAny more flexibility will lead to overfitting.\n\nAny less flexibility will lead to underfitting.\n\nIdeally, if your model configuration is perfectly matched to the DGP, it will have very low bias and very low variance (assuming sufficiently large N)\nThe world is complex. In many instances,\n\nWe can’t perfectly represent the DGP\nWe trade off a little bias for big reduction in variance to produce the most accurate predictions (and stable parameter estimates across samples for explanatory goals)\nOr we trade off a little variance (slightly more flexible model) to get a big reduction in bias\nEither way, we get models that predict well and may be useful for explanatory goals\n\n\n\n\n\n1.6.9 Variance - A deeper dive\n\n\n\n\n\n\nQuestion: Consider example of p = n - 1 in general linear model. What happens in this situation? How is this related to overfitting and model flexibility?\n\n\n\n\n\n\n\nShow Answer\nThe  model will perfectly fit the sample data even when there is no relationship\nbetween the predictors and the outcome.  e.g., Any two points can be fit perfectly\nwith one predictor (line), any three points can be fit perfectly with two predictors\n(plane).  This model will NOT predict well in new data.  This model is overfit \nbecause n-1 predictors is too flexible for the linear model. You will fit the \nnoise in the training data.\n\n\n\n\n\n\nFactors that increase overfitting\n\nSmall N\nComplex models (e.g, many predictors, p relative to n, non-parametric models)\nWeak effects of predictors (lots of noise available to overfit)\nCorrelated predictors (for some algorithms like the GLM)\nChoosing between many model configurations (e.g. different predictors or predictor sets, transformations, types of statistical models) - lets return to this when we consider p-hacking\n\n\nYou might have noticed that many of the above factors contribute to the standard error of a parameter estimate/model coefficient from the GLM\n\nSmall N\nBig p\nSmall \\(R^2\\) (weak effects)\nCorrelated predictors\n\nThe standard error increases as model overfitting increases due to these factors\n\n\n\n\n\n\n\nQuestion: Explain the link between model variance/overfitting, standard errors, and sampling distributions?\n\n\n\n\n\n\n\nShow Answer\nAll parameter estimates have a sampling distribution.  This is the distribution \nof estimates that you would get if you repeatedly fit the same model to new samples.\nWhen a model is overfit, that means that aspects of the model (its parameter \nestimates, its predictions) will vary greatly from sample to sample.  This is \nrepresented by a large standard error (the SD of the sampling distribution) for \nthe model's parameter estimates.  It also means that the predictions you will make\nin new data will be very different depending on the sample that was used to \nestimate the parameters.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Describe problem of p-hacking with respect to overfitting?\n\n\n\n\n\n\n\nShow Answer\nWhen you p-hack, you are overfitting the training set (your sample).  You try out\nmany, many different model configurations and choose the one you like best rather\nthan what works well in new data.  This model likely capitalizes on noise in your\nsample.  It won't fit well in another sample.  In other words, your conclusions \nare not linked to the true DGP and would be different if you used a different sample.\nIn a different vein, your significance test is wrong.  The SE does not reflect \nthe model variance that resulted from testing many different configurations b/c \nyour final model didn't \"know\" about the other models.  Statistically invalid \nconclusion!\n\n\n\n\n\n\nParameter estimates from an overfit model are specific to the sample within which they were trained and are not true for other samples or the population as a whole\n\nParameter estimates from overfit models have big (TRUE) SE and so they may be VERY different in other samples\n\nThough if the overfitting is due to fitting many models, it won’t be reflected in the SE from any one model because each model doesn’t know the other models exist! p-hacking!!\n\nWith traditional (one-sample) statistics, this can lead us to incorrect conclusions about the effect of predictors associated with these parameter estimates (bad for explanatory goals!).\nIf the parameter estimates are very different sample to sample (and different from the true population parameters), this means the model will predict poorly in new samples (bad for prediction goals!). We fix this by using resampling to evaluate model performance.\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag.\n\n\nYarkoni, Tal, and Jacob Westfall. 2017. “Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning.” Perspectives on Psychological Science 12 (6): 1100–1122."
  },
  {
    "objectID": "002_exploratory_data_analysis.html#overview-of-unit",
    "href": "002_exploratory_data_analysis.html#overview-of-unit",
    "title": "2  Exploratory Data Analysis",
    "section": "2.1 Overview of Unit",
    "text": "2.1 Overview of Unit\n\n2.1.1 Learning Objectives\n\nStages of Analysis\nBest practices for data storage, variable classing, data dictionaries\nProblems and solutions regarding data leakage\nKey goals and techniques cleaning EDA\n\nTidying names and response labels\nAppropriate visualizations based on variable class\nSummary statistics based on variable class\n\nProper splitting for training/validation and test sets\nKey goals and techniques modeling EDA\n\nAppropriate visualizations based on variable class\nSummary statistics based on variable class\n\nIntroductory use of recipes for feature engineering\n\n\n\n2.1.2 Readings\n[NOTE: These are short chapters. You are reading to understand the framework of visualizing data in R. Don’t feel like you have to memorize the details. These are reference materials that you can turn back to when you need to write code!]\n\nWickham, Çetinkaya-Rundel, and Grolemund (2023) Chapter 1, Data Visualization\nWickham, Çetinkaya-Rundel, and Grolemund (2023) Chapter 9, Layers\nWickham, Çetinkaya-Rundel, and Grolemund (2023) Chapter 10, Exploratory Data Analysis\n\n\n\n2.1.3 Lecture Videos\n\nLecture 1: Overview of EDA; ~ 24 mins\nLecture 2: EDA for Cleaning; ~ 33 mins\nLecture 3: Introduction to Recipes; ~ 17 mins\nLecture 4: EDA for Modeling Part I; ~ 17 mins\nLecture 5: EDA for Modeling Part II; ~ 22 mins\nDiscussion\n\nPost questions or discuss readings or lectures in Slack\n\n\n2.1.4 Application Assignment\n\ndata\ndata dictionary\ncleaning EDA: rmd html\nmodeling EDA: rmd html\nfun_modeling.R\nsolutions: knit cleaning EDA; knit modeling EDA\n\nSubmit the application assignment and unit quiz by 8 pm on Wednesday, January 31st"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#overview-of-exploratory-data-analysis",
    "href": "002_exploratory_data_analysis.html#overview-of-exploratory-data-analysis",
    "title": "2  Exploratory Data Analysis",
    "section": "2.2 Overview of Exploratory Data Analysis",
    "text": "2.2 Overview of Exploratory Data Analysis\n\n2.2.1 Stages of Data Analysis and Model Development\nThese are the main stages of data analysis for machine learning and the data that are used\n\nEDA: Cleaning (full dataset)\nEDA: Split data into training, validation and test set(s)\nEDA: Modeling (training sets)\nModel Building: Feature engineering (training sets)\nModel Building: Fit many models configurations (training set)\nModel Building: Evaluate many models configurations (validation sets)\nFinal Model Evaluation: Select final/best model configuration (validation sets)\nFinal Model Evaluation: Fit best model configuration (use both training and validation sets)\nFinal Model Evaluation: Evaluate final model configuration (test sets)\nFinal Model Evaluation: Fit best model configuration to ALL data (training, validation, and test sets) if you plan to use it for applications.\n\nThe earlier stages are highly iterative:\n\nYou may iterate some through EDA stages 1-3 if you find further errors to clean in stage 3 [But make sure you resplit into the same sets]\nYou will iterate many times though stages 3-6 as you learn more about your data both through EDA for modeling and evaluating actual models in validation\n\nYou will NOT iterate back to earlier stages after you select a final model configuration\n\nStages 7 - 10 are performed ONLY ONCE\nOnly one model configuration is selected and re-fit and only that model is brought into test for evaluation\nAny more than this is essentially equivalent to p-hacking in traditional analyses\nStep 10 only happens if you plan to use the model in some application\n\n\n\n2.2.2 Data file formats\nWe generally store data as CSV [comma-separated value] files\n\nEasy to view directly in a text editor\nEasy to share because others can use/import into any data analysis platform\nWorks with version control (e.g. git, svn)\nuse read_csv() and write_csv()\n\nExceptions include:\n\nWe may consider binary (.rds) format for very big files because read/write can be slow for csv files.\n\nBinary file format provides a modest additional protection for sensitive data (which we also don’t share)\nuse read_rds() and write_rds()\n\nSee chapter 7 - Data Import in Wickham, Çetinkaya-Rundel, and Grolemund (2023) for more details and advanced techniques for importing data using read_csv()\n\n\n2.2.3 Classing Variables\nWe store and class variables in R based on their data type.\nFor nominal variables:\n\nWe store (in csv files) these variables as character class with descriptive text labels for the levels\n\nEasier to share/document\nReduces errors\n\nWe class these variables in R as factors when we load them (using read_csv())\nIn some cases, we should pay attention to the order of the levels of the variable. e.g.,\n\nFor a dichotomous outcome variable, the positive/event level of dichotomous factor outcome should be first level of the factor\nThe order of levels may also matter for factor predictors (e.g., step_dummy() uses first level as reference).\n\n\nFor ordinal variables:\n\nWe store (in csv files) these variables as character class with descriptive text labels for the levels\n\nEasier to share/document\nReduces errors\n\nWe class these variables in R as factors (just like nominal variables)\n\nIt is easier to do EDA with these variables classes as factors\nWe use standard factors (not ordered)\n\nConfirm that the order of the levels is set up correctly. This is very important for ordinal variables.\nDuring feature engineering stage, we can then either\n\nTreat as a nominal variable and create features using step_dummy()\nTreat as an interval variable using step_ordinalscore()\n\n\nFor interval and ratio variables:\n\nWe store these variables as numeric\nWe class these variables as numeric (either integer or double - let R decide) during the read and clean stage (They are typically already in this class when read in)\n\n\nWe will refer to both nominal and ordinal variables as categorical variables\n\nThey are stored as character\nThey are classed as factors\nThey include a limited number of unique responses\nWe can indicate the order of the levels\nSimilar EDA approaches are used with both\nOrdinal variables may show non-linear relations b/c they may not be evenly spaced. In these instances, we can use feature engineering approaches that are also used for nominal variables\n\nWe will often refer to interval and ratio variables as numeric variables\n\nThe are stored as numeric\nThey are classed with one of two numeric data types\nThey have many (infinite?) unique responses\nSimilar EDA approaches are used with both\nSimilar feature engineering approaches are used with both\n\n\n\n2.2.4 Data Dictionaries\nYou should always make a data dictionary for use with your data files.\n\nIdeally, these are created during the planning phase of your study, prior to the start of data collection\nStill useful if created at the start of data analysis\n\nData dictionaries:\n\nhelp you keep track of your variables and their characteristics (e.g., valid ranges, valid responses)\ncan be used by you to check your data during EDA\ncan be provided to others when you share your data (data are not generally useful to others without a data dictionary)\n\nWe will see a variety of data dictionaries throughout the course. Many are not great as you will learn.\n\n\n2.2.5 The Ames Housing Prices Dataset\nWe will use the Ames Housing Prices dataset as a running example this unit (and some future units and application assignments as well)\n\nYou can read more about the original dataset created by Dean DeCock\nThe data set contains data from home sales of individual residential property in Ames, Iowa from 2006 to 2010\nThe original data set includes 2930 observations of sales price and a large number of explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous)\nThis is the original data dictionary\nThe challenge with this dataset is to build the best possible prediction model for the sale price of the homes.\n\n\nFirst, lets set up our environment with functions from important packages. I strongly recommend reviewing our recommendations for best practices regarding managing function conflicts now. It will save you a lot of headaches in the future.\n\nFirst we set a conflicts policy that will produce errors if we have unanticipated conflicts.\nWe next source a library of functions that we use for common tasks in machine learning. This includes a function (tidymodels_conflictRules()) that sets conflict rules to allow us to attach tidymodels functions without conflicts with tidyverse functions. You can review that function to see what it does (search for that function name at the link)\nThen we use that function\n\n\noptions(conflicts.policy = \"depends.ok\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\n\nℹ SHA-1 hash of file is \"175d942e14f108d74912bfb2593b77637328ecb1\"\n\ntidymodels_conflictRules()\n\nNow we load packages for functions that we will use regularly. There are five things to note RE best practices\n\nIf we will use a lot of functions from a package (e.g., tidyverse, tidymodels), we attach the full package\nIf we will use only several functions from a package (but plan to use them repeatedly), we use the include.only parameter to just attach those functions.\nAt times, if we plan to use a single function from a package only 1-2x times, we may not even attach that function at all. Instead, we just call it using its namespace (i.e. packagename::functionname)\nIf a package has a function that conflicts with our primary packages and we don’t plan to use that function, we load the package but exclude the function. If we really needed it, we can call it with its namespace as per option 3 above.\nPay attention to conflicts that were allowed to make sure you understand and accept them. (I left the package messages and warnings in the book this time to see them. I will hide them to avoid cluttering book in later units but you should always review them.)\n\n\nlibrary(janitor, include.only = \"clean_names\")\nlibrary(cowplot, include.only = \"plot_grid\") # for plot_grid()\nlibrary(kableExtra, exclude = \"group_rows\") # exclude dplyr conflict\nlibrary(tidyverse) # for general data wrangling\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels) # for modeling\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n✔ broom        1.0.4     ✔ rsample      1.1.1\n✔ dials        1.2.0     ✔ tune         1.1.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.0     ✔ yardstick    1.2.0\n✔ recipes      1.0.6     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\n\nThen we source (from github) two other libraries of functions that we use commonly for exploratory data analyses. You should review these function scripts to see how I’ve written this code.\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\n\nℹ SHA-1 hash of file is \"c045eee2655a18dc85e715b78182f176327358a7\"\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\nℹ SHA-1 hash of file is \"def6ce26ed7b2493931fde811adff9287ee8d874\"\n\n\nFinally, we tune our environment a bit more by setting plot themes and print options\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf,\n        pillar.bold = TRUE)\n\nAnd we set a relative path to our data. This assumes you are using an RStudio project with the path to the data relative to that project file. I’ve provided more detail elsewhere on best practices for managing files and paths.\n\npath_data &lt;- \"data\"\n\nHere is a quick glimpse of the subset of observations we will work with in Units 2-3 and the first two application assignments. Note that we re-class all character variables as unordered factors for now\n\n1data_all &lt;- read_csv(here::here(path_data, \"ames_raw_class.csv\"),\n2                     col_types = cols()) |&gt;\n3  mutate(across(where(is.character), factor)) |&gt;\n4  glimpse()\n\n\n1\n\nFirst we read data using a relative path and the here::here() function. This is a replacement for file.path() that works better for both interactive use and rendering in Quarto when using projects.\n\n2\n\nWe use col_types = cols() to let R guess the correct class for each column. This suppresses messages that aren’t important at this point prior to EDA.\n\n3\n\nThen we use a mutate to re-class all character data to factors. This should handle most of our nominal and ordinal variables. We may need to still class some that were missed because they appeared to be numeric. I prefer factor() to forcats::fct() because factor orders the levels alphabetically. Be aware that this could change if your code is used in a region of the world where this sorting is different. I still prefer this to the alternative (in fct()) that orders by the order the levels are found in your data.\n\n4\n\nIt is good practice to always glimpse() data after you read it.\n\n\n\n\nRows: 1,955\nColumns: 81\n$ PID               &lt;fct&gt; 0526301100, 0526350040, 0526351010, 0527105010, 0527…\n$ `MS SubClass`     &lt;fct&gt; 020, 020, 020, 060, 120, 120, 120, 060, 060, 060, 02…\n$ `MS Zoning`       &lt;fct&gt; RL, RH, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, …\n$ `Lot Frontage`    &lt;dbl&gt; 141, 80, 81, 74, 41, 43, 39, 60, 75, 63, 85, NA, 47,…\n$ `Lot Area`        &lt;dbl&gt; 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, …\n$ Street            &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave…\n$ Alley             &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Lot Shape`       &lt;fct&gt; IR1, Reg, IR1, IR1, Reg, IR1, IR1, Reg, IR1, IR1, Re…\n$ `Land Contour`    &lt;fct&gt; Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, Lvl, Lvl, Lv…\n$ Utilities         &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllP…\n$ `Lot Config`      &lt;fct&gt; Corner, Inside, Corner, Inside, Inside, Inside, Insi…\n$ `Land Slope`      &lt;fct&gt; Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gt…\n$ Neighborhood      &lt;fct&gt; NAmes, NAmes, NAmes, Gilbert, StoneBr, StoneBr, Ston…\n$ `Condition 1`     &lt;fct&gt; Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, Nor…\n$ `Condition 2`     &lt;fct&gt; Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm…\n$ `Bldg Type`       &lt;fct&gt; 1Fam, 1Fam, 1Fam, 1Fam, TwnhsE, TwnhsE, TwnhsE, 1Fam…\n$ `House Style`     &lt;fct&gt; 1Story, 1Story, 1Story, 2Story, 1Story, 1Story, 1Sto…\n$ `Overall Qual`    &lt;dbl&gt; 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6…\n$ `Overall Cond`    &lt;dbl&gt; 5, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 2, 5, 6, 6…\n$ `Year Built`      &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993…\n$ `Year Remod/Add`  &lt;dbl&gt; 1960, 1961, 1958, 1998, 2001, 1992, 1996, 1999, 1994…\n$ `Roof Style`      &lt;fct&gt; Hip, Gable, Hip, Gable, Gable, Gable, Gable, Gable, …\n$ `Roof Matl`       &lt;fct&gt; CompShg, CompShg, CompShg, CompShg, CompShg, CompShg…\n$ `Exterior 1st`    &lt;fct&gt; BrkFace, VinylSd, Wd Sdng, VinylSd, CemntBd, HdBoard…\n$ `Exterior 2nd`    &lt;fct&gt; Plywood, VinylSd, Wd Sdng, VinylSd, CmentBd, HdBoard…\n$ `Mas Vnr Type`    &lt;fct&gt; Stone, None, BrkFace, None, None, None, None, None, …\n$ `Mas Vnr Area`    &lt;dbl&gt; 112, 0, 108, 0, 0, 0, 0, 0, 0, 0, 0, 0, 603, 0, 350,…\n$ `Exter Qual`      &lt;fct&gt; TA, TA, TA, TA, Gd, Gd, Gd, TA, TA, TA, TA, Gd, Ex, …\n$ `Exter Cond`      &lt;fct&gt; TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, …\n$ Foundation        &lt;fct&gt; CBlock, CBlock, CBlock, PConc, PConc, PConc, PConc, …\n$ `Bsmt Qual`       &lt;fct&gt; TA, TA, TA, Gd, Gd, Gd, Gd, TA, Gd, Gd, Gd, Gd, Gd, …\n$ `Bsmt Cond`       &lt;fct&gt; Gd, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, …\n$ `Bsmt Exposure`   &lt;fct&gt; Gd, No, No, No, Mn, No, No, No, No, No, Gd, Av, Gd, …\n$ `BsmtFin Type 1`  &lt;fct&gt; BLQ, Rec, ALQ, GLQ, GLQ, ALQ, GLQ, Unf, Unf, Unf, GL…\n$ `BsmtFin SF 1`    &lt;dbl&gt; 639, 468, 923, 791, 616, 263, 1180, 0, 0, 0, 637, 36…\n$ `BsmtFin Type 2`  &lt;fct&gt; Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Un…\n$ `BsmtFin SF 2`    &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0, 0, 0, 0,…\n$ `Bsmt Unf SF`     &lt;dbl&gt; 441, 270, 406, 137, 722, 1017, 415, 994, 763, 789, 6…\n$ `Total Bsmt SF`   &lt;dbl&gt; 1080, 882, 1329, 928, 1338, 1280, 1595, 994, 763, 78…\n$ Heating           &lt;fct&gt; GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA…\n$ `Heating QC`      &lt;fct&gt; Fa, TA, TA, Gd, Ex, Ex, Ex, Gd, Gd, Gd, Gd, TA, Ex, …\n$ `Central Air`     &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y…\n$ Electrical        &lt;fct&gt; SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBr…\n$ `1st Flr SF`      &lt;dbl&gt; 1656, 896, 1329, 928, 1338, 1280, 1616, 1028, 763, 7…\n$ `2nd Flr SF`      &lt;dbl&gt; 0, 0, 0, 701, 0, 0, 0, 776, 892, 676, 0, 0, 1589, 67…\n$ `Low Qual Fin SF` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Gr Liv Area`     &lt;dbl&gt; 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655,…\n$ `Bsmt Full Bath`  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0…\n$ `Bsmt Half Bath`  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Full Bath`       &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 1, 1, 2, 2…\n$ `Half Bath`       &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0…\n$ `Bedroom AbvGr`   &lt;dbl&gt; 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 4, 4, 1, 2, 3, 3…\n$ `Kitchen AbvGr`   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ `Kitchen Qual`    &lt;fct&gt; TA, TA, Gd, TA, Gd, Gd, Gd, Gd, TA, TA, Gd, Gd, Ex, …\n$ `TotRms AbvGrd`   &lt;dbl&gt; 7, 5, 6, 6, 6, 5, 5, 7, 7, 7, 5, 4, 12, 8, 8, 4, 7, …\n$ Functional        &lt;fct&gt; Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Ty…\n$ Fireplaces        &lt;dbl&gt; 2, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1…\n$ `Fireplace Qu`    &lt;fct&gt; Gd, NA, NA, TA, NA, NA, TA, TA, TA, Gd, Po, NA, Gd, …\n$ `Garage Type`     &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attc…\n$ `Garage Yr Blt`   &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993…\n$ `Garage Finish`   &lt;fct&gt; Fin, Unf, Unf, Fin, Fin, RFn, RFn, Fin, Fin, Fin, Un…\n$ `Garage Cars`     &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2…\n$ `Garage Area`     &lt;dbl&gt; 528, 730, 312, 482, 582, 506, 608, 442, 440, 393, 50…\n$ `Garage Qual`     &lt;fct&gt; TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, …\n$ `Garage Cond`     &lt;fct&gt; TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, …\n$ `Paved Drive`     &lt;fct&gt; P, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y…\n$ `Wood Deck SF`    &lt;dbl&gt; 210, 140, 393, 212, 0, 0, 237, 140, 157, 0, 192, 0, …\n$ `Open Porch SF`   &lt;dbl&gt; 62, 0, 36, 34, 0, 82, 152, 60, 84, 75, 0, 54, 36, 12…\n$ `Enclosed Porch`  &lt;dbl&gt; 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `3Ssn Porch`      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Screen Porch`    &lt;dbl&gt; 0, 120, 0, 0, 0, 144, 0, 0, 0, 0, 0, 140, 210, 0, 0,…\n$ `Pool Area`       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Pool QC`         &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence             &lt;fct&gt; NA, MnPrv, NA, MnPrv, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Misc Feature`    &lt;fct&gt; NA, NA, Gar2, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Misc Val`        &lt;dbl&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Mo Sold`         &lt;dbl&gt; 5, 6, 6, 3, 4, 1, 3, 6, 4, 5, 2, 6, 6, 6, 6, 6, 2, 1…\n$ `Yr Sold`         &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010…\n$ `Sale Type`       &lt;fct&gt; WD, WD, WD, WD, WD, WD, WD, WD, WD, WD, WD, WD, WD, …\n$ `Sale Condition`  &lt;fct&gt; Normal, Normal, Normal, Normal, Normal, Normal, Norm…\n$ SalePrice         &lt;dbl&gt; 215000, 105000, 172000, 189900, 213500, 191500, 2365…\n\n\n\nDataset Notes:\n\nDataset has N = 1955 rather than 2930.\n\nI have held out remaining observations to serve as a test set for a friendly competition in Unit 3\nI will judge your models’ performance with this test set at that time!\nMore on the importance of held out test sets as we progress through the course\n\nThis full dataset has 81 variables. For the lecture examples in units 2-3 we will only use a subset of the predictors\nYou will use different predictors in the next two application assignments\n\n\ndata_all &lt;- data_all |&gt; \n  select(SalePrice,\n         `Gr Liv Area`, \n         `Lot Area`, \n         `Year Built`, \n         `Overall Qual`, \n         `Garage Cars`,\n         `Garage Qual`,\n         `MS Zoning`,\n         `Lot Config` ,\n         `Bldg Type`) |&gt; \n  glimpse()\n\nRows: 1,955\nColumns: 10\n$ SalePrice      &lt;dbl&gt; 215000, 105000, 172000, 189900, 213500, 191500, 236500,…\n$ `Gr Liv Area`  &lt;dbl&gt; 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 14…\n$ `Lot Area`     &lt;dbl&gt; 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 100…\n$ `Year Built`   &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, 1…\n$ `Overall Qual` &lt;dbl&gt; 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, 7…\n$ `Garage Cars`  &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2…\n$ `Garage Qual`  &lt;fct&gt; TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA,…\n$ `MS Zoning`    &lt;fct&gt; RL, RH, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL,…\n$ `Lot Config`   &lt;fct&gt; Corner, Inside, Corner, Inside, Inside, Inside, Inside,…\n$ `Bldg Type`    &lt;fct&gt; 1Fam, 1Fam, 1Fam, 1Fam, TwnhsE, TwnhsE, TwnhsE, 1Fam, 1…\n\n\nCoding Sidebar:\n\nNotice the non-standard variable names that include spaces.\n\nWe use the back-ticks around the variable names to allow us reference those variables.\n\nWe will fix this during the cleaning process.\nNever use spaces in variable names when setting up your own data!!! (More on this in a moment…)"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-data-cleaning",
    "href": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-data-cleaning",
    "title": "2  Exploratory Data Analysis",
    "section": "2.3 Exploratory Data Analysis for Data Cleaning",
    "text": "2.3 Exploratory Data Analysis for Data Cleaning\nEDA could be done using either tidyverse packages and functions or tidymodels (mostly using the recipes package.)\n\nWe prefer to use the richer set of functions available in the tidyverse (and dplyr and purrr packages in particular).\nWe will reserve the use of recipes for feature engineering only when we are building features for models that we will fit in our training sets and evaluation in our validation and test sets.\n\n\n2.3.1 Data Leakage Issues\nData leakage refers to a mistake made by the creator of a machine learning model in which they accidentally share information between their training set and held-out validation or test sets\n\nTraining sets are used to fit models with different configurations\nValidation sets are used to select the best model among those with different configurations (not needed if you only have one configuration)\nTest sets are used to evaluate a best model\nWhen splitting data-sets into training, validation and test sets, the goal is to ensure that no data (or information more broadly) are shared between the three\n\nNo data or information from test should influence either fitting or selecting models\nTest should only be used once to evaluate a best/final model\nTrain and validation set also must be segregated (although validation sets may be used to evaluate many models)\nInformation necessary for transformations and other feature engineering (e.g., means/sds for centering/scaling, procedures for missing data imputation) must all be based only on training data.\nData leakage is common if you are not careful.\n\n\nIn particular, if we begin to use test data or information about test during model fitting\n\nWe risk overfitting\nThis is essentially the equivalent of p-hacking in traditional analyses\nWill report too optimistic performance estimate, which could have harmful real-world consequences.\n\n\n\n2.3.2 Tidy variable names\nUse snake case for variable names\n\nclean_names() from janitor package is useful for this.\nMay need to do further correction of variable names using rename()\nSee more details about tidy names for objects (e.g., variables, dfs, functions) per Tidy Style Guide\n\n\ndata_all &lt;- data_all |&gt; \n  clean_names(\"snake\")\n\ndata_all |&gt; names()\n\n [1] \"sale_price\"   \"gr_liv_area\"  \"lot_area\"     \"year_built\"   \"overall_qual\"\n [6] \"garage_cars\"  \"garage_qual\"  \"ms_zoning\"    \"lot_config\"   \"bldg_type\"   \n\n\n\n\n2.3.3 Explore variable classes\nAt this point, variables should be classed as either numeric or factor\n\nInterval and ratio variables use numeric classes (dbl or int)\nNominal and ordinal variable use factor class\nUseful for variable selection later (e.g., where(is.numeric), where(is.factor))\n\nSubsequent cleaning steps are clearer if we have this established/confirmed now\n\nWe have one variable to change to factor class\n\ndata_all |&gt; glimpse()\n\nRows: 1,955\nColumns: 10\n$ sale_price   &lt;dbl&gt; 215000, 105000, 172000, 189900, 213500, 191500, 236500, 1…\n$ gr_liv_area  &lt;dbl&gt; 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 1465…\n$ lot_area     &lt;dbl&gt; 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 10000…\n$ year_built   &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, 199…\n$ overall_qual &lt;dbl&gt; 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, 7, …\n$ garage_cars  &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, …\n$ garage_qual  &lt;fct&gt; TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, T…\n$ ms_zoning    &lt;fct&gt; RL, RH, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, R…\n$ lot_config   &lt;fct&gt; Corner, Inside, Corner, Inside, Inside, Inside, Inside, I…\n$ bldg_type    &lt;fct&gt; 1Fam, 1Fam, 1Fam, 1Fam, TwnhsE, TwnhsE, TwnhsE, 1Fam, 1Fa…\n\n\noverall_qual is ordinal.\n* We should convert it to a factor and confirm the levels are ordered properly.\n* The data dictionary indicates that valid values range from 1 - 10.\n\n1oq_levels &lt;- 1:10\n\nt &lt;-  data_all |&gt; \n  mutate(overall_qual = factor(overall_qual, \n2                               levels = oq_levels)) |&gt;\n  glimpse()\n\n\n1\n\nIt is always best to explicitly set the levels of an ordinal factor in the order you prefer. It is not necessary here because overall_qual was numeric and therefore sorts in the expected order. However, if it had been numbers stores as characters, it could sort incorrectly (e.g., 1, 10, 2, 3, …). And obviously if the orders levels were names, the order would have to be specified.\n\n2\n\nWe indicate the levels here.\n\n\n\n\nRows: 1,955\nColumns: 10\n$ sale_price   &lt;dbl&gt; 215000, 105000, 172000, 189900, 213500, 191500, 236500, 1…\n$ gr_liv_area  &lt;dbl&gt; 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 1465…\n$ lot_area     &lt;dbl&gt; 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 10000…\n$ year_built   &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, 199…\n$ overall_qual &lt;fct&gt; 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, 7, …\n$ garage_cars  &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, …\n$ garage_qual  &lt;fct&gt; TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, TA, T…\n$ ms_zoning    &lt;fct&gt; RL, RH, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, RL, R…\n$ lot_config   &lt;fct&gt; Corner, Inside, Corner, Inside, Inside, Inside, Inside, I…\n$ bldg_type    &lt;fct&gt; 1Fam, 1Fam, 1Fam, 1Fam, TwnhsE, TwnhsE, TwnhsE, 1Fam, 1Fa…\n\n\n\n\n2.3.4 Skimming the data\nskim() from the skimr package is a wonderful and customizable function for summary statistics\n\nIt is highly customizable so we can write our own versions for our own needs\nWe use different versions for cleaning and modeling EDA\nFor cleaning EDA, we just remove some stats that we don’t want to see at this time\nWe can get many of the summary stats for cleaning in one call\nWe have a custom skim defined in the fun_eda.R library that we use regularly. Here is the code but you can use the function directly if you sourced fun_eda.R (as we did above)\n\n\nskim_some &lt;- skim_with(numeric = sfl(mean = NULL, sd = NULL, p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL))\n\n\nHere is what we get with our new skim_some() function\n\nWe will refer to this again for each characteristic we want to review for instructional purposes\nWe can already see that we can use skim_some() to confirm that we only have numeric and character classes\n\n\ndata_all |&gt; \n  skim_some()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndata_all\n\n\n\n\nNumber of rows\n\n\n1955\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n4\n\n\n\n\nnumeric\n\n\n6\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\ngarage_qual\n\n\n109\n\n\n0.94\n\n\nFALSE\n\n\n5\n\n\nTA: 1745, Fa: 79, Gd: 16, Po: 4\n\n\n\n\nms_zoning\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n7\n\n\nRL: 1530, RM: 297, FV: 91, C (: 19\n\n\n\n\nlot_config\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n5\n\n\nIns: 1454, Cor: 328, Cul: 114, FR2: 55\n\n\n\n\nbldg_type\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n5\n\n\n1Fa: 1631, Twn: 145, Dup: 77, Twn: 64\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\np0\n\n\np100\n\n\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n12789\n\n\n745000\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n438\n\n\n5642\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n1476\n\n\n215245\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1875\n\n\n2010\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n1\n\n\n10\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n0\n\n\n4\n\n\n\n\n\n\n\nCoding sidebar 1:\n\nWrite functions whenever you will repeat code often. You can now reuse skim_some()\nskim_with() is an example of a function factory - a function that is used to create a new function\n\npartial() and compose() are two other function factories we will use at times\nMore details on function factories is available in Advanced R\n\n\n\nCoding sidebar 2:\n\nGather useful functions together in a script that you can reuse.\nAll of the reusable functions in this and later units are available to you in one of my public github repositories.\nYou can load these functions into your workspace directly from github using devtools::source_url(). For example: devtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_modeling.R?raw=true\")\nYou should start to gather your favorite custom functions together in your own script(s).\n\nYou can save your own scripts in a local file and load them into your workspace using source() or you can make your own github repo so you can begin to share your code with others!\n\n\n\n2.3.5 Missing Data - All variables\nskim_some() provides us with missing data counts and complete data proportions for each variable\n\ndata_all |&gt; \n  skim_some()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndata_all\n\n\n\n\nNumber of rows\n\n\n1955\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n4\n\n\n\n\nnumeric\n\n\n6\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\ngarage_qual\n\n\n109\n\n\n0.94\n\n\nFALSE\n\n\n5\n\n\nTA: 1745, Fa: 79, Gd: 16, Po: 4\n\n\n\n\nms_zoning\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n7\n\n\nRL: 1530, RM: 297, FV: 91, C (: 19\n\n\n\n\nlot_config\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n5\n\n\nIns: 1454, Cor: 328, Cul: 114, FR2: 55\n\n\n\n\nbldg_type\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n5\n\n\n1Fa: 1631, Twn: 145, Dup: 77, Twn: 64\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\np0\n\n\np100\n\n\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n12789\n\n\n745000\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n438\n\n\n5642\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n1476\n\n\n215245\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1875\n\n\n2010\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n1\n\n\n10\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n0\n\n\n4\n\n\n\n\n\n\n\n\nView full observation for missing value\n\nprint() will print only 20 rows and number of columns that will display for width of page\n\nSet options() if you will do a lot of printing and want full dataframe printed\n\nUse kbl() from kableExtra package for formatted tables\nUse View() interactively in R Studio\n\nOption 1 (Simple): Use print() with options()\n\noptions(tibble.width = Inf, tibble.print_max = Inf)\n\ndata_all |&gt; filter(is.na(garage_cars)) |&gt; \n  print()\n\n# A tibble: 1 × 10\n  sale_price gr_liv_area lot_area year_built overall_qual garage_cars\n       &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1     150909        1828     9060       1923            5          NA\n  garage_qual ms_zoning lot_config bldg_type\n  &lt;fct&gt;       &lt;fct&gt;     &lt;fct&gt;      &lt;fct&gt;    \n1 &lt;NA&gt;        RM        Inside     1Fam     \n\ndata_all |&gt; filter(is.na(garage_qual)) |&gt; \n  print()\n\n# A tibble: 109 × 10\n    sale_price gr_liv_area lot_area year_built overall_qual garage_cars\n         &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n  1     115000         864    10500       1971            4           0\n  2     128950        1225     9320       1959            4           0\n  3      84900        1728    13260       1962            5           0\n  4     116500         858     7207       1958            5           0\n  5      76500        1306     5350       1940            3           0\n  6      76500        2256     9045       1910            5           0\n  7     159900        1560    12900       1912            6           0\n  8      55000        1092     5600       1930            4           0\n  9      93369        1884     6449       1907            4           0\n 10      94000        1020     6342       1875            5           0\n 11     136000        1832    10773       1967            4           0\n 12     100000        1664     9825       1965            5           0\n 13      90000         960     6410       1958            4           0\n 14     100000        1666     9839       1931            5           0\n 15     139000        1824     9400       1971            6           0\n 16      76000        1092     1476       1970            4           0\n 17      75500         630     1491       1972            4           0\n 18      88250        1092     1900       1970            4           0\n 19     136000        1792     9000       1974            5           0\n 20     142000        1114    13072       2004            5           0\n 21      82500         708     5330       1940            4           0\n 22     129000        1464     9900       1910            5           0\n 23      94550        1701     7627       1920            4           0\n 24     103000        1447    10134       1910            5           0\n 25      37900         968     5925       1910            3           0\n 26     113000        1452     4456       1920            4           0\n 27      58500         816     3300       1910            4           0\n 28      34900         720     7879       1920            4           0\n 29      60000         800     6120       1936            2           0\n 30      62500        2128     3000       1922            5           0\n 31      97500        1864     5852       1902            7           0\n 32      70000         892     5160       1923            4           0\n 33     179000        1200    10800       1987            5           0\n 34     179000        1200    10800       1987            5           0\n 35      61000         904    10020       1922            1           0\n 36     118000         698     9405       1947            5           0\n 37      99900         864     4060       1922            5           0\n 38     119900        1678    10926       1959            5           0\n 39     112000         833     8780       1985            5           0\n 40     141000        1080     7500       2004            7           0\n 41     106250        1294    10800       1900            4           0\n 42     130000        1800     8513       1961            5           0\n 43     120000        1027     5400       1920            7           0\n 44      95000        1080     5914       1890            5           0\n 45      65000        1588    12205       1949            3           0\n 46     129400        1540     6000       1905            5           0\n 47     160000        1984     8094       1910            6           1\n 48      89500        1406     7920       1920            6           0\n 49      79900        1198     5586       1920            6           0\n 50      82375        1344    10320       1915            3           0\n 51     127500        1355    10106       1940            5           0\n 52      80000        1006     9000       1959            5           0\n 53     260000        1518    19550       1940            5           0\n 54      99600         864     9350       1975            5           0\n 55     107500        1347     7000       1910            5           0\n 56      79000        1096     9600       1924            6           0\n 57      85000         796     8777       1910            5           0\n 58     145900        2200     8777       1900            5           0\n 59      82000        1152     6040       1955            4           0\n 60      82000        1152     6012       1955            4           0\n 61     118000        1440    12108       1955            4           0\n 62      82500        1152     6845       1955            4           0\n 63      91900         784     6931       1955            4           0\n 64     120000        1053    12180       1938            5           0\n 65      96000        1137     8050       1947            5           0\n 66      98000         864     5604       1925            5           0\n 67      67000         864     8248       1914            3           0\n 68     135900        1716     5687       1912            5           0\n 69     119000        1200     8155       1930            5           0\n 70      81000         630     1890       1972            4           0\n 71     146000        1100     7500       2006            6           0\n 72      64000         670     3500       1945            3           0\n 73     103200         882     5500       1956            4           0\n 74     148000        1534    10800       1895            5           0\n 75     110500         866     3880       1945            5           0\n 76     127000        1355     6882       1914            6           0\n 77     200500        3086    18030       1946            5           0\n 78     150000        1440     7711       1977            4           0\n 79      86000         605     9098       1920            4           0\n 80     123600         990     8070       1994            4           0\n 81      98500        1195     8741       1946            5           0\n 82      79000         774     4270       1931            3           0\n 83     200000        3395    10896       1914            6           0\n 84     150000        2592    10890       1923            5           0\n 85     115000        1517     8500       1919            5           0\n 86     150909        1828     9060       1923            5          NA\n 87     119600        1991     8250       1895            5           0\n 88     147000        1120     8402       2007            5           0\n 89      93900        1092     1495       1970            4           0\n 90      84500         630     1936       1970            4           0\n 91     139500        1142     7733       2005            6           0\n 92     132000        1131    13072       2005            6           0\n 93      85500         869     5900       1923            4           0\n 94     135000        1192    10800       1949            4           0\n 95     119000        1556     8512       1960            5           0\n 96     124000        1025     7000       1962            5           0\n 97      64500        1020     4761       1918            3           0\n 98     100000         788     7446       1941            4           0\n 99      80500         912     6240       1947            4           0\n100      72000         819     9000       1919            5           0\n101     117250         914     8050       2002            6           0\n102      81000        1184     8410       1910            5           0\n103      83000        1414     8248       1922            4           0\n104     102000        1522     6000       1926            5           0\n105      72000         672     8534       1925            4           0\n106     115000        1396     9000       1951            5           0\n107      78000         936     8520       1916            3           0\n108      92000         630     1533       1970            5           0\n109      90500        1092     1936       1970            4           0\n    garage_qual ms_zoning lot_config bldg_type\n    &lt;fct&gt;       &lt;fct&gt;     &lt;fct&gt;      &lt;fct&gt;    \n  1 &lt;NA&gt;        RL        FR2        1Fam     \n  2 &lt;NA&gt;        RL        Inside     1Fam     \n  3 &lt;NA&gt;        RL        Inside     Duplex   \n  4 &lt;NA&gt;        RL        Inside     1Fam     \n  5 &lt;NA&gt;        RL        Inside     1Fam     \n  6 &lt;NA&gt;        RM        Inside     2fmCon   \n  7 &lt;NA&gt;        RM        Inside     1Fam     \n  8 &lt;NA&gt;        RM        Inside     2fmCon   \n  9 &lt;NA&gt;        C (all)   Inside     1Fam     \n 10 &lt;NA&gt;        RL        Inside     1Fam     \n 11 &lt;NA&gt;        RL        Inside     Duplex   \n 12 &lt;NA&gt;        RL        Inside     Duplex   \n 13 &lt;NA&gt;        RL        Inside     1Fam     \n 14 &lt;NA&gt;        RL        Inside     1Fam     \n 15 &lt;NA&gt;        RL        Corner     Duplex   \n 16 &lt;NA&gt;        RM        Inside     Twnhs    \n 17 &lt;NA&gt;        RM        Inside     TwnhsE   \n 18 &lt;NA&gt;        RM        Inside     TwnhsE   \n 19 &lt;NA&gt;        RL        FR2        Duplex   \n 20 &lt;NA&gt;        RL        Inside     1Fam     \n 21 &lt;NA&gt;        RL        Inside     1Fam     \n 22 &lt;NA&gt;        RM        Corner     1Fam     \n 23 &lt;NA&gt;        RM        Corner     2fmCon   \n 24 &lt;NA&gt;        RM        Inside     1Fam     \n 25 &lt;NA&gt;        RM        Inside     1Fam     \n 26 &lt;NA&gt;        RM        Inside     2fmCon   \n 27 &lt;NA&gt;        C (all)   Inside     1Fam     \n 28 &lt;NA&gt;        C (all)   Inside     1Fam     \n 29 &lt;NA&gt;        RM        Inside     1Fam     \n 30 &lt;NA&gt;        RM        Inside     Duplex   \n 31 &lt;NA&gt;        RM        Corner     2fmCon   \n 32 &lt;NA&gt;        RM        Inside     1Fam     \n 33 &lt;NA&gt;        RL        Inside     Duplex   \n 34 &lt;NA&gt;        RL        Inside     Duplex   \n 35 &lt;NA&gt;        RL        Inside     1Fam     \n 36 &lt;NA&gt;        RL        Inside     1Fam     \n 37 &lt;NA&gt;        RL        Corner     1Fam     \n 38 &lt;NA&gt;        RL        Inside     Duplex   \n 39 &lt;NA&gt;        RL        Corner     1Fam     \n 40 &lt;NA&gt;        RL        Inside     1Fam     \n 41 &lt;NA&gt;        RL        Inside     2fmCon   \n 42 &lt;NA&gt;        RL        Corner     Duplex   \n 43 &lt;NA&gt;        RM        Inside     1Fam     \n 44 &lt;NA&gt;        RM        Inside     1Fam     \n 45 &lt;NA&gt;        RM        Inside     1Fam     \n 46 &lt;NA&gt;        RM        Corner     1Fam     \n 47 &lt;NA&gt;        RM        Inside     2fmCon   \n 48 &lt;NA&gt;        RM        Inside     1Fam     \n 49 &lt;NA&gt;        RM        Inside     1Fam     \n 50 &lt;NA&gt;        RM        Inside     2fmCon   \n 51 &lt;NA&gt;        RL        Inside     2fmCon   \n 52 &lt;NA&gt;        RL        Inside     1Fam     \n 53 &lt;NA&gt;        RL        Inside     1Fam     \n 54 &lt;NA&gt;        RL        Inside     Duplex   \n 55 &lt;NA&gt;        RL        Inside     2fmCon   \n 56 &lt;NA&gt;        RL        Corner     1Fam     \n 57 &lt;NA&gt;        RL        Inside     1Fam     \n 58 &lt;NA&gt;        RL        Inside     Duplex   \n 59 &lt;NA&gt;        RL        Inside     Duplex   \n 60 &lt;NA&gt;        RL        Corner     Duplex   \n 61 &lt;NA&gt;        RL        Inside     Duplex   \n 62 &lt;NA&gt;        RL        Inside     Duplex   \n 63 &lt;NA&gt;        RL        Inside     2fmCon   \n 64 &lt;NA&gt;        RL        Inside     1Fam     \n 65 &lt;NA&gt;        RL        Inside     1Fam     \n 66 &lt;NA&gt;        RL        Inside     1Fam     \n 67 &lt;NA&gt;        RL        Inside     1Fam     \n 68 &lt;NA&gt;        RL        Inside     2fmCon   \n 69 &lt;NA&gt;        RM        Inside     1Fam     \n 70 &lt;NA&gt;        RM        Inside     Twnhs    \n 71 &lt;NA&gt;        RL        Inside     1Fam     \n 72 &lt;NA&gt;        RL        Inside     1Fam     \n 73 &lt;NA&gt;        RL        Inside     1Fam     \n 74 &lt;NA&gt;        RL        Inside     1Fam     \n 75 &lt;NA&gt;        RM        Inside     1Fam     \n 76 &lt;NA&gt;        RM        Inside     1Fam     \n 77 &lt;NA&gt;        RL        Inside     1Fam     \n 78 &lt;NA&gt;        RL        Inside     Duplex   \n 79 &lt;NA&gt;        RL        Inside     1Fam     \n 80 &lt;NA&gt;        RL        Inside     1Fam     \n 81 &lt;NA&gt;        RL        Inside     Duplex   \n 82 &lt;NA&gt;        RH        Inside     1Fam     \n 83 &lt;NA&gt;        RH        Inside     2fmCon   \n 84 &lt;NA&gt;        RL        Inside     Duplex   \n 85 &lt;NA&gt;        RM        Corner     1Fam     \n 86 &lt;NA&gt;        RM        Inside     1Fam     \n 87 &lt;NA&gt;        C (all)   Inside     2fmCon   \n 88 &lt;NA&gt;        RL        Inside     1Fam     \n 89 &lt;NA&gt;        RM        Inside     TwnhsE   \n 90 &lt;NA&gt;        RM        Inside     Twnhs    \n 91 &lt;NA&gt;        RL        Inside     1Fam     \n 92 &lt;NA&gt;        RL        Inside     1Fam     \n 93 &lt;NA&gt;        RL        Inside     1Fam     \n 94 &lt;NA&gt;        RL        Inside     1Fam     \n 95 &lt;NA&gt;        RL        Corner     Duplex   \n 96 &lt;NA&gt;        RL        Inside     2fmCon   \n 97 &lt;NA&gt;        C (all)   Corner     1Fam     \n 98 &lt;NA&gt;        RL        Corner     1Fam     \n 99 &lt;NA&gt;        RM        Inside     1Fam     \n100 &lt;NA&gt;        RM        Inside     1Fam     \n101 &lt;NA&gt;        RL        Inside     1Fam     \n102 &lt;NA&gt;        RL        FR2        1Fam     \n103 &lt;NA&gt;        RL        Inside     1Fam     \n104 &lt;NA&gt;        RL        Inside     1Fam     \n105 &lt;NA&gt;        RM        Inside     1Fam     \n106 &lt;NA&gt;        C (all)   Inside     2fmCon   \n107 &lt;NA&gt;        C (all)   Inside     1Fam     \n108 &lt;NA&gt;        RM        Inside     Twnhs    \n109 &lt;NA&gt;        RM        Inside     Twnhs    \n\n\n\nHere are some more advanced options using kbl() for the df with many rows\n\nkable() tables from knitr package and kableExtra extensions (including kbl()) are very useful during EDA and also final publication quality tables\nuse library(kableExtra)\nsee vignettes for kableExtra\n\nOption 2 (more advanced): Use a function for kables that we created. Code is displayed here but the function is available to you if you source fun_eda.R from Github\n\n# Might want to use height = \"100%\" if only printing a few rows\n1print_kbl &lt;- function(data, height = \"500px\") {\n  data |&gt; \n    kbl(align = \"r\") |&gt; \n    kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) |&gt; \n2    scroll_box(height = height, width = \"100%\")\n}\n\n\n1\n\nDefaults to a output box of height = “500px”. Can set to other values if preferred.\n\n2\n\nMight want to use height = \"100%\" if only printing a few rows.\n\n\n\n\n\n\ndata_all |&gt; filter(is.na(garage_qual)) |&gt; \n  print_kbl()\n\n\n\n\n\nsale_price\ngr_liv_area\nlot_area\nyear_built\noverall_qual\ngarage_cars\ngarage_qual\nms_zoning\nlot_config\nbldg_type\n\n\n\n\n115000\n864\n10500\n1971\n4\n0\nNA\nRL\nFR2\n1Fam\n\n\n128950\n1225\n9320\n1959\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n84900\n1728\n13260\n1962\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n116500\n858\n7207\n1958\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n76500\n1306\n5350\n1940\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n76500\n2256\n9045\n1910\n5\n0\nNA\nRM\nInside\n2fmCon\n\n\n159900\n1560\n12900\n1912\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n55000\n1092\n5600\n1930\n4\n0\nNA\nRM\nInside\n2fmCon\n\n\n93369\n1884\n6449\n1907\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n94000\n1020\n6342\n1875\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n136000\n1832\n10773\n1967\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n100000\n1664\n9825\n1965\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n90000\n960\n6410\n1958\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n100000\n1666\n9839\n1931\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n139000\n1824\n9400\n1971\n6\n0\nNA\nRL\nCorner\nDuplex\n\n\n76000\n1092\n1476\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n75500\n630\n1491\n1972\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n88250\n1092\n1900\n1970\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n136000\n1792\n9000\n1974\n5\n0\nNA\nRL\nFR2\nDuplex\n\n\n142000\n1114\n13072\n2004\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n82500\n708\n5330\n1940\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n129000\n1464\n9900\n1910\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n94550\n1701\n7627\n1920\n4\n0\nNA\nRM\nCorner\n2fmCon\n\n\n103000\n1447\n10134\n1910\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n37900\n968\n5925\n1910\n3\n0\nNA\nRM\nInside\n1Fam\n\n\n113000\n1452\n4456\n1920\n4\n0\nNA\nRM\nInside\n2fmCon\n\n\n58500\n816\n3300\n1910\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n34900\n720\n7879\n1920\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n60000\n800\n6120\n1936\n2\n0\nNA\nRM\nInside\n1Fam\n\n\n62500\n2128\n3000\n1922\n5\n0\nNA\nRM\nInside\nDuplex\n\n\n97500\n1864\n5852\n1902\n7\n0\nNA\nRM\nCorner\n2fmCon\n\n\n70000\n892\n5160\n1923\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n179000\n1200\n10800\n1987\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n179000\n1200\n10800\n1987\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n61000\n904\n10020\n1922\n1\n0\nNA\nRL\nInside\n1Fam\n\n\n118000\n698\n9405\n1947\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n99900\n864\n4060\n1922\n5\n0\nNA\nRL\nCorner\n1Fam\n\n\n119900\n1678\n10926\n1959\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n112000\n833\n8780\n1985\n5\n0\nNA\nRL\nCorner\n1Fam\n\n\n141000\n1080\n7500\n2004\n7\n0\nNA\nRL\nInside\n1Fam\n\n\n106250\n1294\n10800\n1900\n4\n0\nNA\nRL\nInside\n2fmCon\n\n\n130000\n1800\n8513\n1961\n5\n0\nNA\nRL\nCorner\nDuplex\n\n\n120000\n1027\n5400\n1920\n7\n0\nNA\nRM\nInside\n1Fam\n\n\n95000\n1080\n5914\n1890\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n65000\n1588\n12205\n1949\n3\n0\nNA\nRM\nInside\n1Fam\n\n\n129400\n1540\n6000\n1905\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n160000\n1984\n8094\n1910\n6\n1\nNA\nRM\nInside\n2fmCon\n\n\n89500\n1406\n7920\n1920\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n79900\n1198\n5586\n1920\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n82375\n1344\n10320\n1915\n3\n0\nNA\nRM\nInside\n2fmCon\n\n\n127500\n1355\n10106\n1940\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n80000\n1006\n9000\n1959\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n260000\n1518\n19550\n1940\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n99600\n864\n9350\n1975\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n107500\n1347\n7000\n1910\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n79000\n1096\n9600\n1924\n6\n0\nNA\nRL\nCorner\n1Fam\n\n\n85000\n796\n8777\n1910\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n145900\n2200\n8777\n1900\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n82000\n1152\n6040\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n82000\n1152\n6012\n1955\n4\n0\nNA\nRL\nCorner\nDuplex\n\n\n118000\n1440\n12108\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n82500\n1152\n6845\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n91900\n784\n6931\n1955\n4\n0\nNA\nRL\nInside\n2fmCon\n\n\n120000\n1053\n12180\n1938\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n96000\n1137\n8050\n1947\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n98000\n864\n5604\n1925\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n67000\n864\n8248\n1914\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n135900\n1716\n5687\n1912\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n119000\n1200\n8155\n1930\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n81000\n630\n1890\n1972\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n146000\n1100\n7500\n2006\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n64000\n670\n3500\n1945\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n103200\n882\n5500\n1956\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n148000\n1534\n10800\n1895\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n110500\n866\n3880\n1945\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n127000\n1355\n6882\n1914\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n200500\n3086\n18030\n1946\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n150000\n1440\n7711\n1977\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n86000\n605\n9098\n1920\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n123600\n990\n8070\n1994\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n98500\n1195\n8741\n1946\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n79000\n774\n4270\n1931\n3\n0\nNA\nRH\nInside\n1Fam\n\n\n200000\n3395\n10896\n1914\n6\n0\nNA\nRH\nInside\n2fmCon\n\n\n150000\n2592\n10890\n1923\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n115000\n1517\n8500\n1919\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n150909\n1828\n9060\n1923\n5\nNA\nNA\nRM\nInside\n1Fam\n\n\n119600\n1991\n8250\n1895\n5\n0\nNA\nC (all)\nInside\n2fmCon\n\n\n147000\n1120\n8402\n2007\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n93900\n1092\n1495\n1970\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n84500\n630\n1936\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n139500\n1142\n7733\n2005\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n132000\n1131\n13072\n2005\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n85500\n869\n5900\n1923\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n135000\n1192\n10800\n1949\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n119000\n1556\n8512\n1960\n5\n0\nNA\nRL\nCorner\nDuplex\n\n\n124000\n1025\n7000\n1962\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n64500\n1020\n4761\n1918\n3\n0\nNA\nC (all)\nCorner\n1Fam\n\n\n100000\n788\n7446\n1941\n4\n0\nNA\nRL\nCorner\n1Fam\n\n\n80500\n912\n6240\n1947\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n72000\n819\n9000\n1919\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n117250\n914\n8050\n2002\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n81000\n1184\n8410\n1910\n5\n0\nNA\nRL\nFR2\n1Fam\n\n\n83000\n1414\n8248\n1922\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n102000\n1522\n6000\n1926\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n72000\n672\n8534\n1925\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n115000\n1396\n9000\n1951\n5\n0\nNA\nC (all)\nInside\n2fmCon\n\n\n78000\n936\n8520\n1916\n3\n0\nNA\nC (all)\nInside\n1Fam\n\n\n92000\n630\n1533\n1970\n5\n0\nNA\nRM\nInside\nTwnhs\n\n\n90500\n1092\n1936\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n\n\n\n\n\nOption 3 (Most advanced): Line by line kable table. You can make this as complicated and customized as you like. This is a simple example of options\n\ndata_all |&gt; filter(is.na(garage_qual)) |&gt; \n  kbl(align = \"r\") |&gt; \n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) |&gt; \n  scroll_box(height = \"500px\", width = \"100%\")\n\n\n\n\n\nsale_price\ngr_liv_area\nlot_area\nyear_built\noverall_qual\ngarage_cars\ngarage_qual\nms_zoning\nlot_config\nbldg_type\n\n\n\n\n115000\n864\n10500\n1971\n4\n0\nNA\nRL\nFR2\n1Fam\n\n\n128950\n1225\n9320\n1959\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n84900\n1728\n13260\n1962\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n116500\n858\n7207\n1958\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n76500\n1306\n5350\n1940\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n76500\n2256\n9045\n1910\n5\n0\nNA\nRM\nInside\n2fmCon\n\n\n159900\n1560\n12900\n1912\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n55000\n1092\n5600\n1930\n4\n0\nNA\nRM\nInside\n2fmCon\n\n\n93369\n1884\n6449\n1907\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n94000\n1020\n6342\n1875\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n136000\n1832\n10773\n1967\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n100000\n1664\n9825\n1965\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n90000\n960\n6410\n1958\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n100000\n1666\n9839\n1931\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n139000\n1824\n9400\n1971\n6\n0\nNA\nRL\nCorner\nDuplex\n\n\n76000\n1092\n1476\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n75500\n630\n1491\n1972\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n88250\n1092\n1900\n1970\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n136000\n1792\n9000\n1974\n5\n0\nNA\nRL\nFR2\nDuplex\n\n\n142000\n1114\n13072\n2004\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n82500\n708\n5330\n1940\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n129000\n1464\n9900\n1910\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n94550\n1701\n7627\n1920\n4\n0\nNA\nRM\nCorner\n2fmCon\n\n\n103000\n1447\n10134\n1910\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n37900\n968\n5925\n1910\n3\n0\nNA\nRM\nInside\n1Fam\n\n\n113000\n1452\n4456\n1920\n4\n0\nNA\nRM\nInside\n2fmCon\n\n\n58500\n816\n3300\n1910\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n34900\n720\n7879\n1920\n4\n0\nNA\nC (all)\nInside\n1Fam\n\n\n60000\n800\n6120\n1936\n2\n0\nNA\nRM\nInside\n1Fam\n\n\n62500\n2128\n3000\n1922\n5\n0\nNA\nRM\nInside\nDuplex\n\n\n97500\n1864\n5852\n1902\n7\n0\nNA\nRM\nCorner\n2fmCon\n\n\n70000\n892\n5160\n1923\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n179000\n1200\n10800\n1987\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n179000\n1200\n10800\n1987\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n61000\n904\n10020\n1922\n1\n0\nNA\nRL\nInside\n1Fam\n\n\n118000\n698\n9405\n1947\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n99900\n864\n4060\n1922\n5\n0\nNA\nRL\nCorner\n1Fam\n\n\n119900\n1678\n10926\n1959\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n112000\n833\n8780\n1985\n5\n0\nNA\nRL\nCorner\n1Fam\n\n\n141000\n1080\n7500\n2004\n7\n0\nNA\nRL\nInside\n1Fam\n\n\n106250\n1294\n10800\n1900\n4\n0\nNA\nRL\nInside\n2fmCon\n\n\n130000\n1800\n8513\n1961\n5\n0\nNA\nRL\nCorner\nDuplex\n\n\n120000\n1027\n5400\n1920\n7\n0\nNA\nRM\nInside\n1Fam\n\n\n95000\n1080\n5914\n1890\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n65000\n1588\n12205\n1949\n3\n0\nNA\nRM\nInside\n1Fam\n\n\n129400\n1540\n6000\n1905\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n160000\n1984\n8094\n1910\n6\n1\nNA\nRM\nInside\n2fmCon\n\n\n89500\n1406\n7920\n1920\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n79900\n1198\n5586\n1920\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n82375\n1344\n10320\n1915\n3\n0\nNA\nRM\nInside\n2fmCon\n\n\n127500\n1355\n10106\n1940\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n80000\n1006\n9000\n1959\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n260000\n1518\n19550\n1940\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n99600\n864\n9350\n1975\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n107500\n1347\n7000\n1910\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n79000\n1096\n9600\n1924\n6\n0\nNA\nRL\nCorner\n1Fam\n\n\n85000\n796\n8777\n1910\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n145900\n2200\n8777\n1900\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n82000\n1152\n6040\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n82000\n1152\n6012\n1955\n4\n0\nNA\nRL\nCorner\nDuplex\n\n\n118000\n1440\n12108\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n82500\n1152\n6845\n1955\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n91900\n784\n6931\n1955\n4\n0\nNA\nRL\nInside\n2fmCon\n\n\n120000\n1053\n12180\n1938\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n96000\n1137\n8050\n1947\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n98000\n864\n5604\n1925\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n67000\n864\n8248\n1914\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n135900\n1716\n5687\n1912\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n119000\n1200\n8155\n1930\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n81000\n630\n1890\n1972\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n146000\n1100\n7500\n2006\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n64000\n670\n3500\n1945\n3\n0\nNA\nRL\nInside\n1Fam\n\n\n103200\n882\n5500\n1956\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n148000\n1534\n10800\n1895\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n110500\n866\n3880\n1945\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n127000\n1355\n6882\n1914\n6\n0\nNA\nRM\nInside\n1Fam\n\n\n200500\n3086\n18030\n1946\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n150000\n1440\n7711\n1977\n4\n0\nNA\nRL\nInside\nDuplex\n\n\n86000\n605\n9098\n1920\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n123600\n990\n8070\n1994\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n98500\n1195\n8741\n1946\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n79000\n774\n4270\n1931\n3\n0\nNA\nRH\nInside\n1Fam\n\n\n200000\n3395\n10896\n1914\n6\n0\nNA\nRH\nInside\n2fmCon\n\n\n150000\n2592\n10890\n1923\n5\n0\nNA\nRL\nInside\nDuplex\n\n\n115000\n1517\n8500\n1919\n5\n0\nNA\nRM\nCorner\n1Fam\n\n\n150909\n1828\n9060\n1923\n5\nNA\nNA\nRM\nInside\n1Fam\n\n\n119600\n1991\n8250\n1895\n5\n0\nNA\nC (all)\nInside\n2fmCon\n\n\n147000\n1120\n8402\n2007\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n93900\n1092\n1495\n1970\n4\n0\nNA\nRM\nInside\nTwnhsE\n\n\n84500\n630\n1936\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n139500\n1142\n7733\n2005\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n132000\n1131\n13072\n2005\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n85500\n869\n5900\n1923\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n135000\n1192\n10800\n1949\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n119000\n1556\n8512\n1960\n5\n0\nNA\nRL\nCorner\nDuplex\n\n\n124000\n1025\n7000\n1962\n5\n0\nNA\nRL\nInside\n2fmCon\n\n\n64500\n1020\n4761\n1918\n3\n0\nNA\nC (all)\nCorner\n1Fam\n\n\n100000\n788\n7446\n1941\n4\n0\nNA\nRL\nCorner\n1Fam\n\n\n80500\n912\n6240\n1947\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n72000\n819\n9000\n1919\n5\n0\nNA\nRM\nInside\n1Fam\n\n\n117250\n914\n8050\n2002\n6\n0\nNA\nRL\nInside\n1Fam\n\n\n81000\n1184\n8410\n1910\n5\n0\nNA\nRL\nFR2\n1Fam\n\n\n83000\n1414\n8248\n1922\n4\n0\nNA\nRL\nInside\n1Fam\n\n\n102000\n1522\n6000\n1926\n5\n0\nNA\nRL\nInside\n1Fam\n\n\n72000\n672\n8534\n1925\n4\n0\nNA\nRM\nInside\n1Fam\n\n\n115000\n1396\n9000\n1951\n5\n0\nNA\nC (all)\nInside\n2fmCon\n\n\n78000\n936\n8520\n1916\n3\n0\nNA\nC (all)\nInside\n1Fam\n\n\n92000\n630\n1533\n1970\n5\n0\nNA\nRM\nInside\nTwnhs\n\n\n90500\n1092\n1936\n1970\n4\n0\nNA\nRM\nInside\nTwnhs\n\n\n\n\n\n\n\n\nCoding sidebar:\n\nIn the above example, we created a function (print_kbl()) from scratch (rather than using a function factory)\nSee functions chapter in Wickham, Çetinkaya-Rundel, and Grolemund (2023) for help.\nSee functionals chapter in Wickham (2019).\n\n\nIn this instance, if we consult our data dictionary, we see that NA for garage_qual should be coded as “no garage”. We will correct this in our data set.\nThis is a pretty poor choice on the part of the researchers who created the dataset because it becomes impossible to distinguish between NA that means no garage vs. true NA for the variable. In fact, if you later do really careful EDA on the full data set with all variables, you will see this problem likely exists in this dataset\nAnyway, let’s correct all the NA for garage_qual to “no_garage” using mutate()\n\ndata_all &lt;- data_all |&gt; \n1  mutate(garage_qual = fct_expand(garage_qual, \"no_garage\"),\n2         garage_qual = replace_na(garage_qual, \"no_garage\"))\n\n\n1\n\nFirst add a new level to the factor\n\n2\n\nThen recode NA to that new level\n\n\n\n\nWe will leave the NA for garage_cars as NA because its not clear if that is truly missing or not, based on further EDA not shown here.\n\nWe have one other issue with garage_qual. It is an ordinal variable but we never reviewed the order of its levels. The data dictionary indicates the levels are ordered (best to worst) as:\n\nEx (excellent)\nGd (good)\nTA (typical/average)\nFa (fair)\nPo (poor)\n\nAnd we might assume that no garage is even worse than a poor garage. Lets see what they are.\n\ndata_all$garage_qual |&gt; levels()\n\n[1] \"Ex\"        \"Fa\"        \"Gd\"        \"Po\"        \"TA\"        \"no_garage\"\n\n\nTo fix this, we can use forcats::fct_relevel().\n\n1gq_levels &lt;- c(\"no_garage\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\")\ndata_all &lt;- data_all |&gt; \n2  mutate(garage_qual = fct_relevel(garage_qual, gq_levels))\n\n3data_all$garage_qual |&gt; levels()\n\n\n1\n\nMake a vector that indicates the valid levels in order\n\n2\n\nPass that into fct_relevel(). See ?fct_relevel for other ways to adjust the levels of a factor.\n\n3\n\nConfirm that the levels are now correct\n\n\n\n\n[1] \"no_garage\" \"Po\"        \"Fa\"        \"TA\"        \"Gd\"        \"Ex\"       \n\n\n\n\n2.3.6 Explore Min/Max Response for Numeric Variables\nWe should explore mins and maxes for all numeric variables to detect out of valid range numeric responses\nCould also do this for ordinal variables that are coded with numbers\n\ne.g., overall_qual (1-10) vs. garage_qual (no_garage, Po, Fa, TA, Gd, Ex)\nThis is only a temporary mutation of overall_qual for this check\n\nWe can use skim_some() again\n\np0 = min\np100 = max\n\n\ndata_all |&gt;\n  mutate(overall_qual = as.numeric(overall_qual)) |&gt; \n  skim_some()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nmutate(data_all, overall_…\n\n\n\n\nNumber of rows\n\n\n1955\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n4\n\n\n\n\nnumeric\n\n\n6\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\ngarage_qual\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n6\n\n\nTA: 1745, no_: 109, Fa: 79, Gd: 16\n\n\n\n\nms_zoning\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n7\n\n\nRL: 1530, RM: 297, FV: 91, C (: 19\n\n\n\n\nlot_config\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n5\n\n\nIns: 1454, Cor: 328, Cul: 114, FR2: 55\n\n\n\n\nbldg_type\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n5\n\n\n1Fa: 1631, Twn: 145, Dup: 77, Twn: 64\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\np0\n\n\np100\n\n\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n12789\n\n\n745000\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n438\n\n\n5642\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n1476\n\n\n215245\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1875\n\n\n2010\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n1\n\n\n10\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n0\n\n\n4\n\n\n\n\n\n\n\n\n\n2.3.7 Explore All Responses for Categorical Variables\nWe should explore all unique responses for nominal variables\nMight also do this for ordinal variables that are coded with labels vs. numbers.\n\ndata_all |&gt; \n  select(where(is.factor)) |&gt;\n  walk(\\(column) print(levels(column)))\n\n[1] \"no_garage\" \"Po\"        \"Fa\"        \"TA\"        \"Gd\"        \"Ex\"       \n[1] \"A (agr)\" \"C (all)\" \"FV\"      \"I (all)\" \"RH\"      \"RL\"      \"RM\"     \n[1] \"Corner\"  \"CulDSac\" \"FR2\"     \"FR3\"     \"Inside\" \n[1] \"1Fam\"   \"2fmCon\" \"Duplex\" \"Twnhs\"  \"TwnhsE\"\n\n\nCoding sidebar:\n\nAbove, we demonstrate the use of an anonymous function (\\(column) print(levels(column))), which is a function we use once that we don’t bother to assign a name (since we won’t reuse it). We often use anonymous functions when using the functions from the purrr package (e.g., map(), walk())\nWe use walk() from the purrr package to apply our anonymous function to all columns of the data frame at once\nJust copy this code for now\nWe will see simpler uses later that will help you understand iteration with purrr functions\nSee the chapter on iteration in R for Data Science (2e) for more info on map() and walk()\n\n\n\n2.3.8 Tidy Responses for Categorical Variables\nFeature engineering with nominal and ordinal variables typically involves\n\nConverting to factors\nOften creating dummy features from these factors\n\nThis feature engineering will use response labels for naming new features\n\nTherefore, it is a good idea to have the responses snake-cased and cleaned up a bit so that these new feature names are clean/clear.\n\nHere is an easy way to convert responses for character variables to snake case using a function (tidy_responses()) we share in fun_eda.R (reproduced here).\n\nThis uses regular expressions (regex), which will will learn about in a later unit on text processing.\nYou could expand this cleaning function if you encounter other issues that need to be cleaned in the factor levels.\n\n\ntidy_responses &lt;- function(column){\n  # replace all non-alphanumeric with _\n  column &lt;- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\W\", \"_\"))\n  # replace whitespace with _\n  column &lt;- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\s+\", \"_\"))\n  # replace multiple _ with single _\n  column &lt;- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\_+\", \"_\"))\n  #remove _ at end of string\n  column &lt;- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\_$\", \"\"))\n  # remove _ at start of string\n  column &lt;- fct_relabel(column, \\(column) str_replace_all(column, \"\\\\^_\", \"\"))\n  # convert to lowercase\n  column &lt;- fct_relabel(column, tolower)\n  factor(column)\n}\n\nLet’s use the function\n\ndata_all &lt;- data_all |&gt; \n  mutate(across(where(is.factor), tidy_responses)) |&gt; \n  glimpse()\n\nRows: 1,955\nColumns: 10\n$ sale_price   &lt;dbl&gt; 215000, 105000, 172000, 189900, 213500, 191500, 236500, 1…\n$ gr_liv_area  &lt;dbl&gt; 1656, 896, 1329, 1629, 1338, 1280, 1616, 1804, 1655, 1465…\n$ lot_area     &lt;dbl&gt; 31770, 11622, 14267, 13830, 4920, 5005, 5389, 7500, 10000…\n$ year_built   &lt;dbl&gt; 1960, 1961, 1958, 1997, 2001, 1992, 1995, 1999, 1993, 199…\n$ overall_qual &lt;dbl&gt; 6, 5, 6, 5, 8, 8, 8, 7, 6, 6, 7, 8, 8, 8, 9, 4, 6, 6, 7, …\n$ garage_cars  &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, …\n$ garage_qual  &lt;fct&gt; ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, t…\n$ ms_zoning    &lt;fct&gt; rl, rh, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl, rl, r…\n$ lot_config   &lt;fct&gt; corner, inside, corner, inside, inside, inside, inside, i…\n$ bldg_type    &lt;fct&gt; 1fam, 1fam, 1fam, 1fam, twnhse, twnhse, twnhse, 1fam, 1fa…\n\n\nCoding sidebar: See more details on the tidy selection helpers like all_of() and where()\n\nAlas, these response labels were pretty poorly chosen so some didn’t convert well. And some are really hard to understand too.\n\nAvoid this problem and choose good response labels from the start for your own data\nHere, we show you what we got from using tidy_responses()\n\n\ndata_all |&gt; \n  select(where(is.factor)) |&gt;\n  walk(\\(column) print(levels(column)))\n\n[1] \"no_garage\" \"po\"        \"fa\"        \"ta\"        \"gd\"        \"ex\"       \n[1] \"a_agr\" \"c_all\" \"fv\"    \"i_all\" \"rh\"    \"rl\"    \"rm\"   \n[1] \"corner\"  \"culdsac\" \"fr2\"     \"fr3\"     \"inside\" \n[1] \"1fam\"   \"2fmcon\" \"duplex\" \"twnhs\"  \"twnhse\"\n\n\n\nLets clean them up a bit more manually\n\ndata_all &lt;- data_all |&gt; \n  mutate(ms_zoning = fct_recode(ms_zoning,\n                                res_low = \"rl\",\n                                res_med = \"rm\",\n                                res_high = \"rh\",\n                                float = \"fv\",\n                                agri = \"a_agr\",\n                                indus = \"i_all\",\n                                commer = \"c_all\"),\n1         bldg_type = fct_recode(bldg_type,\n                                one_fam = \"1fam\",\n                                two_fam = \"2fmcon\",\n                                town_end = \"twnhse\",\n                                town_inside = \"twnhs\"))\n\n\n1\n\nNote that I did not need to list all levels in the recode. Only the levels I wanted to change.\n\n\n\n\nThe full dataset is now clean!\n\n\n2.3.9 Train/Validate/Test Splits\nThe final task we typically do as part of the data preparation process is to split the full dataset into training, validation and test sets.\n\nTest sets are “typically” between 20-30% of your full dataset\n\nThere are costs and benefits to larger test sets\nWe will learn about these costs/benefits in the unit on resampling\nI have already held out the test set\n\nThere are many approaches to validation sets\n\nFor now (until unit 5) we will use a single validation set approach\nWe will use 25% of the remaining data (after holding out the test set) as a validation set for this example\n\nIt is typical to split data on the outcome within strata\n\nFor a categorical outcome, this makes the proportions of the response categories more balanced across the train, validation, and test sets\nFor a numeric outcome, we first break up the distribution into temporary bins (see breaks = 4 below) and then we split within these bins\n\nIMPORTANT: Set a seed so that you can reproduce these splits if you later do more cleaning\n\n\nset.seed(20110522)\nsplits &lt;- data_all |&gt; \n  initial_split(prop = 3/4, strata = \"sale_price\", breaks = 4)\n\n\nWe then extract the training set from the splits and save it\n\nTraining sets are used for “analysis”- hence the name of the function\n\n\nsplits |&gt; \n1  analysis() |&gt;\n  glimpse() |&gt; \n  write_csv(here::here(path_data, \"ames_clean_class_trn.csv\"))\n\n\n1\n\nanalysis() pulls out the training set from our split of data_all\n\n\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;dbl&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nWe will not need the validation set for modeling EDA\n\nIt should NOT be used for anything other than evaluating models to select the best model configuration\nWe do NOT do Modeling EDA or Model Fitting with the validation set\nSave it in this clean form for easy use when you need it\nWe use the validation set to “assess” models that we have fit in training sets - hence the name of the function\n\n\nsplits |&gt; \n1  assessment() |&gt;\n  glimpse() |&gt; \n  write_csv(here::here(path_data, \"ames_clean_class_val.csv\"))\n\n\n1\n\nassessment() pulls out the validation set from our split of data_all\n\n\n\n\nRows: 490\nColumns: 10\n$ sale_price   &lt;dbl&gt; 215000, 189900, 189000, 171500, 212000, 164000, 394432, 1…\n$ gr_liv_area  &lt;dbl&gt; 1656, 1629, 1804, 1341, 1502, 1752, 1856, 1004, 1092, 106…\n$ lot_area     &lt;dbl&gt; 31770, 13830, 7500, 10176, 6820, 12134, 11394, 11241, 168…\n$ year_built   &lt;dbl&gt; 1960, 1997, 1999, 1990, 1985, 1988, 2010, 1970, 1971, 197…\n$ overall_qual &lt;dbl&gt; 6, 5, 7, 7, 8, 8, 9, 6, 5, 6, 7, 9, 8, 8, 7, 8, 6, 5, 5, …\n$ garage_cars  &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 3, 2, 3, 1, 1, 2, …\n$ garage_qual  &lt;fct&gt; ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, t…\n$ ms_zoning    &lt;fct&gt; res_low, res_low, res_low, res_low, res_low, res_low, res…\n$ lot_config   &lt;fct&gt; corner, inside, inside, inside, corner, inside, corner, c…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, one_fam, town_end, one_fam, on…"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-modeling",
    "href": "002_exploratory_data_analysis.html#exploratory-data-analysis-for-modeling",
    "title": "2  Exploratory Data Analysis",
    "section": "2.4 Exploratory Data Analysis for Modeling",
    "text": "2.4 Exploratory Data Analysis for Modeling\nNow let’s begin our Modeling EDA\nWe prefer to write separate scripts for Cleaning vs, Modeling EDA (but not displayed here)\n\nThis keeps these two processes separate in our minds\nCleaning EDA is done with full dataset but Modeling EDA is only done with a training set - NEVER use validation or test set\n\n\nLets re-load (and glimpse) our training set to pretend we are at the start of a new script.\nNow that we are done cleaning the data, we should start to fully class our variables for how we will use them for modeling EDA and modeling more generally.\n\nNumeric predictors are classes as numeric (int or double)\ncategorical predictors are classed as unordered or ordered factors.\n\n\n data_trn &lt;- read_csv(here::here(path_data, \"ames_clean_class_trn.csv\")) |&gt; \n  glimpse()\n\nRows: 1465 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): garage_qual, ms_zoning, lot_config, bldg_type\ndbl (6): sale_price, gr_liv_area, lot_area, year_built, overall_qual, garage...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;dbl&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;chr&gt; \"ta\", \"ta\", \"no_garage\", \"ta\", \"ta\", \"ta\", \"ta\", \"no_gara…\n$ ms_zoning    &lt;chr&gt; \"res_high\", \"res_low\", \"res_low\", \"res_low\", \"res_low\", \"…\n$ lot_config   &lt;chr&gt; \"inside\", \"corner\", \"fr2\", \"fr2\", \"inside\", \"inside\", \"in…\n$ bldg_type    &lt;chr&gt; \"one_fam\", \"one_fam\", \"one_fam\", \"town_inside\", \"town_end…\n\n\nOpps:\n\nNotice that overall_qual is back to being classed as numeric (dbl).\n\nThis is because read_csv() guesses the type for each column and we used only numbers for this ordinal categorical variable.\n\n\nNotice that your factors are back to character\n\nThis is because csv files don’t save anything other than the values (labels for factors). They are the cleaned labels though!\n\nYou should class all variables using the same approach as before (often just a copy/paste). My preferred way is via mutates\n\n\n data_trn &lt;- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n1           col_types = cols()) |&gt;\n2  mutate(across(where(is.character), factor)) |&gt;\n3  mutate(overall_qual = factor(overall_qual),\n4         overall_qual = fct_relevel(overall_qual, as.character(1:10)),\n         garage_qual = fct_relevel(garage_qual, c(\"no_garage\", \"po\", \"fa\", \n5                                                  \"ta\", \"gd\", \"ex\"))) |&gt;\n  glimpse()\n\n\n1\n\nuse col_types = cols() to suppress messages about default class assignments\n\n2\n\nuse mutate() with across() to change all character variables to factors\n\n3\n\nuse mutate() with factor() to change numeric variable to factor.\n\n4\n\nuse mutate() with fct_relevel() to explicitly set levels of an ordered factor\n\n5\n\nnotice the warning about the unknown level. Always explore warnings! In this instance, its fine. There were only two observations with ex and neither ended up in the training split. Still best to include this level to note it exists!\n\n\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nCoding sidebar: We will likely re-class the Ames dataset many times (for training, validation, test). We could copy/paste these mutates each time but whenever you do something more than twice, I recommend writing a function. We might write this one to re-class the ames variables\n\nclass_ames &lt;- function(df){\n  \n  df |&gt;\n    mutate(across(where(is.character), factor)) |&gt; \n    mutate(overall_qual = factor(overall_qual), \n           overall_qual = fct_relevel(overall_qual, as.character(1:10)), \n           garage_qual = fct_relevel(garage_qual, c(\"no_garage\", \"po\", \"fa\", \n                                                    \"ta\", \"gd\", \"ex\")))\n}\n\nNow we can use it every time we read in one of the Ames datasets\n\ndata_trn &lt;- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), col_types = cols()) |&gt; \n  class_ames() |&gt; \n  glimpse()\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nThere are 3 basic types of Modeling EDA you should always do\n\nExplore missingness for predictors\nExplore univariate distributions for outcome and predictors\nExplore bivariate relationships between predictors and outcome\n\nAs a result of this exploration, we will:\n\nIdentify promising predictors\nDetermine appropriate feature engineering for those predictors (e.g., transformations)\nIdentify outliers and consider how to handle when model building\nConsider how to handle imputation for missing data (if any)\n\n\n2.4.1 Overall Summary of Feature Matrix\nBefore we dig into individual variables and their distributions and relationships with the outcome, it’s nice to start with a big picture of the dataset\n\nWe use another customized version of skim() from the skimr package to provide this\nJust needed to augment it with skewness and kurtosis statistics for numeric variables\nand remove histogram b/c we don’t find that small histogram useful\nincluded in fun_eda.R on github\n\n\nskew_na &lt;- partial(e1071::skewness, na.rm = TRUE)\nkurt_na &lt;- partial(e1071::kurtosis, na.rm = TRUE)\n\nskim_all &lt;- skimr::skim_with(numeric = skimr::sfl(skew = skew_na, \n                                                  kurtosis = kurt_na, \n                                                  hist = NULL))\n\n\ndata_trn |&gt; \n  skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndata_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n5\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n10\n\n\n5: 424, 6: 350, 7: 304, 8: 176\n\n\n\n\ngarage_qual\n\n\n0\n\n\n1\n\n\n5\n\n\nta: 1312, no_: 81, fa: 57, gd: 13\n\n\n\n\nms_zoning\n\n\n0\n\n\n1\n\n\n7\n\n\nres: 1157, res: 217, flo: 66, com: 13\n\n\n\n\nlot_config\n\n\n0\n\n\n1\n\n\n5\n\n\nins: 1095, cor: 248, cul: 81, fr2: 39\n\n\n\n\nbldg_type\n\n\n0\n\n\n1\n\n\n5\n\n\none: 1216, tow: 108, dup: 63, tow: 46\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\n\n\n\nCareful review of this output provides a great orientation to our data\n\n\n2.4.2 Univariate Distributions\nExploration of univariate distributions are useful to\n\nUnderstand variation and distributional shape\nMay suggest need to consider transformations as part of feature engineering\nCan identify univariate outliers (valid but disconnected from distribution so not detected in cleaning)\n\nWe generally select different visualizations and summary statistics for categorical vs. numeric variables\n\n2.4.2.1 Categorical Variables\n\n2.4.2.1.1 Barplots\nThe primary visualization for categorical variables is the bar plot\nDefine and customize it within a function for repeated use. We share this and all the remaining plots used in this unit in fun_plot.R. Source it to use them without having to re-code each time\n\nplot_bar &lt;- function(df, x){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\nCoding sidebar: When defining functions, generally put data as first argument so you can pipe in data using tidy pipelines\nThere are pros and cons to writing functions that accept variable names that are quoted vs. unquoted\n\nIt depends a bit on how you will use them.\n.data[[argument]] is used in functions with quoted arguments\nembracing {{}} is used for unquoted arguments\nFor these plot functions, I use quoted variable names and then pipe those into map() to make multiple plots (see below)\nsee ?vignette(\"programming\") or info on tidy evaluation in Wickham, Çetinkaya-Rundel, and Grolemund (2023) for more details\n\n\nBar plots reveal low frequency responses for nominal and ordinal variables\n\nSee bldg_type\n\n\ndata_trn |&gt; plot_bar(\"bldg_type\")\n\n\n\n\n\nBar plots can display distributional shape for ordinal variables. May suggest the need for transformations if we later treat the ordinal variable as numeric\n\nSee overall_qual. Though it is not very skewed.\n\n\ndata_trn |&gt; plot_bar(\"overall_qual\")\n\n\n\n\n\nCoding sidebar:\nWe can make all of our plots iteratively using map() from the `purrr package.\n\ndata_trn |&gt; \n1  select(where(is.factor)) |&gt;\n2  names() |&gt;\n3  map(\\(name) plot_bar(df = data_trn, x = name)) |&gt;\n4  plot_grid(plotlist = _, ncol = 2)\n\n\n1\n\nSelect only the factor columns\n\n2\n\nGet their names as strings (that is why we use quoted variables in these plot functions\n\n3\n\nUse map() to iterative plot_bar() over every column. (see iteration in Wickham, Çetinkaya-Rundel, and Grolemund (2023))\n\n4\n\nUse plot_grid() from cowplot package to display the list of plots in a grid\n\n\n\n\n\n\n\n\n\n2.4.2.1.2 Tables\nWe tend to prefer visualizations vs. summary statistics for EDA. However, tables can be useful.\nHere is a function that was described in Wickham, Çetinkaya-Rundel, and Grolemund (2023) that we like because\n\nIncludes counts and proportions\nIncludes NA as a category\n\nWe have included it in fun_eda.R for your use.\n\ntab &lt;- function(df, var, sort = FALSE) {\n  df |&gt;  dplyr::count({{ var }}, sort = sort) |&gt; \n    dplyr::mutate(prop = n / sum(n))\n} \n\nTables can be used to identify responses that have very low frequency and to think about the need to handle missing values\n\nSee ms_zoning\nMay want to collapse low frequency (or low percentage) categories to reduce the number of features needed to represent the predictor\n\n\ndata_trn |&gt; tab(ms_zoning)\n\n# A tibble: 7 × 3\n  ms_zoning     n     prop\n  &lt;fct&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 agri          2 0.00137 \n2 commer       13 0.00887 \n3 float        66 0.0451  \n4 indus         1 0.000683\n5 res_high      9 0.00614 \n6 res_low    1157 0.790   \n7 res_med     217 0.148   \n\n\n\nor maybe sorted\n\n\ndata_trn |&gt; tab(ms_zoning, sort = TRUE)\n\n# A tibble: 7 × 3\n  ms_zoning     n     prop\n  &lt;fct&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 res_low    1157 0.790   \n2 res_med     217 0.148   \n3 float        66 0.0451  \n4 commer       13 0.00887 \n5 res_high      9 0.00614 \n6 agri          2 0.00137 \n7 indus         1 0.000683\n\n\nbut could see this with plot as well\n\ndata_trn |&gt; plot_bar(\"ms_zoning\")\n\n\n\n\n\n\n\n2.4.2.2 Numeric Variables\n\n2.4.2.2.1 Histograms\nHistograms are a useful/common visualization for numeric variables\nLet’s define a histogram function (included in fun_plots.r)\n\nBin size should be explored a bit to find best representation\nSomewhat dependent on n (my default here is based on this training set)\nThis is one of the limitations of histograms\n\n\nplot_hist &lt;- function(df, x, bins = 100){\n  df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_histogram(bins = bins) +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n\nLet’s look at sale_price\n\nIt is positively skewed\nMay suggest units (dollars) are not interval in nature (makes sense)\nCould cause problems for some algorithms (e.g., lm) when features are normal\n\n\ndata_trn |&gt; plot_hist(\"sale_price\")\n\n\n\n\n\n\n2.4.2.2.2 Smoothed Frequency Polygons\nFrequency polygons are also commonly used\nDefine a frequency polygon function and use it (included in fun_plots.r)\n\nBins may matter again\n\n\nplot_freqpoly &lt;- function(df, x, bins = 50){\n  df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_freqpoly(bins = bins) +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n\n\ndata_trn |&gt; plot_freqpoly(\"sale_price\")\n\n\n\n\n\n\n2.4.2.2.3 Simple Boxplots\nBoxplots display\n\nMedian as line\n25%ile and 75%ile as hinges\nHighest and lowest points within 1.5 * IQR (interquartile-range: difference between scores at 25% and 75%iles)\nOutliers outside of 1.5 * IQR\n\nDefine a boxplot function and use it (included in fun_plots.r)\n\nplot_boxplot &lt;- function(df, x){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1))\n}\n\n\ndata_trn |&gt; plot_boxplot(\"sale_price\")\n\n\n\n\n\n\n2.4.2.2.4 Combined Boxplot and Violin Plots\nThe combination of a boxplot and violin plot is particularly useful\n\nThis is our favorite\nGet all the benefits of the boxplot\nCan clearly see shape of distribution given the violin plot overlay\nCan also clearly see the tails\n\nDefine a combined plot (included in fun_plots.r)\n\nplot_box_violin &lt;- function(df, x){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_violin(aes(y = 0), fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.x = element_text(angle = 90, size = x_label_size, vjust = 0.5, hjust = 1))\n}\n\nHere is the plot for sale_price\n\nIn this instance, the skew is NOT due to only a few outliers\n\n\ndata_trn |&gt; plot_box_violin(\"sale_price\")\n\n\n\n\n\nCoding sidebar:\n\nYou can make figures for all numeric variables at once using select() and map() as before\n\n\ndata_trn |&gt; \n1  select(where(is.numeric)) |&gt;\n  names() |&gt; \n  map(\\(name) plot_box_violin(df = data_trn, x = name)) |&gt; \n  plot_grid(plotlist = _, ncol = 2)\n\n\n1\n\nNow select numeric rather than factor but otherwise same as previous example\n\n\n\n\n\n\n\n\n\n2.4.2.2.5 Summary Statistics\nskim_all() provided all the summary statistics you likely needed for numeric variables\n\nmean & median (p50)\nsd & IQR (see difference between p25 and p75)\nskew & kurtosis\n\nYou can get skim of only numeric variables if you like\n\ndata_trn |&gt; \n  select(where(is.numeric)) |&gt; \n  skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nselect(data_trn, where(is…\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n5\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\n\n\n\n\n\n\n\n2.4.3 Bivariate Relationships with Outcome\nBivariate relationships with the outcome are useful to detect\n\nWhich predictors display some relationship with the outcome\nWhat feature engineering (transformations) might maximize that relationship\nAre there any bivariate (model) outliers\n\nAgain, we prefer visualizations but summary statistics are also available\n\n2.4.3.1 Both Numeric\n\n2.4.3.1.1 Scatterplots\nScatterplots are the preferred visualization when both variables are numeric\nDefine a scatterplot function (included in fun_plots.r)\n\nadd a simple line\nadd a LOWESS line (Locally Weighted Scatterplot Smoothing)\nThese lines are useful for considering shape of relationship\n\n\nplot_scatter &lt;- function(df, x, y){\n  df |&gt;\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"green\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n\n\nLet’s consider relationship between gr_liv_area and sale_price\n\nCare most about influential points (both model outlier and leverage)\nCan be typically spotted in bivariate plots (but could do more sophisticated assessments)\nWe might:\n\nretain as is\ndrop\nbring to fence\n\n\nIf bivariate outliers are detected, you should return to cleaning mode to verify that they aren’t result of scoring/coding errors. If they are:\n\nFix in full dataset\nUse same train/test split after fixing\n\n\ndata_trn |&gt; plot_scatter(\"gr_liv_area\", \"sale_price\")\n\n\n\n\n\nHere is another example where the relationship might be non-linear\n\ndata_trn |&gt; plot_scatter(\"year_built\", \"sale_price\")\n\n\n\n\n\nA transformation of sale_price might help the relationship with \\(year\\_built\\) but might hurt gr_liv_area\nMaybe need to transform both sale_price and gr_liv_area as both were skewed\nThis might require some more EDA but here is a start\n\nQuick and temporary Log (base e) of sale_price\nThis doesn’t seem promising by itself\n\n\ndata_trn |&gt; \n  mutate(sale_price = log(sale_price)) |&gt; \n  plot_scatter(\"gr_liv_area\", \"sale_price\")\n\n\n\ndata_trn |&gt; \n  mutate(sale_price = log(sale_price)) |&gt;\n  plot_scatter(\"year_built\", \"sale_price\")\n\n\n\n\n\nCan make scatterplots for ordered factors as well\n\nBut other (perhaps better) options also exist for this combination of variable classes.\nUse as.numeric() to allow for lm and LOWESS lines on otherwise categorical variable\n\n\ndata_trn |&gt; \n  mutate(overall_qual = as.numeric(overall_qual)) |&gt; \n  plot_scatter(\"overall_qual\", \"sale_price\")\n\n\n\n\n\nCoding sidebar: Use jitter() with x to help with overplotting\n\ndata_trn |&gt; \n  mutate(overall_qual = jitter(as.numeric(overall_qual))) |&gt; \n  plot_scatter(\"overall_qual\", \"sale_price\")\n\n\n\n\n\nWhen the dataset is large, overplotting can even be a problem with true numeric variables\n\nYou can bin the values to remove this problem\nContinue to use lm and LOWESS lines\nThis dataset doesn’t have much of an overplotting problem but…..\n\n\nplot_hexbin &lt;- function(df, x, y){\n  df |&gt;\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_hex() +\n    geom_smooth(method = \"lm\", col = \"red\") +\n    geom_smooth(method = \"loess\", col = \"green\")  +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n}\n\n\ndata_trn |&gt; plot_hexbin(\"gr_liv_area\", \"sale_price\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n2.4.3.1.2 Correlations & Plots\nCorrelations are useful summary statistics for numeric variables\nSome statistical algorithms are sensitive to high correlations among features (multi-collinearity)\nAt best, highly correlated features add unnecessary flexibility and can lead to overfitting\nWe can visualize correlations among predictors/features using corrplot.mixed() from corrplot package\n\nBest for numeric variables\nCan include ordered categorical or two level unordered categorical variables if transformed to numeric\nCan include unordered categorical variables with &gt; 2 levels if first transformed appropriately (e.g., dummy features, not demonstrated yet)\nWorks best with relatively small set of variables\nNeed to consider how to handle missing values\n\n\ndata_trn |&gt; \n  mutate(overall_qual = as.numeric(overall_qual),\n         garage_qual = as.numeric(garage_qual)) |&gt; \n  select(where(is.numeric)) |&gt; \n  cor(use = \"pairwise.complete.obs\") |&gt; \n  corrplot::corrplot.mixed()\n\n\n\n\ncoding sidebar Note use of namespace (corrplot::corrplot.mixed()) to call this function from corrplot\n\n\n\n2.4.3.2 Categorical and Numeric\n\n2.4.3.2.1 Grouped Box + Violin Plots\nA grouped version of the combined box and violin plot is our preferred visualization for relationship between categorical and numeric variables (included in fun_plots.r)\n\nOften best when feature is categorical and outcome is numeric but can reverse\nCan use with ordered or unordered categorical variable\nWickham, Çetinkaya-Rundel, and Grolemund (2023) also describes use of grouped frequency polygons for this combination of variable classes\n\n\nplot_grouped_box_violin &lt;- function(df, x, y){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\nHere is the relationship between overall_qual and sale_price\n\nTend to prefer this over the scatterplot (with as.numeric()) for ordered categorical variables\nIncreasing spread of sale_price at higher levels of overall_qual is clearer in this plot\n\n\ndata_trn |&gt; plot_grouped_box_violin(\"overall_qual\", \"sale_price\")\n\n\n\n\n\nHere is a box + violin with an unordered categorical variable\n\nMore variation and skew in sale_price for one family homes (additional features, moderators?)\nPosition of townhouse (interior vs. exterior) seems to matter (don’t collapse?)\n\n\ndata_trn |&gt; plot_grouped_box_violin(\"bldg_type\", \"sale_price\")\n\n\n\n\n\nWhen we have a categorical predictor (ordered or unordered) and a numeric outcome, we often want to see both the relationship between the variables AND the variability on the categorical variable alone. We like this combined plot enough when doing EDA to provide a specific function (included in fun_plots.r)! It is our go to for understanding the potential effect of a categorical predictor\n\nplot_categorical &lt;- function(df, x, y, ordered = FALSE){\n  if (ordered) {\n    df &lt;- df |&gt;\n      mutate(!!x := fct_reorder(.data[[x]], .data[[y]]))\n  }\n  \n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  p_bar &lt;- df |&gt;\n    ggplot(aes(x = .data[[x]])) +\n    geom_bar()  +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n  \n  p_box &lt;- df |&gt;\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n  \n  return(list(p_bar, p_box))\n}\n\n\ndata_trn |&gt; plot_categorical(\"bldg_type\", \"sale_price\") |&gt; \n  plot_grid(plotlist = _, ncol = 1)\n\n\n\n\n\n\n\n2.4.3.3 Both Categorical\n\n2.4.3.3.1 Stacked Barplots\nStacked Barplots:\n\nCan be useful with both ordered and unordered categorical variables\nCan create with either raw counts or percentages.\n\nDisplays different perspective (particularly with uneven distributions across levels)\nDepends on your question\n\nOften, you will place the outcome on the x-axis and the feature is coded by fill\n\n\nplot_grouped_barplot_count &lt;- function(df, x, y){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[y]], fill = .data[[x]])) +\n    geom_bar(position = \"stack\") +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\nplot_grouped_barplot_percent &lt;- function(df, x, y){\n  x_label_size &lt;- if_else(skimr::n_unique(df[[x]]) &lt; 7, 11, 7)\n  \n  df |&gt;\n    ggplot(aes(x = .data[[y]], fill = .data[[x]])) +\n    geom_bar(position = \"fill\") +\n    labs(y = \"Proportion\") +\n    theme(axis.text.x = element_text(angle = 90, size = x_label_size, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n}\n\n\nFor example, if we wanted to learn about how bldg_type varies by lot_config, see these plots\n\ndata_trn |&gt; plot_grouped_barplot_count(\"lot_config\", \"bldg_type\")\n\n\n\n\n\ndata_trn |&gt; plot_grouped_barplot_percent(\"lot_config\", \"bldg_type\")\n\n\n\n\n\nMay want to plot both ways\n\ndata_trn |&gt; plot_grouped_barplot_percent(\"lot_config\", \"bldg_type\")\n\n\n\n\n\ndata_trn |&gt; plot_grouped_barplot_percent(\"bldg_type\", \"lot_config\")\n\n\n\n\n\n\n2.4.3.3.2 Tile Plot\nTile plots may be useful if both categorical variables are ordered\n\nplot_tile &lt;- function(df, x, y){\n  df |&gt;\n    count(.data[[x]], .data[[y]]) |&gt;\n    ggplot(aes(x = .data[[x]], y = .data[[y]])) +\n    geom_tile(mapping = aes(fill = n))\n}\n\n\ndata_trn |&gt; plot_tile(\"overall_qual\", \"garage_qual\")\n\n\n\n\n\nYou might also consider a scatterplot with jitter in this instance\n\ndata_trn |&gt; \n  mutate(overall_qual = jitter(as.numeric(overall_qual)),\n         garage_qual = as.numeric(garage_qual)) |&gt; \n  plot_scatter(\"overall_qual\", \"garage_qual\")\n\n\n\n\n\n\n2.4.3.3.3 Two-way Tables\nTwo-way tables are sometimes a useful summary statistic for two categorical variables. We can use tabyl() from the janitor package for this\nFor example, the relationship between bldg_type and lot_config\n\ndata_trn |&gt; janitor::tabyl(bldg_type, lot_config)\n\n   bldg_type corner culdsac fr2 fr3 inside\n      duplex     13       0   3   0     47\n     one_fam    221      73  29   1    892\n    town_end      8       5   3   0     92\n town_inside      0       3   3   0     40\n     two_fam      6       0   1   1     24"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#working-with-recipes",
    "href": "002_exploratory_data_analysis.html#working-with-recipes",
    "title": "2  Exploratory Data Analysis",
    "section": "2.5 Working with Recipes",
    "text": "2.5 Working with Recipes\nRecipes are used for feature engineering in tidymodels using the recipes package\n\nUsed for transforming raw predictors into features used in our models\nDescribes all steps to make feature matrix. For example:\n\nTransforming factors into “dummy” features if needed\nLinear and non-linear transformations (e.g., log, box-cox)\nPolynomials and interactions (i.e., x1 * x1 or x1 * x2)\nMissing value imputations\n\nProper use of recipes is an important tool to prevent data leakage between train and either validation or test.\nRecipes use only information from the training set in all feature engineering\nConsider example of standardizing x1 for a feature in train vs. validation and test. Must use mean and sd from TRAIN to standardize x1 in train, validate, and test. VERY IMPORTANT.\n\nWe use recipes in a two step process - prep() and bake()\n\n“Prepping” a recipe involves calculating any statistics needed for the transformations that will be applied to engineer features (e.g., mean and standard deviation to normalize a numeric variable).\n\nPrepping is done with the prep() function.\nPrepping is always done only with training data. A “prepped” recipe does not derive any statistics from validation or test sets.\n\n“Baking” is the process of calculating the features\n\nBaking is done with the bake() function.\nWe used our prepped recipe when we bake.\nWhereas we only prep a recipe with training data, we can use a prepped recipe to bake features from any dataset (training, validation, or test).\n\n\nWe will work with recipes extensively when model building starting in unit 3\nFor now, we will only use the recipe to indicate roles as a gentle introduction.\nWe will expand on this recipe in unit 3\nRecipe syntax is very similar to generic tidyverse syntax (created by same group)\n\nActually a subset of tidyverse functions\nLess flexible/powerful but focused on our needs and easier to learn\nYou will eventually know both\n\n\nRecipes are used in Modeling scripts (which is a third type of script after cleaning and modeling EDA scripts)\n\nLets reload training again to pretend we are starting a new script\n\n\n data_trn &lt;- read_csv(file.path(path_data, \"ames_clean_class_trn.csv\"), \n                      col_types = cols()) |&gt; \n1  class_ames() |&gt;\n  glimpse()\n\n\n1\n\nRemember our function for classing!\n\n\n\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\nRecipes can be used to indicate the outcome and predictors that will be used in the model\n\nCan use . to indicate all predictors\n\nCurrently, our preferred method with some exceptions\nWe can exclude some predictors later by changing their role, removing them with a later recipe step (\\(step\\_rm()\\)), or specifying a more precise formula when we fit the model\nSee Roles in Recipes for more info\n\nCan use specific names of predictors along with \\(+\\) if only a few predictors\n\ne.g., sale_price ~ lot_area + year_built + overall_qual\n\nDo NOT indicate interactions here\n\nAll predictors are combined with +\nInteractions are specified by a later explicit feature engineering step\n\n\n\nrec &lt;- recipe(sale_price ~ ., data = data_trn)\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 9\n\nsummary(rec)\n\n# A tibble: 10 × 4\n   variable     type      role      source  \n   &lt;chr&gt;        &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 gr_liv_area  &lt;chr [2]&gt; predictor original\n 2 lot_area     &lt;chr [2]&gt; predictor original\n 3 year_built   &lt;chr [2]&gt; predictor original\n 4 overall_qual &lt;chr [3]&gt; predictor original\n 5 garage_cars  &lt;chr [2]&gt; predictor original\n 6 garage_qual  &lt;chr [3]&gt; predictor original\n 7 ms_zoning    &lt;chr [3]&gt; predictor original\n 8 lot_config   &lt;chr [3]&gt; predictor original\n 9 bldg_type    &lt;chr [3]&gt; predictor original\n10 sale_price   &lt;chr [2]&gt; outcome   original\n\n\n\n\n2.5.1 Prepping and Baking a Recipe\nLet’s make a feature matrix from the training set\nThere are two discrete (and important) steps * prep() * bake()\nFirst we prep the recipe using the training data\n\n1rec_prep &lt;- rec |&gt;\n2  prep(training = data_trn)\n\n\n1\n\nWe start by prepping our raw/original recipe (rec)\n\n2\n\nWe use the prep() function on on the training data. Recipes are ALWAYS prepped using training data. This makes sure that are recipes will always only use information from the training set when making features for any subsequent dataset.\n\n\n\n\n\nSecond, we bake the training data using this prepped recipe to get a feature matrix from it.\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(new_data = data_trn)\n\n\nFinally, we should generally at least glimpse (and typically do some more EDA) on our features to make sure our recipe is doing what we expect.\n\nfeat_trn |&gt; glimpse()\n\nRows: 1,465\nColumns: 10\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n\nfeat_trn |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n5\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n10\n\n\n5: 424, 6: 350, 7: 304, 8: 176\n\n\n\n\ngarage_qual\n\n\n0\n\n\n1\n\n\n5\n\n\nta: 1312, no_: 81, fa: 57, gd: 13\n\n\n\n\nms_zoning\n\n\n0\n\n\n1\n\n\n7\n\n\nres: 1157, res: 217, flo: 66, com: 13\n\n\n\n\nlot_config\n\n\n0\n\n\n1\n\n\n5\n\n\nins: 1095, cor: 248, cul: 81, fr2: 39\n\n\n\n\nbldg_type\n\n\n0\n\n\n1\n\n\n5\n\n\none: 1216, tow: 108, dup: 63, tow: 46\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\n\n\n\nWe can now use our features from training to train models, but that will take place in the next unit!\nWe could also use the prepped recipe to bake validation or test data to evaluate trained models. That too will happen in the next unit!"
  },
  {
    "objectID": "002_exploratory_data_analysis.html#discussion-topics",
    "href": "002_exploratory_data_analysis.html#discussion-topics",
    "title": "2  Exploratory Data Analysis",
    "section": "2.6 Discussion Topics",
    "text": "2.6 Discussion Topics\n\nHouse keeping\n\nUnit 2 solutions\nQuizzes\nCourse evals for extra credit (to quiz score)!\nUnit 3 homework\n\ntest set predictions\nfree lunch!\n\n\nReview\n\nGoal is to develop model that closely approximates DGP\nGoal is to evaluate (estimate) how close our model is to the DGP (how much error) with as little error as possible\nBias, overfitting/variance for any estimate (model and performance of model)\ncandidate model configurations\nfit, select, evaluate\ntraining, validation, test\n\nReview: 2.2.1 Stages of Data Analysis and Model Development\nBest practices (discuss quickly)\n\ncsv for data sharing, viewing, git (though be careful with data in github or other public repo!)\nvariable values saved as text when nominal and ordinal (self-documenting)\nCreate data dictionary - Documentation is critical!!\nsnake_case for variables and self-documenting names (systematic names too)\n\nReview: 2.3.1 Data Leakage Issues\n\nReview section in webbook\nCleaning EDA is done with full dataset (but univariate). Very limited (variable names, values, find errors)\nModeling EDA is only done with a training set (or even “eyeball” sample) - NEVER use validate or test set\nNever estimate anything with full data set (e.g., missing values, standardize, etc)\nUse recipes, prep (all estimation) with held in data than bake the appropriate set\nPut test aside\nYou work with validation but never explore with validation (will still catch leakage with test but will be mislead to be overly optimistic and spoil test)\n\nFunctions sidenote - fun_modeling.R on github\nReview: 2.4.2 Prepping and Baking a Recipe\n\nReview section in web book\nprep always with held in data, bake with held in & out data.\n\nEDA for modeling\n\nlimitless, just scratched the surface\nDiffers some based on dimenstionality of dataset\nLearning about DGP\n\nunderstand univariate distributions, frequencies\nbivariate relationships\ninteractions (3 or more variables)\npatterns in data\n\n\nExtra topics, time permitting\n\n8.1. Missing data\n* Exclude vs. Impute in training data.  Outcomes?\n* How to impute\n* Missing in validate or test (can't exclude?). Exclude cases with missing outcomes.\n8.2. Outliers * Drop or fix errors! * Goal is always to estimate DGP * Exclude * Retain * Bring to fence * Don’t exclude/change outcome in validate/test\n8.3. Issues with high dimensionality\n* Hard to do predictor level EDA\n* Common choices (normality transformations)\n* observed vs. predicted plots\n* Methods for automated variable selection (glmnet)\n8.4. Distributional Shape\n* Measurement issues (interval scale)\n* Implications for relationships with other variables\n* Solutions?\n8.5. Linearity vs. More Complex Relationships\n* Transformations\n* Choice of statistical algorithm\n* Do you need a linear model?\n8.6. Interactions\n* Domain expertise\n* Visual options for interactions\n* But what do do with high dimensional data?\n* Explanatory vs. prediction goals (algorithms that accommodate interactions)\n8.7. How to handle all of these decisions in the machine learning framework\n* Goal is to develop a model that most closely approximates the DGP\n* How does validation and test help this?\n* Preregistration?\n  * Pre-reg for performance metric, resampling method   \n  * Use of resampling for other decisions\n  * Use of resampling to find correct model to test explanatory goals\n8.8. Model Assumptions\n* Why do we make assumptions?\n  * Inference\n  * Flexibility wrt DGP\n\n\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. https://adv-r.hadley.nz/.\n\n\nWickham, Hadley, Çetinkaya-Rundel Mine, and Garrett Grolemund. 2023. R for Data Science: Visualize, Model, Transform, and Import Data. 2nd ed. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "003_regression.html#overview-of-unit",
    "href": "003_regression.html#overview-of-unit",
    "title": "3  Introduction to Regression Models",
    "section": "3.1 Overview of Unit",
    "text": "3.1 Overview of Unit\n\n3.1.1 Learning Objectives\n\nUse of root mean square error (RMSE) in training and validation sets for model performance evaluation\nThe General Linear Model as a machine learning model\n\nExtensions to categorical variables (Dummy coding features)\nExtensions to interactive and non-linear effects of features\n\nK Nearest Neighbor (KNN)\n\nHyperparameter \\(k\\)\nScaling predictors\nExtensions to categorical variables\n\n\n\n\n3.1.2 Readings\n\nJames et al. (2023) Chapter 3, pp 59 - 109\n\n\n\n3.1.3 Lecture Videos\n\nLecture 1: Overview; ~ 10 mins\nLecture 2: The Simple Linear Model; ~ 25 mins\nLecture 3: Extension to Multiple Predictors; ~ 15 mins\nLecture 4: Extension to Categorical Predictors; ~ 20 mins\nLecture 5: Extension to Interactions and Non-Linear Effects; ~ 12 mins\nLecture 6: Introduction to KNN; ~ 19 mins\nLecture 7: Distance and Scaling in KNN; ~8 mins\nLecture 8: KNN with Ames; ~ 12 mins\nDiscussion\n\n\n\n3.1.4 Application Assignment and Quiz\n\nclean data: train; validate; test\ndata dictionary\nglm rmd\nknn rmd\nsolution: lm; knn\n\nPost questions to the Slack channel for application assignments.\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, February 7th\n\nOur goal in this unit is to build a machine learning regression model that can accurately (we hope) predict the sale_price for future sales of houses (in Iowa? more generally?)\nTo begin this project we need to:\n\nSet up conflicts policies\nWe will hide this in future units\n\n\noptions(conflicts.policy = \"depends.ok\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\n\nℹ SHA-1 hash of file is \"175d942e14f108d74912bfb2593b77637328ecb1\"\n\ntidymodels_conflictRules()\n\n\nLoad the packages we will need\n\n\nlibrary(janitor, include.only = \"clean_names\")\nlibrary(cowplot, include.only = \"plot_grid\") # for plot_grid()\nlibrary(kableExtra, exclude = \"group_rows\") # exclude dplyr conflict\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\n\n\nsource additional class functions libraries\nWe will hide this in future units\n\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\n\nℹ SHA-1 hash of file is \"c045eee2655a18dc85e715b78182f176327358a7\"\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\nℹ SHA-1 hash of file is \"def6ce26ed7b2493931fde811adff9287ee8d874\"\n\n\n\nset display options\nWe will hide this in future units\n\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\n\n\nhandle paths\n\n\npath_data &lt;- \"./data\"\n\n\n\nSet up function to class ames data (copied with one improvement from last unit)\n\n\nclass_ames &lt;- function(df){\n  df |&gt;\n    mutate(across(where(is.character), factor)) |&gt; \n    mutate(overall_qual = factor(overall_qual), \n           overall_qual = fct_relevel(overall_qual, as.character(1:10)), \n1           garage_qual = suppressWarnings(fct_relevel(garage_qual,\n                                                      c(\"no_garage\", \"po\", \"fa\", \n                                                    \"ta\", \"gd\", \"ex\"))))\n}\n\n\n1\n\nWarnings should be considered errors until investigated. Once investigated, they can be ignored. To explicitly ignore, use suppressWarnings()\n\n\n\n\n\n\nOpen the cleaned training set\n\n\ndata_trn &lt;- \n  read_csv(here::here(path_data, \"ames_clean_class_trn.csv\"), \n           col_types = cols()) |&gt;  \n  class_ames() |&gt; \n  glimpse()\n\nRows: 1,465\nColumns: 10\n$ sale_price   &lt;dbl&gt; 105000, 126000, 115000, 120000, 99500, 112000, 122000, 12…\n$ gr_liv_area  &lt;dbl&gt; 896, 882, 864, 836, 918, 1902, 900, 1225, 1728, 858, 1306…\n$ lot_area     &lt;dbl&gt; 11622, 8400, 10500, 2280, 7892, 8930, 9819, 9320, 13260, …\n$ year_built   &lt;dbl&gt; 1961, 1970, 1971, 1975, 1979, 1978, 1967, 1959, 1962, 195…\n$ overall_qual &lt;fct&gt; 5, 4, 4, 7, 6, 6, 5, 4, 5, 5, 3, 5, 4, 5, 3, 5, 2, 6, 5, …\n$ garage_cars  &lt;dbl&gt; 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 1, …\n$ garage_qual  &lt;fct&gt; ta, ta, no_garage, ta, ta, ta, ta, no_garage, no_garage, …\n$ ms_zoning    &lt;fct&gt; res_high, res_low, res_low, res_low, res_low, res_med, re…\n$ lot_config   &lt;fct&gt; inside, corner, fr2, fr2, inside, inside, inside, inside,…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, town_inside, town_end, duplex,…\n\n\n\n\nOpen the cleaned validation set\n\n\ndata_val &lt;- read_csv(here::here(path_data, \"ames_clean_class_val.csv\"),\n                     col_types = cols()) |&gt; \n  class_ames() |&gt; \n  glimpse()\n\nRows: 490\nColumns: 10\n$ sale_price   &lt;dbl&gt; 215000, 189900, 189000, 171500, 212000, 164000, 394432, 1…\n$ gr_liv_area  &lt;dbl&gt; 1656, 1629, 1804, 1341, 1502, 1752, 1856, 1004, 1092, 106…\n$ lot_area     &lt;dbl&gt; 31770, 13830, 7500, 10176, 6820, 12134, 11394, 11241, 168…\n$ year_built   &lt;dbl&gt; 1960, 1997, 1999, 1990, 1985, 1988, 2010, 1970, 1971, 197…\n$ overall_qual &lt;fct&gt; 6, 5, 7, 7, 8, 8, 9, 6, 5, 6, 7, 9, 8, 8, 7, 8, 6, 5, 5, …\n$ garage_cars  &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 3, 2, 3, 1, 1, 2, …\n$ garage_qual  &lt;fct&gt; ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, ta, t…\n$ ms_zoning    &lt;fct&gt; res_low, res_low, res_low, res_low, res_low, res_low, res…\n$ lot_config   &lt;fct&gt; corner, inside, inside, inside, corner, inside, corner, c…\n$ bldg_type    &lt;fct&gt; one_fam, one_fam, one_fam, one_fam, town_end, one_fam, on…\n\n\nNOTE: Remember, I have held back an additional test set that we will use only once to evaluate the final model that we each develop in this unit.\n\nWe will also make a dataframe to track validation error across the models we fit\n\nerror_val &lt;- tibble(model = character(), rmse_val = numeric()) |&gt; \n  glimpse()\n\nRows: 0\nColumns: 2\n$ model    &lt;chr&gt; \n$ rmse_val &lt;dbl&gt; \n\n\n\nWe will fit regression models with various model configurations.\nThese configurations will differ with respect to statistical algorithm:\n\nA General Linear Model (lm) - a parametric approach\nK Nearest Neighbor (KNN) - a non-parametric approach\n\nThese configurations will differ with respect to the features\n\nSingle feature (i.e., simple regression)\nVarious sets of multiple features that vary by:\n\nRaw predictors used\nTransformations applied to those predictors as part of feature engineering\nInclusion (or not) of interactions among features\n\nThe KNN model configurations will also differ with respect to its hyperparameter- \\(k\\)\n\n\nTo build models that will work well in new data (e.g., the data that I have held back from you so far):\n\nWe have split the remaining data into a training and validation set for our own use during model building\nWe will fit models in train\nWe will evaluate them in validation\n\nRemember that we:\n\nUsed a 75/25 stratified (on sale_price) split of the data at the end of cleaning EDA\nAre only using a subset of the available predictors. The same ones I used for the EDA unit\n\nYou will work with all of my predictors and all the predictors you used for your EDA when you do the application assignment for this unit\n\nPause for a moment to answer this question: [Why do we need independent validation data to select the best model configuration? In other words, why cant we just fit and evaluate all of the models in our one training set?].{red} 1\n\nLet’s take a quick look at the available raw predictors in the training set\n\ndata_trn |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndata_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n5\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n10\n\n\n5: 424, 6: 350, 7: 304, 8: 176\n\n\n\n\ngarage_qual\n\n\n0\n\n\n1\n\n\n5\n\n\nta: 1312, no_: 81, fa: 57, gd: 13\n\n\n\n\nms_zoning\n\n\n0\n\n\n1\n\n\n7\n\n\nres: 1157, res: 217, flo: 66, com: 13\n\n\n\n\nlot_config\n\n\n0\n\n\n1\n\n\n5\n\n\nins: 1095, cor: 248, cul: 81, fr2: 39\n\n\n\n\nbldg_type\n\n\n0\n\n\n1\n\n\n5\n\n\none: 1216, tow: 108, dup: 63, tow: 46\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\n\n\n\nRemember from our modeling EDA that we have some issues to address as part of our feature engineering:\n\nMissing values\nPossible transformation of sale_price\nPossible transformation of other numeric predictors\nWe will need to use some feature engineering techniques to handle categorical variables\nWe may need to consider interactions among features\n\nAll of this will be accomplished with a recipe\nBut first, let’s consider our first statistical algorithm"
  },
  {
    "objectID": "003_regression.html#the-simple-general-linear-model-lm",
    "href": "003_regression.html#the-simple-general-linear-model-lm",
    "title": "3  Introduction to Regression Models",
    "section": "3.2 The Simple (General) Linear Model (LM)",
    "text": "3.2 The Simple (General) Linear Model (LM)\nWe will start with a review of the use of the simple (one feature) linear model (LM) as a machine learning model because you should be very familiar with this statistical model at this point\n\\(Y = \\beta_0 + \\beta_1*X_1 + \\epsilon\\)\nApplied to our regression problem, we might fit a model such as:\n\\(sale\\_price = \\beta_0 + \\beta_1*gr\\_liv\\_area + \\epsilon\\)\nThe GLM is a parametric model. We need to estimate two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), using our training dataset.\nYou already know how to do this using lm() in base R. However, we will use the tidymodels modeling approach.\n\nWe use tidymodels because:\n\nIt provides a consistent interface to many (and growing numbers) of statistical algorithms\nIt provides very strong and easy feature engineering routines (e.g., missing data, scaling, transformations, near-zero variance, collinearity)\nIt simplifies model performance evaluation using resampling approaches (that you don’t know yet!)\nIt supports numerous performance metrics\nIt is (mostly) tightly integrated within the tidyverse\nIt is under active development and support\nYou can see documentation for all of the packages at the tidymodels website. It is worth a quick review now to get a sense of what is available\n\n\nTo fit a model with a specific configuration, we need to:\n\nSet up a feature engineering recipe\nUse the recipe to make a feature matrix\n\nprep() it with training data\nbake() it with data you want to use to calculate feature matrix\n\nSelect and define the statistical algorithm\nFit the algorithm in the feature matrix\n\nThese steps are accomplished with functions from the recipes and parsnip packages.\nWe will start with a simple model configuration\n\nGeneral linear model\nOne feature (raw gr_liv_area)\nFit in training data\n\n\nSet up a VERY SIMPLE feature engineering recipe\n\nInclude outcome on the left size of ~\nInclude raw predictors (not yet features) on the right side of ~.\n\nIndicate the training data\n\n\nrec &lt;- \n  recipe(sale_price ~ gr_liv_area, data = data_trn)\n\n\nWe can see a summary of it to verify it is doing what you expect\n\n\nsummary(rec)\n\n# A tibble: 2 × 4\n  variable    type      role      source  \n  &lt;chr&gt;       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 gr_liv_area &lt;chr [2]&gt; predictor original\n2 sale_price  &lt;chr [2]&gt; outcome   original\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\nWe can then prep the recipe and bake the data to make our feature matrix from the training dataset\n\nAgain, remember we always prep a recipe with training data but use the prepped recipe to bake any data\nIn this instance we will prep with data_trn and then bake data_trn\n\n\nrec_prep &lt;- rec |&gt; \n  prep(training = data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(new_data = data_trn)\n\nYou should always review the feature matrix to make sure it looks as you expect\n\nincludes outcome (sale_price)\nincludes expected feature (gr_liv_area)\nSample size is as expected\nNo missing data\n\n\nfeat_trn |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n2\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n2\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\n\n\n\n\nNow let’s consider the statistical algorithm\n\ntidymodels breaks this apart into two pieces for clarity\nFirst, you specify the broad category of algorithm\n\ne.g., linear_reg(), nearest_neighbor(), logistic_reg()\n\nThen you select a function from a specific R package (or base R) that will implement the algorithm\n\ntidymodels calls this setting the engine\ne.g., lm, kknn, glm, glmnet\n\n\n\nYou can see the available engines (and modes: regression vs. classification) for the broad classes of algorithms\nWe will work with many of these algorithms later in the course\n\nshow_engines(\"linear_reg\")\n\n# A tibble: 7 × 2\n  engine mode      \n  &lt;chr&gt;  &lt;chr&gt;     \n1 lm     regression\n2 glm    regression\n3 glmnet regression\n4 stan   regression\n5 spark  regression\n6 keras  regression\n7 brulee regression\n\nshow_engines(\"nearest_neighbor\")\n\n# A tibble: 2 × 2\n  engine mode          \n  &lt;chr&gt;  &lt;chr&gt;         \n1 kknn   classification\n2 kknn   regression    \n\nshow_engines(\"logistic_reg\")\n\n# A tibble: 7 × 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification\n\nshow_engines(\"decision_tree\")\n\n# A tibble: 5 × 2\n  engine mode          \n  &lt;chr&gt;  &lt;chr&gt;         \n1 rpart  classification\n2 rpart  regression    \n3 C5.0   classification\n4 spark  classification\n5 spark  regression    \n\nshow_engines(\"rand_forest\")\n\n# A tibble: 6 × 2\n  engine       mode          \n  &lt;chr&gt;        &lt;chr&gt;         \n1 ranger       classification\n2 ranger       regression    \n3 randomForest classification\n4 randomForest regression    \n5 spark        classification\n6 spark        regression    \n\nshow_engines(\"mlp\")\n\n# A tibble: 6 × 2\n  engine mode          \n  &lt;chr&gt;  &lt;chr&gt;         \n1 keras  classification\n2 keras  regression    \n3 nnet   classification\n4 nnet   regression    \n5 brulee classification\n6 brulee regression    \n\n\nThere are some additional engines we will use in the course in the discrim package\n\nlibrary(discrim, exclude = \"smoothness\") \nshow_engines(\"discrim_linear\") \n\n# A tibble: 4 × 2\n  engine        mode          \n  &lt;chr&gt;         &lt;chr&gt;         \n1 MASS          classification\n2 mda           classification\n3 sda           classification\n4 sparsediscrim classification\n\nshow_engines(\"discrim_regularized\") \n\n# A tibble: 1 × 2\n  engine mode          \n  &lt;chr&gt;  &lt;chr&gt;         \n1 klaR   classification\n\nshow_engines(\"naive_Bayes\")\n\n# A tibble: 2 × 2\n  engine     mode          \n  &lt;chr&gt;      &lt;chr&gt;         \n1 klaR       classification\n2 naivebayes classification\n\n\n\nYou can also better understand how the engine will be called using translate()\nNot useful here but will be with more complicated algorithms\n\nlinear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n\n\nLet’s combine our feature matrix with an algorithm to fit a model in our training set features using only raw gr_liv_area as a feature\nNote the specification of\n\nThe category of algorithm\nThe engine (no need to set mode of engine b/c lm are only for the regression mode)\nThe use of the . to indicate all features in the matrix.\n\nno needed here because there is only one feature: gr_liv_area\nwill be useful when we have many features in the matrix\n\nWe use the the feature matrix (rather than raw data) from the training set to fit the model.\n\n\nfit_lm_1 &lt;- \n1  linear_reg() |&gt;\n2  set_engine(\"lm\") |&gt;\n3  fit(sale_price ~ ., data = feat_trn)\n\n\n1\n\ncategory of algorithm\n\n2\n\nengine\n\n3\n\nuse of . for all features and use of feature matrix from training set\n\n\n\n\n\nWe can get the parameter estimates, standard errors, and statistical tests for each \\(\\beta\\) = 0 for this model using tidy() from the broom package (loaded as part of the tidyverse)\n\nfit_lm_1 |&gt;  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   16561.   4537.        3.65 2.72e-  4\n2 gr_liv_area     109.      2.85     38.2  4.78e-222\n\n\n\nThere are a variety of ways to pull out the estimates for each feature (and intercept)\nOption 1: Pull all estimates from the tidy object\n\nfit_lm_1 |&gt; \n  tidy() |&gt; \n  pull(estimate)\n\n[1] 16560.9991   108.9268\n\n\nOption 2: Extract a single estimate using $ and row number. Be careful that order of features won’t change! This assumes the feature coefficient for the relevant feature is always the second coefficient.\n\ntidy(fit_lm_1)$estimate[[2]]\n\n[1] 108.9268\n\n\nOption 3: Extract a single estimate using $ and match to term. Safer\n\nfit_lm_1 |&gt; \n  tidy() |&gt; \n  filter(term == \"gr_liv_area\") |&gt; \n  pull(estimate)\n\n[1] 108.9268\n\n\n\nBest to write a function if we plan to do this a lot. This function in included in fun_ml.R\n\nget_estimate &lt;- function(the_fit, the_term){\n  the_fit |&gt; \n    tidy() |&gt; \n    filter(term == the_term) |&gt; \n    pull(estimate)\n}\n\nNow we can use this function whenever we need a coefficient\n\nget_estimate(fit_lm_1, \"gr_liv_area\")\n\n[1] 108.9268\n\nget_estimate(fit_lm_1, \"(Intercept)\")\n\n[1] 16561\n\n\nRegardless of the method, we now have a simple parametric model for sale_price\n\\(\\hat{sale\\_price} = 1.6561\\times 10^{4} + 108.9 * gr\\_liv\\_area\\)\n\nWe can get the predicted values for sale_price (i.e., \\(\\hat{sale\\_price}\\)) in our validation set using predict()\nHowever, we first need to make a feature matrix from our validation set\n\nWe the recipe that we previously prepped with our training set\nWe bake the training data (data_val) to create features for the training set (feat_val)\n\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(new_data = data_val)\n\n\nAs always, we should skim these new features\n\nSample size matches what we expect for validation set\nNo missing data\nIncludes expected outcome and features\n\n\nfeat_val |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_val\n\n\n\n\nNumber of rows\n\n\n490\n\n\n\n\nNumber of columns\n\n\n2\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n2\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1493.0\n\n\n483.78\n\n\n480\n\n\n1143.5\n\n\n1436\n\n\n1729.5\n\n\n3608\n\n\n0.92\n\n\n1.16\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n178512.8\n\n\n75493.59\n\n\n35311\n\n\n129125.0\n\n\n160000\n\n\n213000.0\n\n\n556581\n\n\n1.42\n\n\n2.97\n\n\n\n\n\n\n\n\nNow we can get predictions using our model with validation features\npredict() returns a dataframe with one column named .pred and one row for every observation in dataframe (e.g., validation feature set)\n\npredict(fit_lm_1, feat_val)\n\n# A tibble: 490 × 1\n     .pred\n     &lt;dbl&gt;\n 1 196944.\n 2 194003.\n 3 213065.\n 4 162632.\n 5 180169.\n 6 207401.\n 7 218729.\n 8 125923.\n 9 135509.\n10 133004.\n# ℹ 480 more rows\n\n\n\nWe can visualize how well this model performs in the validation set by plotting predicted sale_price (\\(\\hat{sale\\_price}\\)) vs. sale_price (ground truth in machine learning terminology) for these data\nWe might do this a lot so let’s write a function (We have included this function in fun_ml.R)\n\nplot_truth &lt;- function(truth, estimate) {\n  ggplot(mapping = aes(x = truth, y = estimate)) + \n    geom_abline(lty = 2) + \n    geom_point(alpha = 0.5) + \n    labs(y = \"predicted outcome\", x = \"outcome\") +\n    coord_obs_pred()   # scale axes uniformly\n}\n\n\nNow make the plot\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_1, feat_val)$.pred)\n\n\n\n\nPerfect performance would have all the points right on the dotted line (same value for actual and predicted outcome)\n\nOur model doesn’t do that well yet. Not surprising\nPattern also has some indication of fanning of residuals with higher outcome scores that suggests need for a power transformation of outcome (e.g., log)\nThis is consistent with our earlier modeling EDA\nPerhaps not that bad here b/c both sale_price and gr_liv_area were positively skewed\nWe will need consider this eventually\n\n\nWe can quantify model performance by selecting a performance metric\n\nThe yardstick package within the tidymodels framework supports calculation of many performance metrics for regression and classification models\nSee the list of all currently available metrics\n\nRoot mean square error (RMSE) is a common performance metric for regression models\n\nYou focused on a related metric, sum of squared error (SSE), in PSY 610/710\nRMSE simply divides SSE by N (to get mean squared error; MSE) and then takes the square root to return the metric to the original units for the outcome variable\nIt is easy to calculate using rmse_vec() from the yardstick package\n\n\nrmse_vec(truth = feat_val$sale_price, \n         estimate = predict(fit_lm_1, feat_val)$.pred)\n\n[1] 51375.08\n\n\n\nLet’s record how well this model performed in validation so we can compare it to subsequent models\n\nerror_val &lt;- bind_rows(error_val,\n                       tibble(model = \"simple linear model\",\n                              rmse_val = rmse_vec(truth = feat_val$sale_price,\n                                                  estimate = predict(fit_lm_1, feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 1 × 2\n  model               rmse_val\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 simple linear model   51375.\n\n\n\nFor explanatory purposes, we might want to visualize the relationship between a raw predictor and the outcome (in addition to examining the parameter estimates and the associated statistical tests)\n\nHere is a plot of \\(\\hat{sale\\_price}\\) by gr_liv_area superimposed over a scatterplot of the raw data from the validation set\n\n\nfeat_val |&gt; \n  ggplot(aes(x = gr_liv_area)) +\n    geom_point(aes(y = sale_price), color = \"gray\") +\n    geom_line(aes(y = predict(fit_lm_1, data_val)$.pred), \n              linewidth = 1.25, color = \"blue\") + \n    ggtitle(\"Validation Set\")\n\n\n\n\n\nAs expected, there is a moderately strong positive relationship between gr_liv_area and sale_price.\n\nWe can also again see the heteroscadasticity in the errors that might be corrected by a power transformation of sale_price (and possibly gr_liv_area)"
  },
  {
    "objectID": "003_regression.html#extension-of-lm-to-multiple-predictors",
    "href": "003_regression.html#extension-of-lm-to-multiple-predictors",
    "title": "3  Introduction to Regression Models",
    "section": "3.3 Extension of LM to Multiple Predictors",
    "text": "3.3 Extension of LM to Multiple Predictors\nWe can improve model performance by moving from simple linear model to a linear model with multiple features derived from multiple predictors\nWe have many other numeric variables available to use, even in this pared down version of the dataset.\n\ndata_trn |&gt;  skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndata_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n10\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n5\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n10\n\n\n5: 424, 6: 350, 7: 304, 8: 176\n\n\n\n\ngarage_qual\n\n\n0\n\n\n1\n\n\n5\n\n\nta: 1312, no_: 81, fa: 57, gd: 13\n\n\n\n\nms_zoning\n\n\n0\n\n\n1\n\n\n7\n\n\nres: 1157, res: 217, flo: 66, com: 13\n\n\n\n\nlot_config\n\n\n0\n\n\n1\n\n\n5\n\n\nins: 1095, cor: 248, cul: 81, fr2: 39\n\n\n\n\nbldg_type\n\n\n0\n\n\n1\n\n\n5\n\n\none: 1216, tow: 108, dup: 63, tow: 46\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n1\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\n\n\n\n\nLet’s expand our model to also include lot_area, year_built, and garage_cars\nAgain, we need:\n\nA feature engineering recipe\nTraining (and eventually validation) feature matrices\nAn algorithm to fit in training feature matrix\n\n\nWith the addition of new predictors, we now have a feature engineering task\n\nWe have missing data on garage_cars in the training set\nWe need to decide how we will handle it\n\nA simple solution is to do median imputation - substitute the median of the non-missing scores for any missing score.\n\nThis is fast and easy to understand\nIt works OK (but there are certainly better options that we will consider later in the course)\n\nsee other options in Step Functions - Imputation section on tidymodels website\n\nThere is only one missing value so it likely doesn’t matter much anyway\n\nLet’s add this to our recipe. All of the defaults are appropriate but you should see ?step_impute_median() to review them\n\nrec &lt;- \n1  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars,\n         data = data_trn) |&gt; \n  step_impute_median(garage_cars)\n\n\n1\n\nNotice we now list four predictors for our recipe using + between them\n\n\n\n\n\nWe can review this recipe to see what it will do\n\nsummary(rec)\n\n# A tibble: 5 × 4\n  variable    type      role      source  \n  &lt;chr&gt;       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 gr_liv_area &lt;chr [2]&gt; predictor original\n2 lot_area    &lt;chr [2]&gt; predictor original\n3 year_built  &lt;chr [2]&gt; predictor original\n4 garage_cars &lt;chr [2]&gt; predictor original\n5 sale_price  &lt;chr [2]&gt; outcome   original\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Operations \n\n\n• Median imputation for: garage_cars\n\n\n\nNow we need to\n\nPrep the recipe in training set\n\n\nrec_prep &lt;- rec |&gt; \n1  prep(data_trn)\n\n\n1\n\nWe will no longer list training = to save some typing\n\n\n\n\n\nBake (and skim) the training set to make training features\n\n\nfeat_trn &lt;- rec_prep |&gt; \n1  bake(data_trn)\n\nfeat_trn |&gt; skim_all()\n\n\n1\n\nWe will no longer list new_data = to save some typing\n\n\n\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n5\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n0\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\n\n\n\n\nBake (and skim) the validation set to make validation features\n\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\nNow let’s combine our algorithm and training features to fit this model configuration with 4 features\n\nfit_lm_4 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n1  fit(sale_price ~ ., data = feat_trn)\n\n\n1\n\nthe . is a bit more useful now\n\n\n\n\n\nThis yields these parameter estimates (which as we know from 610/710 were selected to minimize SSE in the training set):\n\nfit_lm_4 |&gt; tidy()\n\n# A tibble: 5 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -1665041.    89370.       -18.6  1.14e- 69\n2 gr_liv_area       76.8       2.62      29.3  3.56e-149\n3 lot_area           0.514     0.146      3.51 4.60e-  4\n4 year_built       854.       46.2       18.5  7.66e- 69\n5 garage_cars    22901.     1964.        11.7  4.09e- 30\n\n\n\nHere is our parametric model\n\\(\\hat{sale\\_price} = -1.6650409\\times 10^{6} + 76.8 * gr\\_liv\\_area + 0.5 * lot\\_area + 854.3 * year\\_built + 2.29008\\times 10^{4} * garage\\_cars\\)\nCompared with our previous simple regression model:\n\\(\\hat{sale\\_price} = 1.6561\\times 10^{4} + 108.9 * gr\\_liv\\_area\\)\n\nOf course, these four features are correlated both with sale_price but also with each other\nLet’s look at correlations in the training set.\n\nfeat_trn |&gt; \n  cor() |&gt; \n  corrplot::corrplot.mixed()\n\n\n\n\n[What are the implications of the correlations among many of these predictors?].{red} 2\n\nHow well does this more complex model perform in validation? Let’s compare the previous and current visualizations of sale_price vs. \\(\\hat{sale\\_price}\\)\n\nLooks like the errors are smaller (closer to the diagonal line that would represent prefect prediction)\nClear signs of non-linearity are now present as well. Time for more Modeling EDA!!\nAnd two outliers perhaps?\n\n\nplot_1 &lt;- plot_truth(truth = feat_val$sale_price, \n                     estimate = predict(fit_lm_1, feat_val)$.pred)\n\nplot_4 &lt;- plot_truth(truth = feat_val$sale_price, \n                     estimate = predict(fit_lm_4, feat_val)$.pred)\n\nplot_grid(plot_1, plot_4, \n          labels = list(\"1 feature\", \"4 features\"), hjust = -1.5)\n\n\n\n\nCoding sidebar: Notice the use of plot_grid() from the cowplot package to make side by side plots. This also required returning the individual plots as objects (just assign to a object name, e.g., plot_1)\n\nLet’s compare model performance for the two models using RMSE in the validation set\n\nThe one feature simple linear model\n\n\nrmse_vec(feat_val$sale_price, \n         predict(fit_lm_1, feat_val)$.pred)\n\n[1] 51375.08\n\n\n\nThe four feature linear model. A clear improvement!\n\n\nrmse_vec(feat_val$sale_price, \n         predict(fit_lm_4, feat_val)$.pred)\n\n[1] 39903.25\n\n\nLet’s bind it to our results table\n\nerror_val &lt;- \n  bind_rows(error_val, \n            tibble(model = \"4 feature linear model\", \n                   rmse_val = rmse_vec(feat_val$sale_price, predict(fit_lm_4, feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 2 × 2\n  model                  rmse_val\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 simple linear model      51375.\n2 4 feature linear model   39903.\n\n\n\nGiven the non-linearity suggested by the truth vs. estimate plots, we might wonder if we could improve the fit if we transformed our features to be closer to normal\nThere are a number of recipe functions that do transformations (see Step Functions - Individual Transformations)\nWe will apply step_YeoJohnson(), which is similar to a Box-Cox transformation but can be more broadly applied because the scores don’t need to be strictly positive\nLet’s do it all again!\n\nDefine the feature engineering recipe\n\n\nrec &lt;- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars, \n         data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_YeoJohnson(lot_area, gr_liv_area, year_built, garage_cars)\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Operations \n\n\n• Median imputation for: garage_cars\n\n\n• Yeo-Johnson transformation on: lot_area, gr_liv_area, year_built, ...\n\n\n\nPrep the recipe with training set\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nUse prepped recipe to bake the training set into features\n\nNotice the features are now less skewed\n\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_trn |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n5\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n5.22\n\n\n0.16\n\n\n4.60\n\n\n5.11\n\n\n5.23\n\n\n5.33\n\n\n5.86\n\n\n0.00\n\n\n0.12\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n14.10\n\n\n1.14\n\n\n10.32\n\n\n13.69\n\n\n14.20\n\n\n14.64\n\n\n21.65\n\n\n0.08\n\n\n5.46\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880.00\n\n\n1953.00\n\n\n1972.00\n\n\n2000.00\n\n\n2010.00\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n0\n\n\n1\n\n\n2.12\n\n\n0.98\n\n\n0.00\n\n\n1.11\n\n\n2.37\n\n\n2.37\n\n\n5.23\n\n\n-0.03\n\n\n0.04\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789.00\n\n\n129500.00\n\n\n160000.00\n\n\n213500.00\n\n\n745000.00\n\n\n1.64\n\n\n4.60\n\n\n\n\n\n\n\nUse same prepped recipe to bake the validation set into features\n\nAgain, features are less skewed\n\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\nfeat_val |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_val\n\n\n\n\nNumber of rows\n\n\n490\n\n\n\n\nNumber of columns\n\n\n5\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n5\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n5.22\n\n\n0.16\n\n\n4.65\n\n\n5.11\n\n\n5.23\n\n\n5.32\n\n\n5.66\n\n\n-0.17\n\n\n0.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n14.14\n\n\n1.17\n\n\n10.57\n\n\n13.69\n\n\n14.24\n\n\n14.72\n\n\n22.44\n\n\n0.11\n\n\n6.12\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.08\n\n\n30.96\n\n\n1875.00\n\n\n1954.00\n\n\n1975.00\n\n\n2000.00\n\n\n2010.00\n\n\n-0.66\n\n\n-0.41\n\n\n\n\ngarage_cars\n\n\n0\n\n\n1\n\n\n2.06\n\n\n0.96\n\n\n0.00\n\n\n1.11\n\n\n2.37\n\n\n2.37\n\n\n5.23\n\n\n0.01\n\n\n0.24\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n178512.82\n\n\n75493.59\n\n\n35311.00\n\n\n129125.00\n\n\n160000.00\n\n\n213000.00\n\n\n556581.00\n\n\n1.42\n\n\n2.97\n\n\n\n\n\n\n\n\nFit model\n\n\nfit_lm_4yj &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\nView truth vs. estimate plot\n\n\nplot_truth(truth = feat_val$sale_price,\n           estimate = predict(fit_lm_4yj, feat_val)$.pred)\n\n\n\n\n\nand look at the error\n\n\nerror_val &lt;- bind_rows(error_val, \n                        tibble(model = \"4 feature linear model with YJ\", \n                               rmse_val = rmse_vec(feat_val$sale_price, \n                                                   predict(fit_lm_4yj, feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 3 × 2\n  model                          rmse_val\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 simple linear model              51375.\n2 4 feature linear model           39903.\n3 4 feature linear model with YJ   41660.\n\n\nThat didn’t help at all.\nWe may need to consider\n\na transformation of sale_price (We will leave that to you for the application assignment!)\nor a different algorithm that can handle non-linear relationships better"
  },
  {
    "objectID": "003_regression.html#extension-to-categorical-predictors",
    "href": "003_regression.html#extension-to-categorical-predictors",
    "title": "3  Introduction to Regression Models",
    "section": "3.4 Extension to Categorical Predictors",
    "text": "3.4 Extension to Categorical Predictors\nMany important predictors in our models may be categorical (ordered and unordered)\n\nSome statistical algorithms (e.g., random forest) can accept even unordered categorical predictors as features without any further feature engineering\nBut many cannot. Linear models cannot.\nThe type of feature engineering may differ for unordered vs. ordered categorical predictors\nFor unordered categorical predictors:\n\nWe need to learn a common approach to transform them to numeric features - dummy coding. We will learn the concept in general AND how to accomplish within a feature engineering recipe.\n\nFor ordered categorical predictors:\n\nWe can treat them like numeric predictors\nWe can treat them like unordered categorical predictors\n\nSee article on Categorical Predictors on the tidymodels website for more details\n\n\n3.4.1 Dummy Coding\nFor many algorithms, we will need to use feature engineering to convert a categorical predictor to numeric features. One common technique is to use dummy coding. When dummy coding a predictor, we transform the original categorical predictor with m levels into m-1 dummy coded features.\nTo better understand how and why we do this, lets consider a version of ms_zoning in the Ames dataset.\n\ndata_trn |&gt; \n  pull(ms_zoning) |&gt; \n  table()\n\n\n    agri   commer    float    indus res_high  res_low  res_med \n       2       13       66        1        9     1157      217 \n\n\nWe will recode ms_zoning to have only 3 levels to make our example simple (though dummy codes can be used for predictors with any number of levels)\n\ndata_dummy &lt;- data_trn |&gt; \n1  select(sale_price, ms_zoning)  |&gt;\n2  mutate(ms_zoning3 = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n3                                 \"floating\" = \"float\")) |&gt;\n4  select(-ms_zoning)\n\n\n1\n\nMake a df (dataframe) with only sale_price and ms_zoning\n\n2\n\nfct_collapse() from the forcats package is our preferred way to collapse levels of a factor. See fct_recode() for more generic recoding of levels.\n\n3\n\nWe could have left this line out and float would have stayed as a level named float\n\n4\n\nRemove original ms_zoning predictor\n\n\n\n\nTake a look at the new predictor\n\ndata_dummy |&gt; \n  pull(ms_zoning3) |&gt; \n  table() \n\n\n commercial    floating residential \n         16          66        1383 \n\n\n\nWe need to convert this three-level, unordered categorical predictor (currently classed as a factor) to numeric features to allow us to use it in statistical algorithms like the general linear model.\n[Why can’t we simply recode each level with a different consecutive value (e.g., commercial = 1, floating =2 , residential = 3)?].{red} 3\n\nImagine fitting a straight line to predict sale_price from ms_zoning3 using these three different ways to arbitrarily assign numbers to levels.\n\ndata_dummy |&gt; \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 1,\n                                ms_zoning3 == \"commercial\" ~ 2,\n                                ms_zoning3 == \"floating\" ~ 3)) |&gt; \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n\n\n\ndata_dummy |&gt; \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 2,\n                                ms_zoning3 == \"commercial\" ~ 1,\n                                ms_zoning3 == \"floating\" ~ 3)) |&gt; \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n\n\n\ndata_dummy |&gt; \n  mutate(ms_zoning3 = case_when(ms_zoning3 == \"residential\" ~ 3,\n                                ms_zoning3 == \"commercial\" ~ 1,\n                                ms_zoning3 == \"floating\" ~ 2)) |&gt; \n  ggplot(aes(x = ms_zoning3, y = sale_price)) +\n    geom_bar(stat=\"summary\", fun = \"mean\")\n\n\n\n\nDummy coding resolves this issue.\n\nWhen using dummy codes, we transform (i.e., feature engineer) our original m-level categorical predictor to m-1 dummy features.\n\nEach of these m-1 features represents a contrast between a specific level of the categorical variable and a reference level\nThe full set of m-1 features represents the overall effect of the categorical predictor variable.\nWe assign values of 0 or 1 to each observation on each feature in a meaningful pattern (see below)\n\n\nFor example, with our three-level predictor: ms_zoning3\n\nWe need 2 features (f1, f2) to represent this 3-level categorical predictor\nFeature 1 is coded 1 for residential and 0 for all other levels\nFeature 2 is coded 1 for floating and 0 for all other levels\n\nHere is this coding scheme displayed in a table\n\n\n# A tibble: 3 × 3\n  ms_zoning3     f1    f2\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 commercial      0     0\n2 residential     1     0\n3 floating        0     1\n\n\n\nWith this coding:\n\nCommercial properties are coded 0 for both f1 and f2.\n\nThis means that commercial properties will become the reference level against which both residential and floating village are compared.\nBecause we are focused on prediction, the choice of reference level is mostly arbitrary. For explanatory goals, you might consider which level is best suited to be the reference.\n3.5 There is much deeper coverage of dummy and other contrast coding in 610/710\n\nWe can add these two features manually to the data frame and view a handful of observations to make this coding scheme more concrete\n\n\n# A tibble: 8 × 4\n  sale_price ms_zoning3     f1    f2\n       &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1     105000 residential     1     0\n2     126000 residential     1     0\n3      13100 commercial      0     0\n4     115000 residential     1     0\n5     149500 floating        0     1\n6      40000 commercial      0     0\n7     120000 residential     1     0\n8     151000 floating        0     1\n\n\n\nIf we now fit a model where we predict sale_price from these two dummy coded features, each feature would represent the contrast of the mean sale_price for the level coded 1 vs. the mean sale_price for the level that is coded 0 for all features (i.e., commercial)\n\nf1 is the contrast of mean sale_price for residential vs. commercial\nf2 is the contrast of mean sale_price for floating vs. commercial\nThe combined effect of these two features represents the overall effect of ms_zoning3 on sale_price\n\n\nLets do this quickly in base r using lm() as you have done previously in 610.\n\nm &lt;- lm(sale_price ~ f1 + f2, data = data_dummy) \n\nm |&gt; summary()\n\n\nCall:\nlm(formula = sale_price ~ f1 + f2, data = data_dummy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-166952  -50241  -20241   31254  565259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    81523      19409   4.200 2.83e-05 ***\nf1             98219      19521   5.031 5.47e-07 ***\nf2            143223      21634   6.620 5.03e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 77640 on 1462 degrees of freedom\nMultiple R-squared:  0.03151,   Adjusted R-squared:  0.03018 \nF-statistic: 23.78 on 2 and 1462 DF,  p-value: 6.858e-11\n\n\nThe mean sale price of residential properties is 9.8219^{4} dollars higher than commercial properties.\nThe mean sale price of floating villages is 1.43223^{5} dollars higher than commercial properties.\n\nTo understand this conceptually, it is easiest to visualize the linear model that would predict sale_price with these two dichotomous features.\n\nThere are only three columns of sale_price because the only possible values for f1 and f2 (which are both dichotomous) are\n\n0,0 (commercial)\n1,0 (residential)\n0,1 (floating village)\n\nThis regression with two features yields a prediction plane (displayed)\nThe left/right tilt of the plane will be the parameter estimate for f1 and it is the contrast of residential vs. commercial\nThe front/back tilt of the plane will be the parameter estimate for f2 and it is the contrast of floating village vs. commercial\n\n\n\n\n\n\n\nStatistical sidebar:\n\nAny full rank (# levels - 1) set of features regardless of coding system predicts exactly the same (e.g., dummy, helmert, contrast coding)\nPreference among coding systems is simply to get single df contrasts of theoretical importance (i.e., for explanation rather than prediction)\nFinal (mth) dummy feature is not included b/c its is completely redundant (perfectly multicollinear) with other dummy features. This would also prevent a linear model from fitting (‘dummy variable trap’).\nHowever, some statistical algorithms do not have problems with perfect multicollinearity (e.g., LASSO, ridge regression).\n\nFor these algorithms, you will sometimes see modified version of dummy coding called one-hot coding.\n\nThis approach uses one additional dummy coded feature for the final category.\n\nWe won’t spend time on this but you should be familiar with the term b/c it is often confused with dummy coding.\n\n\n\nCoding Sidebar\nWhen creating dummy coded features from factors that have levels with infrequent observations, you may occasionally end up with novel levels in your validation or test sets that were not present in your training set.\n\nThis will cause you issues.\n\nThese issues are mostly resolved if you make sure to explicitly list all possible levels for a factor when classing that factor in the training data, even if the level doesn’t exist in the training data.\n\nWe provide more detail on this issue in an appendix.\n\n\n\n3.5.1 Unordered Categorical Predictors\nNow that we understand how to use dummy coding to feature engineer unordered categorical predictors, let’s consider some potentially important ones that are available to us. Are any promising?\nLets return first to ms_zoning\nData dictionary entry: Identifies the general zoning classification of the sale.\n\nagri: Agriculture\ncommer: Commercial\nfloat: Floating Village Residential\nindus: Industrial\nres_high: Residential High Density\nres_med: Residential Medium Density\nres_low: Residential Low Density\n\nWe might:\n\nRepresent it with 6 dummy features (because there are 7 raw levels) but many of the categories are very low n - won’t account for much variance?\nCombine all the commercial categories (agri, commer, indus), which would take care of most of the low n groups. They also all tend to have the lower prices.\nCombine all the residential to get a better feature to variance accounted ratio. They all tend to have similar prices on average and res_high is also pretty low n. \n\n\ndata_trn |&gt; \n  plot_categorical(\"ms_zoning\", \"sale_price\") |&gt; \n  plot_grid(plotlist = _, ncol = 2)\n\n\n\n\n\nlot_config\n\nMost are inside lots, some of the lot categories are low n\nMedian sale_price is not very different between configurations\nNot very promising but could help some (particularly given the large sample size)\n\n\ndata_trn |&gt; \n  plot_categorical(\"lot_config\", \"sale_price\") |&gt; \n  plot_grid(plotlist = _, ncol = 2)\n\n\n\n\nData dictionary entry: Lot configuration\n\ninside: Inside lot\ncorner: Corner lot\nculdsac: Cul-de-sac\nfr2: Frontage on 2 sides of property\nfr3: Frontage on 3 sides of property\n\n\nbldg_type\n\nMost of the houses are in one category - one_fam\nThere is not much difference in median sale_price among categories\nNot very promising\n\n\ndata_trn |&gt; \n  plot_categorical(\"bldg_type\", \"sale_price\") |&gt; \n  plot_grid(plotlist = _, ncol = 2)\n\n\n\n\nData dictionary entry: Type of dwelling\n\none_fam: Single-family Detached\n\ntwo_fam: Two-family Conversion; originally built as one-family dwelling\nduplex: Duplex\ntown_end: Townhouse End Unit\ntown_inside: Townhouse Inside Unit\n\n\nLet’s do some feature engineering with ms_zoning. We can now do this formally in a recipe so that it can be used in our modeling workflow.\nFirst, if you noticed earlier, there are some levels for ms_zoning that are pretty infrequent. Lets make sure both data_trn and data_val have all levels set for this factor.\n\ndata_trn |&gt; pull(ms_zoning) |&gt; levels()\n\n[1] \"agri\"     \"commer\"   \"float\"    \"indus\"    \"res_high\" \"res_low\"  \"res_med\" \n\ndata_val |&gt; pull(ms_zoning) |&gt; levels()\n\n[1] \"commer\"   \"float\"    \"indus\"    \"res_high\" \"res_low\"  \"res_med\" \n\n\nAs expected, we are missing a level (agri) in data_val. Lets fix that.\n\ndata_val &lt;- data_val |&gt; \n  mutate(ms_zoning = factor(ms_zoning, \n                            levels = c(\"agri\", \"commer\", \"float\", \"indus\", \"res_high\", \"res_low\", \"res_med\")))\n\n\nWith that fixed, let’s proceed:\n\nWe will collapse categories down to three levels (commercial, residential, floating village) as before but now using step_mutate() combined with fct_collapse() to do this inside of our recipe.\n\nWe will convert to dummy features using step_dummy(). The first level of the factor will be set to the reference level when we call step_dummy().\nstep_dummy() is a poor choice for function name. It actually uses whatever contrast coding we have set up in R. However, the default is are dummy coded contrasts (R calls this treatment contrasts). See ?contrasts and options(\"contrasts\") for more info.\n\n\nrec &lt;- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + ms_zoning, \n         data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\")) |&gt;\n  step_dummy(ms_zoning)\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5\n\n\n\n\n\n── Operations \n\n\n• Median imputation for: garage_cars\n\n\n• Variable mutation for: fct_collapse(ms_zoning, residential = c(\"res_high\",\n  \"res_med\", \"res_low\"), commercial = c(\"agri\", \"commer\", \"indus\"), floating =\n  \"float\")\n\n\n• Dummy variables from: ms_zoning\n\n\n\nCoding Sidebar\nYou should also read more about some other step_() functions that you might use for categorical predictors: - step_other() to combine all low frequency categories into a single “other” category. - step_unknown() to assign missing values their own category - You can use selector functions. For example, you could make dummy variables out of all of your factors using step_dummy(all_nominal_predictors()).\nSee the Step Functions - Dummy Variables and Encoding section on the tidymodels website for additional useful functions.\nWe have also described these in the section on factor steps in Appendix 1\n\nLet’s see if the addition of ms_zoning helped\n\nNotice the addition of the dummy coded features to the feature matrix\nNotice the removal of the factor ms_zoning\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\nfeat_trn |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n7\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n7\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n0\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\nms_zoning_floating\n\n\n0\n\n\n1\n\n\n0.05\n\n\n0.21\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n4.38\n\n\n17.22\n\n\n\n\nms_zoning_residential\n\n\n0\n\n\n1\n\n\n0.94\n\n\n0.23\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-3.86\n\n\n12.90\n\n\n\n\n\n\nfeat_val |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_val\n\n\n\n\nNumber of rows\n\n\n490\n\n\n\n\nNumber of columns\n\n\n7\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n7\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1493.00\n\n\n483.78\n\n\n480\n\n\n1143.5\n\n\n1436.0\n\n\n1729.50\n\n\n3608\n\n\n0.92\n\n\n1.16\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10462.08\n\n\n10422.55\n\n\n1680\n\n\n7500.0\n\n\n9563.5\n\n\n11780.75\n\n\n215245\n\n\n15.64\n\n\n301.66\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.08\n\n\n30.96\n\n\n1875\n\n\n1954.0\n\n\n1975.0\n\n\n2000.00\n\n\n2010\n\n\n-0.66\n\n\n-0.41\n\n\n\n\ngarage_cars\n\n\n0\n\n\n1\n\n\n1.74\n\n\n0.76\n\n\n0\n\n\n1.0\n\n\n2.0\n\n\n2.00\n\n\n4\n\n\n-0.24\n\n\n0.22\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n178512.82\n\n\n75493.59\n\n\n35311\n\n\n129125.0\n\n\n160000.0\n\n\n213000.00\n\n\n556581\n\n\n1.42\n\n\n2.97\n\n\n\n\nms_zoning_floating\n\n\n0\n\n\n1\n\n\n0.05\n\n\n0.22\n\n\n0\n\n\n0.0\n\n\n0.0\n\n\n0.00\n\n\n1\n\n\n4.07\n\n\n14.58\n\n\n\n\nms_zoning_residential\n\n\n0\n\n\n1\n\n\n0.93\n\n\n0.25\n\n\n0\n\n\n1.0\n\n\n1.0\n\n\n1.00\n\n\n1\n\n\n-3.51\n\n\n10.33\n\n\n\n\n\n\n\nNow lets fit a model with these features\n\nfit_lm_6 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_6, feat_val)$.pred)\n\n\n\nerror_val &lt;- error_val |&gt; \n  bind_rows(tibble(model = \"6 feature linear model w/ms_zoning\", \n                   rmse_val = rmse_vec(feat_val$sale_price,\n                                       predict(fit_lm_6,\n                                               feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 4 × 2\n  model                              rmse_val\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 simple linear model                  51375.\n2 4 feature linear model               39903.\n3 4 feature linear model with YJ       41660.\n4 6 feature linear model w/ms_zoning   39846.\n\n\nRemoving Yeo Johnson transformation but adding dummy coded ms_zoning may have helped a little\n[Will the addition of new predictors/features to a model always reduce RMSE in train? in validation?].{red} 4\n\n\n3.5.2 Ordered Categorical Predictors\nWe have two paths to pursue for ordered categorical predictors\n\nWe can treat them like unordered categorical predictors (e.g., dummy code)\nWe can treat them like numeric predictors (either raw or with an added transformation if needed)\n\n\nLet’s consider overall_qual\n\ndata_trn |&gt; \n  plot_categorical(\"overall_qual\", \"sale_price\") |&gt; \n  plot_grid(plotlist = _, ncol = 2)\n\n\n\n\nObservations:\n\nLow frequency for low and to some degree high quality response options. If dummy coding, may want to collapse some (1-2)\nThere is a monotonic relationship (mostly linear) with sale_price. Treat as numeric?\nNot skewed so doesn’t likely need to be transformed if treated as numeric\nNumeric will take one feature vs. many (9?) features for dummy codes.\n\nDummy codes are more flexible but we may not need this flexibility (and unnecessary flexibility increases overfitting)\n\n\nLet’s add overall_qual to our model as numeric\nRemember that this predictor was ordinal so we paid special attention to the order of the levels when we classed this factor. Lets confirm they are in order\n\ndata_trn |&gt; pull(overall_qual) |&gt; levels()\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n\n\nTo convert this to numeric (with levels in this order), we can use another simple mutate inside our recipe.\n\nrec &lt;- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |&gt;\n  step_dummy(ms_zoning)\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 6\n\n\n\n\n\n── Operations \n\n\n• Median imputation for: garage_cars\n\n\n• Variable mutation for: fct_collapse(ms_zoning, residential = c(\"res_high\",\n  \"res_med\", \"res_low\"), commercial = c(\"agri\", \"commer\", \"indus\"), floating =\n  \"float\"), as.numeric(overall_qual)\n\n\n• Dummy variables from: ms_zoning\n\n\nCoding Sidebar\nThere is a step function called step_ordinalscore() but it requires that the factor is classed as an ordered factor. It is also more complicated than needed in our opinion. Just use as.numeric()\n\nLet’s evaluate this model\n\nMaking features\nSkipping the skim to save space (we promised we checked it previously!)\nFitting model\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\nfit_lm_7 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\nPlotting results\nQuantifying held out error\n\n\nplot_truth(truth = feat_val$sale_price, \n                                 estimate = predict(fit_lm_7, feat_val)$.pred)\n\n\n\nerror_val &lt;- bind_rows(error_val, \n                        tibble(model = \"7 feature linear model\", \n                               rmse_val = rmse_vec(feat_val$sale_price, predict(fit_lm_7, feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 5 × 2\n  model                              rmse_val\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 simple linear model                  51375.\n2 4 feature linear model               39903.\n3 4 feature linear model with YJ       41660.\n4 6 feature linear model w/ms_zoning   39846.\n5 7 feature linear model               34080.\n\n\nThat helped!"
  },
  {
    "objectID": "003_regression.html#extensions-to-interactive-models-and-non-linear-models",
    "href": "003_regression.html#extensions-to-interactive-models-and-non-linear-models",
    "title": "3  Introduction to Regression Models",
    "section": "3.6 Extensions to Interactive Models and Non-linear Models",
    "text": "3.6 Extensions to Interactive Models and Non-linear Models\n\n3.6.1 Interactions\nThere may be interactive effects among our predictors\n\nSome statistical algorithms (e.g., KNN) can naturally accommodate interactive effects without any feature engineering\nLinear models cannot\nNothing to fear, tidymodels makes it easy to feature engineer interactions\n[BUT - as we will learn, we generally think that if you expect lots of interactions, the linear model may not be the best model to use]\n\n\nFor example, it may be that the relationship between year_built and sale_price depends on overall_qual.\n\nOld houses are expensive if they are in good condition\nbut old houses are very cheap if they are in poor condition\n\nIn the tidymodels framework\n\nCoding interactions is done by feature engineering, not by formula (Note that formula does not change below in recipe)\nThis seems appropriate to us as we are making new features to represent interactions\nWe still use an R formula like interface to specify the interaction term features that will be created\nsee more details on the tidymodels website\n\n\nrec &lt;- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |&gt;\n  step_dummy(ms_zoning) |&gt; \n  step_interact(~ overall_qual:year_built)\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 6\n\n\n\n\n\n── Operations \n\n\n• Median imputation for: garage_cars\n\n\n• Variable mutation for: fct_collapse(ms_zoning, residential = c(\"res_high\",\n  \"res_med\", \"res_low\"), commercial = c(\"agri\", \"commer\", \"indus\"), floating =\n  \"float\"), as.numeric(overall_qual)\n\n\n• Dummy variables from: ms_zoning\n\n\n• Interactions with: overall_qual:year_built\n\n\n\nLet’s prep, bake, fit, and evaluate!\n\nNote the new interaction term (we just skim feat_trn here)\nNamed using “x” to specify the interaction\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\nfeat_trn |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n9\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n9\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n0\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n6.08\n\n\n1.41\n\n\n1\n\n\n5\n\n\n6\n\n\n7\n\n\n10\n\n\n0.20\n\n\n-0.03\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\nms_zoning_floating\n\n\n0\n\n\n1\n\n\n0.05\n\n\n0.21\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n4.38\n\n\n17.22\n\n\n\n\nms_zoning_residential\n\n\n0\n\n\n1\n\n\n0.94\n\n\n0.23\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-3.86\n\n\n12.90\n\n\n\n\noverall_qual_x_year_built\n\n\n0\n\n\n1\n\n\n12015.69\n\n\n2907.93\n\n\n1951\n\n\n9800\n\n\n11808\n\n\n14021\n\n\n20090\n\n\n0.24\n\n\n-0.11\n\n\n\n\n\n\n\n\nfix model\nplot\ncalculate held out error\n\n\nfit_lm_8 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(sale_price ~., \n      data = feat_trn)\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_8, feat_val)$.pred)\n\n\n\nerror_val &lt;- bind_rows(error_val, \n                      tibble(model = \n                               \"8 feature linear model w/interaction\", \n                             rmse_val = rmse_vec(feat_val$sale_price,\n                                                 predict(fit_lm_8,\n                                                         feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 6 × 2\n  model                                rmse_val\n  &lt;chr&gt;                                   &lt;dbl&gt;\n1 simple linear model                    51375.\n2 4 feature linear model                 39903.\n3 4 feature linear model with YJ         41660.\n4 6 feature linear model w/ms_zoning     39846.\n5 7 feature linear model                 34080.\n6 8 feature linear model w/interaction   32720.\n\n\n\nThat helped!\n\n\nYou can also feature engineer interactions with categorical predictors\n\nThe categorical predictors should first be converted to dummy code features\nYou will indicate the interactions using the variable names that will be assigned to these dummy code features\nUse starts_with() or matches() to make it easy if there are many features associated with a categorical predictor\nCan use “~ .^2” to include all two way interactions (be careful if you have dummy coded features!)\n\nLet’s code an interaction between ms_zoning & year_built.\n\nOld homes are cool\nOld commercial spaces are never cool\nMaybe this is why the main effect of ms_zoning wasn’t useful\n\n\nrec &lt;- \n  recipe(sale_price ~  ~ gr_liv_area + lot_area + year_built + garage_cars + \n           ms_zoning + overall_qual, data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\"),\n              overall_qual = as.numeric(overall_qual)) |&gt;\n  step_dummy(ms_zoning) |&gt; \n  step_interact(~ overall_qual:year_built) |&gt; \n  step_interact(~ starts_with(\"ms_zoning_\"):year_built)  \n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 6\n\n\n\n\n\n── Operations \n\n\n• Median imputation for: garage_cars\n\n\n• Variable mutation for: fct_collapse(ms_zoning, residential = c(\"res_high\",\n  \"res_med\", \"res_low\"), commercial = c(\"agri\", \"commer\", \"indus\"), floating =\n  \"float\"), as.numeric(overall_qual)\n\n\n• Dummy variables from: ms_zoning\n\n\n• Interactions with: overall_qual:year_built\n\n\n• Interactions with: starts_with(\"ms_zoning_\"):year_built\n\n\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\nYup, we have two new interaction features as expected\n\n\nfeat_trn |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n11\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n11\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n1506.84\n\n\n511.44\n\n\n438\n\n\n1128\n\n\n1450\n\n\n1759\n\n\n5642\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n10144.16\n\n\n8177.55\n\n\n1476\n\n\n7500\n\n\n9375\n\n\n11362\n\n\n164660\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n1971.35\n\n\n29.65\n\n\n1880\n\n\n1953\n\n\n1972\n\n\n2000\n\n\n2010\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n0\n\n\n1\n\n\n1.78\n\n\n0.76\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n-0.26\n\n\n0.10\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n6.08\n\n\n1.41\n\n\n1\n\n\n5\n\n\n6\n\n\n7\n\n\n10\n\n\n0.20\n\n\n-0.03\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789\n\n\n129500\n\n\n160000\n\n\n213500\n\n\n745000\n\n\n1.64\n\n\n4.60\n\n\n\n\nms_zoning_floating\n\n\n0\n\n\n1\n\n\n0.05\n\n\n0.21\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n4.38\n\n\n17.22\n\n\n\n\nms_zoning_residential\n\n\n0\n\n\n1\n\n\n0.94\n\n\n0.23\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-3.86\n\n\n12.90\n\n\n\n\noverall_qual_x_year_built\n\n\n0\n\n\n1\n\n\n12015.69\n\n\n2907.93\n\n\n1951\n\n\n9800\n\n\n11808\n\n\n14021\n\n\n20090\n\n\n0.24\n\n\n-0.11\n\n\n\n\nms_zoning_floating_x_year_built\n\n\n0\n\n\n1\n\n\n90.29\n\n\n415.84\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2009\n\n\n4.38\n\n\n17.22\n\n\n\n\nms_zoning_residential_x_year_built\n\n\n0\n\n\n1\n\n\n1860.03\n\n\n453.95\n\n\n0\n\n\n1948\n\n\n1968\n\n\n1997\n\n\n2010\n\n\n-3.83\n\n\n12.78\n\n\n\n\n\n\n\n\nFit model\nPlot\nQuantify held out error\n\n\nfit_lm_10 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_lm_10, feat_val)$.pred)\n\n\n\nerror_val &lt;- error_val |&gt; \n  bind_rows(tibble(model = \"10 feature linear model w/interactions\", \n                   rmse_val = rmse_vec(feat_val$sale_price,\n                                       predict(fit_lm_10,\n                                               feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 7 × 2\n  model                                  rmse_val\n  &lt;chr&gt;                                     &lt;dbl&gt;\n1 simple linear model                      51375.\n2 4 feature linear model                   39903.\n3 4 feature linear model with YJ           41660.\n4 6 feature linear model w/ms_zoning       39846.\n5 7 feature linear model                   34080.\n6 8 feature linear model w/interaction     32720.\n7 10 feature linear model w/interactions   32708.\n\n\n\nNot really any better\nShouldn’t just include all interactions without reason\n\nEither you have done EDA to support them or\nYou have substantive interest in them (explanatory question)\nIf you want all interactions, use a statistical algorithm that supports those relationships without feature engineering (e.g., KNN, random forest and other decision trees)\n\n\n\n\n\n3.6.2 Non-linear Models\nWe may also want to model non-linear effects of our predictors\n\nSome non-parametric models can accommodate non-linear effects without feature engineering (e.g., KNN, Random Forest).\nNon-linear effects can be accommodated in a linear model with feature engineering\n\nTransformations of Y or X. See Step Functions - Individual Transformations on tidymodels website\nOrdinal predictors can be coded with dummy variables\nQuantitative variables can be split at threshold\nPolynomial contrasts for quantitative or categorical predictors (see step_poly())\n\nWe will continue to explore these options throughout the course"
  },
  {
    "objectID": "003_regression.html#knn-regression",
    "href": "003_regression.html#knn-regression",
    "title": "3  Introduction to Regression Models",
    "section": "3.7 KNN Regression",
    "text": "3.7 KNN Regression\nK Nearest Neighbor\n\nIs a non-parametric regression and classification statistical algorithm\n\nIt does not yield specific parameter estimates for features/predictors (or statistical tests for those parameter estimates)\nThere are still ways to use it to address explanatory questions (visualizations, model comparisons, feature importance)\n\nVery simple but also powerful (listed commonly among top 10 algorithms)\n\nBy powerful, it is quite flexible and can accommodate many varied DGPs without the need for much feature engineering with its predictors\nMay not need most transformations of X or Y\nMay not need to model interactions\nStill need to handle missing data, outliers, and categorical predictors\n\nAlgorithm “memorizes” the training set (lazy learning)\n\nLazy learning is most useful for large, continuously changing datasets with few attributes (features) that are commonly queried (e.g., online recommendation systems)\n\nPrediction for any new observation is based on \\(k\\) most similar observations from the dataset\n\n\\(k\\) provides direct control over the bias-variance trade-off for this algorithm\n\n\n\nTo better understand KNN let’s simulate training data for three different DGPs (linear - y, polynomial - y2, and step - y3)\n\nLet’s start with a simple example where the DGP for Y is linear on one predictor (X)\nThis figure displays:\n\nDGP\nPrediction line from a simple linear model\nRed lines to represent three new observations (X = 10, 50, 90) we want to make predictions for via a standard KNN\n\n[What would 5-NN predictions look like for each of these three new values of X?].{red} 5\nDGP: \\(y = rnorm(150, x, 10)\\)\n\n\n\n\n\n\nKNN can easily accommodate non-linear relationships between numeric predictors and outcomes without any feature engineering for predictors\nIn fact, it can flexibly handle any shape of relationship\nDGP: \\(y2 = rnorm(150, x^4 / 800000, 8)\\)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\nDGP: \\(y3 = if\\_else(x &lt; 40, rnorm(150, 25, 10), rnorm(150, 75, 10))\\)\n\n\nWarning in geom_vline(xintercept = 90, color = \"red\", linewith = 1.5): Ignoring\nunknown parameters: `linewith`\n\n\n\n\n\n\n\n3.7.1 The hyperparameter k\nKNN is our first example of a statistical algorithm that includes a hyperparameter, in this case \\(k\\)\n\nAlgorithm hyperparameters differ from parameters in that they cannot be estimated while fitting the algorithm to the training set\nThey must be set in advance\nk = 5 is the default for kknn(), the engine from the kknn package that we will use to fit a KNN within tidymodels.\n\n\\(kknn()\\) weights observations (neighbors) based on distance.\n\nAn option exists for unweighted as well but not likely used much (default is optimal weighting, use it!).\n\n\n\nUsing the polynomial DGP above, let’s look at a 5-NN yields\n\nNote the new category of algorithm, new engine, and the need to set a mode (because KNN can be used for regression and classification)\nWe can look in the package documentation to better understand what is being done (?kknn::train.kknn).\n\n\nnearest_neighbor() |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  translate()\n\nK-Nearest Neighbor Model Specification (regression)\n\nComputational engine: kknn \n\nModel fit template:\nkknn::train.kknn(formula = missing_arg(), data = missing_arg(), \n    ks = min_rows(5, data, 5))\n\n\nSet up simple feature engineering recipe and get training features (nothing happening but let’s follow normal routine anyway)\n\nrec &lt;- \n  recipe(y2 ~ x, data = data_trn_demo)\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn_demo)\n\nfeat_trn_demo &lt;- rec_prep |&gt; \n  bake(data_trn_demo)\n\n\nFit 5NN\n\nfit_5nn_demo &lt;- \n  nearest_neighbor() |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(y2 ~ ., data = feat_trn_demo)\n\n\nGet a validation set (a new sample using same polynomial DGP)\n\ndata_val_demo &lt;- tibble(x = runif(200, 1, 100),\n               y = rnorm(200, x, 10),\n               y_dgp = rnorm(200, x ,0),\n               y2 = rnorm(200, x^4 / 800000, 8),\n               y2_dgp = rnorm(200, x^4 / 800000 ,0),\n               y3 = if_else(x &lt; 40, rnorm(200, 25, 10),  rnorm(200, 75, 10)),\n               y3_dgp = if_else(x &lt; 40, rnorm(200, 25, 0),  rnorm(200, 75, 0)))\n\nfeat_val_demo &lt;- rec_prep |&gt; \n  bake(data_val_demo)\n\nDisplay 5NN predictions in validation\n\nKNN (with k = 5) does a pretty good job of representing the shape of the DGP (low bias)\nKNN displays some (but minimal) evidence of overfitting\nSimple linear model does not perform well (clear/high bias)\n\n\nfeat_val_demo |&gt; \n  bind_cols(data_val_demo |&gt; select(y, y2_dgp)) |&gt; # add in other outcomes from data\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_smooth(aes(color = \"green\"), method = 'lm', formula = y ~ x,  se = FALSE) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_val_demo)$.pred, color = \"red\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"green\", \"red\"),\n                       labels = c(\"DGP\", \"linear model\", \"k = 5\"),\n                       guide = \"legend\")\n\n\n\n\n\nLet’s pause and consider our conceptual understanding of the impact of \\(k\\) on the bias-variance trade-off\n[How will the size of \\(k\\) influence model performance (e.g., bias, overfitting/variance)?].{red} 6\n[How will k = 1 perform in training and validation sets?].{red} 7\n\nk = 1\nFit new model\nRecipe and features have not changed\n\nfit_1nn_demo &lt;- \n1  nearest_neighbor(neighbors = 1) |&gt;\n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(y2 ~ ., data = feat_trn_demo)\n\n\n1\n\nSet k with neighbors =\n\n\n\n\n\nVisualize prediction models in Train and Validation\n\n\n\n\n\n\nCalculate RMSE in validation for two KNN models\nk = 1\n\nrmse_vec(feat_val_demo$y2, \n         predict(fit_1nn_demo, feat_val_demo)$.pred)\n\n[1] 10.91586\n\n\nk = 5\n\nrmse_vec(feat_val_demo$y2, \n         predict(fit_5nn_demo, feat_val_demo)$.pred)\n\n[1] 8.387035\n\n\n\nWhat if we go the other way and increase \\(k\\) to 75\n\nfit_75nn_demo &lt;- \n  nearest_neighbor(neighbors = 75) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(y2 ~ ., data = feat_trn_demo)\n\n\nVisualize prediction models in Train and Validation\n\n#|echo: false\nplot_train &lt;- feat_trn_demo |&gt; \n  bind_cols(data_trn_demo |&gt; select(y, y2_dgp)) |&gt; # add in other outcomes for fig\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_trn_demo)$.pred, color = \"red\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_1nn_demo, feat_trn_demo)$.pred, color = \"green\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_75nn_demo, feat_trn_demo)$.pred, color = \"yellow\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n    scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"red\", \"green\", \"yellow\"),\n                       labels = c(\"DGP\", \"k = 5\", \"k = 1\", \"k = 75\"),\n                       guide = \"legend\")\n\nplot_val &lt;- feat_val_demo |&gt; \n  bind_cols(data_val_demo |&gt; select(y, y2_dgp)) |&gt; # add in other outcomes for fig\n  ggplot(aes(x = x, y = y2)) +\n    geom_line(aes(x = x, y = y2_dgp, color = \"blue\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_5nn_demo, feat_val_demo)$.pred, color = \"red\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_1nn_demo, feat_val_demo)$.pred, color = \"green\"), linewidth = 1.5) +\n    geom_line(aes(x = x, y = predict(fit_75nn_demo, feat_val_demo)$.pred, color = \"yellow\"), linewidth = 1.5) +\n    geom_point(aes(y = y2), color = \"black\") +\n     scale_color_identity(name = \"Model\",\n                       breaks = c(\"blue\", \"red\", \"green\", \"yellow\"),\n                       labels = c(\"DGP\", \"k = 5\", \"k = 1\", \"k = 75\"),\n                       guide = \"legend\")\n\nplot_grid(plot_train, plot_val, labels = list(\"Training Set\", \"Validation Set\"), ncol = 2, nrow = 1, hjust = -1)\n\n\n\n\n\nCalculate RMSE in validation for three KNN models\nThis is the bias-variance trade-off in action\nk = 1\nhigh variance\n\nrmse_vec(feat_val_demo$y2, \n         predict(fit_1nn_demo, feat_val_demo)$.pred)\n\n[1] 10.91586\n\n\nk = 5\njust right (well better at least)\n\nrmse_vec(feat_val_demo$y2, \n         predict(fit_5nn_demo, feat_val_demo)$.pred)\n\n[1] 8.387035\n\n\nk = 75\nhigh bias\n\nrmse_vec(feat_val_demo$y2, \n         predict(fit_75nn_demo, feat_val_demo)$.pred)\n\n[1] 15.34998\n\n\n\n\n\n3.7.2 Defining “Nearest”\nTo make a prediction for some new observation, we need to identify the observations from the training set that are nearest to it\n\nNeed a distance measure to define “nearest”\nThere are a number of different distance measures available (e.g., Euclidean, Manhattan, Chebyshev, Cosine, Minkowski)\nEuclidean is most commonly used in KNN\nIMPORTANT: We care only about:\n\nDistance between a validation observation and all the training observations\nNeed to find the \\(k\\) observations in training that are nearest to the validation observation (i.e., its neighbors)\nDistance is defined based on these observations’ features, not their outcomes\n\n\n\nEuclidean distance between any two points is an n-dimensional extension of the Pythagorean formula (which applies explicitly with 2 features/2 dimensional space).\n\\(C^2 = A^2 + B^2\\)\n\\(C = \\sqrt{A^2 + B^2}\\)\n…where C is the distance between two points\n\nThe Euclidean distance between 2 points (p and q) in two dimensions (2 predictors, x1 = A, x2 = B)\n\\(Distance = \\sqrt{A^2 + B^2}\\)\n\\(Distance = \\sqrt{(q1 - p1)^2 + (q2 - p2)^2}\\)\n\\(Distance = \\sqrt{(2 - 1)^2 + (5 - 2)^2}\\)\n\\(Distance = 3.2\\)\n\n\n\n\n\n\nOne dimensional (one feature) is simply the subtraction of scores on that feature (x1) between p and q\n\\(Distance = \\sqrt{(q1 - p1)^2}\\)\n\\(Distance = \\sqrt{(2 - 1)^2}\\)\n\\(Distance = 1\\)\n\n\n\n\n\nN-dimensional generalization for n features:\n\\(Distance = \\sqrt{(q1 - p1)^2 + (q2 - p2)^2 + ... + (qn - pn)^2}\\)\n\nManhattan distance is also referred to as city block distance\n\nTravel down the “A” street for 1 unit\nTravel down the “B” street for 3 units\nTotal distance = 4 units\n\nFor two features/dimensions\n\\(Distance = |A + B|\\)\n\n\n\n\n\n\nkknn() uses Minkowski distance (see Wikipedia or less mathematical description)\n\nIt is a more complex parameterized distance formula\n\nThis parameter is called p, referred to as distance in kknn()\n\nEuclidean and Manhattan distances are special cases where p = 2 and 1, respectively\nThe default p in kknn() = 2 (Euclidean distance)\n\nThis default (like all defaults) can be changed when you define the algorithm using nearest_neighbor()\n\n\n\n3.7.3 Scaling X\nDistance is dependent on scales of all the features. We need to put all features on the same scale\n\nScale all features to SD = 1 (using step_scale(all_numeric_predictors()))\nRange correct [0, 1] all features (using step_range(all_numeric_predictors()))\n\n\n\n3.7.4 Categorical Predictors\nKNN requires numeric features (for distance calculation).\n\nFor categorical predictors, you will need to use dummy coding or other feature engineering that results in numeric features.\ne.g., step_dummy(all_factor_predictors())\n\n\n\n\n3.7.5 KNN with Ames Housing Prices\nLet’s use KNN with Ames\n\nTrain a model using only numeric predictors and overall_qual as numeric\nUse the default k = 5 algorithm\nrange correct all features\n\n\nrec &lt;- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + overall_qual, \n         data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(overall_qual = as.numeric(overall_qual)) |&gt; \n1  step_scale(all_numeric_predictors())\n\nrec\n\n\n1\n\nRemember to take advantage of these selectors for easier code! See ?has_role for more details\n\n\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5\n\n\n\n\n\n── Operations \n\n\n• Median imputation for: garage_cars\n\n\n• Variable mutation for: as.numeric(overall_qual)\n\n\n• Scaling for: all_numeric_predictors()\n\n\n\nrec_prep &lt;- rec |&gt; \n  prep(feat_trn)\n\nfeat_trn &lt;- rec_prep |&gt;\n  bake(data_trn)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\nSkim training features. Note all SD = 1\n\nfeat_trn |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_trn\n\n\n\n\nNumber of rows\n\n\n1465\n\n\n\n\nNumber of columns\n\n\n6\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n6\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n2.95\n\n\n1.00\n\n\n0.86\n\n\n2.21\n\n\n2.84e+00\n\n\n3.44\n\n\n11.03\n\n\n1.43\n\n\n5.19\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n1.24\n\n\n1.00\n\n\n0.18\n\n\n0.92\n\n\n1.15e+00\n\n\n1.39\n\n\n20.14\n\n\n11.20\n\n\n182.91\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n66.48\n\n\n1.00\n\n\n63.40\n\n\n65.86\n\n\n6.65e+01\n\n\n67.45\n\n\n67.79\n\n\n-0.54\n\n\n-0.62\n\n\n\n\ngarage_cars\n\n\n0\n\n\n1\n\n\n2.33\n\n\n1.00\n\n\n0.00\n\n\n1.31\n\n\n2.62e+00\n\n\n2.62\n\n\n5.23\n\n\n-0.26\n\n\n0.10\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n4.30\n\n\n1.00\n\n\n0.71\n\n\n3.54\n\n\n4.24e+00\n\n\n4.95\n\n\n7.07\n\n\n0.20\n\n\n-0.03\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n180696.15\n\n\n78836.41\n\n\n12789.00\n\n\n129500.00\n\n\n1.60e+05\n\n\n213500.00\n\n\n745000.00\n\n\n1.64\n\n\n4.60\n\n\n\n\n\n\n\nSkim validation features. Note SD. [Why not exactly 1?].{red}\n\nfeat_val |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_val\n\n\n\n\nNumber of rows\n\n\n490\n\n\n\n\nNumber of columns\n\n\n6\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n6\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\ngr_liv_area\n\n\n0\n\n\n1\n\n\n2.92\n\n\n0.95\n\n\n0.94\n\n\n2.24\n\n\n2.81\n\n\n3.38\n\n\n7.05\n\n\n0.92\n\n\n1.16\n\n\n\n\nlot_area\n\n\n0\n\n\n1\n\n\n1.28\n\n\n1.27\n\n\n0.21\n\n\n0.92\n\n\n1.17\n\n\n1.44\n\n\n26.32\n\n\n15.64\n\n\n301.66\n\n\n\n\nyear_built\n\n\n0\n\n\n1\n\n\n66.47\n\n\n1.04\n\n\n63.23\n\n\n65.90\n\n\n66.61\n\n\n67.45\n\n\n67.79\n\n\n-0.66\n\n\n-0.41\n\n\n\n\ngarage_cars\n\n\n0\n\n\n1\n\n\n2.27\n\n\n0.99\n\n\n0.00\n\n\n1.31\n\n\n2.62\n\n\n2.62\n\n\n5.23\n\n\n-0.24\n\n\n0.22\n\n\n\n\noverall_qual\n\n\n0\n\n\n1\n\n\n4.28\n\n\n0.98\n\n\n0.71\n\n\n3.54\n\n\n4.24\n\n\n4.95\n\n\n7.07\n\n\n0.00\n\n\n0.35\n\n\n\n\nsale_price\n\n\n0\n\n\n1\n\n\n178512.82\n\n\n75493.59\n\n\n35311.00\n\n\n129125.00\n\n\n160000.00\n\n\n213000.00\n\n\n556581.00\n\n\n1.42\n\n\n2.97\n\n\n\n\n\n\n\n\nFit 5NN\n\nfit_5nn_5num &lt;- \n  nearest_neighbor() |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\nerror_val &lt;- bind_rows(error_val, \n                        tibble(model = \"5 numeric predictor 5nn\", \n                               rmse_val = rmse_vec(feat_val$sale_price, predict(fit_5nn_5num, feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 8 × 2\n  model                                  rmse_val\n  &lt;chr&gt;                                     &lt;dbl&gt;\n1 simple linear model                      51375.\n2 4 feature linear model                   39903.\n3 4 feature linear model with YJ           41660.\n4 6 feature linear model w/ms_zoning       39846.\n5 7 feature linear model                   34080.\n6 8 feature linear model w/interaction     32720.\n7 10 feature linear model w/interactions   32708.\n8 5 numeric predictor 5nn                  32837.\n\n\n\nNot bad!\n\n\nKNN also mostly solved the linearity problem\n\nWe might be able to improve the linear models with better transformations of X and Y\nHowever, this wasn’t needed for KNN!\n\n\nplot_truth(truth = feat_val$sale_price, \n           estimate = predict(fit_5nn_5num, feat_val)$.pred)\n\n\n\n\n\nBut 5NN may be overfit. k = 5 is pretty low\nAgain with k = 20\n\nfit_20nn_5num &lt;- \n  nearest_neighbor(neighbors = 20) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\nerror_val &lt;- error_val |&gt; \n  bind_rows(tibble(model = \"5 numeric predictor 20nn\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_20nn_5num, \n                                               feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 9 × 2\n  model                                  rmse_val\n  &lt;chr&gt;                                     &lt;dbl&gt;\n1 simple linear model                      51375.\n2 4 feature linear model                   39903.\n3 4 feature linear model with YJ           41660.\n4 6 feature linear model w/ms_zoning       39846.\n5 7 feature linear model                   34080.\n6 8 feature linear model w/interaction     32720.\n7 10 feature linear model w/interactions   32708.\n8 5 numeric predictor 5nn                  32837.\n9 5 numeric predictor 20nn                 30535.\n\n\n\nThat helped some\n\n\nOne more time with k = 50 to see where we are in the bias-variance function\n\nfit_50nn_5num &lt;- \n  nearest_neighbor(neighbors = 50) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\nerror_val &lt;- error_val |&gt; \n  bind_rows(tibble(model = \"5 numeric predictor 50nn\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_50nn_5num, \n                                               feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 10 × 2\n   model                                  rmse_val\n   &lt;chr&gt;                                     &lt;dbl&gt;\n 1 simple linear model                      51375.\n 2 4 feature linear model                   39903.\n 3 4 feature linear model with YJ           41660.\n 4 6 feature linear model w/ms_zoning       39846.\n 5 7 feature linear model                   34080.\n 6 8 feature linear model w/interaction     32720.\n 7 10 feature linear model w/interactions   32708.\n 8 5 numeric predictor 5nn                  32837.\n 9 5 numeric predictor 20nn                 30535.\n10 5 numeric predictor 50nn                 31055.\n\n\n\nToo high, now we have bias……\nWe will learn a more rigorous method for selecting the optimal value for \\(k\\) (i.e., tuning this hyperparameter) in unit 5\n\n\nTo better understand bias-variance trade-off, let’s look at error across these three values of \\(k\\) in train and validation for Ames\nTraining\n\nRemember that training error would be 0 for k = 1\nTraining error is increasing as \\(k\\) increases b/c it KNN is overfitting less (so its not fitting the noise in train as well)\n\n\nrmse_vec(feat_trn$sale_price, \n         predict(fit_5nn_5num, feat_trn)$.pred)\n\n[1] 19012.94\n\nrmse_vec(feat_trn$sale_price, \n         predict(fit_20nn_5num, feat_trn)$.pred)\n\n[1] 27662.2\n\nrmse_vec(feat_trn$sale_price, \n         predict(fit_50nn_5num, feat_trn)$.pred)\n\n[1] 31069.12\n\n\nValidation\n\nValidation error is first going down as \\(k\\) increases (and it would have been very high for k = 1)\nBias is likely increasing a bit\nBut this is compensated by big decreases in overfitting variance\nThe trade-off is good for k = 20 relative to 5 and 1\nAt some point, as \\(k\\) increases the increase in bias outweighed the decrease in variance and validation error increased too.\n\n\nrmse_vec(feat_val$sale_price, \n         predict(fit_5nn_5num, feat_val)$.pred)\n\n[1] 32837.37\n\nrmse_vec(feat_val$sale_price, \n         predict(fit_20nn_5num, feat_val)$.pred)\n\n[1] 30535.04\n\nrmse_vec(feat_val$sale_price, \n         predict(fit_50nn_5num, feat_val)$.pred)\n\n[1] 31054.6\n\n\n\nLet’s do one final example and add one of our unordered categorical variables into the model: ms_zoning\n\nNeed to collapse levels and then dummy\n\n\nrec &lt;- \n  recipe(sale_price ~ gr_liv_area + lot_area + year_built + garage_cars + \n           overall_qual + ms_zoning, data = data_trn) |&gt; \n  step_impute_median(garage_cars) |&gt; \n  step_mutate(overall_qual = as.numeric(overall_qual)) |&gt; \n  step_mutate(ms_zoning = fct_collapse(ms_zoning,\n                                 \"residential\" = c(\"res_high\", \"res_med\", \"res_low\"),\n                                 \"commercial\" = c(\"agri\", \"commer\", \"indus\"),\n                                 \"floating\" = \"float\")) |&gt;\n  step_dummy(ms_zoning) |&gt; \n  step_scale(all_numeric_predictors())\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 6\n\n\n\n\n\n── Operations \n\n\n• Median imputation for: garage_cars\n\n\n• Variable mutation for: as.numeric(overall_qual)\n\n\n• Variable mutation for: fct_collapse(ms_zoning, residential = c(\"res_high\",\n  \"res_med\", \"res_low\"), commercial = c(\"agri\", \"commer\", \"indus\"), floating =\n  \"float\")\n\n\n• Dummy variables from: ms_zoning\n\n\n• Scaling for: all_numeric_predictors()\n\n\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\nFit and evaluate\n\nfit_20nn_5num_mszone &lt;- \n  nearest_neighbor(neighbors = 20) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(sale_price ~ ., data = feat_trn)\n\n\nerror_val &lt;- error_val |&gt; \n  bind_rows(tibble(model = \"5 numeric predictor 20nn with ms_zoning\", \n                   rmse_val = rmse_vec(feat_val$sale_price, \n                                       predict(fit_20nn_5num_mszone, \n                                               feat_val)$.pred)))\n\nerror_val\n\n# A tibble: 11 × 2\n   model                                   rmse_val\n   &lt;chr&gt;                                      &lt;dbl&gt;\n 1 simple linear model                       51375.\n 2 4 feature linear model                    39903.\n 3 4 feature linear model with YJ            41660.\n 4 6 feature linear model w/ms_zoning        39846.\n 5 7 feature linear model                    34080.\n 6 8 feature linear model w/interaction      32720.\n 7 10 feature linear model w/interactions    32708.\n 8 5 numeric predictor 5nn                   32837.\n 9 5 numeric predictor 20nn                  30535.\n10 5 numeric predictor 50nn                  31055.\n11 5 numeric predictor 20nn with ms_zoning   30172.\n\n\n\nNow it helps.\n\nMight have to do with interactions with other predictors that we didn’t model in the linear model\nKNN automatically accommodates interactions. Why?\nThis model is a bit more complex and might benefit further from higher \\(k\\)\n\nAs a teaser, here is another performance metric for this model - \\(R^2\\). Not too shabby! Remember, there is certainly some irreducible error in sale_price that will put a ceiling on \\(R^2\\) and a floor on RMSE\n\nrsq_vec(feat_val$sale_price, \n        predict(fit_20nn_5num_mszone, feat_val)$.pred)\n\n[1] 0.8404044\n\n\nOverall, we now have a model that predicts housing prices with about 30K of RMSE and accounting for 84% of the variance. I am sure you can improve on this!"
  },
  {
    "objectID": "003_regression.html#discussion",
    "href": "003_regression.html#discussion",
    "title": "3  Introduction to Regression Models",
    "section": "3.8 Discussion",
    "text": "3.8 Discussion\n\nAnnouncements\n\n\nBug in print method for recipes\n\nFor bug report with (long) reprex:\nFor now use: remotes::install_github(\"tidymodels/recipes#1084\")\n\nReprex\n\nRead new appendix\nWe now expect reprex for help on application assignments\nWill review in lab next week\n\nHomework is basically same for unit 4\n\nNew dataset - titanic\nDo EDA but we don’t need to see it\nFit KNN and RDA models (will learn about LDA, QDA and RDA in unit)\nSubmit predictions. Free lunch!\nWill know about first free lunch by next Thursday\n\nCourse feedback in quiz as trial\n\n\nQuestions\n\n\nWhat are two broad sources of error?\nWhat are two broad sources of reducible error?\nWhy do we need independent validation data to select the best model configuration?\nWhat factors make the need for a validation set even greater?\nWhat is RMSE? Connect it to metric you already know? How is it being used in lm (two ways)?; in knn (one way)?\nHow does bias and variance manifest when you look at your performance metric (RMSE) in training and validation sets?\nWill the addition of new predictors/features to a (lm?) model always reduce RMSE in train? in validation? Connect to concepts of bias and variance\n\n\nk\n\n\nWhat is it and how does it get used when making predictions?\nWhat is the impact of k on bias and variance/overfitting?\nk=1 - performance in train? in val?\n\n\nInteraction in KNN - Consider bias first (but also variance) in this example\n\n\nSimulate data\nFit models for lm and knn with and without interaction\nTook some shortcuts (no recipe, predict back into train)\n\n\nn &lt;- 200\nset.seed(5433)\n# simulate data as linear model\ndata &lt;- tibble(x1 = runif(n, 0,100), # uniform\n               x2 = rep(c(0,1), n/2), # dichotomous\n               x1_x2 = x1*x2, # interaction\n               y = rnorm(n, 0 + 1*x1 + 10*x2 + 10* x1_x2, 20)) #DGP + noise\n\nfit_lm &lt;- \n  linear_reg() |&gt;   \n  set_engine(\"lm\") |&gt;   \n  fit(y ~ x1 + x2, data = data)\n\nfit_lm_int &lt;- \n  linear_reg() |&gt;   \n  set_engine(\"lm\") |&gt;   \n  fit(y ~ x1 + x2 + x1_x2, data = data)\n\nfit_knn &lt;- \n  nearest_neighbor(neighbors = 20) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(y ~ x1 + x2, data = data)\n\nfit_knn_int &lt;- \n  nearest_neighbor(neighbors = 20) |&gt;   \n  set_engine(\"kknn\") |&gt;   \n  set_mode(\"regression\") |&gt; \n  fit(y ~ x1 + x2 + x1_x2, data = data)\n\ndata &lt;- data |&gt; \n  mutate(pred_lm = predict(fit_lm, data)$.pred,\n         pred_lm_int = predict(fit_lm_int, data)$.pred,\n         pred_knn = predict(fit_knn, data)$.pred,\n         pred_knn_int = predict(fit_knn_int, data)$.pred)\n\n\nPredictions from linear model with and without interaction\n\n\ndata |&gt; \n  ggplot(aes(x = x1, group = factor(x2), color = factor(x2))) +\n    geom_line(aes(y = pred_lm)) +\n    geom_point(aes(y = y)) +\n    ggtitle(\"lm without interaction\") +\n    ylab(\"y\") +\n    scale_color_discrete(name = \"x2\")\n\n\n\ndata |&gt; \n  ggplot(aes(x = x1, group = factor(x2), color = factor(x2))) +\n    geom_line(aes(y = pred_lm_int)) +\n    geom_point(aes(y = y)) +\n    ggtitle(\"lm with interaction\") +\n    ylab(\"y\") +\n    scale_color_discrete(name = \"x2\")\n\n\n\n\n\nPredictions from linear model with and without interaction\n\n\ndata |&gt; \n  ggplot(aes(x = x1, group = factor(x2), color = factor(x2))) +\n    geom_line(aes(y = pred_knn)) +\n    geom_point(aes(y = y)) +\n    ggtitle(\"KNN without interaction\") +\n    ylab(\"y\") +\n    scale_color_discrete(name = \"x2\")\n\n\n\ndata |&gt; \n  ggplot(aes(x = x1, group = factor(x2), color = factor(x2))) +\n    geom_line(aes(y = pred_knn_int)) +\n    geom_point(aes(y = y)) +\n    ggtitle(\"KNN with interaction\") +\n    ylab(\"y\") +\n    scale_color_discrete(name = \"x2\")\n\n\n\n\n\nWhat are implications of this for when to use flexible algorithm like knn vs. lm?\n\nif data are high dimensional?\nroutine use?\n\n\n\nTransformations of numeric predictors\n\n\nUse of plot_truth() [predicted vs. observed]\nResiduals do not have mean of 0 for every \\(\\hat{y}\\)\n\nConsequence: biased parameter estimates. Linear is bad DGP\nAlso bad test of questions RE the predictor (underestimate? misinform)\n\nNon-normal residuals\n\nConsequence: lm parameter estimates still unbiased (for linear DGP) but more “efficient” solutions exist\nBad for prediction b/c higher variance than other solutions\nMay suggest omission of variables\n\nHeteroscasticity\n\nConsequence: Inefficient and inaccurate standard errors.\nStatistical tests wrong\nPoor prediction for some (where larger variance of resituals) \\(\\hat{y}\\)\nhigher variance overall than other solutions - bad again for prediction\n\nTransformation of outcome?\n\nmetric\nback to raw predictions\n\n\n\nKNN (black box) for explanatory purposes\n\n\nVisualizations (think of interaction plot above) make clear the effect\nWill learn more (better visualizations, variable importance, model comparisons) in later unit\n\n\nExploration\n\n\n“I feel that I can come up with models that decrease the RMSE, but I don’t have good priors on whether adding any particular variable or observation will result in an improved model. I still feel a little weird just adding and dropping variables into a KNN and seeing what gets the validation RMSE the lowest (even though because we’re using validation techniques it’s a fine technique)”\n\nExploration is learning. This is research. If you knew the answer you wouldn’t be doing the study\nDomain knowledge is still VERY important\nSome algorithms (LASSO, glmnet) will help with feature selection\nstaying organized\n\nScript structure\nGood documentation - RMD as analysis notebook\n\nSome overfitting to validation will occur? Consequence? Solutions?\n\n\n\nLM vs. KNN better with predictors or overall\n\n\n“Why do some features seem to improve performance more in linear models or only in KNNs?”\n“What are some contexts where KNN doesn’t work well? In other words, what are the advantages/disadvantages of using KNN?”\n\nAlways comes down to bias vs. variance\nFlexibility and N are key moderators of these two key factors.\n\nk? - impact on bias, variance?\n\n\n\n“In GLM, why correlation/collinearity among predictors will cause larger variance? Is it because of overfitting?”\n“Curse of dimensionality” - Bias vs. variance\n\n\nMissing features produce biased models.\nUnnecessary features or even many features relative to N produce variance\nDoes your available N in your algorithm support the features you need to have low bias.\n\nMostly an empirical question - can’t really tell otherwise outside of simulated data. Validation set is critical!\nFlexible models often need more N holding predictors constant\nRegularization (unit 6) will work well when lots of predictors\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag."
  },
  {
    "objectID": "003_regression.html#footnotes",
    "href": "003_regression.html#footnotes",
    "title": "3  Introduction to Regression Models",
    "section": "",
    "text": "These models will all overfit the dataset within which they are fit to some degree (in other words, they will predict both systematic variance and some noise in the training set). However, they will differ in how much they overfit the training set. As the models get more flexible they will have the potential to overfit to a greater degree. Models with a larger number of features (e.g., more predictors, features based on interactions as well as raw predictors) will overfit to a greater degree. All other things equal, the non-parametric KNN will also be more flexible than the general linear model so it may overfit to a greater degree as well if the true DGP is linear on the features. Therefore, just because a model fits the training set well does not mean it will work well in new data because the noise will be different in every new dataset.↩︎\n“The multiple regression model coefficients represent unique effects, controlling for all other variables in the model. You can see how the unique effect of gr_liv_area is smaller than its overall effect from the simple regression. This also means that the overall predictive strength of the model won’t be a sum of the effects of each predictor considered in isolation - it will likely be less. Also, if the correlations are high, problems with multicollinearity will emerge. This will yield large standard errors which means that the models will start to have more variance when fit in different training datasets! We will soon learn about other regularized versions of the GLM that don’t have these issues with correlated predictors.↩︎\nThere is no meaningful way to order the numbers that we assign to the levels of this unordered categorical predictor. The shape and strength of the relationship between it and sale_price will completely change based on arbitrary ordering of the levels.↩︎\nAs you know, the estimation procedure in linear models is OLS. Parameter estimates are derived to minimize the SSE in the data set in which they are derived. For this reason, adding a predictor will never increase RMSE in the training set and it will usually lower it even when it is not part of the DGP. However, this is not true in validation. A predictor will only meaningfully lower RMSE in validation if it is part of the DGP. Also, a bad predictor could even increase RMSE in validation due to overfitting.↩︎\nFor x = 10, find the five observations that have X values closest to 10. Average the Y values for those 5 observations and that is your prediction (\\(\\hat{Y}\\)) associated with that new value of X. Repeat to make predictions for \\(\\hat{Y}\\) for any other value of X, e.g., 50, 90, or any other value↩︎\nSmaller values of \\(k\\) will tend to increase overfitting (and therefore variance across training samples) but decrease bias. Larger values of k will tend to decrease overfitting but increase bias. We need to find the Goldilocks “sweet spot”↩︎\n\\(k\\) = 1 will perfectly fit the training set. Therefore it is very dependent on the training set (high variance). It will fit both the DGP and the noise in the training set. Clearly it will likely not do as well in validation (it will be overfit to training). \\(k\\) needs to be larger if there is more noise (to average over more cases). \\(k\\) needs to be smaller if the relationships are complex. (More on choosing \\(k\\) by resampling in unit 5.↩︎"
  },
  {
    "objectID": "004_classification.html#unit-overview",
    "href": "004_classification.html#unit-overview",
    "title": "4  Introduction to Classification Models",
    "section": "4.1 Unit Overview",
    "text": "4.1 Unit Overview\n\n4.1.1 Learning Objectives\n\nBayes classifier\nLogistic regression\n\nprobability, odds, and logit models\ndefinitions of odds and odds ratios\n\nK nearest neighbors\nLinear discriminant analysis\nQuadratic discriminant analysis\nRegularized discriminant analysis\nDecision boundaries in the two feature space\nRelative costs and benefits of these different statistical algorithms\n\n\n\n4.1.2 Readings\n\nJames et al. (2023) Chapter 4, pp 129 - 164\n\n\n\n4.1.3 Lecture Videos\n\nLecture 1: The Bayes Classifier ~ 5 mins\nLecture 2: Conceptual Overview of Logistic Regression ~ 17 mins\nLecture 3: EDA with the Cars Dataset ~17 mins\nLecture 4: Logistic Regression with Cars Dataset ~32 mins\nLecture 5: KNN with Cars Dataset ~14 mins\nLecture 6: LDA, DQA, RDA with Cars Dataset ~12 mins\nLecture 7: Comparisons among Classifiers ~14 mins\nDiscussion Post questions or discuss readings or lectures on the appropriate Slack channel\n\n\n\n4.1.4 Application Assignment and Quiz\n\ndata: raw; test\ndata dictionary\ncleaning EDA rmd\nrda rmd\nknn rmd\nfun_modeling.R\nsolution: modeling EDA rda; knn\n\nPost questions to application_assignments\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, February 14th\n\nOur eventual goal in this unit is to build a machine learning classification model that can accurately predict who lived vs. died on the titanic.\nTo begin, we need to:\n\nSet up conflicts policies\nWe will hide this in future units (showing one last time)\n\n\noptions(conflicts.policy = \"depends.ok\")\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\")\n\nℹ SHA-1 hash of file is \"175d942e14f108d74912bfb2593b77637328ecb1\"\n\ntidymodels_conflictRules()\n\n\nLoad the packages we will need\n\n\nlibrary(janitor, include.only = \"clean_names\")\nlibrary(cowplot, include.only = \"plot_grid\") # for plot_grid()\nlibrary(kableExtra, exclude = \"group_rows\") # exclude dplyr conflict\nlibrary(tidyverse) # for general data wrangling\nlibrary(tidymodels) # for modeling\n\n\nsource additional class functions libraries\nWe will hide this in future units (showing one last time)\n\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\n\nℹ SHA-1 hash of file is \"c045eee2655a18dc85e715b78182f176327358a7\"\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\")\n\nℹ SHA-1 hash of file is \"def6ce26ed7b2493931fde811adff9287ee8d874\"\n\n\n\nset display options\nWe will hide this in future units (showing one last time)\n\n\ntheme_set(theme_classic())\noptions(tibble.width = Inf)\n\n\nhandle paths\n\n\npath_data &lt;- \"./data\""
  },
  {
    "objectID": "004_classification.html#bayes-classifier",
    "href": "004_classification.html#bayes-classifier",
    "title": "4  Introduction to Classification Models",
    "section": "4.2 Bayes classifier",
    "text": "4.2 Bayes classifier\nFirst, lets introduce the Bayes classifier, which is the classifier that will have the lowest error rate of all classifiers using the same set of features.\nThe figure below displays simulated data for a classification problem for K = 2 classes as a function of X1 and X2\n\n\n\n\n\nThe Bayes classifier assigns each observation its most likely class given its conditional probabilities for the values for X1 and X2\n\n\\(Pr(Y = k | X = x_0) for\\:k = 1:K\\)\nFor K = 2, this means assigning to the class with Pr &gt; .50\nThis decision boundary for the two class problem is displayed in the figure\n\n\nThe Bayes classifier provides the minimum error rate for test data\n\nError rate for any \\(x_0\\) will be \\(1 - max (Pr( Y = k | X = x_0))\\)\nOverall error rate will be the average of this across all possible X\nThis is the irreducible error for classification problems\nThis is a theoretical model b/c (except for simulated data), we don’t know the conditional probabilities based on X\nMany classification models try to estimate these conditionals\n\nLet’s talk now about some of these classification models"
  },
  {
    "objectID": "004_classification.html#logistic-regression-a-conceptual-review",
    "href": "004_classification.html#logistic-regression-a-conceptual-review",
    "title": "4  Introduction to Classification Models",
    "section": "4.3 Logistic regression: A Conceptual Review",
    "text": "4.3 Logistic regression: A Conceptual Review\nLogistic regression (a special case of the generalized linear model) estimates the conditional probability for each class given X (a specific set of values for our features)\n\nIn the binary outcome case, we will often refer to the two outcomes as the positive class and the negative class\nThis makes most sense in some applied settings where we are most interested in predicting if one of the two classes is likely, e.g.,\n\nPresence of heart disease\nPositive for some psychiatric diagnoses\nLapse back to alcohol use in patients with alcohol use disorder\n\n\nWe prefer to use logistic regression over the general linear model with unordered categorical outcomes because:\n\nThe General Linear Model makes predictions that are not bounded by [0, 1] and do not represent true estimates of conditional probability for each class\nLogistic regression approaches can be modified to accommodate multi-class outcomes (i.e., more than two levels) even when those outcomes are unordered\n\nNonetheless, the general linear model is still used at times to predict binary outcomes (see Linear Probability Model) so you should be aware of it. We won’t discuss it further here.\n\n\n\n\n\n\nLogistic regression provides predicted conditional probabilities for one class (positive class) for any specific set of values for our features (X)\n\\(Pr(positive\\:class | X) = \\frac{e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}{1 + e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}\\)\nThese conditional probabilities are bounded by [0, 1]\nTo maximize accuracy (as per Bayes classifier), we predict the positive case if Pr(positive class | X) &gt; .5 for any specific X, otherwise we predict the negative class\n\nAs a simple parametric model, logistic regression is commonly used for explanatory purposes as well as prediction\nFor these reasons, it is worthwhile to fully understand how to work with the logistic function to quantify and describe the effects of your features/predictors in terms of\n\nProbability\nOdds\n[Log-odds or logit]\nOdds ratio\n\n\nThe logistic function yields the probability of the positive class given X\nHowever, in some instances, it may make more sense to describe the odds of the positive case occurring (e.g., horse racing) rather than probability\nOdds are defined with respect to probabilities as follows:\n\\(odds = \\frac{Pr(positive\\:class|X)} {1 - Pr(positive\\:class|X)}\\)\nFor example, if the UW Badgers have a .5 probability of winning some upcoming game based on X, their odds of winning are 1 (to 1)\n\n\\(\\frac{0.5} {1 - 0.5}\\)\n\nIf the UW Badgers have a .75 probability of winning some upcoming game based on X, their odds of winning are 3 (:1; ‘3 to 1’)\n\n\\(\\frac{0.75} {1 - 0.75}\\)\n\nIf the UW Badgers have a .25 probability of winning some upcoming game based on X, their odds of winning are .33 (1:3)\n\n\\(\\frac{0.25} {1 - 0.25}\\)\n\n\nThe logistic function can be modified to provide odds directly:\nLogistic function: \\(Pr(positive\\:class | X) = \\frac{e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}{1 + e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}}\\)\nDefinition of odds:\n\\(odds = \\frac{Pr(positive\\:class|X)} {1 - Pr(positive\\:class|X)}\\)\nSubstitute logistic function for \\(Pr(positive\\:class|X)\\) on top and bottom and simplify to get:\n\\(odds(positive\\:class|X) = e^{\\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p}\\)\nOdds are bounded by [0, \\(\\infty\\)]\n\nThe logistic function can be modified further such that the outcome (log-odds/logit) is a linear function of your features\nIf we take the natural log (base e) of both sides of the odds equation, we get:\n\\(log(odds(positive\\:class|X)) = \\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_p * X_p\\)\nLog-odds are unbounded \\([-\\infty, \\infty]\\)\nUse of logit transformation provides the connection between the general linear model and generalized linear models (in this case with link = logit, family = binomial). Notice that the logit/log-odds is a linear combination of the features (just like in the general linear model)\n\nOdds and probability are descriptive but they are not linear functions of X\n\nTherefore, parameter estimates from these models aren’t very useful to describe the effect of features\nThis is because unit change in Y per unit change in any specific feature is not the same for all values of the feature\n\nLog-odds are a linear function of X\n\nTherefore, you can say that log-odds of positive class increases by \\(\\beta_1\\) for every one unit increase in \\(x_1\\)\nHowever, log-odds are NOT very descriptive/intuitive so they are not that useful for explanatory purposes\n\n\nThe odds ratio addresses these problems\nOdds are defined at a specific set of values across the features in your model. For example, with one feature:\n\\(odds = \\frac{Pr(positive\\:class|x_1)} {1 - Pr(positive\\:class|x_1)}\\)\nThe odds ratio describes the change in odds for a change of c units in your feature\nWith some manipulation:\n\n\\(Odds\\:ratio = \\frac{odds(x+c)}{odds(x)}\\)\n\\(Odds\\:ratio = \\frac{e^{\\beta_0 + \\beta_1*(x_1 + c)}}{e^{\\beta_0 + \\beta_1*x_1}}\\)\n\\(Odds\\:ratio = e^{c*\\beta_1}\\)\n\n\nAs an example, if we fit a logistic regression model to predict the probability of the Badgers winning a home football game given the attendance (measured in individual spectators at the game), we might find \\(\\beta_1\\) = .000075.\nGiven this, the odds ratio associated with every increase in 10,000 spectators:\n\n\\(= e^{c * \\beta_1}\\)\n\\(= e^{10000 * .000075}\\)\n\\(= 2.1\\)\nFor every increase of 10,000 spectators, the odds of the Badgers winning doubles"
  },
  {
    "objectID": "004_classification.html#the-cars-dataset",
    "href": "004_classification.html#the-cars-dataset",
    "title": "4  Introduction to Classification Models",
    "section": "4.4 The Cars Dataset",
    "text": "4.4 The Cars Dataset\nLet’s put it all of this together in the cars dataset from Carnegie Mellon’s StatLib Dataset Archive Our goal is to build a classifier (machine learning model for a categorical outcome) that classifies cars as either high or low mpg.\n\n\n4.4.1 Cleaning EDA\nLet’s start with some cleaning EDA\n\nOpen and “skim” it RE variable names, classes & missing data\nVariable names are already tidy\nno missing data\nmins (p0) and maxes(p100) for numeric look good\n\n\ndata_all &lt;- read_csv(here::here(path_data, \"auto_all.csv\"),\n                     col_types = cols()) |&gt; \n  glimpse()\n\nRows: 392\nColumns: 9\n$ mpg          &lt;chr&gt; \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"…\n$ cylinders    &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 6, 6, 6, 4, …\n$ displacement &lt;dbl&gt; 307, 350, 318, 304, 302, 429, 454, 440, 455, 390, 383, 34…\n$ horsepower   &lt;dbl&gt; 130, 165, 150, 150, 140, 198, 220, 215, 225, 190, 170, 16…\n$ weight       &lt;dbl&gt; 3504, 3693, 3436, 3433, 3449, 4341, 4354, 4312, 4425, 385…\n$ acceleration &lt;dbl&gt; 12.0, 11.5, 11.0, 12.0, 10.5, 10.0, 9.0, 8.5, 10.0, 8.5, …\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 7…\n$ origin       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, …\n$ name         &lt;chr&gt; \"chevrolet chevelle malibu\", \"buick skylark 320\", \"plymou…\n\ndata_all |&gt; skim_some()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndata_all\n\n\n\n\nNumber of rows\n\n\n392\n\n\n\n\nNumber of columns\n\n\n9\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\ncharacter\n\n\n2\n\n\n\n\nnumeric\n\n\n7\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: character\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmin\n\n\nmax\n\n\nempty\n\n\nn_unique\n\n\nwhitespace\n\n\n\n\n\n\nmpg\n\n\n0\n\n\n1\n\n\n3\n\n\n4\n\n\n0\n\n\n2\n\n\n0\n\n\n\n\nname\n\n\n0\n\n\n1\n\n\n6\n\n\n36\n\n\n0\n\n\n301\n\n\n0\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\np0\n\n\np100\n\n\n\n\n\n\ncylinders\n\n\n0\n\n\n1\n\n\n3\n\n\n8.0\n\n\n\n\ndisplacement\n\n\n0\n\n\n1\n\n\n68\n\n\n455.0\n\n\n\n\nhorsepower\n\n\n0\n\n\n1\n\n\n46\n\n\n230.0\n\n\n\n\nweight\n\n\n0\n\n\n1\n\n\n1613\n\n\n5140.0\n\n\n\n\nacceleration\n\n\n0\n\n\n1\n\n\n8\n\n\n24.8\n\n\n\n\nyear\n\n\n0\n\n\n1\n\n\n70\n\n\n82.0\n\n\n\n\norigin\n\n\n0\n\n\n1\n\n\n1\n\n\n3.0\n\n\n\n\n\n\n\n\n\nAfter reviewing the data dictionary, we see that origin is an unordered categorical variable that is coded numeric (where 1 = American, 2 = European, and 3 = Japanese). Let’s recode as character with meaningful labels and then convert to factor\nIn fact, lets change all our categorical variables (mgp, origin, and name) to factors.\n\nmpg is ordinal, so lets set the levels to indicate the order.\n\n\ndata_all &lt;- data_all |&gt; \n  mutate(origin = case_when(origin == 1 ~ \"american\", \n                            origin == 2 ~ \"european\", \n                            origin == 3 ~ \"japanese\")) |&gt;\n  mutate(mpg = factor(mpg, levels = c(\"low\", \"high\")),\n         origin = factor(origin),\n         name = factor(name)) |&gt; \n  glimpse()\n\nRows: 392\nColumns: 9\n$ mpg          &lt;fct&gt; low, low, low, low, low, low, low, low, low, low, low, lo…\n$ cylinders    &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 6, 6, 6, 4, …\n$ displacement &lt;dbl&gt; 307, 350, 318, 304, 302, 429, 454, 440, 455, 390, 383, 34…\n$ horsepower   &lt;dbl&gt; 130, 165, 150, 150, 140, 198, 220, 215, 225, 190, 170, 16…\n$ weight       &lt;dbl&gt; 3504, 3693, 3436, 3433, 3449, 4341, 4354, 4312, 4425, 385…\n$ acceleration &lt;dbl&gt; 12.0, 11.5, 11.0, 12.0, 10.5, 10.0, 9.0, 8.5, 10.0, 8.5, …\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 7…\n$ origin       &lt;fct&gt; american, american, american, american, american, america…\n$ name         &lt;fct&gt; chevrolet chevelle malibu, buick skylark 320, plymouth sa…\n\n\n\n\nExplore responses for categorical variables\n\nOther than name, responses for all other variables are tidy\n\n\n\ndata_all |&gt; \n  select(where(is.factor)) |&gt;\n  walk(\\(column) print(levels(column)))\n\n[1] \"low\"  \"high\"\n[1] \"american\" \"european\" \"japanese\"\n  [1] \"amc ambassador brougham\"             \n  [2] \"amc ambassador dpl\"                  \n  [3] \"amc ambassador sst\"                  \n  [4] \"amc concord\"                         \n  [5] \"amc concord d/l\"                     \n  [6] \"amc concord dl 6\"                    \n  [7] \"amc gremlin\"                         \n  [8] \"amc hornet\"                          \n  [9] \"amc hornet sportabout (sw)\"          \n [10] \"amc matador\"                         \n [11] \"amc matador (sw)\"                    \n [12] \"amc pacer\"                           \n [13] \"amc pacer d/l\"                       \n [14] \"amc rebel sst\"                       \n [15] \"amc spirit dl\"                       \n [16] \"audi 100 ls\"                         \n [17] \"audi 100ls\"                          \n [18] \"audi 4000\"                           \n [19] \"audi 5000\"                           \n [20] \"audi 5000s (diesel)\"                 \n [21] \"audi fox\"                            \n [22] \"bmw 2002\"                            \n [23] \"bmw 320i\"                            \n [24] \"buick century\"                       \n [25] \"buick century 350\"                   \n [26] \"buick century limited\"               \n [27] \"buick century luxus (sw)\"            \n [28] \"buick century special\"               \n [29] \"buick electra 225 custom\"            \n [30] \"buick estate wagon (sw)\"             \n [31] \"buick lesabre custom\"                \n [32] \"buick opel isuzu deluxe\"             \n [33] \"buick regal sport coupe (turbo)\"     \n [34] \"buick skyhawk\"                       \n [35] \"buick skylark\"                       \n [36] \"buick skylark 320\"                   \n [37] \"buick skylark limited\"               \n [38] \"cadillac eldorado\"                   \n [39] \"cadillac seville\"                    \n [40] \"capri ii\"                            \n [41] \"chevroelt chevelle malibu\"           \n [42] \"chevrolet bel air\"                   \n [43] \"chevrolet camaro\"                    \n [44] \"chevrolet caprice classic\"           \n [45] \"chevrolet cavalier\"                  \n [46] \"chevrolet cavalier 2-door\"           \n [47] \"chevrolet cavalier wagon\"            \n [48] \"chevrolet chevelle concours (sw)\"    \n [49] \"chevrolet chevelle malibu\"           \n [50] \"chevrolet chevelle malibu classic\"   \n [51] \"chevrolet chevette\"                  \n [52] \"chevrolet citation\"                  \n [53] \"chevrolet concours\"                  \n [54] \"chevrolet impala\"                    \n [55] \"chevrolet malibu\"                    \n [56] \"chevrolet malibu classic (sw)\"       \n [57] \"chevrolet monte carlo\"               \n [58] \"chevrolet monte carlo landau\"        \n [59] \"chevrolet monte carlo s\"             \n [60] \"chevrolet monza 2+2\"                 \n [61] \"chevrolet nova\"                      \n [62] \"chevrolet nova custom\"               \n [63] \"chevrolet vega\"                      \n [64] \"chevrolet vega (sw)\"                 \n [65] \"chevrolet vega 2300\"                 \n [66] \"chevrolet woody\"                     \n [67] \"chevy c10\"                           \n [68] \"chevy c20\"                           \n [69] \"chevy s-10\"                          \n [70] \"chrysler cordoba\"                    \n [71] \"chrysler lebaron medallion\"          \n [72] \"chrysler lebaron salon\"              \n [73] \"chrysler lebaron town @ country (sw)\"\n [74] \"chrysler new yorker brougham\"        \n [75] \"chrysler newport royal\"              \n [76] \"datsun 1200\"                         \n [77] \"datsun 200-sx\"                       \n [78] \"datsun 200sx\"                        \n [79] \"datsun 210\"                          \n [80] \"datsun 210 mpg\"                      \n [81] \"datsun 280-zx\"                       \n [82] \"datsun 310\"                          \n [83] \"datsun 310 gx\"                       \n [84] \"datsun 510\"                          \n [85] \"datsun 510 (sw)\"                     \n [86] \"datsun 510 hatchback\"                \n [87] \"datsun 610\"                          \n [88] \"datsun 710\"                          \n [89] \"datsun 810\"                          \n [90] \"datsun 810 maxima\"                   \n [91] \"datsun b-210\"                        \n [92] \"datsun b210\"                         \n [93] \"datsun b210 gx\"                      \n [94] \"datsun f-10 hatchback\"               \n [95] \"datsun pl510\"                        \n [96] \"dodge aries se\"                      \n [97] \"dodge aries wagon (sw)\"              \n [98] \"dodge aspen\"                         \n [99] \"dodge aspen 6\"                       \n[100] \"dodge aspen se\"                      \n[101] \"dodge challenger se\"                 \n[102] \"dodge charger 2.2\"                   \n[103] \"dodge colt\"                          \n[104] \"dodge colt (sw)\"                     \n[105] \"dodge colt hardtop\"                  \n[106] \"dodge colt hatchback custom\"         \n[107] \"dodge colt m/m\"                      \n[108] \"dodge coronet brougham\"              \n[109] \"dodge coronet custom\"                \n[110] \"dodge coronet custom (sw)\"           \n[111] \"dodge d100\"                          \n[112] \"dodge d200\"                          \n[113] \"dodge dart custom\"                   \n[114] \"dodge diplomat\"                      \n[115] \"dodge magnum xe\"                     \n[116] \"dodge monaco (sw)\"                   \n[117] \"dodge monaco brougham\"               \n[118] \"dodge omni\"                          \n[119] \"dodge rampage\"                       \n[120] \"dodge st. regis\"                     \n[121] \"fiat 124 sport coupe\"                \n[122] \"fiat 124 tc\"                         \n[123] \"fiat 124b\"                           \n[124] \"fiat 128\"                            \n[125] \"fiat 131\"                            \n[126] \"fiat strada custom\"                  \n[127] \"fiat x1.9\"                           \n[128] \"ford country\"                        \n[129] \"ford country squire (sw)\"            \n[130] \"ford escort 2h\"                      \n[131] \"ford escort 4w\"                      \n[132] \"ford f108\"                           \n[133] \"ford f250\"                           \n[134] \"ford fairmont\"                       \n[135] \"ford fairmont (auto)\"                \n[136] \"ford fairmont (man)\"                 \n[137] \"ford fairmont 4\"                     \n[138] \"ford fairmont futura\"                \n[139] \"ford fiesta\"                         \n[140] \"ford futura\"                         \n[141] \"ford galaxie 500\"                    \n[142] \"ford gran torino\"                    \n[143] \"ford gran torino (sw)\"               \n[144] \"ford granada\"                        \n[145] \"ford granada ghia\"                   \n[146] \"ford granada gl\"                     \n[147] \"ford granada l\"                      \n[148] \"ford ltd\"                            \n[149] \"ford ltd landau\"                     \n[150] \"ford maverick\"                       \n[151] \"ford mustang\"                        \n[152] \"ford mustang gl\"                     \n[153] \"ford mustang ii\"                     \n[154] \"ford mustang ii 2+2\"                 \n[155] \"ford pinto\"                          \n[156] \"ford pinto (sw)\"                     \n[157] \"ford pinto runabout\"                 \n[158] \"ford ranger\"                         \n[159] \"ford thunderbird\"                    \n[160] \"ford torino\"                         \n[161] \"ford torino 500\"                     \n[162] \"hi 1200d\"                            \n[163] \"honda accord\"                        \n[164] \"honda accord cvcc\"                   \n[165] \"honda accord lx\"                     \n[166] \"honda civic\"                         \n[167] \"honda civic (auto)\"                  \n[168] \"honda civic 1300\"                    \n[169] \"honda civic 1500 gl\"                 \n[170] \"honda civic cvcc\"                    \n[171] \"honda prelude\"                       \n[172] \"maxda glc deluxe\"                    \n[173] \"maxda rx3\"                           \n[174] \"mazda 626\"                           \n[175] \"mazda glc\"                           \n[176] \"mazda glc 4\"                         \n[177] \"mazda glc custom\"                    \n[178] \"mazda glc custom l\"                  \n[179] \"mazda glc deluxe\"                    \n[180] \"mazda rx-4\"                          \n[181] \"mazda rx-7 gs\"                       \n[182] \"mazda rx2 coupe\"                     \n[183] \"mercedes benz 300d\"                  \n[184] \"mercedes-benz 240d\"                  \n[185] \"mercedes-benz 280s\"                  \n[186] \"mercury capri 2000\"                  \n[187] \"mercury capri v6\"                    \n[188] \"mercury cougar brougham\"             \n[189] \"mercury grand marquis\"               \n[190] \"mercury lynx l\"                      \n[191] \"mercury marquis\"                     \n[192] \"mercury marquis brougham\"            \n[193] \"mercury monarch\"                     \n[194] \"mercury monarch ghia\"                \n[195] \"mercury zephyr\"                      \n[196] \"mercury zephyr 6\"                    \n[197] \"nissan stanza xe\"                    \n[198] \"oldsmobile cutlass ciera (diesel)\"   \n[199] \"oldsmobile cutlass ls\"               \n[200] \"oldsmobile cutlass salon brougham\"   \n[201] \"oldsmobile cutlass supreme\"          \n[202] \"oldsmobile delta 88 royale\"          \n[203] \"oldsmobile omega\"                    \n[204] \"oldsmobile omega brougham\"           \n[205] \"oldsmobile starfire sx\"              \n[206] \"oldsmobile vista cruiser\"            \n[207] \"opel 1900\"                           \n[208] \"opel manta\"                          \n[209] \"peugeot 304\"                         \n[210] \"peugeot 504\"                         \n[211] \"peugeot 504 (sw)\"                    \n[212] \"peugeot 505s turbo diesel\"           \n[213] \"peugeot 604sl\"                       \n[214] \"plymouth 'cuda 340\"                  \n[215] \"plymouth arrow gs\"                   \n[216] \"plymouth champ\"                      \n[217] \"plymouth cricket\"                    \n[218] \"plymouth custom suburb\"              \n[219] \"plymouth duster\"                     \n[220] \"plymouth fury\"                       \n[221] \"plymouth fury gran sedan\"            \n[222] \"plymouth fury iii\"                   \n[223] \"plymouth grand fury\"                 \n[224] \"plymouth horizon\"                    \n[225] \"plymouth horizon 4\"                  \n[226] \"plymouth horizon miser\"              \n[227] \"plymouth horizon tc3\"                \n[228] \"plymouth reliant\"                    \n[229] \"plymouth sapporo\"                    \n[230] \"plymouth satellite\"                  \n[231] \"plymouth satellite custom\"           \n[232] \"plymouth satellite custom (sw)\"      \n[233] \"plymouth satellite sebring\"          \n[234] \"plymouth valiant\"                    \n[235] \"plymouth valiant custom\"             \n[236] \"plymouth volare\"                     \n[237] \"plymouth volare custom\"              \n[238] \"plymouth volare premier v8\"          \n[239] \"pontiac astro\"                       \n[240] \"pontiac catalina\"                    \n[241] \"pontiac catalina brougham\"           \n[242] \"pontiac firebird\"                    \n[243] \"pontiac grand prix\"                  \n[244] \"pontiac grand prix lj\"               \n[245] \"pontiac j2000 se hatchback\"          \n[246] \"pontiac lemans v6\"                   \n[247] \"pontiac phoenix\"                     \n[248] \"pontiac phoenix lj\"                  \n[249] \"pontiac safari (sw)\"                 \n[250] \"pontiac sunbird coupe\"               \n[251] \"pontiac ventura sj\"                  \n[252] \"renault 12 (sw)\"                     \n[253] \"renault 12tl\"                        \n[254] \"renault 5 gtl\"                       \n[255] \"saab 99e\"                            \n[256] \"saab 99gle\"                          \n[257] \"saab 99le\"                           \n[258] \"subaru\"                              \n[259] \"subaru dl\"                           \n[260] \"toyota carina\"                       \n[261] \"toyota celica gt\"                    \n[262] \"toyota celica gt liftback\"           \n[263] \"toyota corolla\"                      \n[264] \"toyota corolla 1200\"                 \n[265] \"toyota corolla 1600 (sw)\"            \n[266] \"toyota corolla liftback\"             \n[267] \"toyota corolla tercel\"               \n[268] \"toyota corona\"                       \n[269] \"toyota corona hardtop\"               \n[270] \"toyota corona liftback\"              \n[271] \"toyota corona mark ii\"               \n[272] \"toyota cressida\"                     \n[273] \"toyota mark ii\"                      \n[274] \"toyota starlet\"                      \n[275] \"toyota tercel\"                       \n[276] \"toyouta corona mark ii (sw)\"         \n[277] \"triumph tr7 coupe\"                   \n[278] \"vokswagen rabbit\"                    \n[279] \"volkswagen 1131 deluxe sedan\"        \n[280] \"volkswagen 411 (sw)\"                 \n[281] \"volkswagen dasher\"                   \n[282] \"volkswagen jetta\"                    \n[283] \"volkswagen model 111\"                \n[284] \"volkswagen rabbit\"                   \n[285] \"volkswagen rabbit custom\"            \n[286] \"volkswagen rabbit custom diesel\"     \n[287] \"volkswagen rabbit l\"                 \n[288] \"volkswagen scirocco\"                 \n[289] \"volkswagen super beetle\"             \n[290] \"volkswagen type 3\"                   \n[291] \"volvo 144ea\"                         \n[292] \"volvo 145e (sw)\"                     \n[293] \"volvo 244dl\"                         \n[294] \"volvo 245\"                           \n[295] \"volvo 264gl\"                         \n[296] \"volvo diesel\"                        \n[297] \"vw dasher (diesel)\"                  \n[298] \"vw pickup\"                           \n[299] \"vw rabbit\"                           \n[300] \"vw rabbit c (diesel)\"                \n[301] \"vw rabbit custom\"                    \n\n\n\nname has many different responses\nWe won’t know how to handle this until we get to later units in the class on natural language processing!\nRemove name\n\n\ndata_all &lt;- data_all |&gt; \n  select(-name)\n\n\nFinally, let’s make and save our training and validation sets. If we were doing model building for prediction we would also need a test set but we will focus this unit on just selecting the best model but not rigorously evaluating it.\n\nLet’s use a 75/25 split, stratified on mpg\nDon’t forget to set a seed in case you need to resplit again in the future!\n\n\nset.seed(20110522) \n\nsplits &lt;- data_all |&gt; \n  initial_split(prop = .75, strata = \"mpg\")\n\nsplits |&gt; \n  analysis() |&gt; \n  glimpse() |&gt; \n  write_csv(here::here(path_data, \"auto_trn.csv\"))\n\nRows: 294\nColumns: 8\n$ mpg          &lt;fct&gt; high, high, high, high, high, high, high, high, high, hig…\n$ cylinders    &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ displacement &lt;dbl&gt; 113.0, 97.0, 97.0, 110.0, 107.0, 104.0, 121.0, 97.0, 140.…\n$ horsepower   &lt;dbl&gt; 95, 88, 46, 87, 90, 95, 113, 88, 90, 95, 86, 90, 70, 76, …\n$ weight       &lt;dbl&gt; 2372, 2130, 1835, 2672, 2430, 2375, 2234, 2130, 2264, 222…\n$ acceleration &lt;dbl&gt; 15.0, 14.5, 20.5, 17.5, 14.5, 17.5, 12.5, 14.5, 15.5, 14.…\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 7…\n$ origin       &lt;fct&gt; japanese, japanese, european, european, european, europea…\n\nsplits |&gt; \n  assessment() |&gt; \n  glimpse() |&gt; \n  write_csv(here::here(path_data, \"auto_val.csv\"))\n\nRows: 98\nColumns: 8\n$ mpg          &lt;fct&gt; low, low, low, low, low, low, low, low, low, low, low, lo…\n$ cylinders    &lt;dbl&gt; 8, 8, 8, 8, 8, 6, 6, 6, 6, 8, 8, 4, 4, 4, 8, 8, 8, 4, 8, …\n$ displacement &lt;dbl&gt; 350, 304, 302, 440, 455, 198, 200, 225, 250, 400, 318, 14…\n$ horsepower   &lt;dbl&gt; 165, 150, 140, 215, 225, 95, 85, 105, 100, 175, 150, 72, …\n$ weight       &lt;dbl&gt; 3693, 3433, 3449, 4312, 4425, 2833, 2587, 3439, 3329, 446…\n$ acceleration &lt;dbl&gt; 11.5, 12.0, 10.5, 8.5, 10.0, 15.5, 16.0, 15.5, 15.5, 11.5…\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 72, 7…\n$ origin       &lt;fct&gt; american, american, american, american, american, america…\n\n\n[Any concerns about using this training-validation split?].{red} 1\n\n\n\n4.4.2 Modeling EDA\nLet’s do some quick modeling EDA to get a sense of the data. We will keep it quick and dirty.\n\nOpen train ( we don’t need validate for modeling EDA)\n[We are pretending this is a new script….]\nLet’s make sure to class all variable appropriately\n\n\ndata_trn &lt;- read_csv(here::here(path_data, \"auto_trn.csv\"),\n                     col_type = cols()) |&gt; \n  mutate(mpg = factor (mpg, levels  = c(\"low\", \"high\")),\n         origin = factor(origin)) |&gt; \n  glimpse()\n\nRows: 294\nColumns: 8\n$ mpg          &lt;fct&gt; high, high, high, high, high, high, high, high, high, hig…\n$ cylinders    &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ displacement &lt;dbl&gt; 113.0, 97.0, 97.0, 110.0, 107.0, 104.0, 121.0, 97.0, 140.…\n$ horsepower   &lt;dbl&gt; 95, 88, 46, 87, 90, 95, 113, 88, 90, 95, 86, 90, 70, 76, …\n$ weight       &lt;dbl&gt; 2372, 2130, 1835, 2672, 2430, 2375, 2234, 2130, 2264, 222…\n$ acceleration &lt;dbl&gt; 15.0, 14.5, 20.5, 17.5, 14.5, 17.5, 12.5, 14.5, 15.5, 14.…\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 7…\n$ origin       &lt;fct&gt; japanese, japanese, european, european, european, europea…\n\n\n\n\nBar plot for outcome\n\nOutcome is balanced\nUnbalanced outcomes can be more complicated (more on this later)\n\n\n\ndata_trn |&gt; plot_bar(\"mpg\")\n\n\n\n\n\n\nGrouped (by mpg) box/violin plots for numeric predictors\n\n\ndata_trn |&gt; \n  select(where(is.numeric)) |&gt; \n  names() |&gt; \n  map(~ plot_grouped_box_violin(df = data_trn, x = \"mpg\", y = .x)) |&gt; \n  plot_grid(plotlist = _, ncol = 2)\n\n\n\n\n\n\nGrouped barplot for origin\n\n\ndata_trn |&gt; \n  plot_grouped_barplot_percent(x = \"origin\", y = \"mpg\")\n\n\n\n\n\ndata_trn |&gt; \n  plot_grouped_barplot_percent(x = \"mpg\", y = \"origin\")\n\n\n\n\n\n\nCorrelation plots for numeric\nCan dummy code categorical predictors to examine correlations (point biserial, phi) including them\nWe will do this manually using tidyverse here.\nSet american to be reference level (its the first level)\n\n\ndata_trn |&gt; \n1  mutate(mpg_high = if_else(mpg == \"high\", 1, 0),\n         origin_japan = if_else(origin == \"japanese\", 1, 0),\n         origin_europe = if_else(origin == \"european\", 1, 0)) |&gt; \n2  select(-origin, -mpg) |&gt;\n  cor() |&gt; \n3  corrplot::corrplot.mixed()\n\n\n1\n\nIf manually coding an binary outcome variable, best practice is to set the positive class to be 1\n\n2\n\nRemove origin after convert to dummy-coded features\n\n3\n\nuse namespace:: because didnt load the corrplot package\n\n\n\n\n\n\n\n\ncylinders, displacement, horsepower, and weight are all essentially the same construct (big, beefy car!)\nAll are strongly correlated with mpg"
  },
  {
    "objectID": "004_classification.html#logistic-regression---model-building",
    "href": "004_classification.html#logistic-regression---model-building",
    "title": "4  Introduction to Classification Models",
    "section": "4.5 Logistic Regression - Model Building",
    "text": "4.5 Logistic Regression - Model Building\nNow that we understand our data a little better, let’s build some models\nLet’s also try to simultaneously pretend that we:\n\nAre building and selecting a best prediction model that will be evaluated with some additional held out test data\nHave an explanatory question about production year - Are we more likely to have efficient cars more recently because of improvements in “technology”, above and beyond broad car characteristics (e.g., the easy stuff like weight, displacement, etc.)\n\nTo be clear, prediction and explanation goals are often separate (though prediction is an important foundation explanation)\nEither way, we need a validation set\n\nOpen it and class variables\nPerhaps we should have written a function for this? (I would if we were going to do this any more times!)\n\n\ndata_val &lt;- read_csv(here::here(path_data, \"auto_val.csv\"),\n                     col_type = cols()) |&gt; \n  mutate(mpg = factor (mpg, levels = c(\"low\", \"high\")),\n         origin = factor(origin)) |&gt; \n  glimpse()\n\nRows: 98\nColumns: 8\n$ mpg          &lt;fct&gt; low, low, low, low, low, low, low, low, low, low, low, lo…\n$ cylinders    &lt;dbl&gt; 8, 8, 8, 8, 8, 6, 6, 6, 6, 8, 8, 4, 4, 4, 8, 8, 8, 4, 8, …\n$ displacement &lt;dbl&gt; 350, 304, 302, 440, 455, 198, 200, 225, 250, 400, 318, 14…\n$ horsepower   &lt;dbl&gt; 165, 150, 140, 215, 225, 95, 85, 105, 100, 175, 150, 72, …\n$ weight       &lt;dbl&gt; 3693, 3433, 3449, 4312, 4425, 2833, 2587, 3439, 3329, 446…\n$ acceleration &lt;dbl&gt; 11.5, 12.0, 10.5, 8.5, 10.0, 15.5, 16.0, 15.5, 15.5, 11.5…\n$ year         &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 72, 7…\n$ origin       &lt;fct&gt; american, american, american, american, american, america…\n\n\n\nLet’s predict high mpg from car beefiness and year\nWe likely want to combine the beefy variables into one construct/factor (beef)\n\nWe can use PCA to extract one factor\nInput variables should be centered and scaled for PCA. Easy peasy\nPCA will be calculated with data_trn during prep(). These statistics from data_trn will be used to bake()\n\n\n1rec &lt;- recipe(mpg ~ ., data = data_trn) |&gt;\n  step_pca(cylinders, displacement, horsepower, weight, \n           options = list(center = TRUE, scale. = TRUE), \n           num_comp = 1, \n2           prefix = \"beef_\")\n\n3rec\n\n\n1\n\nUsing . now in the recipe to leave all variables in the feature matrix. We can later select the ones we want use for prediction\n\n2\n\nWe will have one PCA component, it will be called beef_1\n\n3\n\nNotice that right now we have 7 predictors. We will need to remember to select down to only beef_ and year\n\n\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 7\n\n\n\n\n\n── Operations \n\n\n• PCA extraction with: cylinders, displacement, horsepower, weight\n\n\nNow, prep() the recipe and bake() some feature for training and validation sets\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\nLets skim our training features\nWe can see there are features in there we won’t use\nmpg, year, and beef_1 look good\n\n\nfeat_trn |&gt; skim_all()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nfeat_trn\n\n\n\n\nNumber of rows\n\n\n294\n\n\n\n\nNumber of columns\n\n\n5\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n2\n\n\n\n\nnumeric\n\n\n3\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\norigin\n\n\n0\n\n\n1\n\n\n3\n\n\name: 187, jap: 56, eur: 51\n\n\n\n\nmpg\n\n\n0\n\n\n1\n\n\n2\n\n\nlow: 147, hig: 147\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nskew\n\n\nkurtosis\n\n\n\n\n\n\nacceleration\n\n\n0\n\n\n1\n\n\n15.54\n\n\n2.64\n\n\n8.00\n\n\n14.00\n\n\n15.50\n\n\n17.0\n\n\n24.80\n\n\n0.24\n\n\n0.58\n\n\n\n\nyear\n\n\n0\n\n\n1\n\n\n75.94\n\n\n3.68\n\n\n70.00\n\n\n73.00\n\n\n76.00\n\n\n79.0\n\n\n82.00\n\n\n0.03\n\n\n-1.17\n\n\n\n\nbeef_1\n\n\n0\n\n\n1\n\n\n0.00\n\n\n1.92\n\n\n-2.36\n\n\n-1.64\n\n\n-0.85\n\n\n1.4\n\n\n4.36\n\n\n0.63\n\n\n-0.96\n\n\n\n\n\n\n\n\nFit the model configuration in train\n\nfit_lr_2 &lt;- \n1  logistic_reg() |&gt;\n2  set_engine(\"glm\") |&gt;\n3  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\n1\n\ncategory of algorithm is logistics regression\n\n2\n\nSet engine to be generalized linear model. No need to `set_mode(“classification”) because logistic regression glm is only used for classification\n\n3\n\nNotice that we explicitly indicate which features to use as predictors because our feature matrix has more than just these two in it because we used . in our recipe and our datasets have more columns.\n\n\n\n\n\nLet’s look at the logistic model and its parameter estimates from train\n\nfit_lr_2 |&gt; tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -23.4      5.55       -4.23 2.38e- 5\n2 beef_1        -2.49     0.316      -7.87 3.49e-15\n3 year           0.292    0.0718      4.07 4.71e- 5\n\n\n\\(Pr(high|(beef, year)) = \\frac{e^{b0 + b1*beef + b2*year}}{1 + e^{b0 + b1*beef + b2*year}}\\)\n\nUse predict() to get these probabilities (or predicted class, more on that later) directly from the model for any data\n\nLet’s get \\(Pr(high|year)\\) holding beef constant at its mean\nLet’s also get the 95% conf int around these probabilities\npredict() returns probabilities for high and low. We will select out the low columns\nAmbiguity is removed by their choice to label by the actual outcome labels (Nice!)\n\n\n1p_high &lt;-\n  tibble(year = seq(min(feat_val$year), max(feat_val$year), .1),\n         beef_1 = mean(feat_val$beef_1)) \n\n2p_high &lt;- p_high |&gt;\n  bind_cols(predict(fit_lr_2, new_data = p_high, type = \"prob\")) |&gt; \n  bind_cols(predict(fit_lr_2, new_data = p_high, type = \"conf_int\")) |&gt; \n  select(-.pred_low, -.pred_lower_low, -.pred_upper_low) |&gt; \n  glimpse()\n\n\n1\n\nFirst, make a dataframe with feature values to get predictions\n\n2\n\nThen bind in predictions and remove unnecessary columns\n\n\n\n\nRows: 121\nColumns: 5\n$ year             &lt;dbl&gt; 70.0, 70.1, 70.2, 70.3, 70.4, 70.5, 70.6, 70.7, 70.8,…\n$ beef_1           &lt;dbl&gt; 0.008438869, 0.008438869, 0.008438869, 0.008438869, 0…\n$ .pred_high       &lt;dbl&gt; 0.04733721, 0.04867270, 0.05004389, 0.05145161, 0.052…\n$ .pred_lower_high &lt;dbl&gt; 0.01495325, 0.01557265, 0.01621651, 0.01688572, 0.017…\n$ .pred_upper_high &lt;dbl&gt; 0.1398943, 0.1419807, 0.1440989, 0.1462495, 0.1484331…\n\n\nCan plot this relationship using the predicted probabilities\n\nggplot() +\n  geom_point(data = feat_val, aes(x = year, y = 2 - as.numeric(mpg)), \n             position = position_jitter(height = 0.05, width = 0.05)) +\n  geom_line(data = p_high, mapping = aes(x = year, y = .pred_high)) +\n  geom_ribbon(data = p_high, \n              aes(x = year, ymin = .pred_lower_high, ymax = .pred_upper_high), \n              linetype = 2, alpha = 0.1) +\n  scale_x_continuous(name = \"Production Year (19XX)\", breaks = seq(70,82,2)) +\n  ylab(\"Pr(High MPG)\")\n\n\n\n\n\nCan plot odds instead of probabilities using odds equation and coefficients from the model\n\\(odds = \\frac{Pr(positive\\:class|x_1)} {1 - Pr(positive\\:class|x_1)}\\)\nNOTE: Harder to also plot raw data as scatter plot when plotting odds vs. probabilities. Maybe use second Y axis?\n\np_high &lt;- p_high |&gt; \n  mutate(.odds_high = .pred_high / (1 - .pred_high),\n         .odds_lower_high = .pred_lower_high / (1 - .pred_lower_high),\n         .odds_upper_high = .pred_upper_high / (1 - .pred_upper_high))\nggplot() +\n  geom_line(data = p_high, mapping = aes(x = year, y = .odds_high)) +\n  geom_ribbon(data = p_high, \n              aes(x = year, ymin = .odds_lower_high, ymax = .odds_upper_high), \n              linetype = 2, alpha = 0.1) +\n  scale_x_continuous(name = \"Production Year (19XX)\", breaks = seq(70,82,2)) +\n  ylab(\"Odds(High MPG)\")\n\n\n\n\n\nOdds ratio for year:\n\\(Odds\\:ratio = e^{c*\\beta_1}\\)\n\nGet the parameter estimate/coefficient from the model\n\n\nexp(get_estimate(fit_lr_2, \"year\"))\n\n[1] 1.33943\n\n\nFor every one year increase (holding beef constant), the odds of a car being categorized as high mpg increase by a factor of 1.3394301\n\nYou can do this for other values of c as well (not really needed here because a 1 year unit makes sense)\n\\(Odds\\:ratio = e^{c*\\beta_1}\\)\n\nexp(10 * get_estimate(fit_lr_2, \"year\"))\n\n[1] 18.58663\n\n\nFor every 10 year increase (holding beef constant), the odds of a car being categorized as high mpg increase by a factor of 18.5866278\n\nTesting parameter estimates\n\ntidy() provides z-tests for the parameter estimates\nThis is not the recommended statistical test\n\n\nfit_lr_2 |&gt; tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -23.4      5.55       -4.23 2.38e- 5\n2 beef_1        -2.49     0.316      -7.87 3.49e-15\n3 year           0.292    0.0718      4.07 4.71e- 5\n\n\nThe preferred test for parameter estimates from logistic regression is the likelihood ratio test\n\nYou can get this using Anova() from the car package (as you learn in 610/710)\nThe glm object is returned using $fit\n\n\ncar::Anova(fit_lr_2$fit, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\n       LR Chisq Df Pr(&gt;Chisq)    \nbeef_1  239.721  1  &lt; 2.2e-16 ***\nyear     20.317  1  6.562e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAccuracy is a very common performance metric for classification models\nHow accurate is our two feature model?\n\nNote the use of type = \"class\"\n\n\n# in train\naccuracy_vec(feat_trn$mpg, predict(fit_lr_2, feat_trn, type = \"class\")$.pred_class)\n\n[1] 0.9217687\n\n# in validate\naccuracy_vec(feat_val$mpg, predict(fit_lr_2, feat_val, type = \"class\")$.pred_class)\n\n[1] 0.8877551\n\n\nYou can see some evidence of over-fitting to the training set in that model performance is a bit lower in validation\n\nLet’s take a look at the decision boundary for this model in the validation set\nWe need a function to plot the decision boundary because we will use it repeatedly to compare decision boundaries across statistical models\n\nplot_decision_boundary &lt;- function(data, model, x_names, y_name, n_points = 100) {\n  \n  preds &lt;- crossing(X1 = seq(min(data[[x_names[1]]]), \n                                 max(data[[x_names[1]]]), \n                                 length = n_points),\n                   X2 = seq(min(data[[x_names[2]]]), \n                                 max(data[[x_names[2]]]), \n                                 length = n_points))\n  names(preds) &lt;- x_names\n  preds[[y_name]] &lt;- predict(model, preds)$.pred_class\n  preds[[y_name]] &lt;- as.numeric(preds[[y_name]])-1\n  \n  ggplot(data = data, \n         aes(x = .data[[x_names[1]]], \n             y = .data[[x_names[2]]], \n             color = .data[[y_name]])) +\n    geom_contour(data = preds, \n                 aes(x = .data[[x_names[1]]], \n                     y = .data[[x_names[2]]], \n                     z = .data[[y_name]]), \n                 color = \"black\", breaks = .5, linewidth = 2) +\n    geom_point(size = 2, alpha = .5) +\n    labs(x = x_names[1], y = x_names[2], color = y_name)\n}\n\n\nLogistic regression produces a linear decision boundary when you consider a scatter plot of the points by the two features. Points on one side of the line are assigned to one class and points on the other side of the line are assigned to the other class. This decision boundary would be a plane if there were three features Harder to visualize in higher dimensional space.\nWe will contrast this decision boundary from logistic regression with other statistical algorithms in a bit.\nHere is the decision boundary for this model in both train and validation sets\n\np_train &lt;- feat_trn |&gt; \n  plot_decision_boundary(fit_lr_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\np_val &lt;- feat_val |&gt; \n  plot_decision_boundary(fit_lr_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\nplot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)\n\n\n\n\n\nWhat if you wanted to try to improve this models predictions?\nWhat if you wanted to have the best set of covariates to test the effect of year. [Assuming the best covariates are those that account for the most variance in mpg]?\nFor either prediction or explanation, you need to find this best model\nWe can compare model performance in validation set to find this best model\n\nWe can use that one for our prediction goal\nWe can test the effect of year in that model for our explanation goal\n\nThis is a principled way to decide on the best model for our explanatory goal (vs. p-hacking)\nWe get to explore and we end up with the best model to provide our focal test\n\n\nLet’s quickly fit another model we might have considered. This model will contain the 4 variables from our PCA but as individual features rather than one PCA component score (beef)\n\nMake features for this model - No feature engineering needed because raw variables are all numeric already) - We will give the features dfs new names to retain the old features that included beef_1\n\nrec_raw &lt;- recipe(mpg ~ ., data = data_trn) \n\nrec_raw_prep &lt;- rec_raw |&gt; \n  prep(data_trn)\n\nfeat_raw_trn &lt;- rec_raw_prep |&gt; \n  bake(data_trn)\n\nfeat_raw_val &lt;- rec_raw_prep |&gt; \n  bake(data_val)\n\nFit PCA features individually + year\n\nfit_lr_raw &lt;- \n  logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(mpg ~ cylinders + displacement + horsepower + weight + year, \n      data = feat_raw_trn)\n\n\nWhich is the best prediction model?\n\nBeef PCA\n\n\naccuracy_vec(feat_val$mpg, predict(fit_lr_2, feat_val)$.pred_class)\n\n[1] 0.8877551\n\n\n\nIndividual raw features from PCA\n\n\naccuracy_vec(feat_raw_val$mpg, predict(fit_lr_raw, feat_raw_val)$.pred_class)\n\n[1] 0.8673469\n\n\nThe PCA beef model fits best. The model with individual features likely increases overfitting a bit but doesn’t yield a reduction in bias because all the other variables are so highly correlated.\n\nfit_lr_2 is your choice for best model for prediction (at least it is descriptively better)\nIf you had an explanatory question about year, how would you have chosen between these two tests of year in these two different but all reasonable models\n\nYou might have chose the model with individual features because the effect of year is stronger. That is NOT the model that comes closes to the DGP. We believe the appropriate model is the beef model that has higher overall accuracy!\n\n\ncar::Anova(fit_lr_2$fit, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\n       LR Chisq Df Pr(&gt;Chisq)    \nbeef_1  239.721  1  &lt; 2.2e-16 ***\nyear     20.317  1  6.562e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(fit_lr_raw$fit, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\n             LR Chisq Df Pr(&gt;Chisq)    \ncylinders      0.4199  1    0.51697    \ndisplacement   1.3239  1    0.24990    \nhorsepower     6.4414  1    0.01115 *  \nweight        15.6669  1  7.553e-05 ***\nyear          27.0895  1  1.942e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis is a start for us to start to consider the use of resampling methods to make decisions about how to best pursue explanatory goals.\nCould you now test your year effect in the full sample? Let’s discuss."
  },
  {
    "objectID": "004_classification.html#k-nearest-neighbors",
    "href": "004_classification.html#k-nearest-neighbors",
    "title": "4  Introduction to Classification Models",
    "section": "4.6 K nearest neighbors",
    "text": "4.6 K nearest neighbors\n\n4.6.1 Conceptual Overview\nLet’s switch gears to a non-parametric method we already know. KNN can be used as a classifier as well as for regression problems\nKNN tries to determine conditional class possibilities for any set of features by looking at observed classes for similar values of for these features in the training set\n\n\n\n\n\n\nThe above figure illustrates the application of 3-NN to a small sample training set (N = 12) with 2 predictors\n\nFor test observation X in the left panel, we would predict class = blue because blue is the majority class (highest probability) among the 3 nearest training observations\nIf we calculated these probabilities for all possible combinations of the two predictors in the training set, it would yield the decision boundaries depicted in the right panel\n\n\nKNN can produce complex decision boundaries\n\nThis makes it flexible (can reduce bias)\nThis makes it susceptible to variance/overfitting problems\n\nRemember that we can control this bias-variance trade-off with K. As K increases, variance reduces (but bias may increase). As K decreases, bias may be reduced but variance increases. Choose a K that produces good performance in new (validation or test) data\n\nThese figures depict the KNN (and Bayes classifier) decision boundaries for the earlier simulated 2 class problem with X1 and X2\n\nK = 10 appears to provide the sweet spot b/c it closely approximates the Bayes decision boundary\nOf course, you wouldn’t know the true Bayes decision boundary if the data were real (not simulated)\nBut K = 10 would also yield the lowest test error (which is how it should be chosen)\n\nBayes classifier test error: .1304\nK = 10 test error: 1363\nK = 1 test error: .1695\nK = 100 test err: .1925\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can NOT make the decision about K based on training error\nThis figure depicts training and test error for simulated data example as function of 1/K\n\nTraining error decreases as 1/K increases. At 1 (K=1) training error is 0\nTest error shows expected inverted U\n\nFor high K (left side), error is high because of high variance\nAs move right (lower K), variance is reduced rapidly with little increase in bias. Error is reduced.\nEventually, there is diminishing return from reducing variance but bias starts to increase rapidly. Error increases again.\n\n\n\n\n\n\n\n\n\n4.6.2 A Return to Cars\nLet’s demonstrate KNN using the Cars dataset\n\nCalculate beef_1 PCA component\nScale both features\nMake train and validation feature matrices\n\n\nrec &lt;- recipe(mpg ~ ., data = data_trn) |&gt; \n  step_pca(cylinders, displacement, horsepower, weight,\n           options = list(center = TRUE, scale. = TRUE), \n           num_comp = 1, prefix = \"beef_\") |&gt; \n  step_scale(year, beef_1)\n\nrec_prep &lt;- rec |&gt; \n  prep(data = data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\nFit models with varying K.\n\n# K = 1\nfit_1nn_2 &lt;- \n  nearest_neighbor(neighbors = 1) |&gt; \n  set_engine(\"kknn\") |&gt;\n1  set_mode(\"classification\") |&gt;\n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n# K = 5\nfit_5nn_2 &lt;- \n  nearest_neighbor(neighbors = 5) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;   \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n# K = 10\nfit_10nn_2 &lt;- \n  nearest_neighbor(neighbors = 10) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;   \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n# K = 20\nfit_20nn_2 &lt;- \n  nearest_neighbor(neighbors = 20) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;   \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\n1\n\nNotice that we now use set_mode(\"classification\")\n\n\n\n\n\nOf course, training accuracy goes down with decreasing flexibility as k increases\n\n# K = 1\naccuracy_vec(feat_trn$mpg, predict(fit_1nn_2, feat_trn)$.pred_class)\n\n[1] 1\n\n# K = 5\naccuracy_vec(feat_trn$mpg, predict(fit_5nn_2, feat_trn)$.pred_class)\n\n[1] 0.9489796\n\n# K = 10\naccuracy_vec(feat_trn$mpg, predict(fit_10nn_2, feat_trn)$.pred_class)\n\n[1] 0.9455782\n\n# K = 20\naccuracy_vec(feat_trn$mpg, predict(fit_20nn_2, feat_trn)$.pred_class)\n\n[1] 0.9285714\n\n\nIn contrast, validation accuracy first increases and then eventually decreases as k increase.\nK = 10 is preferred based on validation accuracy\n\n# K = 1\naccuracy_vec(feat_val$mpg, predict(fit_1nn_2, feat_val)$.pred_class)\n\n[1] 0.8163265\n\n# K = 5\naccuracy_vec(feat_val$mpg, predict(fit_5nn_2, feat_val)$.pred_class)\n\n[1] 0.877551\n\n# K = 10\naccuracy_vec(feat_val$mpg, predict(fit_10nn_2, feat_val)$.pred_class)\n\n[1] 0.8877551\n\n# K = 20\naccuracy_vec(feat_val$mpg, predict(fit_20nn_2, feat_val)$.pred_class)\n\n[1] 0.8673469\n\n\n\nLet’s look at the decision boundaries for 10-NN in training and validation sets\n\nA very complex decision boundary\nClearly trying hard to segregate the points\nIn Ames, the relationships were non-linear and therefore KNN did much better than the linear model\nHere, the decision boundary is pretty linear so the added flexibility of KNN doesn’t get us much.\n\nMaybe we gain a little in bias reduction but lose a little in overfitting\nEnds up performing comparable to logistic regression (a generalized linear model)\n\n\n\np_train &lt;- feat_trn |&gt; \n  plot_decision_boundary(fit_10nn_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\")\n\np_val &lt;- feat_val |&gt; \n  plot_decision_boundary(fit_10nn_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\")\n\nplot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)\n\n\n\n\n\n[How do you think the parametric logistic regression compares to the non-parametric KNN with respect to explanatory goals? Consider our (somewhat artificial) question about the effect of year.].{red} 2\n\nPlot of probability of high_mpg by year, holding beef_1 constant at its mean based on 10-NN\n\nGet \\(Pr(high|year)\\) holding beef constant at its mean\npredict() returns probabilities for high and low.\n\n\np_high &lt;- \n  tibble(year = seq(min(feat_val$year), max(feat_val$year), .1),\n         beef_1 = mean(feat_val$beef_1))\np_high &lt;- p_high |&gt; \n  bind_cols(predict(fit_10nn_2, p_high, type = \"prob\")) |&gt; \n  glimpse()\n\nRows: 33\nColumns: 4\n$ year       &lt;dbl&gt; 18.99759, 19.09759, 19.19759, 19.29759, 19.39759, 19.49759,…\n$ beef_1     &lt;dbl&gt; 0.004400185, 0.004400185, 0.004400185, 0.004400185, 0.00440…\n$ .pred_low  &lt;dbl&gt; 0.96, 0.99, 0.99, 0.99, 0.97, 0.87, 0.81, 0.81, 0.81, 0.81,…\n$ .pred_high &lt;dbl&gt; 0.04, 0.01, 0.01, 0.01, 0.03, 0.13, 0.19, 0.19, 0.19, 0.19,…\n\n\n\nggplot() +\n  geom_point(data = feat_val, aes(x = year, y = 2 - as.numeric(mpg)), \n             position = position_jitter(height = 0.05, width = 0.05)) +\n  geom_line(data = p_high, mapping = aes(x = year, y = .pred_high)) +\n  scale_x_continuous(name = \"Production Year (19XX)\", \n                     breaks = seq(0, 1, length.out = 7), \n                     labels = as.character(seq(70, 82, length.out = 7))) +\n  ylab(\"Pr(High MPG)\")\n\n\n\n\nIn a later unit, we will learn about feature ablation that we can combine with model comparisons to potentially test predictor effects in non-parametric models"
  },
  {
    "objectID": "004_classification.html#linear-and-quadratic-discriminant-analysis",
    "href": "004_classification.html#linear-and-quadratic-discriminant-analysis",
    "title": "4  Introduction to Classification Models",
    "section": "4.7 Linear and Quadratic Discriminant Analysis",
    "text": "4.7 Linear and Quadratic Discriminant Analysis\n\n4.7.1 Linear Discriminant Analysis\nLDA models the distributions of the Xs separately for each class\nThen uses Bayes theorem to estimate \\(Pr(Y = k | X)\\) for each k and assigns the observation to the class with the highest probability\n\\(Pr(Y = k|X) = \\frac{\\pi_k * f_k(X)}{\\sum_{l = 1}^{K} f_l(X)}\\)\nwhere\n\n\\(\\pi_k\\) is the prior probability that an observation comes from class k (estimated from frequencies of k in training)\n\\(f_k(X)\\) is the density function of X for an observation from class k\n\n\\(f_k(X)\\) is large if there is a high probability that an observation in class k has that set of values for X and small if that probability is low\n\\(f_k(X)\\) is difficult to estimate unless we make some simplifying assumptions (i.e., X is multivariate normal and common covariance matrix (\\(\\sum\\)) across K classes)\nWith these assumptions, we can estimate \\(\\pi_k\\), \\(\\mu_k\\), and \\(\\sigma^2\\) from the training set and calculate \\(Pr(Y = k|X)\\) for each k\n\n\n\nWith a single feature, the probability of any class k, given X is:\n\\(Pr(Y = k|X) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2)}{\\sum_{l=1}^{K}\\pi_l\\frac{1}{2\\sigma^2}\\exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)}\\)\nLDA is a parametric model, but is it interpretable?\n\nApplication of LDA to Cars data set with two predictors\nNotice that LDA produces linear decision boundary (see James et al. (2023) for formula for discriminant function derived from the probability function on last slide)\n\nrec &lt;- recipe(mpg ~ ., data = data_trn) |&gt; \n  step_pca(cylinders, displacement, horsepower, weight, \n           options = list(center = TRUE, scale. = TRUE), \n           num_comp = 1, \n           prefix = \"beef_\")\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_val &lt;- rec_prep |&gt; \n  bake(data_val)\n\n\nFit the LDA in train\n\nlibrary(discrim, exclude = \"smoothness\")\n\ndiscrim_linear() |&gt; \n  set_engine(\"MASS\") |&gt; \n  translate()\n\nLinear Discriminant Model Specification (classification)\n\nComputational engine: MASS \n\nModel fit template:\nMASS::lda(formula = missing_arg(), data = missing_arg())\n\nfit_lda_2 &lt;- \n  discrim_linear() |&gt; \n  set_engine(\"MASS\") |&gt; \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\nAccuracy and decision boundary\n\naccuracy_vec(feat_val$mpg, predict(fit_lda_2, feat_val)$.pred_class)\n\n[1] 0.8469388\n\n\n\np_train &lt;- feat_trn |&gt; \n  plot_decision_boundary(fit_lda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\np_val &lt;- feat_val |&gt; \n  plot_decision_boundary(fit_lda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\nplot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)\n\n\n\n\n\n\n4.7.2 Quadratic Discriminant Analysis\nQDA relaxes one restrictive assumption of LDA\n\nStill required multivariate normal X\nBut it allows each class to have its own \\(\\sum\\)\nThis makes it:\n\nMore flexible\nAble to model non-linear decision boundaries (see formula for discriminant in James et al. (2023))\nBut requires substantial increase in parameter estimation (more potential to overfit)\n\n\n\nApplication of RDA (Regularized Discriminant Analysis) algorithm to Car data set with two features\n\nThe algorithm that is available in tidymodels is actually a regularized discriminant analysis, rda() from the klaR package\nThere are two hyperparameters, frac_common_cov and frac_identity, that can each vary between 0 - 1\n\nWhen frac_common_cov = 1 and frac_identity = 0, this is an LDA\nWhen frac_common_cov = 0 and frac_identity = 0, this is a QDA\nThese hyperparameters can be tuned to different values to improve the fit dependent on the true DGP\nMore on hyperparameter tuning in unit 5\nThis is a flexible algorithm that likely replaces the need to fit separate LDA and QDA models\n\nsee https://discrim.tidymodels.org/reference/discrim_regularized.html\n\nHere is a true QDA using frac_common_cov = 1 and frac_identity = 0\n\ndiscrim_regularized(frac_common_cov = 0, frac_identity = 0) |&gt; \n  set_engine(\"klaR\") |&gt; \n  translate()\n\nRegularized Discriminant Model Specification (classification)\n\nMain Arguments:\n  frac_common_cov = 0\n  frac_identity = 0\n\nComputational engine: klaR \n\nModel fit template:\nklaR::rda(formula = missing_arg(), data = missing_arg(), lambda = 0, \n    gamma = 0)\n\nfit_qda_2 &lt;- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) |&gt; \n  set_engine(\"klaR\") |&gt; \n  fit(mpg ~ beef_1 + year, data = feat_trn)\n\n\nAccuracy and decision boundary\n\naccuracy_vec(feat_val$mpg, \n             predict(fit_qda_2, feat_val)$.pred_class)\n\n[1] 0.877551\n\n\n\np_train &lt;- feat_trn |&gt; \n  plot_decision_boundary(fit_qda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\np_val &lt;- feat_val |&gt; \n  plot_decision_boundary(fit_qda_2, x_names = c(\"year\", \"beef_1\"), y_name = \"mpg\", n_points = 400)\n\nplot_grid(p_train, p_val, labels = list(\"Train\", \"Validation\"), hjust = -1.5)"
  },
  {
    "objectID": "004_classification.html#comparisons-between-these-four-classifiers",
    "href": "004_classification.html#comparisons-between-these-four-classifiers",
    "title": "4  Introduction to Classification Models",
    "section": "4.8 Comparisons between these four classifiers",
    "text": "4.8 Comparisons between these four classifiers\n\nBoth logistic and LDA are linear functions of X and therefore produce linear decision boundaries\nLDA makes additional assumptions about X (multivariate normal and common \\(\\sum\\)) beyond logistic regression. Relative performance is based on the quality of this assumption\nQDA relaxes the LDA assumption about common \\(\\sum\\) (and RDA can relax it partially)\n\nThis also allows for nonlinear decision boundaries\nQDA is therefore more flexible, which means possibly less bias but more potential for overfitting\n\nBoth QDA and LDA assume multivariate normal X so may not accommodate categorical predictors very well. Logistic and KNN do accommodate categorical predictors\nKNN is non-parametric and therefore the most flexible\n\nIncreased overfitting, decreased bias?\nNot very interpretable. But LDA/QDA, although parametric, aren’t as interpretable as logistic regression\n\nLogistic regression fails when classes are perfectly separated (but does that ever happen?) and is less stable when classes are well separated\nLDA, KNN, and QDA naturally accommodate more than two classes\n\nLogistic requires additional tweak (Briefly describe: multiple one vs other classes models approach)\n\nLogisitic regression requires relatively large sample sizes. LDA/QDA may perform better with smaller sample sizes if X is multivariate normal."
  },
  {
    "objectID": "004_classification.html#a-quick-tour-of-many-classifiers",
    "href": "004_classification.html#a-quick-tour-of-many-classifiers",
    "title": "4  Introduction to Classification Models",
    "section": "4.9 A quick tour of many classifiers",
    "text": "4.9 A quick tour of many classifiers\nThe Cars dataset had strong predictors and a mostly linear decision boundary for the two predictors we considered\nThis will not be true in many cases\nLet’s consider a more complex two predictor decision boundary in the circle dataset from the mlbench package (lots of cool datasets for ML)\nThis will hopefully demonstrate that the key is to have a algorithm that can model the DGP\n\nThis is NO best algorithm\nThe best algorithm depends on\n\nThe DGP\nThe goal (prediction vs. explanation)\n\n\nIt will also demonstrate the power of tidymodels to allow us to fit many different statistical algorithms which all have their own syntax using a common syntax provided by tidymodels.\nExample adapted to tidymodels from a demonstration by Michael Hahsler\n\nlibrary(mlbench)\nset.seed(20140102)\ndata_trn &lt;- as_tibble(mlbench.circle(200)) |&gt; \n  rename(x_1 = x.1, x_2 = x.2) |&gt; \n  glimpse()\n\nRows: 200\nColumns: 3\n$ x_1     &lt;dbl&gt; -0.514721263, 0.763312056, 0.312073042, 0.162981535, -0.320294…\n$ x_2     &lt;dbl&gt; 0.47513532, 0.65803777, -0.89824011, 0.38680494, -0.47313964, …\n$ classes &lt;fct&gt; 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2,…\n\ntest &lt;- as_tibble(mlbench.circle(200)) |&gt; \n  rename(x_1 = x.1, x_2 = x.2)\n\ndata_trn |&gt; ggplot(aes(x = x_1, y = x_2, color = classes)) +\n  geom_point(size = 2, alpha = .5)\n\n\n\n\n\n4.9.1 Logistic Regression\nFit\n\nfit_lr_bench &lt;- \n  logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\nAccuracy and Decision Boundary\n\naccuracy_vec(test$classes, \n             predict(fit_lr_bench, test)$.pred_class)\n\n[1] 0.62\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_lr_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_lr_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\nplot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n4.9.2 KNN with K = 5 (somewhat arbitrary default)\nFit\n\nfit_knn_bench &lt;- \n  nearest_neighbor() |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\nAccuracy and Decision Boundary\n\naccuracy_vec(test$classes, \n             predict(fit_knn_bench, test)$.pred_class)\n\n[1] 0.985\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_knn_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_knn_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\nplot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n4.9.3 Linear Discriminant Analysis\nFit\n\nfit_lda_bench &lt;- \n  discrim_linear() |&gt; \n  set_engine(\"MASS\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\nAccuracy and Decision Boundary\n\naccuracy_vec(test$classes, \n             predict(fit_lda_bench, test)$.pred_class)\n\n[1] 0.62\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_lda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_lda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\nplot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n4.9.4 Regularized Discrinant Analysis\nLetting rda() select optimal hyperparameter values\nFit\n\nfit_rda_bench &lt;- \n  discrim_regularized() |&gt; \n  set_engine(\"klaR\") |&gt;\n  fit(classes ~ x_1 + x_2, data = data_trn)\n\nAccuracy and Decision Boundary\n\naccuracy_vec(test$classes, \n             predict(fit_rda_bench, test)$.pred_class)\n\n[1] 0.985\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_rda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_rda_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\nplot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n4.9.5 Naive Bayes Classifier\nThe Naïve Bayes classifier is a simple probabilistic classifier which is based on Bayes theorem\nBut, we assume that the predictor variables are conditionally independent of one another given the response value\n\nThis algorithm can be fit with either klaR or naivebayes engines\nThere are many tutorials available on this classifier\n\nFit\n\nfit_bayes_bench &lt;- \n  naive_Bayes() |&gt; \n  set_engine(\"naivebayes\") |&gt;\n  fit(classes ~ x_1 + x_2, data = data_trn)\n\nAccuracy and Decision Boundary\n\naccuracy_vec(test$classes, \n             predict(fit_bayes_bench, test)$.pred_class)\n\n[1] 0.99\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_bayes_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_bayes_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\nplot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n4.9.6 Random Forest\n\nRandom Forest is a variant of decision trees\nIt uses bagging which involves resampling the data to produce many trees and then aggregating across trees for the final classification\nWe will discuss Random Forest in a later unit\nHere we fit it with defaults for its hyperparameters\n\nFit\n\nfit_rf_bench &lt;- \n  rand_forest() |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\nAccuracy and Decision Boundary\n\naccuracy_vec(test$classes, \n             predict(fit_rf_bench, test)$.pred_class)\n\n[1] 0.985\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_rf_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_rf_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\nplot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n4.9.7 Neural networks\nIt is easy to fit a single layer neural network\nWe do this below with varying number of hidden units and all other hyperparameters set to defaults\n\n4.9.7.1 Single Layer NN with 1 hidden unit:\nFit\n\nfit_nn1_bench &lt;- \n mlp(hidden_units = 1) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\nAccuracy and Decision Boundary\n\naccuracy_vec(test$classes, \n             predict(fit_nn1_bench, test)$.pred_class)\n\n[1] 0.66\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_nn1_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_nn1_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\nplot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n4.9.7.2 Single Layer NN with 2 hidden units\n\nfit_nn2_bench &lt;- \n mlp(hidden_units = 2) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\nAccuracy and Decision Boundary\n\naccuracy_vec(test$classes, \n             predict(fit_nn2_bench, test)$.pred_class)\n\n[1] 0.765\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_nn2_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_nn2_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\nplot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)\n\n\n\n\n\n\n4.9.7.3 Single Layer NN with 5 hidden units\n\nfit_nn5_bench &lt;- \n mlp(hidden_units = 5) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\") |&gt; \n  fit(classes ~ x_1 + x_2, data = data_trn)\n\nAccuracy and Decision Boundary\n\naccuracy_vec(test$classes, \n             predict(fit_nn5_bench, test)$.pred_class)\n\n[1] 0.96\n\np_train &lt;- data_trn |&gt; \n  plot_decision_boundary(fit_nn5_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\np_test &lt;- test |&gt; \n  plot_decision_boundary(fit_nn5_bench, x_names = c(\"x_1\", \"x_2\"), y_name = \"classes\", n_points = 400)\n\nplot_grid(p_train, p_test, labels = list(\"Train\", \"Test\"), hjust = -1.5)"
  },
  {
    "objectID": "004_classification.html#discussion",
    "href": "004_classification.html#discussion",
    "title": "4  Introduction to Classification Models",
    "section": "4.10 Discussion",
    "text": "4.10 Discussion\n\n4.10.1 And the winner is….\n\nTop 3:\nNext 3:\nEveryone else:\n\n\n\n4.10.2 Other announcements\n\nUpdated use of discussion\nQuiz performance\nFeedback, captions, slack feedback\nDummy coding concepts (options for instruction)\n\n\n\n4.10.3 Student Questions\n\nBayes classifier\n\nCould you explain this statement about the Bayes classifier?: Pr(Y=k|X=x0) for k=1:K\nI really don’t understand Bayes classifier after searching additional information. terminology to this concept is confusing.\n\nLogistic Regression\n\nLogistic Regression and its figure\n\nLDA/QDA\n\nIt would be great if you could talk through the broad logic of what generative classifiers are doing (e.g., James pg 141-2). I feel like I sort of get it; instead of estimating distributions of classes conditioned on features, they’re estimating distributions of features conditioned on classes, and then using Bayes’ rule? but it would be helpful to talk through this in the context of an example.\nWhat is the difference between LDA and QDA? Comparison\nWhat makes QDA model more flexible? And why that makes less bias but more potential for overfitting in QDA?\n\nComparisons among algorithms\n\nI do not understand how logistic regression is different than linear discriminant analysis? It seems in LDA, the goal is to find an optimal line to classify on. However, isn’t logistic regression effectively doing the same by trying to create a model that is most predictive? How is the boundary line different?\nI think I understand when you would use a LDA versus QDA, but I would like to know more about when to choose the logistic regression. Are LDA and QDAs completely different from the logistic regression conceptually?\nWe haven’t talk about logistic regression yet in psych610. What are some advantages and disadvantages of logistic regression compared to other classification methods?\nCould you please go over the differences between LDA/QDA and logistic regression. I understand these parametric models are intermediate models between logistic regression and knn, but I don’t understand the advantage of using them over knn or logistic regression.\n\nCategorical predictors\n\nCan knn classification deal with data having a 2+ category outcome variable?\nAlso, best practices for dummy coding or creating grouped levels or if this is just trial and error and what seems to make the most sense.\n\nMissing data\n\nWe had a lot of missing data this week. I am curious about the best ways to impute this data, and whether or not that should differ between classification and regression models\nHow to select the right algorithm for missing data?\n\nGeneric modeling questions\n\nHow do I figure out why my model is doing worse? How to think about this without visualization?\nI’m still a bit confused about what would take place in modeling EDA vs. model fitting and evaluation. For example, if I want to separate the Titanic variable cabin into cabin letter and number, would that be a step that takes place in recipes only?\nHow can we quickly decide which predictors to include in our model when we have so many predictors and multicollinearity issues may happen?\nCan we extrapolate more classification questions from the data we already collected? For example, if you are collecting heart rate of a person, while exercising. Can you then create a new variable that says if the person was tired or not?\n\nOther questions\n\nIn 610, we plotted predicted points along with our predicted data points along in our final graphs. Do you recommend plotting predicted points on our graphs for publication?\nCan you review the variable “beef” that you create in the “step_pca” function? How do the numeric variables get combined?\nuse cases for random forest models\nFor all of these classifiers, will they work differently on large datasets with different types of data such as texts and images? If yes, which ones may work faster with large text data and large image data?\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag."
  },
  {
    "objectID": "004_classification.html#footnotes",
    "href": "004_classification.html#footnotes",
    "title": "4  Introduction to Classification Models",
    "section": "",
    "text": "These sample sizes are starting to get a little small. Fitted models will have higher variance with only 75% (N = 294) observations and performance in validation (with only N = 98 observations) may not be a very precise estimate of true validation error. We will learn more robust methods in the unit on resampling.↩︎\nThe logistic regression provides coefficients (parameter estimates) that can be used to describe changes in probability, odds and odds ratio associated with change in year. These parameter estimates can be tested via inferential procedures. KNN does not provide any parameter estimates. With KNN, we can visualize decision boundary (only in 2 or three dimensions) or the predicted outcome by any feature, controlling for other features but these relationships may be complex in shape. Of course, if the relationships are complex, we might not want to hide that. We will learn more about feature importance for explanation in a later unit.↩︎"
  },
  {
    "objectID": "005_resampling.html#overview-of-unit",
    "href": "005_resampling.html#overview-of-unit",
    "title": "5  Resampling Methods for Model Selection and Evaluation",
    "section": "5.1 Overview of Unit",
    "text": "5.1 Overview of Unit\n\n5.1.1 Learning Objectives\n\nBias vs. variance wrt model performance estimates\n\nHow is this different from bias vs. variable of model itself\n\nHow to do parallel processing\nTypes of resampling\n\nValidation set approach\nLeave One Out CV\nK-Fold and Repeated K-Fold\nGrouped K-Fold\nBootstrap resampling\n\nUse of resampling for tuning hyperparameters\n\nCombining these resampling approaches with a Test set\n\nUsed for simultaneous model selection and evaluation\nSingle independent test set\nAdvanced topic: Nested resampling\n\n\n\n\n5.1.2 Readings\n\nKuhn and Johnson (2018) Chapter 4, pp 61 - 80\nSupplemental: * James et al. (2023) Chapter 5, pp 197 - 208 186\n\n\n\n5.1.3 Lecture Videos\n\nLecture 1: Overview & Parallel Processing\nLecture 2: Single Validation/Test Set Approach\nLecture 3: Leave One Out Cross Validation\nLecture 4: K-fold Cross Validation Approaches\nLecture 5: Bootstrap Resampling\nLecture 6: Tuning Hyperparameters via Resampling\nLecture 7: Resampling for Both Model Selection and Evaluation\nLecture 8: Nested Resampling\nDiscussion\n\nPost questions or discuss readings or lectures on Slack\n\n\n5.1.4 Application Assignment and Quiz\n\ndata\ndata dictionary\nrmd shell\nsolution\n\nPost questions to application_assignments Slack channel\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, February 21st\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag.\n\n\nKuhn, Max, and Kjell Johnson. 2018. Applied Predictive Modeling. 1st ed. 2013, Corr. 2nd printing 2018 edition. New York: Springer."
  },
  {
    "objectID": "006_regularization.html#overview-of-unit",
    "href": "006_regularization.html#overview-of-unit",
    "title": "6  Regularization and Penalized Models",
    "section": "6.1 Overview of Unit",
    "text": "6.1 Overview of Unit\nLearning Objectives\n\nSubsetting approaches: Forward, Backward, Best Subset (covered in reading only)\nCost and Loss functions\n\nWhat are they and how are they used\nWhat are the specific formulas for linear model, logistic regression, and variants of glmnet (ridge, LASSO, full elasticnet)\n\nWhat is regularization\n\nWhat are its benefits?\nWhat are its costs?\n\nHow does lambda affect bias-variance trade-off in glmnet\nWhat does alpha do?\nFeature engineering approaches for dimensionality reduction: PCA (covered in reading only)\nOther algorithms that do feature selection/dimensionality reduction: PCR and PLS (covered in reading only)\nContrasts of PCA, PCR, PLS, and glmnet/LASSO for dimensionality reduction (covered in reading only)\n\nReadings\n\nJames et al. (2023) Chapter 6, pp 225 - 267\n\nLecture Videos\n\nLecture 1: An Introduction to Penalized/Regularized Algorithms\nLecture 2: Cost Functions\nLecture 3: Ridge Regression\nLecture 4: LASSO\nLecture 5: The Elastic net\nLecture 6: Emprical Example - Many good predictors\nLecture 7: Emprical Example - Good and zero predictors\nLecture 8: Emprical Example - LASSO for covariate selection\nDiscussion\n\nCoding Assignment\n\ndata\ndata dictionary\nrmd shell\nfun_modeling.R\nsolution\n\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, February 28th :w\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag."
  },
  {
    "objectID": "007_midterm.html#applications-take-home-exam",
    "href": "007_midterm.html#applications-take-home-exam",
    "title": "7  Midterm Exam",
    "section": "7.1 Applications (take-home) Exam",
    "text": "7.1 Applications (take-home) Exam\nThe Applications exam is due at 8 pm on Wednesday, March 6th.\nAs the name suggests, it will focus primary applications. However, within the RMD file there are questions that also assess your understanding of what you are doing and why.\nYou should complete the exam as you have previously completed the application assignments:\n\nDownload the datasets and rmd file below\nProvide the requested code in the empty code chunks that are included in the rmd file\nAnswer the questions posed outside the code chunks within the text sections of the rmd file, immediately after the question.\nWhen you are done, knit the file to html and upload this knit file through the Canvas\nThe exam is due on Wednesday, March 10th, at 8pm\n\nIn contrast to the application assignments, the TAs and I will not be able to answer substantive questions about the exam. However, if you need us to clarify what we are requesting you to do for any specific question or believe you have found an error in the exam, please post your question to the exam_quizzes channel in Slack and we will respond ASAP\nExam rmd shell\nData:\n\ndataset 1; data dictionary\ndataset 2\ndataset 3"
  },
  {
    "objectID": "007_midterm.html#conceptual-exam",
    "href": "007_midterm.html#conceptual-exam",
    "title": "7  Midterm Exam",
    "section": "7.2 Conceptual Exam",
    "text": "7.2 Conceptual Exam\nThe conceptual exam will be held in class during the Discussion section on Thursday, March 7th."
  },
  {
    "objectID": "008_advanced_performance_metrics.html#overview-of-unit",
    "href": "008_advanced_performance_metrics.html#overview-of-unit",
    "title": "8  Advanced Performance Metrics",
    "section": "8.1 Overview of Unit",
    "text": "8.1 Overview of Unit\nLearning Objectives\n\nUnderstand costs and benefits of accuracy\nUse of a confusion matrix\nUnderstand costs and benefits of other performanc metrics\nThe ROC curve and area under the curve\nModel selection using other performance metrics\nHow to address class imbalance\n\nSelection of performance metric\nSelection of classification threshold\nSampling and resampling approaches\n\n\nReadings\n\nKuhn and Johnson (2018) Chapter 11, pp 247-266\nKuhn and Johnson (2018) Chapter 16, pp 419-435\n\nLecture Videos\n\nLecture 1: Unit Introduction\nLecture 2: The Confusion Matrix\nLecture 3: Metrics from the Confusion Matrix, Part 1\nLecture 4: Metrics from the Confusion Matrix, Part 2\nLecture 5: The Receiver Operating Characteristic (ROC) Curve\nLecture 6: Selecting Model Configurations with Other Metrics\nLecture 7: Addressing Class Imbalance\nDiscussion\n\nCoding Assignment\n\ndata\nrmd shell\nsolution: rmd; html\n\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, March 20th\n\n\n\n\nKuhn, Max, and Kjell Johnson. 2018. Applied Predictive Modeling. 1st ed. 2013, Corr. 2nd printing 2018 edition. New York: Springer."
  },
  {
    "objectID": "009_model_comparisons.html#overview-of-unit",
    "href": "009_model_comparisons.html#overview-of-unit",
    "title": "9  Model Comparisons and Other Explanatory Goals",
    "section": "9.1 Overview of Unit",
    "text": "9.1 Overview of Unit\nLearning Objectives\n\nUse of the permutation test to evaluate overall model performance\n\nUsing test performance\nUsing resampled performance\n\nUse of model comparisons to compare models\n\nFeature ablation\nFrequentist correlated t-test using CV\nBayesian estimation for model comparisons\n\nROPE\n\n\nVariable important metrics for explanation\n\nPermutation feature importance\nShapley values\n\nlocal importance\nglobal importance\n\n\nVisual approaches for explanation\n\nPartial dependence plots\nAccumulated Local Effects (ALE) plots\n\n\nReadings\n\nBenavoli et al. (2017) paper: Read pages 1-9 that describe the correlated t-test and its limitations.\nKruschke (2018) paper: Describes Bayesian estimation and the ROPE (generally, not in the context of machine learning and model comparisons)\nMolnar (2023) Chapter 3 - Interpretability\nMolnar (2023) Chapter 6 - Model-Agnostic Methods\nMolnar (2023) Chapter 8 - Global Model Agnostic Methods: Read setions 8.1, 8.2, 8.3, and 8.5\nMolnar (2023) Chapter 9 - Local Model-Agnostic Methods: Read section 9.5\n\nLecture Videos\n\nIntroduction to the Permutation Test\nPermutation Test with Single Held-Out Test set\nPermutation Test with Resampling for Selection and Evaluation\nIntroduction to Model Comparisons\nThe Nadeau & Bengio Correlated t-test for Model Comparisons\nBayesian Estimation for Model Comparisons\nFeature Importance Metrics to Understand Models\nVisual Approaches to Understand Models\nExploring Interactions\nDiscussion\n\nApplication Assignment\n\ndata: admissions.csv\nassignment rmd\nsolution html: rmd; html\n\nPost questions to the application_assignments Slack channel\nSubmit the application assignment here and complete theunit quiz by 8 pm on Wednesday, March 27\n\n\n\n\nBenavoli, Alessio, Giorgio Coraniy, Janez Demsar, and Marco Zaffalon. 2017. “Time for a Change: A Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis.” Journal of Machine Learning Research 18: 1–36.\n\n\nKruschke, John K. 2018. “Rejecting or Accepting Parameter Values in Bayesian Estimation.” Advances in Methods and Practices in Psychological Science 1: 270–80.\n\n\nMolnar, Christoph. 2023. Intepretable Machine Learning: A Guide for Makiong Black Box MOdels Explainable. 2nd ed. https://christophm.github.io/interpretable-ml-book/."
  },
  {
    "objectID": "010_random_forest.html#overview-of-unit",
    "href": "010_random_forest.html#overview-of-unit",
    "title": "10  Advanced Models: Decision Trees, Bagging Trees, and Random Forest",
    "section": "10.1 Overview of Unit",
    "text": "10.1 Overview of Unit\nLearning Objectives\n\nDecision trees\nBagged trees\nHow to bag models and the benefits\nRandom Forest\nHow Random Forest extends bagged trees\nFeature interpretation with:\n\nDecision tree plots\nVariable importance\n\n\nReadings\n\nJames et al. (2023) Chapter 8, Tree Based Methods; pp 327 - 352\n\nIn addition, much of the content from this unit has been drawn from four chapters in a book called Hands On Machine Learning In R. It is a great book and I used it heavily (and at times verbatim) b/c it is quite clear in its coverage of these algorithms. If you want more depth, you might read chapters 9-12 from this book as a supplement to this unit in our course.\nLecture Videos\n\nLecture 1: Decision Trees\nLecture 2: Decision Trees in Ames\nLecture 3: Bagged Trees\nLecture 4: Bagged Trees in Ames\nLecture 5: Random Forest\nLecture 6: Random Forest in Ames\nDiscussion\n\nCoding Assignment\n\ndata\nrmd shell\nsolution: rmd; html\n\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, April 3rd\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York: Springer-Verlag."
  },
  {
    "objectID": "011_neural_networks.html#overview-of-unit",
    "href": "011_neural_networks.html#overview-of-unit",
    "title": "11  Advanced Models: Neural Networks",
    "section": "11.1 Overview of Unit",
    "text": "11.1 Overview of Unit\nLearning Objectives\n\nWhat are neural networks\nTypes of neural networks\nNeural network architecture\n\nlayers and units\nweights and biases\nactivation functions\ncost functions\noptimization\n\nepochs\nbatches\nlearning rate\n\n\nHow to fit 3 layer MLPs in tidymodels using Keras\n\nReadings\n\nNeural Networks and Deep Learning, Chapter 1: Using neural networks to recognize handwritten digits\n\nLecture Videos\n\nLecture 1: But what is a Neural Network?\nLecture 2: Gradient descent, how neural networks learn\nLecture 3: What is backpropagation really doing?\nOptional Lecture 4: Backpropagation calculus\nLecture 5: Introduction and the MNIST dataset\nLecture 6: Fitting neural networks in tidymodels with Keras - Part 1\nLecture 7: Fitting neural networks in tidymodels with Keras - Part 2\nLecture 8: Addressing overfitting\nLecture 9: Selecting model configurations and final remarks\nDiscussion\n\nCoding Assignment\n\nwine quality datasets: training;test\nrmd shell\nsolution: rmd; html\n\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, April 10th"
  },
  {
    "objectID": "012_nlp.html#overview-of-unit",
    "href": "012_nlp.html#overview-of-unit",
    "title": "12  Natural Language Processing: Text Processing and Feature Engineering",
    "section": "12.1 Overview of Unit",
    "text": "12.1 Overview of Unit\nLearning Objectives\n\nObjective 1\nObjective 2\n\nReadings\n\nHvitfeldt and Silge (2022) Chapter 2: Tokenization\nHvitfeldt and Silge (2022) Chapter 5: Word Embeddings\n\nNOTES: Please read the above chapters more with an eye toward concepts and issues rather than code. I will demonstrate a minimum set of functions to accomplish the NLP modeling tasks for this unit.\nAlso know that the entire Hvitfeldt and Silge (2022, book) is really mandatory reading. I would also strongly recommend this entire Silge and Robinson (2017) book. Both will be important references at a minimum.\nLecture Videos\n\nLecture 1: General Text (Pre-) Processing - the stringr package\nLecture 2: General Text (Pre-) Processing - regular expressions\nLecture 3: The IMDB Reviews Dataset\nLecture 4: Tokenization- Part 1\nLecture 5: Tokenization- Part 2\nLecture 6: Stopwords\nLecture 7: Stemming\nLecture 8: Bag of Words\nLecture 9: NLP in Action - Part 1\nLecture 10: NLP in Action - Part 2\n\nCoding Assignment\n\ndata\nrmd shell\nGloVE embeddings\nsolution: rmd; html\n\nSubmit the application assignment here and complete the unit quiz by 8 pm on Wednesday, April 17th\n\n\n\n\nHvitfeldt, Emil, and Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. https://smltar.com/.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. 1rst ed. Beijing; Boston: O’Reilly Media."
  },
  {
    "objectID": "013_applications.html#overview-of-unit",
    "href": "013_applications.html#overview-of-unit",
    "title": "13  Applications for Machine Learning: Synthesis and Concept Generalization",
    "section": "13.1 Overview of Unit",
    "text": "13.1 Overview of Unit\nLearning Objectives\nThis week we will read a short “book” by Andrew Ng, a very well known computer scientist in the field of AI who also offers one of the best known MOOCs on machine learning at Coursera\nThis book is very application oriented and practical. The goals for reading it are:\n\nTo encourage generalization of your learning - some terminology and perspectives will be different here from the authors you have been reading. The concepts are all the same. We hope that seeing these ideas in a new context will help you recognize and generalize them.\nThe book is applied. It will help you consider carefully how to make best use of your data for model selection, evaluation, error analysis and other related tasks to figure out what to do when your models don’t perform well.\nIt is a “course in a book”, such that we hope this will encourage you to see the big picture of how all the pieces from our course fit together.\n\nFinally, it is a quick read so don’t worry that it is a full book. Also, you have no lectures or application assigment this week. It is all about consolidation now!!!! You are in the home stretch.\nReadings\n\nNg (2018) pdf\n\nLecture Videos\nNo lectures this week. Only lab and discussion section.\nApplication Assignment\nNo assignment this week!\nThe unit quiz is due by 8 pm on Wednesday, April 24th\n\n\n\n\nNg, Andrew. 2018. Machine Learning Yearning: Technical Strategy for AI Engineers in the Age of Deep Learning. DeepLearning.AI."
  },
  {
    "objectID": "014_ethics.html#overview-of-unit",
    "href": "014_ethics.html#overview-of-unit",
    "title": "14  Ethical Issues in Machine Learning Research and Applications",
    "section": "14.1 Overview of Unit",
    "text": "14.1 Overview of Unit\nLearning Objectives\nAll semester, we have been learning how to develop and evaluate machine learning models. In this final unit, we will now consider the impact that these models have had to date on society. Of course, there is the potential for many benefits from these models but they have also produced substantial harm. It is important that we recognize what contributes to their harm so that we can strive to avoid these problems in our own work.\nReadings\nThe readings this week will come from O’Neil (2016); We will read the introduction, chapters 1, 3, 5, and the conclusion and afterword sections.\nLecture Videos\nThere are no lecture videos, application assignment, or quiz this week. We will meet on Tuesday for lab and Thursday for discussion.\n\n\n\n\nO’Neil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Reprint Edition. Broadway Books."
  },
  {
    "objectID": "101_novel_levels.html",
    "href": "101_novel_levels.html",
    "title": "16  Handling Novel Values in Held Out Sets",
    "section": "",
    "text": "When you have nominal/ordinal predictors that have levels that are infrequent, you will occassionally find that an infrequent level appears in your held out set (i.e., validation or test) but not in your training set. This can cause problems when you try to make predictions for these new values. Specifcally, the feature values for this level will be set to NA and therefore, you will get predictions of NA for these observations.\nIn this appendix, we demonstrate this problem and our preferred solution given our workflow of classing all nominal/ordinal predictors as factors in our dataframes.\n\nlibrary(tidyverse) \nlibrary(tidymodels) \ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\n\nMake simple data sets with an outcome (y) and one nominal predictor (x). Note that x will have a novel value (foo) in the test set that wasnt present in the training set.\n\nn &lt;- 6\ndata_trn &lt;- tibble(y = rnorm(n), \n                   x = rep (c(\"a\", \"b\", \"c\"), n/3)) |&gt;\n  mutate(x = factor(x, levels = c(\"a\", \"b\", \"c\"))) |&gt; \n  print()\n\n# A tibble: 6 × 2\n        y x    \n    &lt;dbl&gt; &lt;fct&gt;\n1  0.364  a    \n2  2.67   b    \n3 -0.0509 c    \n4 -0.369  a    \n5 -0.0319 b    \n6  0.347  c    \n\ndata_test &lt;- tibble(y = c(rnorm(n), rnorm(1)),\n                    x = c(rep (c(\"a\", \"b\", \"c\"), n/3), \"foo\")) |&gt; \n  mutate(x = factor(x, levels = c(\"a\", \"b\", \"c\", \"foo\"))) |&gt; \n  print()\n\n# A tibble: 7 × 2\n       y x    \n   &lt;dbl&gt; &lt;fct&gt;\n1  0.881 a    \n2  2.15  b    \n3  1.07  c    \n4 -1.04  a    \n5 -0.477 b    \n6 -0.312 c    \n7  0.973 foo  \n\n\nMake a recipe\n\nrec &lt;- recipe(y ~ x, data = data_trn) %&gt;% \n  step_dummy(x)\n\nPrep the recipe with training data\n\nrec_prep &lt;- rec |&gt; \n  prep(data_trn)\n\nFeatures for training set. No problems\n\nfeat_trn &lt;- rec_prep |&gt; \n  bake(data_trn)\n\nfeat_trn |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn\n\n\nNumber of rows\n6\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ny\n0\n1\n0.49\n1.10\n-0.37\n-0.05\n0.16\n0.36\n2.67\n1.17\n-0.38\n\n\nx_b\n0\n1\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\nx_c\n0\n1\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\n\n\n\nFeatures for test set.\n\nNow we see the problem indicated by the warning about new level in test.\nWe see that one observation is missing for x in test. If we looked closer, we would see this is the observation for foo\n\n\nfeat_test &lt;- rec_prep |&gt; \n  bake(data_test)\n\nWarning: There are new levels in a factor: foo\n\nfeat_test |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_test\n\n\nNumber of rows\n7\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ny\n0\n1.00\n0.46\n1.11\n-1.04\n-0.39\n0.88\n1.02\n2.15\n0.06\n-1.63\n\n\nx_b\n1\n0.86\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\nx_c\n1\n0.86\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\n\n\n\nWe solve this problem but just making sure this level was listed when we created the factor in training (e.g., use this mutate earlier when classing x in data_trn: mutate(x = factor(x, levels = c(\"a\", \"b\", \"c\", \"foo\"))).\nOr we can add the level after the fact, when we discover the problem (as below).\n\ndata_trn1 &lt;- data_trn |&gt; \n  mutate(x = factor(x, levels = c(\"a\", \"b\", \"c\", \"foo\")))\n\nNow prep recipe with this updated training set that includes foo level\n\nrec_prep1 &lt;- rec |&gt; \n  prep(data_trn1)\n\nFeatures for training as before\n\nWe now have a feature for this new level\nIt is set to 0 for all observations (because there are no observations with a value of foo in training set)\n\n\nfeat_trn1 &lt;- rec_prep1 |&gt; \n  bake(data_trn1)\n\nfeat_trn1 |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_trn1\n\n\nNumber of rows\n6\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ny\n0\n1\n0.49\n1.10\n-0.37\n-0.05\n0.16\n0.36\n2.67\n1.17\n-0.38\n\n\nx_b\n0\n1\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\nx_c\n0\n1\n0.33\n0.52\n0.00\n0.00\n0.00\n0.75\n1.00\n0.54\n-1.96\n\n\nx_foo\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nNaN\nNaN\n\n\n\n\n\nNow there is no problem when we find this value for an observation in the test set.\n\nfeat_test1 &lt;- rec_prep1 |&gt; \n  bake(data_test)\n\nfeat_test1 |&gt; skim_all()\n\n\nData summary\n\n\nName\nfeat_test1\n\n\nNumber of rows\n7\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nskew\nkurtosis\n\n\n\n\ny\n0\n1\n0.46\n1.11\n-1.04\n-0.39\n0.88\n1.02\n2.15\n0.06\n-1.63\n\n\nx_b\n0\n1\n0.29\n0.49\n0.00\n0.00\n0.00\n0.50\n1.00\n0.75\n-1.60\n\n\nx_c\n0\n1\n0.29\n0.49\n0.00\n0.00\n0.00\n0.50\n1.00\n0.75\n-1.60\n\n\nx_foo\n0\n1\n0.14\n0.38\n0.00\n0.00\n0.00\n0.00\n1.00\n1.62\n0.80\n\n\n\n\n\nAll is good. BUT, there are still some complexities when we fit this model in train and predict into test. In training, the x_foo feature is a constant (all 0) so this will present some issues for some statistical algorithms. Lets see what happens when we fit a linear model and use it to predict into test.\n\nfit1 &lt;-\n  linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(y ~ ., data = feat_trn1)\n\nIf we look at the parameter estimates, we see that the algorithm was unable to estimate a parameter for x_foo because it was a constant in train\n\nfit1 %&gt;% tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -0.00229     0.817  -0.00281   0.998\n2 x_b          1.32        1.15    1.14      0.335\n3 x_c          0.150       1.15    0.130     0.905\n4 x_foo       NA          NA      NA        NA    \n\n\nThis will generate a warning (“prediction from a rank-deficient fit has doubtful cases”) when you use this model to make predictions for values it didnt see in the training set.\n\nThe consequence is that the model will predict a y value associated with the reference level (coded 0 for all other dummy features) for all foo observations. This is probably the best we can do for these new (previously unseen) values for x.\nalso note that the column name for predictions, which is usually called .pred, is now called .pred_res. You will need to accomodate this in your code as well. Just rename it.\n\n\npredict(fit1, feat_test1) |&gt;  \n  bind_cols(feat_test1)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from rank-deficient fit; attr(*, \"non-estim\") has\ndoubtful cases\n\n\n# A tibble: 7 × 5\n  .pred_res      y   x_b   x_c x_foo\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  -0.00229  0.881     0     0     0\n2   1.32     2.15      1     0     0\n3   0.148    1.07      0     1     0\n4  -0.00229 -1.04      0     0     0\n5   1.32    -0.477     1     0     0\n6   0.148   -0.312     0     1     0\n7  -0.00229  0.973     0     0     1\n\n\nThis is our preferred solution when new/previously unseen values exist in held out data. A comparable solution is offered as a recipe step. See step_novel()"
  },
  {
    "objectID": "test.html#standards",
    "href": "test.html#standards",
    "title": "21  Test",
    "section": "21.1 Standards",
    "text": "21.1 Standards\n\n21.1.1 For displaying figures:\n\n\n\n\n\n\n\n21.1.2 For displaying code and even variable names\ndemo &lt;- function(x) variable_2\n\n\n21.1.3 For code annotation\n\nlibrary(tidyverse)\n\n\n1\n\nmake a df\n\n2\n\nglimpse a df\n\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n1d &lt;- tibble (x = 1:5) |&gt;\n2  glimpse()\n\nRows: 5\nColumns: 1\n$ x &lt;int&gt; 1, 2, 3, 4, 5\n\n\n\n\n21.1.4 For displaying color (like red questions)\nthis sentence is in red\nThis is an alternative method but more complex: I love R.\nred yellow green blue\nyellow background for better contrast\n\n\n21.1.5 Callouts for questions\nHTML:\n\n\n\n\n\n\nThis is the question you want displayed?\n\n\n\n\n\nHere’s the answer! This can be as long as you want.\n\n\n\nRevealJS does not yet have callout collapse implemented. A workaround is to use code folding:\n\n\n\n\n\n\nIf the DGP for y is a cubic function of x, what do we know about the expected bias for our three candidate model configurations in this example?\n\n\n\n\n\n\n\nShow Answer\nThe simple linear model will underfit the true DGP and therefore it will be biased b/c \nit can only represent y as a linear function of x.  \n\nThe two polynomial models will be generally unbiased b/c they have x represented \nwith 20th order polynomials.  \n\nLASSO will be slightly biased due to regularization but more on that in a later unit\n\n\n\n\n\n\n\n21.1.6 Conditional Content\nUse divs to specify content to only appear in certain formats:\nWill only appear in HTML."
  }
]