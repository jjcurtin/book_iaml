---
title: "lab_unit_08"
format: html
self-contained: true
editor: visual
---

```{r}
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
```

```{r}
library(tidyverse) 
library(tidymodels)
library(ggplot2)
library(xfun, include.only = "cache_rds")

devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
```

```{r}
theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max=Inf)
```

### function, vis, eda

#### CM, `autoplot`

-   I haven't started the coding assignment yet (because of the 710 midterm this week), but I'd like to know if autoplot is only used to plot confusion matrices or if we can use it to plot other things? And if so, what? - We can also use `autoplot` for simple plotting of appropriate data type

```{r}
?autoplot

# in ggplot2
# autoplot: Create a complete ggplot appropriate to a particular data type
```

-   Maybe provide some functions we can play around about modifying confusion matrix to make it publication quality?

```{r}
# Sample multi-class data
set.seed(421)
df <- tibble(
  truth = factor(sample(c("A", "B", "C", "D"), 150, replace = TRUE)),  # True Labels
  predicted = factor(sample(c("A", "B", "C", "D"), 150, replace = TRUE))  # Model Predictions
)

# Compute confusion matrix
cm <- conf_mat(df, truth, predicted)

# Default autoplot
autoplot(cm)
```

We can plot confusin matrix for multi-category models and modify the aesthetics like color, label and fonts, etc.

```{r}
autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low = "yellow", high = "blue") +  # Custom color scale
  theme_minimal() +
  labs(title = "Confusion Matrix",
       fill = "Count") +  # Adjust labels
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"))

```

#### predictor variance

-   **Scale of variance**: In the homework there is a question about variance. I had trouble understanding that and would like to hear more about it. It was not easy to tell visually whether variance was an issue for the predictors because some predictors appeared to have a wide spread but the scale was only like 0.1 from min to max. Others had what appeared to be a very compressed spread but the scale was in the hundreds. Do we consider absolute variance or the relative variance here?

> We inspect on the variance of the training data when doing EDA and also conduct feature engineering when creating recipes. Here are some points we pay attention to for variance:
>
> 1\. The features we select to create the recipe/model should be variable so that it helps to differentiate the outcome variable. For example, if almost all the participants for a study are around 20-year-old, the variable `age` might not be a very predictive and influential feature.
>
> 2\. The variance will affect the performance of linear models, which is based on a normal hypothesis of the prediction error (and reflected on the distribution of the features). So if the distribution is skewed, we'll be careful for possible transformation steps.
>
> 3\. For the scale, both glmnet and knn ask for a common range of the features. For knn, it's based on the distance calculation. So we should range the features to make sure of the influence of each feature on the distance is the same. For glmnet, because we're (by default) imposing same penalty on the feature coefficients, we should make sure they are in the same range to penalize consistently. And thus, the scale of the variance will be ranged accordingly with the feature engineering.

#### correlation plot

-   Also, is there a better way to print the correlation matrices when there are many predictors? With many predictors it just look like a mess. I can't read the variable names or make out anything useful. Even zooming in and making it fullscreen does not help.

> An alternative for the correlation plot function could be the GGally functions, which return some comprehensive correlation information for the variable pairs. Besides, if using the corr plots, we could tweak on the format to show the numeric numbers of the correlation or other shapes.

```{r}
# corr plot example w/ numbers
M <- cor(mtcars)
corrplot::corrplot(M, method = "number", type = "upper") # colorful number

# explore the funcion
??corrplot::corrplot
```

#### grid plot

-   I would also be open to looking over how we can make our glmnet hyperparameter graphs look better. It was kind of confusing to fix.
-   perhaps a way to visualize the best hyperparameter indices for resampling.

> John's function to plot the performance as y, a hyperparamter as the x and another hyperparameter as colorful curves could provide an overall trend of the perforamance when tuning the hyperparameters.

> Another choice might be using a heatmap.

```{r}
# create a fake df
set.seed(123)  # Ensure reproducibility

# Generate a fake dataset
n <- 100  # Number of samples
p <- 20   # Number of features

# Generate informative features (correlated with outcome)
X <- as.data.frame(matrix(rnorm(n * p), nrow = n, ncol = p))
colnames(X) <- paste0("V", 1:p)  # Assign column names

# Create a linear combination of some features as the signal
signal <- 0.8 * X$V1 + 0.6 * X$V2 - 0.5 * X$V3 + 0.4 * X$V4

# Generate probabilities using logistic function
prob <- 1 / (1 + exp(-signal))  # Sigmoid function

# Assign binary labels based on probability
y <- rbinom(n, 1, prob) |> 
  as.factor()

# Combine into a dataframe
df <- cbind(X, y = y)

# Print first few rows
head(df)

```

```{r}
set.seed(12345)

splits_test <- df |>  
  initial_split(prop = 3/4, strata = "y")

d_trn <- splits_test |> 
  analysis()

d_test <- splits_test |> 
  assessment()
```

```{r}
# set up resampling and hp grid
set.seed(12345)

splits_boot <- d_trn |> 
  bootstraps(times = 10, strata = "y") 

grid_glmnet <- expand_grid(penalty = exp(seq(-6, 3, length.out = 100)),
                           mixture = seq(0, 1, length.out = 6))
```

```{r}
rec <- recipe(y ~ ., data = d_trn)

fits_glmnet <- cache_rds(
  expr = {
    logistic_reg(penalty = tune(), 
                 mixture = tune()) |> 
      set_engine("glmnet") |> 
      set_mode("classification") |> 
      tune_grid(preprocessor = rec, 
                resamples = splits_boot, grid = grid_glmnet, 
                metrics = metric_set(accuracy))
  },
  dir = "cache/",
  file = "fits_glmnet_lab_8",
  rerun = F)
```

```{r}
plot_hyperparameters(fits_glmnet, hp1 = "penalty", hp2 = "mixture", 
                     metric = "accuracy", log_hp1 = TRUE)
```

```{r}
metric_data <- tune::collect_metrics(fits_glmnet)

metric_data |> 
  ggplot(aes(x = log(penalty), y = mixture, fill = mean)) +
  geom_tile() +
  scale_fill_gradient()
```

#### facet_wrap

-   Can we go over the type of arguments that go into facet_wrap? In histograms?

```{r}
# Create a fake dataset
set.seed(123)
df <- data.frame(
  category = rep(c("A", "B", "C"), each = 100),
  value = c(rnorm(100, mean = 50, sd = 10),
            rnorm(100, mean = 70, sd = 15),
            rnorm(100, mean = 30, sd = 5))
)

# Plot histograms with facet_wrap()
ggplot(df, aes(x = value)) +
  geom_histogram(binwidth = 5, fill = "steelblue", color = "white") +
  facet_wrap(~ category) +
  theme_bw()
```

#### function construction

-   I still confused about building a function in R.

```{r}
# define a function
demo_fun <- function(x = 10, y = 10){
  # some operations on the inputs
  z1 <- x * y
  z2 <- x + y
  
  # return outputs
  return(c(z1, z2))
}
```

```{r}
demo_fun()
# 100 20
demo_fun(5, 5)
# 25 10
```

#### function conflict

-   Why do I still need to specify function conflicts even though I have only libraried tidyverse and tidymodel. I also included John's function. The one function which has the conflict is the select().

> Maybe load the conflict rule polices before loading any packages.

### regularization, tuning

#### grid

-   I would like to know more about how to specify the numeric values for the penalty values that are inside the `expand_grid` function. What does it mean to have `seq(-8, 3, length.out = 200)`?

> This means to generate a sequence from -8 to 3, including 200 values between them.

```{r}
?seq

# ## Default S3 method:
# seq(from = 1, to = 1, by = ((to - from)/(length.out - 1)), length.out = NULL, along.with = NULL, ...)

# length.out	
# desired length of the sequence. A non-negative number, which for seq and seq.int will be rounded up if fractional.
```

### balanced sampling

#### over/under_ratio

-   In downsampling what does under_ratio = 1 mean? What would different values mean. Is it just the number of samples?

> It means the ratio of the #majority/#minority

```{r}
?themis::step_downsample

# under_ratio	
# A numeric value for the ratio of the majority-to-minority frequencies. 
# The default value (1) means that all other levels are sampled down to have the same frequency as the least occurring level. 
# A value of 2 would mean that the majority levels will have (at most) (approximately) twice as many rows than the minority level.
```

-   How does change over_ratio to some values other than 1 make a difference?

```{r}
?themis::step_upsample

# over_ratio	
# A numeric value for the ratio of the minority-to-majority frequencies. 
# The default value (1) means that all other levels are sampled up to have the same frequency as the most occurring level. 
# A value of 0.5 would mean that the minority levels will have (at most) (approximately) half as many rows than the majority level.
```

#### examples to implement up/down-sampling

-   Upsampling and downsampling; more examples of how this would be implemented.
-   How to use the different resampling techniques to address uneven class distribution? examples for the same.
-   Can you talk more on the step_upsample()?

> We use them in the recipe by calling `themis::step_upsample` or `themis::step_downsample`

#### details of up/down-sampling and SMOTE

-   The underlying procedure of step_downsample and step_upsample. Why we use this function after step_dummy and before step_normalize in recipe?

> We randomly remove/sample from existing instances of the majority/minority class. And the order is intended to maintain the pattern of the original data, such as the overall status of the distribution of the feature values.

-   I think I would like to look more at the smote technique for resampling. I find this topic a little bit confusing.

> It's a special upsampling for minority class, where the upsampled instances are synthesized from the feature values limited by the existing instances.

-   Implementing SMOTE for class imbalance correctly while avoiding data leakage.

> Only prepare the recipe with trianing data. In our workflow, practically this will be specifying the parameter `new_data` as NULL for `bake` before training. If NULL is given, the function will interanlly pass this information to the `skip` parameter for `step_upsample` (or other balanced resampling steps) to resample and prepare the data. If some other data is given, the function will skip the resampling step to avoid data leakage in resampling.

-   How do we correctly implement up-sampling, down-sampling, and SMOTE in tidymodels, ensuring that resampling affects only the training set and not the test set?

> Check out the key.

-   **tuning with resampled data**: What are the key considerations when tuning models on resampled data?

> (1) Referring to the hyperparameter tuning using resampling techniques, we shoudl pay attention to the range the of the grid. It should wide enough to include a local extremum. A footnote from John on this will be the selection of the hyperparameters should be inclusive to accept a range within one standard deviation around the "best", because the resampling may introduce some uncertainty in this optimization.
> (2) the inverse) is tunable. Because the ratio in the real world data is usually not very ideally balanced, the best resampling ratio may not be 1 in practice.
> (3) Do not up/downresample the validation data and test data. And the pratice suggestion is above in the leakage avoiding question.

### metrics

#### ROC

-   What do the arguments for building the ROC plot mean?

```{r}
#| eval: false
roc_plot <- 
  tibble(truth = feat_test$diagnosis,
         prob = predict(fit_best_knn, feat_test, type = "prob")$.pred_malignant) |> 
  roc_curve(prob, truth = truth)

roc_plot |>
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .threshold)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(x = "1 - Specificity (FPR)",
       y = "Sensitivity (TPR)")
```

> In the dataframe, `truth` refers to the ground truth, and `prob` refers to the probability of the prediction to be positive. We pipes it into roc_curve to compute specificity and sensitivity.

-   Using and interpreting the results for ROC and auROC.

> The ROC provides a more formal method to visualize the trade-offs between sensitivity and specificity across all possible thresholds for classification.

> auROC is the probability that the classifier will rank/predict a randomly selected true positive observation higher than a randomly selected true negative observation. Alternatively, it can be thought of as the average sensitivity across all decision thresholds. It summarizes performance (sensitivity vs. specificity trade-off) across all possible thresholds and is not affected by class imbalances in contrast to many other metrics.

#### multiple metrics usage

-   `metrics = metric_set(roc_auc, accuracy, sens, spec, bal_accuracy)`: I still don't really understand why we run all of these metrics at once, is it just to see which one is the best? And if you can do that why would we not always include all of the metrics to see which one is most accurate apart from specific scenarios?

> If we're selecting a best model connfiguration, we should make sure the criterion is the same across all candidate configurations. And for different aims, we might use different metric as this criterion (for more comprehensive selection, there are also metrics like balanced accuracy). We assess the model on its universal performance using all those metrics.

### parallel processing, cache

#### specify parallel processing and cache

-   I am still unsure about parallel processing, do we simply include those 2 lines from the slides or do we need to do a similar process as caching where we wrap the expression in a code? - I am a little confused how parallel processing works, and if we have to integrate it into the other code chunks.

> The parallel processing is a global setting for the script, so we only need to specify it at the beginning of the computation. Normally, computations run sequentially (one after another). With parallel processing, multiple tasks run simultaneously, leveraging multi-cores.

> Cache is case-specific for chunks/computational steps that need a long time and may be repeated in the future, so it needs to be set for a specific process.

### Questions on application midterm and keras installation
