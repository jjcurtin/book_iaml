---
title: "Unit 06 Lab Agenda"
author: "Coco Yu"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: FALSE
#| message: FALSE
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
library(tidyverse) 
library(tidymodels)
theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max=Inf)

devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true",
                     sha1 = "c045eee2655a18dc85e715b78182f176327358a7")
```

- Is it possible to use both step_novel and step_impute_mode in a recipe for dealing with unseen levels in the test data and missing values for categorical variables? When I tried this in the assignment I got this error: 

Error in bake():
! Can't convert from new_data[[col_name]] <factor<ac8fc>> to <factor<a9e24>> due to loss of generality.
â€¢ Locations: 377
Run rlang::last_trace() to see where the error occurred.
> rlang::last_trace()
<error/vctrs_error_cast_lossy>
Error in bake():
! Can't convert from new_data[[col_name]] <factor<ac8fc>> to <factor<a9e24>> due to loss of generality.
â€¢ Locations: 377
---
Backtrace:
    â–†
 1. â”œâ”€recipes::bake(rec_prep, data_test)
 2. â””â”€recipes:::bake.recipe(rec_prep, data_test)
 3.   â”œâ”€recipes::bake(step, new_data = new_data)
 4.   â””â”€recipes:::bake.step_impute_mode(step, new_data = new_data)
 
- 


> ??? *Maybe having a demo of going over the steps of PCR and PLS regression.*

## Homework specific

> *I was wondering how to remove the x in both ms_sub_class and pid? I am still working on the homework assignment and will probably figure this out as I complete it. But so far I have not been able to remove it after turning it into a factor.*

`str_replace_all(text, "x", "")`


## Parallel Processing & Cache

> *Is there any way that I can set my R to run things faster in coding?*

Yes! We can use parallel processing and/or caching to speed up computation time. You can find explanations in lab 5 too (I'm copying and pasting the examples Zihan put up there!).

### Parallel processing

Parallel processing allows us to speed up computation by executing multiple tasks **simultaneously** with multiple CPU cores.

We can use parallel processing when:

- The fitting process for each of these configurations is independent for the others

- The order that the configurations are fit doesnâ€™t matter either

- When these two criteria are met, the processes can be run in parallel with an often big time savings

<font style = "color:green"> ***Find more information on [here](https://tune.tidymodels.org/articles/extras/optimizations.html)ðŸ˜Š!!*** </font>

**Set up parallel processing**

```{r}
cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
```

### Using cache

> *Can we review how to use the cache in our code?*

> *I'm just curious about the rerun = rerun_setting and how this makes the model rerun or of it is skipped. Is it just if data is changed or adjusting parameter it should be rerun?*

When we're doing caching, we store the results of the processing steps as objects. And then, we could read them in directly once we run through the process. It could save us a lot of time when working with large scale data and repetitive procedures.

If you want to set up the rerun option globally, assign it to a variable.

**Set up cache**

```{r}
library(xfun, include.only = "cache_rds")
rerun_setting <- FALSE
```

**Example code**

```{r}
results <- cache_rds(
  expr = {
    Sys.sleep(10)
    results <- c(1, 2, 3)
  }, 
  dir = "cache/",
  file = "cache_demo_xfun",
  rerun = rerun_setting)

results
```

<font style = "color:blue">When you set `rerun_setting = TRUE`, you will overwrite anything stored in your cache file and run your codes from the start. When it is set to `FALSE`, you will read in whatever is stored in your cache file and keeps running what's remaining. It is important to note that you should overwrite your cache file (i.e., `rerun_setting = TRUE`) every time you change something in your codes.</font>

## Generating Data

> *it would be great to cover some technique for generating own fake data like MASS function.*

```{r}
set.seed(123)

n <- 1000

# Generate 10 correlated numeric predictors
mu <- c(50, 100, 10, 5000, 0.5, 75, 5, 20, 2, 1)  # define mean for each predictor
sigma <- matrix(.7, nrow = 10, ncol = 10) # base correlation
diag(sigma) <- 1

data <- as.data.frame(MASS::mvrnorm(n, mu = mu, Sigma = sigma))


# Generate 5 independent numeric variables
data <- data |> bind_cols(
  independent_numeric <- tibble(
  runif(n, min = 0, max = 1),      # Uniform distribution (0â€“1)
  rnorm(n, mean = 200, sd = 50),  # Normal distribution (high mean)
  rpois(n, lambda = 5),           # Poisson-distributed data
  rbeta(n, shape1 = 2, shape2 = 5) * 100, # Beta distribution scaled
  sample(1000:5000, n, replace = TRUE)  # Discrete uniform
))

# Generate categorical predictors
data <- data |> bind_cols(tibble(
  sample(c("A", "B", "C", "D"), n, replace = TRUE,     # Nominal
         prob = c(0.6, 0.2, 0.02, 0.18)), # make the data sparse
  sample(c("Low", "Medium", "High"), n, replace = TRUE),  # Ordinal
  sample(letters[1:5], n, replace = TRUE),  # Random categorical levels
  sample(0:1, n, replace = TRUE, prob = c(0.7, 0.3)),  # Binary
  sample(c("Yes", "No"), n, replace = TRUE)  # Binary categorical
))

names(data) <- paste0("x", 1:20)

data$y <- rnorm(n, 1000, 300)
```

**Introduce random missingness to the data**

```{r}
set.seed(123)

data$x2[sample(1:1000, size = 5)] <- NA
data$x3[sample(1:1000, size = 15)] <- NA
data$x8[sample(1:1000, size = 80)] <- NA
data$x12[sample(1:1000, size = 2)] <- NA
data$x18[sample(1:1000, size = 25)] <- NA
```

```{r}
skimr::skim_without_charts(data)
```

## Data Cleaning

> *Cleaning the data. The application assignment due this Friday required a lot of data cleaning with the variable names and the data values. I was wondering if we could have a refresher about what functions to use to clean the data (i.e. remove quotation marks, unnecessary leading x's, etc.)*

> *More on cleaning data. Good practice? Concise methods? The data this week is dreadful to look at.*

> *One question I'm curious about comes from ames dataset for this week. In this dataset, there are ordinal variables such as "electrical" which has very low frequency count at certain level: only one value for "ms" level. When spliting the data into training and test sets, this one value was assigned to test set, resulting in the missing of a level in this column, which leads to the creation of new level called "NA" in this column within the training set. I wonder if this could become a problem for the model performance and if you have any suggestions to resovle such situation. *

### Clean variable name

```{r}
data <- data |> janitor::clean_names()
```

### Class variable

```{r}
data <- data |> 
  mutate(
    x16 = factor(x16),
    x17 = factor(x17),
    x18 = factor(x18),
    x19 = factor(x19),
    x20 = factor(x20)
  ) |> 
  glimpse()
```

### Clean variable response

```{r}
data <- data |> mutate(across(where(is.factor), tidy_responses)) |> 
  glimpse()
```

### Relevel factors if deem necessary

I'm releveling x17 here because I want to transform it into numeric values later

```{r}
data <- data |> mutate(x17 = fct_relevel(x17, c("low", "medium", "high"))) |> 
  glimpse()
```

## Train/validation/test split

> ???*Can you do grouped kfold CV with multiple groups?*

`group_vfold_cv()` only allows <font style = "color:red">one grouping variable</font> at a time. However, we can do a workaround to group kfold CV with multiple groups.

**For example, say if we want to group observations based on x16 and x20**

```{r}
group_cv_folds <- data |> 
  mutate(combined_group = paste0(x16, "_", x20)) |> 
  group_vfold_cv(group = combined_group, v = 5)

print(group_cv_folds)
```

> ???*I experienced a data leakage in an assignment a few unit ago, is it a good idea to make all the training, validation, and test set to read-only before the feature engineering? if so, how to do that.*

## Feature Engineering

### Standardization

> *Regarding standardizing variables before regularization, I wonder if the same applies to categorical variables. Should we also standardize categorical variables? If so, wouldn't that make interpretability difficult?*

If we dummy code category variables, we don't need to standardize them. However, if we use other encoding methods (e.g., ordinal encoding), we should standardize them.

> *What is the difference between step_normalize and step_normalize() and step_scale(). I feel like we have used both and they serve a similar purpose. Is there any difference or any circumstance where one is better than the other?*

> *What's the difference between step_scale and step_range?*

`step_normalize()` creates a specification of a recipe step that will normalize numeric data to have a standard deviation of one and a mean of zero. `step_scale()` creates a specification of a recipe step that will normalize numeric data to have a standard deviation of one. `step_range()` creates a specification of a recipe step that will normalize numeric data to be within a pre-defined range of values.

`step_normalize()` and `step_scale()` are similar in that they both standardize the data, but `step_normalize()` also centers the data by subtracting the mean. `step_range()` is different because it scales the data to a specific range, rather than standardizing it.

| Formula | Description | When to use |
|---------|-------------|--------|
| $x' = \frac{x - \mu}{\sigma}$ | centers data by mean; scale data by SD | when applying to models sensitive to magnitude (e.g., linear regression, PCA, ridge/lasso regression) |
| $x' = \frac{x}{\sigma}$ | scale data by SD (without centering) | when want to adjust scale without shifting center |
| $x' = \frac{x - min(x)}{max(x) - min(x)}$ | scale data to a specific range | when working with models that require a specific range (e.g., neural networks, knn, svm) |


ðŸ’» **Sample Codes**

```{r}
# step_normalize()
data_normalize <- recipe(y ~ ., data = data) |> 
  step_normalize(x14) |> 
  prep(data) |> 
  bake(NULL) 

# step_scale()
data_scale <- recipe(y ~ ., data = data) |> 
  step_scale(x14) |> 
  prep(data) |> 
  bake(NULL) 

# step_range()
data_range <- recipe(y ~ ., data = data) |> 
  step_range(x14) |> 
  prep(data) |> 
  bake(NULL) 
```

```{r}
cowplot::plot_grid(
  data |> ggplot(aes(x = x14)) + geom_histogram() + labs(title = "Original Data"),
  data_normalize |> ggplot(aes(x = x14)) + geom_histogram() + labs(title = "step_normalize()"),
  data_scale |> ggplot(aes(x = x14)) + geom_histogram() + labs(title = "step_scale()"),
  data_range |> ggplot(aes(x = x14)) + geom_histogram() + labs(title = "step_range()")
)

bind_rows(
  data |> 
    summarize(x14_mean = mean(x14, na.rm = TRUE), 
              x14_sd = sd(x14, na.rm = TRUE)),
  data_normalize |>
    summarize(x14_mean = mean(x14, na.rm = TRUE), 
              x14_sd = sd(x14, na.rm = TRUE)),
  data_scale |>
    summarize(x14_mean = mean(x14, na.rm = TRUE), 
              x14_sd = sd(x14, na.rm = TRUE)),
  data_range |>
    summarize(x14_mean = mean(x14, na.rm = TRUE), 
              x14_sd = sd(x14, na.rm = TRUE))
) |> 
  mutate(method = c("Original Data", "step_normalize()", "step_scale()", "step_range()")) |> 
  select(method, everything())
```

### Visualization

> ???*When applying transformation and scaling in recipe function, is there a method to measure or visualize how well are they doing? Or should this be conducted when checking the univariate stats?*

## Hyperparameter tuning

### Selecting best range

> *Selecting values for lambda and alpha (i.e. tuning for the hyperspace grid). How can one try to choose the best range of values?*

> *I would hope to have more explanation on tuning the penalty hyperparameter*

> ???*Is there a way of getting a good alpha or lambda value that does not include guessing?*

```{r}
recipe <- recipe(y ~ ., data = data) |> 
  step_rm(x16, x20) |> 
  step_mutate(x17 = as.numeric(x17)) |>
  step_impute_mean(all_numeric_predictors()) |> 
  step_impute_mode(all_nominal_predictors()) |> 
  step_normalize(all_numeric_predictors()) |> 
  # step_other(x16) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_nzv(all_predictors())

grid <- expand_grid(
  penalty = exp(seq(-5, 7, length.out = 500)),
  mixture = seq(0, 1, length = 10)
)

fit <- linear_reg(penalty = tune(), mixture = tune()) |> 
  set_engine("glmnet") |> 
  tune_grid(
    preprocessor = recipe,
    resamples = group_cv_folds,
    grid = grid,
    metrics = metric_set(rmse)
  )

plot_hyperparameters(fit, hp1 = "penalty", hp2 = "mixture", metric = "rmse")
```

> *More on the expand_grid stuff and looking at multiple dimensions there.*

> *When using linear_reg() with set_engine("glmnet" and tune_grid(), how does tune_grid() choose the best model?*

```{r}
grid |> head(500) |> print_kbl()
show_best(fit, metric = "rmse", n = 10) |> print_kbl()
```

> *Why would we need to tune both the mixture hyperparameter and the penalty hyperparameter in elastic net regression but only penalty hyperparameter in LASSO model?*

In LASSO regression, the penalty hyperparameter controls the amount of regularization applied to the model. In elastic net regression, both the penalty and mixture hyperparameters control the amount of regularization applied to the model. The mixture hyperparameter controls the balance between LASSO and ridge regression, while the penalty hyperparameter controls the overall amount of regularization. A mixture of 1 in elastic net regression is equivalent to LASSO regression, while a mixture of 0 is equivalent to ridge regression.

## Regularization

> ???*The web book mentions using penalty.factor to prevent the IV from being penalized, but are there any best practices for tuning this? Could we allow some shrinkage of the IVâ€™s effect while still maintaining an unbiased estimate?*

> ???*How can we use LASSO for feature selection in a dataset where the independent variable (IV) is dichotomous (e.g., an experimental manipulation), and we want to select the best covariates to predict a quantitative outcome (y)? I particularly want to focus on bootstrapping to find the best lambda value and following up with a linear model. *

> *is it correct that we only need to consider cost function but not loss function and glmnet packaget only has arguments for cost function?*

Loss function is the function that measures how well our predictions match the actual values (e.g., MSE, rmse). Cost function is the function that we want to minimize in order to find the best model parameters (e.g., loss function and the regularization term in glmnet). In glmnet, we only need to specify the cost function because it is already minimizing the loss function as part of the optimization process.

> ???*Interaction terms in High-Dimension regularization*











