---
title: "Unit 11 Lab Agenda"
author: "Coco Yu"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: FALSE
#| message: FALSE
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
library(tidyverse) 
library(tidymodels)
library(baguette)
theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max=Inf)

devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true",
                     sha1 = "c045eee2655a18dc85e715b78182f176327358a7")
```

- What does a wrapper function do, like in accuracy_wrapper? I couldn't follow what was happening there.

- Could we go over the output of contrast_models? Should we set a new seed for each process we use that involves randomization? or should we just set one seed at the start of our script? 

- Can we demo calculating and interpreting the Friedman's H statistic and deciding from that what interactions to include in a model?

- Can we cover Shapley values?

- Will we get into/see an example of how to use Bayesian estimation for model comparisons?

- I'd like to know if we should set ROPE to 1% by default or change this value in other circumstances.

- Could you explain again how to analyze each type of explanatory plot

- I would be interested to have some more samples on the permutation feature importance in coding.

- why do we do wrappers?

- I'm still confusing about why we need customized wrapper functions and those vector transformations when calculating feature importance?

- cv_compact <- collect_metrics(fits_compact, summarize = FALSE) |>

Links to an external site. filter(.config == hp_best_compact$.config) |>

Links to an external site. pull(.estimate) Links to an external site. Links to an external site.cv_full |> print()

 

i could not understand the meaning of output of this code.

ppt 33p

- I know there is more info in a link on the website, but how are nest and unnest used (best practices)? 

- I did not understand in the Bayesian posterior probabilities code the function command. I understand it is specifying the structure of the data with the id and id2 part but where does statistic ~ model come from? Those are not columns in the df from what I can tell.

- perf_mod usage and possible inputs.

- I would like to know some suggestions about selection and order of implementing these explantory appraoches we've learned. In specific, I would like to learn what factors, such as computation cost or data structure, would determine our selection and order. 

- For Bayesian model comparison, I didn't fully understand what hetero_var = TRUE/FALSE actually does. I was a bit confused about why we set hetero_var = TRUE for binary outcome measures. 

- The lectures mentioned using the vip package to extract model specific feature importance metrics. I was wondering if we might take a look at those in lab.

- I'm curious about efficient implementation strategies for conditional permutation feature importance with correlated features, since the standard approach seems problematic when features are dependent on each other.

- I was having trouble using the explainer in my code, but only when trying to render the document. When I ran the code chunks in order it was fine but when trying to render it said that the object could not be found. I am still working on this bug but not really sure what to change.

- How can we effectively use Shapley values to interpret feature importance in a model with highly correlated features, and what are the computational challenges when applying this method to large datasets?

- My hyper parameter graph is stuck with the same range, even when I tweak my hyper parameter grid values (particularly trying to tweak "penalty"). Is there an input to plot_hyperparameter that could solve this issue?

- I don't really have one besides column names being replaced with X1...Xn with the actual names being shifted down one. Not sure why it happens but not too hard to get rid of it.

- if not setting hetero_var = TRUE , will it consider heterogeneity by default?

- How do early stopping parameters (e.g., maxdepth, minsplit) interact with cost-complexity pruning (cp) when both are included in the model tuning grid? Should we tune them together or separately, and what strategies work best for selecting optimal combinations via cross-validation?

- What does this code do?

cv_full <- collect_metrics(fits_full, summarize = FALSE) |> Links to an external site. filter(.config == hp_best_full$.config) |> Links to an external site. pull(.estimate) Links to an external site. Links to an external site.cv_full |> print()

- different ggplot configurations for displaying shapley values.

- Why do we need to define our own performance metric functions when using permutation importance test?

- How to apply Bayesian estimation for model comparisons -  coding examples

- How would this be different if we did different k-fold values? Like if we did 10 folds like in the lecture?

- What should be our first steps when we run in to convergence issues with the fits?



