---
title: "Unit 04 Lab Agenda"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: FALSE
#| message: FALSE
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
library(tidyverse) 
library(tidymodels)
theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max=Inf)
```

-   When I included all of the predictors in my recipe (survived \~ ., data = data_trn) I would make it to the data validation bake and get an error that I had columns that weren't accounted for and when I looked it up, it said to change my dataset. Could we discuss troubleshooting that or best practices for the order to put our step functions.

-   Why the transformations and PCA don't seem to help with our current data. When is this the case? What should we do?

-   For the previous assignment, I tested some of the same models used in the lectures (using the exact same procedures/code) but got different RMSE--why?

-   I think we need to learn more techniques for loading different types of datasets into RStudio. For example, when I tried to load the car dataset from the StatLib Dataset Archive, which was used in the video lecture, I had trouble because it contained Unix code instead of consisting with plain CSV file.

-   For LDA and QDA, is centering/scaling not important? Also, I am curious if model configuration will be more prone to errors if key predictors have highly different scales (e.g., income and age).

-   Can you visualize classification/decision boundaries for more than 2 variables? I feel like that would be helpful for tuning the models.

> *Q: - I am interested in how to find the optimal K value in knn. Last semester, we did a visualization about how to find optimal k in knn in Python, and I am wondering if that works the same in R.*

## 1. Handle Missingness

> *Q: Still, I would like to learn more about how to properly use feature engineering functions such as step_impute series and how to handle missing values, specifically for categorical variables.*

### Most common step functions to impute missing values

-   `step_impute_median()`: creates a specification of a recipe step that will substitute missing values of numeric variables by the training set **median** of those variables

-   `step_impute_mean()`: creates a specification of a recipe step that will substitute missing values of numeric variables by the training set **mean** of those variables

-   `step_impute_mode()`: creates a specification of a recipe step that will substitute missing values of nominal variables by the training set **mode** of those variables

-   `step_impute_linear()`: creates a specification of a recipe step that will create **linear regression models** to impute missing data

-   `step_impute_knn()`: creates a specification of a recipe step that will impute missing data using **nearest neighbors**

-   `step_impute_bag()`: creates a specification of a recipe step that will create **bagged tree models** to impute missing data

### Numeric vs. nominal predictors

<font style = "color:red">**Numeric predictors:** </font>`step_impute_median()`, `step_impute_mean()`, `step_impute_linear()`, `step_impute_knn()`, `step_impute_bag()`

<font style = "color:red">**Nominal predictors:** </font> `step_impute_mode()`, `step_impute_knn()`, `step_impute_bag()`

## 2. Mismatch of category levels in training, validation and test data

> *Q: What is an easier way to check if we need to ensure the levels from the train, validation and the test set of the data match? In another word when do I know I will need to manually mutate the level for a categorical variable for one predictor in a specific data set among those three to match the levels of the others.*

<font style = "color:green"> ***For more detailed steps to handle novel levels in held-out data, please consult [John's page](https://jjcurtin.github.io/book_iaml/app_dummy_coding.html)😊!!*** </font>

### Step 1: Check if levels match across held-in and held-out datasets

```{r}
#| eval: FALSE
levels(data_trn$x)
levels(data_validation$x)
levels(data_test$x)
```

### Step 2: Create new levels when building recipes (only do this step when there're novel levels in your held-out data)

```{r}
#| eval: FALSE
rec <- recipe(y ~ ., data = data_trn) |>  
  step_novel(x) |> 
  step_dummy(x)
```

❗ You should always create novel levels before dummy coding.

❗ When there are multiple novel levels in your held-out data, they would all collapse into a single class called "new" (and it's fine!).

## 3. Handle Multicollinearity

> *Q: Error in solve.default(reg.cov) : system is computationally singular: reciprocal condition number = 1.08418e-37*

> *Q: Error handling with QDA models, there were a ton of "perfectly correlated" predictors, or something of the like, that felt really hard to get around, even with step_ncz.*

*Quick note: `step_nzv()` removes variables that have very low variance (highly sparse and unbalanced); it cannot help with multicollinearity*

### Step 1: Inspect correlations during modeling EDA

<font style = "color:darkblue">💡 Remove highly correlated predictors before applying LDA/QDA</font>

**Correlation matrix**

```{r}
#| eval: FALSE
data_trn |> 
  select(where(is.numeric)) |> 
  cor(use = "pairwise.complete.obs") |> 
  corrplot::corrplot.mixed()
```

*Sample graph*:

```{r}
#| echo: FALSE
#| message: FALSE
iris |> 
  select(where(is.numeric)) |> 
  cor(use = "pairwise.complete.obs") |> 
  corrplot::corrplot.mixed()
```

**VIF (regression models only)**

```{r}
#| eval: FALSE
# linear regression
lm(y ~ ., data) |> car::vif()
# logistic regression
glm(y ~ . , data, family = binomial) |> car::vif()
```

### Handle Multicollinearity when building recipes

-   `step_pca()`: creates a specification of a recipe step that will convert numeric variables into one or more principal components (<font style = "color: red">don't forget to center and scale your variables before applying pca!!</font>)

    -   PCA can transform correlated predictors into uncorrelated principle components

-   `step_corr`: creates a specification of a recipe step that will potentially remove variables that have large absolute correlations with other variables

### Regularization Methods

Regularization is a technique to prevent overfitting by adding a penalty to the model's complexity. This helps with handling high-dimensional data or multicollinearity.

For example, RDA uses two regularization parameters -- $\alpha$ to control the mix between LDA and QDA, and $\lambda$ to shrink the covariance matrix towards a diagonal matrix which helps handle multicollinearity

## 4. Step Functions (and ordering)

`step_impute_**()`, `step_YeonJohnson()`, `step_mutate()`, `step_dummy()`, `step_other()`, `step_novel()`, `step_ordinalscore()`, `step_interact()`, `step_poly()`, `step_scale()`, `step_range()`, `step_pca()`

```{r}
#| eval: FALSE
rec <- recipe(y ~ ., data_trn) |> 
  # handle missing values first
  step_impte_median(all_numeric_predictors()) |> 
  step_impute_mean(all_numeric_predictors()) |> 
  step_impute_mode(all_nominal_predictors()) |> 
  step_impute_linear() |> 
  step_impute_knn() |> 
  step_impute_bag() |> 
  
  # convert nominal to numeric if needed
  step_ordinalscore() |> 
  
  # transformation
  step_YeoJohnson() |> # best for models that assume normality (linear models, 
                       # discriminant analysis, bayesian models, etc.), may not
                       # needed for knn
  
  # feature engineering to create new features
  step_mutate() |> 
  
  # encoding categorical variables
  step_novel() |> 
  step_other() |> 
  step_dummy() |> # not needed for tree-based models and bayesian models
  
  # interactions & polynomial features
  step_interact() |> # may not needed for knn
  step_poly() |> 
  
  # scaling & normalization -- suitable for regularization-based models (knn, etc.) 
  # and regularized models (ridge, lasso, RDA, etc.), PCA, and neural networks
  # not needed for tree-based models
  step_scale() |> 
  step_range() |> 
  
  # dimensionality reduction
  step_pca() # suitable for high dimensional data
```

## 5. Models and their parameters

> *Q: Can we discuss more about parameters in KNN, LR, LDA/QDA/RDA and what we should be and should not be tuning?*

### Linear Regression

-   penalty: amount of regularization

-   mixture: proportion of L1 regularization

```{r}
#| eval: FALSE
linear_reg(penalty = NULL, mixture = NULL) |> 
  set_engine("lm")
```

### KNN

-   neighbors: number of neighbors to consider

-   weight_func (`rectangular`, `triangular`, `epanechnikov`, `biweight`, `triweight`, `cos`, `inv`, `gaussian`, `rank`, `optimal`): the function used to weight distances

-   dist_power: for calculating Minkowski distance

```{r}
#| eval: FALSE
nearest_neighbor(neighbors = NULL, weight_func = NULL, dist_power = NULL) |> 
  set_engine("kknn") |> 
  set_mode("regression") # or set_mode("classification")
```

### Logistic Regression

-   penalty: amount of regularization

-   mixture: proportion of L1 regularization

```{r}
#| eval: FALSE
logistic_reg(penalty = NULL, mixture = NULL) |> 
  set_engine("glm")
```

### discriminant analysis

#### LDA

-   penalty: amount of regularization

-   regularization_method (`diagonal`, `min_distance`, `shrink_cov`, `shrink_mean`): type of regularized estimation

```{r}
#| eval: FALSE
discrim_linear(penalty = NULL, regularization_method = NULL) |> 
  set_engine("MASS")
```

#### QDA

-   frac_common_cov

-   frac_identity

```{r}
#| eval: FALSE
discrim_regularized(frac_common_cov =  NULL, frac_identity = NULL) |> 
  set_engine("klaR")
```

### Naive bayes classifier

-   smoothness: relative smoothness of the class boundary

-   Laplace: Laplace correction to smoothing low-frequency counts

```{r}
#| eval: FALSE
naive_Bayes(smoothness = NULL, Laplace = NULL) |> 
  set_engine("naivebayes")
```

### Random Forest

-   mtry: number of predictors at each split

-   trees: number of trees in the ensemble

-   min_n: required minimum number of data points in a node

```{r}
#| eval: FALSE
rand_forest(mtry = NULL, trees = NULL, min_n = NULL) |> 
  set_engine("ranger") |> 
  set_mode("classification") # or set_mode("regression")
```

### Neural Networks

-   hidden_units: number of units in the hidden model

-   penalty: amount of weight decay

-   dropout: proportion of model parameters randomly set to zero

-   epochs: number of training iterations

-   activation (`linear`, `softmax`, etc): type of relationship between the original predictors and the hidden unit layer

-   learn_rate: rate at which the boosting algorithm adapts from iteration-to-iteration

```{r}
#| eval: FALSE
mlp(hidden_units = NULL, penalty = NULL, dropout = NULL, epochs = NULL,
    activaation = NULL, learn_rate = NULL) |> 
  set_engine("nnet") |> 
  set_mode("classification") # or set_mode("regression")
```

## 6. Multinomial regression, ordinal logistic regression

> *Q: When would we use multinomial logistic regression vs. ordinal logistic regression?*

**Multinomial regression**: when there are multiple predictors

**Ordinal regression**: when the outcome variable is ordered

## 7. Pipeline of selecting the best model and generating test predictions

![](images/clipboard-2055670079.png)

