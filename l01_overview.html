<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Overview of Machine Learning – Introduction to Applied Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./l02_exploratory_data_analysis.html" rel="next">
<link href="./course_materials.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="book.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./l01_overview.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Overview of Machine Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Applied Machine Learning</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/jjcurtin/book_iaml" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Syllabus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./course_materials.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Materials</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l01_overview.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Overview of Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l02_exploratory_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</a></li>
  <li><a href="#an-introductory-framework-for-machine-learning" id="toc-an-introductory-framework-for-machine-learning" class="nav-link" data-scroll-target="#an-introductory-framework-for-machine-learning"><span class="header-section-number">1.2</span> An Introductory Framework for Machine Learning</a></li>
  <li><a href="#more-details-on-supervised-techniques" id="toc-more-details-on-supervised-techniques" class="nav-link" data-scroll-target="#more-details-on-supervised-techniques"><span class="header-section-number">1.3</span> More Details on Supervised Techniques</a>
  <ul class="collapse">
  <li><a href="#how-do-we-estimate-f" id="toc-how-do-we-estimate-f" class="nav-link" data-scroll-target="#how-do-we-estimate-f"><span class="header-section-number">1.3.1</span> How Do We Estimate <span class="math inline">\(f\)</span>?</a></li>
  <li><a href="#how-do-we-assess-model-performance" id="toc-how-do-we-assess-model-performance" class="nav-link" data-scroll-target="#how-do-we-assess-model-performance"><span class="header-section-number">1.3.2</span> How Do We Assess Model Performance?</a></li>
  </ul></li>
  <li><a href="#key-terminology-in-context" id="toc-key-terminology-in-context" class="nav-link" data-scroll-target="#key-terminology-in-context"><span class="header-section-number">1.4</span> Key Terminology in Context</a></li>
  <li><a href="#an-example-of-the-bias-variance-trade-off" id="toc-an-example-of-the-bias-variance-trade-off" class="nav-link" data-scroll-target="#an-example-of-the-bias-variance-trade-off"><span class="header-section-number">1.5</span> An Example of the Bias-Variance Trade-off</a>
  <ul class="collapse">
  <li><a href="#overview-of-example" id="toc-overview-of-example" class="nav-link" data-scroll-target="#overview-of-example"><span class="header-section-number">1.5.1</span> Overview of Example</a></li>
  <li><a href="#stimulation-steps" id="toc-stimulation-steps" class="nav-link" data-scroll-target="#stimulation-steps"><span class="header-section-number">1.5.2</span> Stimulation Steps</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/jjcurtin/book_iaml/edit/main/l01_overview.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Overview of Machine Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="learning-objectives" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</h2>
<ul>
<li>Understand uses for machine learning models</li>
<li>Become familiar with key terminology (presented in bold throughout this unit)</li>
<li>Understand differences between models
<ul>
<li>Supervised vs.&nbsp;unsupervised</li>
<li>Regression vs.&nbsp;classification</li>
<li>Options for statistical algorithms</li>
<li>Features vs.&nbsp;predictors</li>
</ul></li>
<li>Relationships between:
<ul>
<li>Data generating processes</li>
<li>Statistical algorithms</li>
<li>Model flexibility</li>
<li>Model interpretability</li>
<li>Prediction vs.&nbsp;explanation</li>
</ul></li>
<li>Understand Bias-Variance Trade-off
<ul>
<li>Reducible and irreducible error</li>
<li>What is bias and variance?</li>
<li>What affects bias and variance?</li>
<li>What is overfitting and how does it relate to bias, variance, and also p-hacking</li>
<li>Use of training and test sets to assess bias and variance</li>
</ul></li>
</ul>
<hr>
</section>
<section id="an-introductory-framework-for-machine-learning" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="an-introductory-framework-for-machine-learning"><span class="header-section-number">1.2</span> An Introductory Framework for Machine Learning</h2>
<p>Machine (Statistical) learning techniques have developed in parallel in statistics and computer science</p>
<p>Techniques can be coarsely divided into supervised and unsupervised approaches</p>
<ul>
<li><strong>Supervised approaches</strong> involve models that predict an outcome using features</li>
<li><strong>Unsupervised approaches</strong> involve finding structure (e.g., clusters, factors) among a set of variables without any specific outcome specified</li>
<li>This course will focus primarily on supervised machine learning problems</li>
<li>However supervised approaches often use unsupervised approaches in early stages as part of <strong>feature engineering</strong></li>
</ul>
<hr>
<p>Examples of <strong>supervised approaches</strong> include:</p>
<ol type="1">
<li>Predicting relapse day-by-day among recovering patients with substance use disorders based on cellular communications and GPS.</li>
<li>Screening someone as positive or negative for substance use disorder based on their Facebook activity</li>
<li>Predicting the sale price of a house based on characteristics of the house and its neighborhood</li>
</ol>
<p>Examples of <strong>unsupervised approaches</strong> include:</p>
<ol type="1">
<li>Determining the factor structure of a set of personality items</li>
<li>Identifying subgroups among patients with alcohol use disorder based on demographics, use history, addiction severity, and other patient characteristics</li>
<li>Identifying the common topics present in customer reviews of some new product or app</li>
</ol>
<hr>
<p>Supervised machine learning approaches can be categorized as either regression or classification techniques</p>
<ul>
<li><strong>Regression techniques</strong> involve numeric (quantitative) outcomes.
<ul>
<li>Regression techniques are NOT limited to “regression” (i.e., the general linear model)<br>
</li>
<li>There are many more types of statistical models that are appropriate for numeric outcomes</li>
</ul></li>
<li><strong>Classification techniques</strong> involve nominal (categorical) outcomes</li>
<li>Most regression and classification techniques can handle categorical predictors</li>
</ul>
<p>Among the earlier supervised model examples, predicting sale price was a regression technique and screening individuals as positive or negative for substance use disorder was a classification technique</p>
<hr>
</section>
<section id="more-details-on-supervised-techniques" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="more-details-on-supervised-techniques"><span class="header-section-number">1.3</span> More Details on Supervised Techniques</h2>
<p>For supervised machine learning problems, we assume <span class="math inline">\(Y\)</span> (outcome) is a function of some <strong>data generating process</strong> (DGP, <span class="math inline">\(f\)</span>) involving a set of Xs (features) plus the addition of random error (<span class="math inline">\(\epsilon\)</span>) that is independent of X and with mean of 0</p>
<p><span class="math inline">\(Y = f(X) + \epsilon\)</span></p>
<p><img src="figs/unit1_data_dgp.png" height="400"></p>
<p><strong>Terminology sidebar</strong>: Throughout the course we will distinguish between the raw <strong>predictors</strong> available in a dataset and the <strong>features</strong> that are derived from those raw predictors through various transformations.</p>
<hr>
<p>We estimate <span class="math inline">\(f\)</span> (the DGP) for two main reasons: <strong>prediction</strong> and/or <strong>inference</strong> (i.e., explanation per Yarkoni and Westfall, 2017)</p>
<p><span class="math inline">\(\hat{Y} = \hat{f}(X)\)</span></p>
<p>For <strong>prediction</strong>, we are most interested in the accuracy of <span class="math inline">\(\hat{Y}\)</span> and typically treat <span class="math inline">\(\hat{f}\)</span> as a black box</p>
<p>For <strong>inference</strong>, we are typically interested in the way that <span class="math inline">\(Y\)</span> is affected by <span class="math inline">\(X\)</span></p>
<ul>
<li>Which predictors are associated with <span class="math inline">\(Y\)</span>?</li>
<li>Which are the strongest/most important predictors of <span class="math inline">\(Y\)</span></li>
<li>What is the relationship between the outcome and the features associated with each predictor. Is the overall relationship between a predictor and <span class="math inline">\(Y\)</span> positive, negative, dependent on other predictors? What is the shape of relationship (e.g., linear or more complex)?</li>
<li>Does the model as a whole improve prediction beyond a null model (no features from predictors) or beyond a compact model?</li>
<li>We care about good (low error) predictions even when we care about inference (we want small <span class="math inline">\(\epsilon\)</span>)
<ul>
<li>They will also be tested with low power</li>
<li>Parameter estimates from models that don’t predict well may be incorrect or at least imprecise</li>
</ul></li>
</ul>
<hr>
<p>Model error includes both <strong>reducible</strong> and <strong>irreducible</strong> error.</p>
<p>If we consider both <span class="math inline">\(X\)</span> and <span class="math inline">\(\hat{f}\)</span> to be fixed, then:</p>
<ul>
<li><span class="math inline">\(E(Y - \hat{Y})^2 = (f(X) + \epsilon - \hat{f}(X))^2\)</span></li>
<li><span class="math inline">\(E(Y - \hat{Y})^2 = [f(X) - \hat{f}(X)]^2 + Var(\epsilon)\)</span></li>
</ul>
<p><span class="math inline">\(Var(\epsilon)\)</span> is irreducible</p>
<ul>
<li>Irreducible error results from other important <span class="math inline">\(X\)</span> that we fail to measure and from measurement error in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></li>
<li>Irreducible error serves as an (unknown) bounds for model accuracy (without collecting additional Xs)</li>
</ul>
<p><span class="math inline">\([f(X) - \hat{f}(X)]^2\)</span> is reducible</p>
<ul>
<li>Reducible error results from a mismatch between <span class="math inline">\(\hat{f}\)</span> and the true <span class="math inline">\(f\)</span></li>
<li>This course will focus on techniques to estimate <span class="math inline">\(f\)</span> with the goal of minimizing reducible error</li>
</ul>
<hr>
<section id="how-do-we-estimate-f" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="how-do-we-estimate-f"><span class="header-section-number">1.3.1</span> How Do We Estimate <span class="math inline">\(f\)</span>?</h3>
<ul>
<li><p>We need a sample of <span class="math inline">\(N\)</span> observations of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> that we will call our <strong>training</strong> set</p></li>
<li><p>There are two types of statistical algorithms that we can use for <span class="math inline">\(\hat{f}\)</span>:</p>
<ul>
<li><strong>Parametric</strong> algorithms</li>
<li><strong>Non-parametric</strong> algorithms</li>
</ul></li>
</ul>
<hr>
<p><strong>Parametric</strong> algorithms:</p>
<ul>
<li>First, make an assumption about the functional form or shape of <span class="math inline">\(f\)</span>.<br>
</li>
<li>For example, the general linear model assumes: <span class="math inline">\(f(X) = \beta_0 + \beta_1*X_1 + \beta_2*X2 + ... + \beta_p*X_p\)</span></li>
<li>Next, a model using that algorithm is <strong>fit</strong> to the <strong>training set</strong>. In other words, the parameter estimates (e.g., <span class="math inline">\(\beta_0, \beta_1\)</span>) are derived to minimize some <strong>cost function</strong> (e.g., mean squared error for the linear model)</li>
<li>Parametric algorithms reduce the problem of estimating <span class="math inline">\(f\)</span> down to one of only estimating some set of parameters for a chosen model</li>
<li>Parametric algorithms often yield more interpretable models</li>
<li>But they are often not very flexible. If you chose the wrong algorithm (shape for <span class="math inline">\(\hat{f}\)</span> that does not match <span class="math inline">\(f\)</span>) the model will not fit well in the training set (and more importantly not in the new <strong>test set</strong> either)</li>
</ul>
<p><strong>Terminology sidebar</strong>: A <strong>training set</strong> is a subset of your full dataset that is used to fit a model. In contrast, a <strong>validation set</strong> is a subset that has not been included in the training set and is used to select a best model from among competing model configurations. A <strong>test set</strong> is a third subset of the full dataset that has not been included in either the training or validation sets and is used for evaluating the performance of your fitted final/best model.</p>
<hr>
<p><strong>Non-parametric</strong> algorithms:</p>
<ul>
<li>Do not make any assumption about the form/shape of <span class="math inline">\(f\)</span></li>
<li>Can fit well for a wide variety of forms/shapes for <span class="math inline">\(f\)</span></li>
<li>This flexibility comes with costs
<ul>
<li>They generally require larger <span class="math inline">\(N\)</span> in the training set than parametric algorithms to achieve comparable performance</li>
<li>They may <strong>overfit</strong> the training set. This happens when they begin to fit the noise in the training set. This will yield low error in training set but much higher error in new validation or test sets.</li>
<li>They are often less interpretable</li>
</ul></li>
</ul>
<hr>
<p>Generally:</p>
<ul>
<li>Flexibility and interpretability are inversely related</li>
<li>Models need to be flexible enough to fit <span class="math inline">\(f\)</span> well</li>
<li>Additional flexibility beyond this can produce overfitting</li>
<li>Parametric algorithms are generally less flexible than non-parametric algorithms</li>
<li>Parametric algorithms can become more flexible by increasing the number of features (<span class="math inline">\(p\)</span> from 610/710; e.g., using more predictors, more complex, non-linear forms to when deriving features from predictors)</li>
<li>Parametric algorithms can be made less flexible through <strong>regularization</strong>. There are techniques to make some non-parametric algorithms less flexible as well</li>
<li>You want the sweet spot for prediction. You may want even less flexible for inference in increase interpretability.</li>
</ul>
<p><img src="figs/unit1_flex_interpret.png" height="400"></p>
<hr>
</section>
<section id="how-do-we-assess-model-performance" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="how-do-we-assess-model-performance"><span class="header-section-number">1.3.2</span> How Do We Assess Model Performance?</h3>
<p>There is no universally best statistical algorithm</p>
<ul>
<li>Depends on the true <span class="math inline">\(f\)</span> and your goal (prediction or inference)</li>
<li>We often compare multiple statistical algorithms (various parametric and non-parametric options) and model configurations more generally (combinations of different algorithms with different sets of features)</li>
<li>When comparing models/configurations, we need to both <strong>fit</strong> these models and then <strong>select</strong> the best one</li>
</ul>
<hr>
<p><strong>Best</strong> needs to be defined with respect to some <strong>performance metric</strong> in new (<strong>validation or test set</strong>) data</p>
<ul>
<li>There are many performance metrics you might use</li>
<li><strong>Root Mean squared error (RMSE)</strong> is common for regression problems</li>
<li><strong>Accuracy</strong> is common for classification problems</li>
</ul>
<p>We will learn many other performance metrics in a later unit</p>
<hr>
<p>Two types of performance problems are typical</p>
<ol type="1">
<li>Models are <strong>underfit</strong> if they don’t adequately represent the true <span class="math inline">\(f\)</span>, typically because they have oversimplied the relationship (e.g., linear function fit to quadratic DGP, missing key interaction terms)
<ul>
<li>Underfit models will yield <strong>biased</strong> predictions. In other words, they will systematically either under-predict or over-predict <span class="math inline">\(Y\)</span> in some regions of the function.</li>
<li>Biased models will perform poorly in both training and test sets</li>
</ul></li>
<li>Models are <strong>overfit</strong> if they are too flexible and begin to fit the noise in the training set.
<ul>
<li>Overfit models will perform well (too well actually) in the training set but poorly in test or validation sets</li>
<li>They will show high <strong>variance</strong> such that the model and its predictions change drastically depending on the training set where it is fit</li>
</ul></li>
</ol>
<hr>
<p>More generally, these problems and their consequences for model performance are largely inversely related</p>
<ul>
<li>This is known as the <strong>Bias-Variance trade-off</strong></li>
<li>We previously discussed reducible and irreducible error
<ul>
<li>Reducible error can be parsed into components due to bias and variance</li>
<li>Goal is to minimize the sum of bias and variance error (i.e., the reducible error overall)</li>
<li>We will often trade off a little bias if it provides a big reduction in variance</li>
</ul></li>
</ul>
<p>But before we dive further into the Bias-Variance trade-off, lets review some key terminology that we will use throughout this course.</p>
<hr>
</section>
</section>
<section id="key-terminology-in-context" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="key-terminology-in-context"><span class="header-section-number">1.4</span> Key Terminology in Context</h2>
<p>In the following pages:</p>
<ul>
<li>We will present the broad steps for developing and evaluating machine learning models</li>
<li>We will situate key terms in this context (along with other synonymous terms used by others) and highlight them in <strong>bold</strong>.</li>
</ul>
<p>Machine learning has emerged in parallel from developments in statistics and computer science.</p>
<ul>
<li>As a result, there is a lot of terminology and often multiple terms used for the same concept. This is not my fault!<br>
</li>
<li>I will try to use one set of terms, but you need to be familiar with other terms you will encounter</li>
</ul>
<hr>
<p>When developing a supervised machine learning model to <strong>predict</strong> or <strong>explain</strong> an <strong>outcome</strong> (also called DV, label, output):</p>
<ul>
<li>Our goal is for the model to match as close as possible (given the limits due to <strong>irreducible error</strong>) the true data generating process for Y.</li>
<li>We typically consider multiple (often many) <strong>candidate model configurations</strong> to achieve this goal.</li>
</ul>
<hr>
<p><strong>Candidate model configurations</strong> can vary with respect to:</p>
<ul>
<li>the <strong>statistical algorithm</strong> used</li>
<li>the algorithm’s <strong>hyperparameters</strong></li>
<li>the <strong>features</strong> used in the model to predict the outcome</li>
</ul>
<hr>
<p><strong>Statistical algorithms</strong> can be coarsely categorized as parametric or non-parametric.</p>
<p>But we will mostly focus on a more granular description of the specific algorithm itself</p>
<p>Examples of specific statistical algorithms we will learn in this course include the linear model, generalized linear model, elastic net, LASSO, ridge regression, neural networks, KNN, random forest.</p>
<hr>
<p>The set of candidate model configurations often includes variations of the same statistical algorithm with different <strong>hyperparameter</strong> (also called tuning parameter) values that control aspects of the algorithm’s operation.</p>
<ul>
<li>Examples include <span class="math inline">\(k\)</span> in the KNN algorithm and <span class="math inline">\(lambda\)</span> in LASSO, Ridge and Elastic Net algorithms.</li>
<li>We will learn more about hyperparameters and their effects later in this course.</li>
</ul>
<hr>
<p>The set of candidate model configurations can vary with respect to the <strong>features</strong> that are included.</p>
<ul>
<li>A recipe describes how to transform raw data for <strong>predictors</strong> (also called IVs) into features (also called regressors, inputs) that are included in the <strong>feature matrix</strong> (also called design matrix, model matrix).<br>
</li>
<li>This process of transforming predictors into features in a feature matrix is called <strong>feature engineering</strong>.</li>
</ul>
<hr>
<p>Crossing variation on statistical algorithms, hyperparameter values, and alternative sets of features can increase the number of candidate model configurations dramatically</p>
<ul>
<li><p>developing a machine learning model can easily involve fitting thousands of model configurations.</p></li>
<li><p>In most implementations of machine learning, the number of candidate model configurations nearly ensures that some fitted models will <strong>overfit</strong> the dataset in which they are developed such that they capitalize on noise that is unique to the dataset in which they were fit.<br>
</p></li>
<li><p>For this reason, model configurations are assessed and selected on the basis of their relative performance for new data (observations that were not involved in the fitting process).</p></li>
</ul>
<hr>
<p>We have ONE full dataset but we use <strong>resampling techniques</strong> to form subsets of that dataset to enable us to assess models’ performance in new data.</p>
<p><strong>Cross-validation</strong> and <strong>bootstrapping</strong> are both examples of classes of resampling techniques that we will learn in this course.</p>
<p>Broadly, resampling techniques create multiple subsets that consist of random samples of the full dataset. These different subsets can be used for <strong>model fitting</strong>, <strong>model selection</strong>, and <strong>model evaluation</strong>.</p>
<ul>
<li><p><strong>Training sets</strong> are subsets that are used for <strong>model fitting</strong> (also called <strong>model training</strong>). During model fitting, models with each candidate model configuration are fit to the data in the training set. For example, during fitting, model parameters are estimated for regression algorithms, and weights are established for neural network algorithms. Some non-parametric algorithms, like k-nearest neighbors, do not estimate parameters but simply “memorize” the training sets for subsequent predictions.</p></li>
<li><p><strong>Validation sets</strong> are subsets that are used for <strong>model selection</strong> (or, more accurately, for model configuration selection). During model selection, each (fitted) model — one for every candidate model configuration — is used to make predictions for observations in a validation set that, importantly, does not overlap with the model’s training set. On the basis of each model’s performance in the validation set, the relatively best model configuration (i.e., the configuration of the model that performs best relative to all other model configurations) is identified and selected. If you have only one model configuration, validation set(s) are not needed because there is no need to select among model configurations.</p></li>
<li><p><strong>Test sets</strong> are subsets that are used for <strong>model evaluation</strong>. Generally, a model with the previously identified best configuration is re-fit to all available data other than the test set. This fitted model is used to predict observations in the test set to estimate how well this model is expected to perform for new observations.</p></li>
</ul>
<hr>
<p>There are three broad steps to develop and evaluate a machine learning model:</p>
<ol type="1">
<li><p>Fitting models with multiple candidate model configurations (in <strong>training set(s)</strong>)</p></li>
<li><p>Assessing each model to select the best configuration (in <strong>validation set(s)</strong>)</p></li>
<li><p>Evaluating how well a model with that best configuration will perform with new observations (in <strong>test sets(s)</strong>)</p></li>
</ol>
<hr>
</section>
<section id="an-example-of-the-bias-variance-trade-off" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="an-example-of-the-bias-variance-trade-off"><span class="header-section-number">1.5</span> An Example of the Bias-Variance Trade-off</h2>
<section id="overview-of-example" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="overview-of-example"><span class="header-section-number">1.5.1</span> Overview of Example</h3>
<p>The concepts of underfitting vs.&nbsp;overfitting and the bias-variance trade-off are critical to understand</p>
<ul>
<li><p>It is also important to understand how model flexibility can affect both the bias and variance of that model’s performance</p></li>
<li><p>It can help to make these abstract concepts concrete by exploring real models that are fit in actual data</p></li>
<li><p>We will conduct a very simple simulation to demonstrate these concepts</p></li>
</ul>
<p>The code in this example is secondary to understanding the concepts of underfittinng, overfitting, bias, variance, and the bias-variance trade-off</p>
<ul>
<li>We will not display much of it so that you can maintain focus on the concepts</li>
<li>You will have plenty of time to learn the underlying</li>
</ul>
<hr>
<p>When modeling, our goal is typically to approximate the <strong>data generating process (DGP)</strong> as close as possible, but in the real world we never know the true DGP.</p>
<p>A key advantage of many simulations is that we do know the DGP because we define it ourselves.</p>
<ul>
<li>For example, in this simulation, we know that <span class="math inline">\(Y\)</span> is a cubic function of <span class="math inline">\(X\)</span> and noise (random error).</li>
<li>In fact, we know the exact equation for calculating <span class="math inline">\(Y\)</span> as a function of <span class="math inline">\(X\)</span>.<br>
</li>
<li><span class="math inline">\(y = 1100 - 4.0 * x - 0.4 * x^2 + 0.1 * (x - h)^3 + noise\)</span>, where:
<ul>
<li><code>b0 = 1100</code></li>
<li><code>b1 = -4.0</code></li>
<li><code>b2 = -0.4</code></li>
<li><code>b3 = 0.1</code></li>
<li><code>h = -20.0</code></li>
<li>noise has <code>mean = 0</code> and <code>sd = 150</code></li>
</ul></li>
</ul>
<hr>
<p>We will attempt to model this cubic DGP with three different model configurations</p>
<ul>
<li>A <strong>simple linear model</strong> that uses only <span class="math inline">\(X\)</span> as a feature</li>
<li>A (20th order) <strong>polynomial linear model</strong> that uses 20 polynomials of <span class="math inline">\(X\)</span> as features</li>
<li>A (20th order) <strong>polynomial LASSO model</strong> that uses the same 20 polynomials of <span class="math inline">\(X\)</span> as features but “regularizes” to remove unimportant features from the model</li>
</ul>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: If the DGP for y is a cubic function of x, what do we know about the expected bias for our three candidate model configurations in this example?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>The simple linear model will underfit the true DGP and therefore it will be biased b/c it can only represent Y as a linear function of X.</p>
<p>The two polynomial models will be generally unbiased b/c they have X represented with 20th order polynomials.</p>
<p>LASSO will be slightly biased due to regularization but more on that in a later unit.</p>
</div>
<hr>
</section>
<section id="stimulation-steps" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="stimulation-steps"><span class="header-section-number">1.5.2</span> Stimulation Steps</h3>
<p>With that introduction complete, lets start our simulation of the bias-variance trade-off</p>
<ol type="1">
<li>Lets simulate four separate research teams, each working to estimate the DGP for Y</li>
</ol>
<ul>
<li>Each team will get their own random sample of <strong>training</strong> data (N = 100) to fit models<br>
</li>
<li>Here are plots of these four simulated training sets (one for each team) with a dotted line for the data generating process (DGP)</li>
</ul>
<div>

</div>
<div class="cell quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-7-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-7-3.png" class="img-fluid" width="672"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-7-4.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<hr>
<ol start="2" type="1">
<li>We get one more large random sample (N = 5000) with the same DGP to use as a test set to evaluate all the models that will be fit in the separate training sets across the four teams.</li>
</ol>
<ul>
<li>We will let each team use this same test set to keep things simple</li>
<li>The key is that the test set contains new observations not present in any of the training sets</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<hr>
<ol start="3" type="1">
<li><p>Each of the four teams fit their three model configurations in their training sets</p></li>
<li><p>They use the resulting models to make predictions for observations in the same training set in which they were fit</p></li>
</ol>
<div>

</div>
<div class="cell quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-9-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-9-3.png" class="img-fluid" width="672"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-9-4.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: Can you see evidence of bias for any model configuration? Look in any training set.
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fraqment uwred">
<p>The simple linear model is clearly biased. It systemically underestimates Y in some portions of the X distribution and overestimates Y in other portions of the X distribution. This is true across training sets for all teams.</p>
</div>
<div class="{fragment}">
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: Can you see any evidence of overfitting for any model configuration?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>The polynomial linear model appears to overfit the data in the training set. In other words, it seems to follow both the signal/DGP and the noise. However, in practice none of the teams could not be certain of this with only their training set.<br>
It is possible that the wiggles in the prediction line represent the real DGP. They need to look at the model’s performance in the test set to be certain about the degree of overfitting. (Of course, we know because these are simulated data and we know the DGP.)</p>
</div>
</div>
<hr>
<ol start="5" type="1">
<li>Now the teams use their 3 trained models to make predictions for new observations in the test set</li>
</ol>
<ul>
<li><p>Remember that the <strong>test set has NEW observations of X and Y</strong> that weren’t used for fitting any of the models.</p></li>
<li><p>Lets look at each model configuration’s performance in test separately</p></li>
</ul>
<hr>
<ul>
<li>Here are predictions from the four <strong>simple linear models</strong> (fit in the training sets for each team) in the <strong>test set</strong></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-10-1.png" width="672" height="400" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: Can you see evidence of bias for the simple linear models?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>Yes, consistent with what we saw in the training sets, the simple linear model systematically overestimates Y in some places and underestimates it in others. The DGP is clearly NOT linear but this simple model can only make linear predictions. It is a fairly biased model that underfits the true DGP. This bias will make a large contribution to the reducible error of the model</p>
</div>
<div class="fragment">
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: How much variance across the simple linear models is present?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>There is not much variance in the prediction lines across the models that were fit by different teams in different training sets. The slopes are very close across the different team’s models and the intercepts only vary by a small amount. The simple linear model configuration does not appear to have high variance (across teams) and therefore model variance will not contribute much to its reducible error.</p>
</div>
</div>
<hr>
<ul>
<li>Here are predictions from the <strong>polynomial linear models</strong> from the four teams in the <strong>test set</strong></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-11-1.png" width="672" height="400" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: Are these polynomial models systematically biased?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>There is not much systematic bias. The overall function is generally cubic for all four teams - just like the DGP. Bias will not contribute much to the model’s reducible error.</p>
</div>
<div class="fragment">
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: How does the variance of these polynomial models compare to the variance of the simple linear models?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>There is much higher model variance for this polynomial linear model relative to the simple linear model. Although all four models generally predict Y as a cubic function of X, there is also a non-systematic wiggle that is different for each team’s models.</p>
</div>
</div>
<div class="fragment">
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: How does this demonstrate the connection between model overfitting and model variance?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>Model variance (across teams) is a result of overfitting to the training set. If a model fits noise in its training set, that noise will be different in every dataset. Therefore, you end up with different models depending on the training set in which they are fit. And none of those models will do well with new data as you can see in this test set because noise is random and different in each dataset.</p>
</div>
</div>
<hr>
<ul>
<li>Here are predictions from the <strong>polynomial LASSO models</strong> from each team in the <strong>test set</strong></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-12-1.png" width="672" height="400" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: How does their bias compare to the simple and polynomial linear models?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>The LASSO models have low bias much like the polynomial linear model. They are able to capture the true cubic DGP fairly well. The regularization process slightly reduced the magnitude of the cubic (the prediction line is a little straighter than it should be), but not by much.</p>
</div>
<div class="fragment">
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: How does their variance compare to the simple and polynomial linear models?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>All four LASSO models, fit in different training sets, resulted in very similar prediction lines. Therefore, these LASSO models have low variance, much like the simple linear model. In contrast, the LASSO model variance is clearly lower than the more flexible polynomimal model.</p>
</div>
</div>
<hr>
<ol start="6" type="1">
<li>Now we will <strong>quantify</strong> the performance of these models in training and test sets with the <strong>root mean square error</strong> performance metric. This is the standard deviation of the error when comparing the predicted values for Y to the actual values (ground truth) for Y.</li>
</ol>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: What do we expect about RMSE for the three models in train and test?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>The simple linear model is underfit to the TRUE DGP. Therfore it is systematically biased everywhere it is used. It won’t fit well in train or test for this reason. However, it’s not very flexible so it won’t be overfit to the noise in train and therefore should fit comparably in train and test.</p>
<p>The polynomial linear model will not be biased at all given that the DGP is polynomial.</p>
<p>However, it is overly flexible (20th order) and so will substantially overfit the training data such that it will show high variance and its performance will be poor in test.</p>
<p>The polynomial LASSO will be the sweet spot in bias-variance trade-off. It has a little bias but not much. However, it is not as flexible due to regularization by lambda so it won’t be overfit to its training set. Therefore, it should do well in the test set.</p>
</div>
<hr>
<p>To better understand this:</p>
<ul>
<li>Compare RMSE across the three model configurations within the training sets (turquoise line)</li>
<li>Compare how RMSE changes for each model configuration across its training set and the test set</li>
<li>Compare RMSE across the three model configurations within the test set (red line)?</li>
<li>Specifically compare the performance of simple linear model (least flexible) with the polynomial linear model (most flexible)</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="l01_overview_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question: Would these observations about bias and variance of these three model configurations always be the same regardless of the DGP?
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="fragment uwred">
<p>No.&nbsp;A model configuration needs to be flexible enough and/or well designed to represent the DGP for the data that you are modeling. The two polynomial models in this example were each able to represent a cubic DGP. The simple linear model was not. The polynomial linear model was too flexible for a cubic given that it had 20 polynomials of X. Therefore, it was overfit to its training set and had high variance. However, if the DGP was a different shape, the story would be different. If the DGP was linear the simple linear model would not have been biased and would have performed best. If this DGP was some other form (step function), it may be that none of the models would work well.</p>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/github\.io\/jjcurtin\/book_iaml");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./course_materials.html" class="pagination-link" aria-label="Course Materials">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Course Materials</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./l02_exploratory_data_analysis.html" class="pagination-link" aria-label="Exploratory Data Analysis">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/jjcurtin/book_iaml/edit/main/l01_overview.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>